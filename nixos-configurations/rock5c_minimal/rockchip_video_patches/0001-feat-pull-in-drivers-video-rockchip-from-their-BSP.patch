From 72b0d1d74e527f02bb898e8dc4158fcfd5afda21 Mon Sep 17 00:00:00 2001
From: John Rinehart <johnrichardrinehart@gmail.com>
Date: Tue, 19 Aug 2025 21:10:17 -0700
Subject: [PATCH 01/14] feat: pull in drivers/video/rockchip from their BSP

---
 drivers/video/Kconfig                         |    4 +
 drivers/video/Makefile                        |    1 +
 drivers/video/rockchip/Kconfig                |   11 +
 drivers/video/rockchip/Makefile               |   11 +
 drivers/video/rockchip/dvbm/Kconfig           |   18 +
 drivers/video/rockchip/dvbm/Makefile          |    5 +
 drivers/video/rockchip/dvbm/rockchip_dvbm.c   |  752 +++
 drivers/video/rockchip/dvbm/rockchip_dvbm.h   |  214 +
 drivers/video/rockchip/iep/Kconfig            |   10 +
 drivers/video/rockchip/iep/Makefile           |    4 +
 .../video/rockchip/iep/hw_iep_config_addr.h   |   99 +
 drivers/video/rockchip/iep/hw_iep_reg.c       | 1530 +++++
 drivers/video/rockchip/iep/hw_iep_reg.h       |  525 ++
 drivers/video/rockchip/iep/iep.h              |  276 +
 drivers/video/rockchip/iep/iep_drv.c          | 1321 ++++
 drivers/video/rockchip/iep/iep_drv.h          |  159 +
 drivers/video/rockchip/iep/iep_iommu_drm.c    |  464 ++
 drivers/video/rockchip/iep/iep_iommu_ops.c    |  244 +
 drivers/video/rockchip/iep/iep_iommu_ops.h    |  121 +
 drivers/video/rockchip/mpp/Kconfig            |   83 +
 drivers/video/rockchip/mpp/Makefile           |   32 +
 .../video/rockchip/mpp/hack/mpp_hack_px30.c   |  245 +
 .../video/rockchip/mpp/hack/mpp_hack_px30.h   |   27 +
 .../video/rockchip/mpp/hack/mpp_hack_rk3576.c |  193 +
 .../video/rockchip/mpp/hack/mpp_hack_rk3576.h |   35 +
 .../mpp/hack/mpp_rkvdec2_hack_rk3568.c        |  732 +++
 .../mpp/hack/mpp_rkvdec2_link_hack_rk3568.c   |  213 +
 drivers/video/rockchip/mpp/mpp_av1dec.c       | 1062 +++
 drivers/video/rockchip/mpp/mpp_common.c       | 2688 ++++++++
 drivers/video/rockchip/mpp/mpp_common.h       |  873 +++
 drivers/video/rockchip/mpp/mpp_debug.h        |  138 +
 drivers/video/rockchip/mpp/mpp_iep2.c         | 1166 ++++
 drivers/video/rockchip/mpp/mpp_iommu.c        |  689 ++
 drivers/video/rockchip/mpp/mpp_iommu.h        |  190 +
 drivers/video/rockchip/mpp/mpp_jpgdec.c       |  660 ++
 drivers/video/rockchip/mpp/mpp_jpgenc.c       |  651 ++
 drivers/video/rockchip/mpp/mpp_rkvdec.c       | 1928 ++++++
 drivers/video/rockchip/mpp/mpp_rkvdec2.c      | 2132 +++++++
 drivers/video/rockchip/mpp/mpp_rkvdec2.h      |  233 +
 drivers/video/rockchip/mpp/mpp_rkvdec2_link.c | 2677 ++++++++
 drivers/video/rockchip/mpp/mpp_rkvdec2_link.h |  254 +
 drivers/video/rockchip/mpp/mpp_rkvenc.c       | 1466 +++++
 drivers/video/rockchip/mpp/mpp_rkvenc2.c      | 2978 +++++++++
 drivers/video/rockchip/mpp/mpp_service.c      |  533 ++
 drivers/video/rockchip/mpp/mpp_vdpp.c         |  828 +++
 drivers/video/rockchip/mpp/mpp_vdpu1.c        |  973 +++
 drivers/video/rockchip/mpp/mpp_vdpu2.c        |  809 +++
 drivers/video/rockchip/mpp/mpp_vepu1.c        |  796 +++
 drivers/video/rockchip/mpp/mpp_vepu2.c        | 1281 ++++
 .../video/rockchip/mpp/rockchip_iep2_regs.h   |  184 +
 drivers/video/rockchip/mpp_osal/Kconfig       |    6 +
 drivers/video/rockchip/mpp_osal/Makefile      |    2 +
 drivers/video/rockchip/mpp_osal/mpp_osal.c    |   43 +
 drivers/video/rockchip/mpp_osal/mpp_osal.h    |   19 +
 drivers/video/rockchip/rga/Kconfig            |   10 +
 drivers/video/rockchip/rga/Makefile           |    4 +
 drivers/video/rockchip/rga/RGA_API.c          |  201 +
 drivers/video/rockchip/rga/RGA_API.h          |   40 +
 drivers/video/rockchip/rga/rga.h              |  507 ++
 drivers/video/rockchip/rga/rga_drv.c          | 2574 ++++++++
 drivers/video/rockchip/rga/rga_mmu_info.c     | 1325 ++++
 drivers/video/rockchip/rga/rga_mmu_info.h     |   24 +
 drivers/video/rockchip/rga/rga_reg_info.c     | 1587 +++++
 drivers/video/rockchip/rga/rga_reg_info.h     |  467 ++
 drivers/video/rockchip/rga/rga_rop.h          |   56 +
 drivers/video/rockchip/rga/rga_type.h         |   49 +
 drivers/video/rockchip/rga2/Kconfig           |   30 +
 drivers/video/rockchip/rga2/Makefile          |    5 +
 drivers/video/rockchip/rga2/RGA2_API.c        |   23 +
 drivers/video/rockchip/rga2/RGA2_API.h        |   59 +
 drivers/video/rockchip/rga2/rga2.h            |  792 +++
 drivers/video/rockchip/rga2/rga2_debugger.c   |  395 ++
 drivers/video/rockchip/rga2/rga2_debugger.h   |  120 +
 drivers/video/rockchip/rga2/rga2_drv.c        | 2277 +++++++
 drivers/video/rockchip/rga2/rga2_mmu_info.c   | 1843 ++++++
 drivers/video/rockchip/rga2/rga2_mmu_info.h   |   35 +
 drivers/video/rockchip/rga2/rga2_reg_info.c   | 1699 +++++
 drivers/video/rockchip/rga2/rga2_reg_info.h   |  332 +
 drivers/video/rockchip/rga2/rga2_rop.h        |   56 +
 drivers/video/rockchip/rga2/rga2_type.h       |   49 +
 drivers/video/rockchip/rga3/Kconfig           |   37 +
 drivers/video/rockchip/rga3/Makefile          |    9 +
 drivers/video/rockchip/rga3/include/rga.h     | 1000 +++
 .../rockchip/rga3/include/rga2_reg_info.h     |  529 ++
 .../rockchip/rga3/include/rga3_reg_info.h     |  521 ++
 .../video/rockchip/rga3/include/rga_common.h  |   86 +
 .../rockchip/rga3/include/rga_debugger.h      |  145 +
 .../video/rockchip/rga3/include/rga_dma_buf.h |   49 +
 drivers/video/rockchip/rga3/include/rga_drv.h |  485 ++
 .../video/rockchip/rga3/include/rga_fence.h   |  101 +
 .../rockchip/rga3/include/rga_hw_config.h     |   94 +
 .../video/rockchip/rga3/include/rga_iommu.h   |   79 +
 drivers/video/rockchip/rga3/include/rga_job.h |   58 +
 drivers/video/rockchip/rga3/include/rga_mm.h  |   67 +
 drivers/video/rockchip/rga3/rga2_reg_info.c   | 3273 ++++++++++
 drivers/video/rockchip/rga3/rga3_reg_info.c   | 2246 +++++++
 drivers/video/rockchip/rga3/rga_common.c      |  927 +++
 drivers/video/rockchip/rga3/rga_debugger.c    | 1003 +++
 drivers/video/rockchip/rga3/rga_dma_buf.c     |  594 ++
 drivers/video/rockchip/rga3/rga_drv.c         | 1633 +++++
 drivers/video/rockchip/rga3/rga_fence.c       |  145 +
 drivers/video/rockchip/rga3/rga_hw_config.c   |  673 ++
 drivers/video/rockchip/rga3/rga_iommu.c       |  423 ++
 drivers/video/rockchip/rga3/rga_job.c         | 1551 +++++
 drivers/video/rockchip/rga3/rga_mm.c          | 2489 ++++++++
 drivers/video/rockchip/rga3/rga_policy.c      |  493 ++
 drivers/video/rockchip/rve/Kconfig            |   29 +
 drivers/video/rockchip/rve/Makefile           |    9 +
 drivers/video/rockchip/rve/include/rve.h      |   72 +
 .../video/rockchip/rve/include/rve_debugger.h |  132 +
 drivers/video/rockchip/rve/include/rve_drv.h  |  332 +
 .../video/rockchip/rve/include/rve_fence.h    |   32 +
 drivers/video/rockchip/rve/include/rve_job.h  |   53 +
 drivers/video/rockchip/rve/include/rve_reg.h  |   88 +
 drivers/video/rockchip/rve/rve_debugger.c     |  566 ++
 drivers/video/rockchip/rve/rve_drv.c          |  897 +++
 drivers/video/rockchip/rve/rve_fence.c        |  136 +
 drivers/video/rockchip/rve/rve_job.c          | 1028 +++
 drivers/video/rockchip/rve/rve_reg.c          |  277 +
 drivers/video/rockchip/vehicle/Kconfig        |   46 +
 drivers/video/rockchip/vehicle/Makefile       |   29 +
 .../vehicle/vehicle-csi2-dphy-common.h        |  385 ++
 drivers/video/rockchip/vehicle/vehicle_ad.h   |   84 +
 .../video/rockchip/vehicle/vehicle_ad_7181.c  |  608 ++
 .../video/rockchip/vehicle/vehicle_ad_7181.h  |   19 +
 .../rockchip/vehicle/vehicle_ad_gc2145.c      | 1149 ++++
 .../rockchip/vehicle/vehicle_ad_gc2145.h      |   18 +
 .../rockchip/vehicle/vehicle_ad_max96714.c    |  539 ++
 .../rockchip/vehicle/vehicle_ad_max96714.h    |   18 +
 .../rockchip/vehicle/vehicle_ad_nvp6188.c     | 1206 ++++
 .../rockchip/vehicle/vehicle_ad_nvp6188.h     |   18 +
 .../rockchip/vehicle/vehicle_ad_nvp6324.c     | 2238 +++++++
 .../rockchip/vehicle/vehicle_ad_nvp6324.h     |   18 +
 .../rockchip/vehicle/vehicle_ad_tp2825.c      | 1039 +++
 .../rockchip/vehicle/vehicle_ad_tp2825.h      |   18 +
 .../rockchip/vehicle/vehicle_ad_tp2855.c      |  843 +++
 .../rockchip/vehicle/vehicle_ad_tp2855.h      |   18 +
 drivers/video/rockchip/vehicle/vehicle_cfg.h  |  168 +
 drivers/video/rockchip/vehicle/vehicle_cif.c  | 5665 +++++++++++++++++
 drivers/video/rockchip/vehicle/vehicle_cif.h  |  191 +
 .../video/rockchip/vehicle/vehicle_cif_regs.h |   19 +
 drivers/video/rockchip/vehicle/vehicle_dev.c  |  116 +
 .../video/rockchip/vehicle/vehicle_flinger.c  | 1519 +++++
 .../video/rockchip/vehicle/vehicle_flinger.h  |  115 +
 .../rockchip/vehicle/vehicle_generic_sensor.c |  488 ++
 drivers/video/rockchip/vehicle/vehicle_gpio.c |  178 +
 drivers/video/rockchip/vehicle/vehicle_gpio.h |   33 +
 drivers/video/rockchip/vehicle/vehicle_main.c |  501 ++
 drivers/video/rockchip/vehicle/vehicle_main.h |   19 +
 .../vehicle/vehicle_samsung_dcphy_common.h    |  246 +
 .../video/rockchip/vehicle/vehicle_version.h  |   78 +
 drivers/video/rockchip/vtunnel/Kconfig        |   12 +
 drivers/video/rockchip/vtunnel/Makefile       |    3 +
 drivers/video/rockchip/vtunnel/rkvtunnel.c    | 1537 +++++
 drivers/video/rockchip/vtunnel/rkvtunnel.h    |   81 +
 155 files changed, 88770 insertions(+)
 create mode 100644 drivers/video/rockchip/Kconfig
 create mode 100644 drivers/video/rockchip/Makefile
 create mode 100644 drivers/video/rockchip/dvbm/Kconfig
 create mode 100644 drivers/video/rockchip/dvbm/Makefile
 create mode 100644 drivers/video/rockchip/dvbm/rockchip_dvbm.c
 create mode 100644 drivers/video/rockchip/dvbm/rockchip_dvbm.h
 create mode 100644 drivers/video/rockchip/iep/Kconfig
 create mode 100644 drivers/video/rockchip/iep/Makefile
 create mode 100644 drivers/video/rockchip/iep/hw_iep_config_addr.h
 create mode 100644 drivers/video/rockchip/iep/hw_iep_reg.c
 create mode 100644 drivers/video/rockchip/iep/hw_iep_reg.h
 create mode 100644 drivers/video/rockchip/iep/iep.h
 create mode 100644 drivers/video/rockchip/iep/iep_drv.c
 create mode 100644 drivers/video/rockchip/iep/iep_drv.h
 create mode 100644 drivers/video/rockchip/iep/iep_iommu_drm.c
 create mode 100644 drivers/video/rockchip/iep/iep_iommu_ops.c
 create mode 100644 drivers/video/rockchip/iep/iep_iommu_ops.h
 create mode 100644 drivers/video/rockchip/mpp/Kconfig
 create mode 100644 drivers/video/rockchip/mpp/Makefile
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_hack_px30.c
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_hack_px30.h
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.c
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.h
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_rkvdec2_hack_rk3568.c
 create mode 100644 drivers/video/rockchip/mpp/hack/mpp_rkvdec2_link_hack_rk3568.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_av1dec.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_common.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_common.h
 create mode 100644 drivers/video/rockchip/mpp/mpp_debug.h
 create mode 100644 drivers/video/rockchip/mpp/mpp_iep2.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_iommu.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_iommu.h
 create mode 100644 drivers/video/rockchip/mpp/mpp_jpgdec.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_jpgenc.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvdec.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvdec2.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvdec2.h
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvdec2_link.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvdec2_link.h
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvenc.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_rkvenc2.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_service.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_vdpp.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_vdpu1.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_vdpu2.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_vepu1.c
 create mode 100644 drivers/video/rockchip/mpp/mpp_vepu2.c
 create mode 100644 drivers/video/rockchip/mpp/rockchip_iep2_regs.h
 create mode 100644 drivers/video/rockchip/mpp_osal/Kconfig
 create mode 100644 drivers/video/rockchip/mpp_osal/Makefile
 create mode 100644 drivers/video/rockchip/mpp_osal/mpp_osal.c
 create mode 100644 drivers/video/rockchip/mpp_osal/mpp_osal.h
 create mode 100644 drivers/video/rockchip/rga/Kconfig
 create mode 100644 drivers/video/rockchip/rga/Makefile
 create mode 100644 drivers/video/rockchip/rga/RGA_API.c
 create mode 100644 drivers/video/rockchip/rga/RGA_API.h
 create mode 100644 drivers/video/rockchip/rga/rga.h
 create mode 100644 drivers/video/rockchip/rga/rga_drv.c
 create mode 100644 drivers/video/rockchip/rga/rga_mmu_info.c
 create mode 100644 drivers/video/rockchip/rga/rga_mmu_info.h
 create mode 100644 drivers/video/rockchip/rga/rga_reg_info.c
 create mode 100644 drivers/video/rockchip/rga/rga_reg_info.h
 create mode 100644 drivers/video/rockchip/rga/rga_rop.h
 create mode 100644 drivers/video/rockchip/rga/rga_type.h
 create mode 100644 drivers/video/rockchip/rga2/Kconfig
 create mode 100644 drivers/video/rockchip/rga2/Makefile
 create mode 100644 drivers/video/rockchip/rga2/RGA2_API.c
 create mode 100644 drivers/video/rockchip/rga2/RGA2_API.h
 create mode 100644 drivers/video/rockchip/rga2/rga2.h
 create mode 100644 drivers/video/rockchip/rga2/rga2_debugger.c
 create mode 100644 drivers/video/rockchip/rga2/rga2_debugger.h
 create mode 100644 drivers/video/rockchip/rga2/rga2_drv.c
 create mode 100644 drivers/video/rockchip/rga2/rga2_mmu_info.c
 create mode 100644 drivers/video/rockchip/rga2/rga2_mmu_info.h
 create mode 100644 drivers/video/rockchip/rga2/rga2_reg_info.c
 create mode 100644 drivers/video/rockchip/rga2/rga2_reg_info.h
 create mode 100644 drivers/video/rockchip/rga2/rga2_rop.h
 create mode 100644 drivers/video/rockchip/rga2/rga2_type.h
 create mode 100644 drivers/video/rockchip/rga3/Kconfig
 create mode 100644 drivers/video/rockchip/rga3/Makefile
 create mode 100644 drivers/video/rockchip/rga3/include/rga.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga2_reg_info.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga3_reg_info.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_common.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_debugger.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_dma_buf.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_drv.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_fence.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_hw_config.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_iommu.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_job.h
 create mode 100644 drivers/video/rockchip/rga3/include/rga_mm.h
 create mode 100644 drivers/video/rockchip/rga3/rga2_reg_info.c
 create mode 100644 drivers/video/rockchip/rga3/rga3_reg_info.c
 create mode 100644 drivers/video/rockchip/rga3/rga_common.c
 create mode 100644 drivers/video/rockchip/rga3/rga_debugger.c
 create mode 100644 drivers/video/rockchip/rga3/rga_dma_buf.c
 create mode 100644 drivers/video/rockchip/rga3/rga_drv.c
 create mode 100644 drivers/video/rockchip/rga3/rga_fence.c
 create mode 100644 drivers/video/rockchip/rga3/rga_hw_config.c
 create mode 100644 drivers/video/rockchip/rga3/rga_iommu.c
 create mode 100644 drivers/video/rockchip/rga3/rga_job.c
 create mode 100644 drivers/video/rockchip/rga3/rga_mm.c
 create mode 100644 drivers/video/rockchip/rga3/rga_policy.c
 create mode 100644 drivers/video/rockchip/rve/Kconfig
 create mode 100644 drivers/video/rockchip/rve/Makefile
 create mode 100644 drivers/video/rockchip/rve/include/rve.h
 create mode 100644 drivers/video/rockchip/rve/include/rve_debugger.h
 create mode 100644 drivers/video/rockchip/rve/include/rve_drv.h
 create mode 100644 drivers/video/rockchip/rve/include/rve_fence.h
 create mode 100644 drivers/video/rockchip/rve/include/rve_job.h
 create mode 100644 drivers/video/rockchip/rve/include/rve_reg.h
 create mode 100644 drivers/video/rockchip/rve/rve_debugger.c
 create mode 100644 drivers/video/rockchip/rve/rve_drv.c
 create mode 100644 drivers/video/rockchip/rve/rve_fence.c
 create mode 100644 drivers/video/rockchip/rve/rve_job.c
 create mode 100644 drivers/video/rockchip/rve/rve_reg.c
 create mode 100644 drivers/video/rockchip/vehicle/Kconfig
 create mode 100644 drivers/video/rockchip/vehicle/Makefile
 create mode 100644 drivers/video/rockchip/vehicle/vehicle-csi2-dphy-common.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_7181.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_7181.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_gc2145.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_gc2145.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_max96714.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_max96714.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_tp2825.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_tp2825.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_tp2855.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_ad_tp2855.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_cfg.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_cif.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_cif.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_cif_regs.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_dev.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_flinger.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_flinger.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_generic_sensor.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_gpio.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_gpio.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_main.c
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_main.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_samsung_dcphy_common.h
 create mode 100644 drivers/video/rockchip/vehicle/vehicle_version.h
 create mode 100644 drivers/video/rockchip/vtunnel/Kconfig
 create mode 100644 drivers/video/rockchip/vtunnel/Makefile
 create mode 100644 drivers/video/rockchip/vtunnel/rkvtunnel.c
 create mode 100644 drivers/video/rockchip/vtunnel/rkvtunnel.h

diff --git a/drivers/video/Kconfig b/drivers/video/Kconfig
index 5df981920a945..5785f72fba5fb 100644
--- a/drivers/video/Kconfig
+++ b/drivers/video/Kconfig
@@ -49,6 +49,10 @@ endmenu
 
 source "drivers/video/backlight/Kconfig"
 
+menu "Rockchip Misc Video driver"
+source "drivers/video/rockchip/Kconfig"
+endmenu
+
 config VGASTATE
        tristate
        default n
diff --git a/drivers/video/Makefile b/drivers/video/Makefile
index ffbac4387c670..02052f7afafd8 100644
--- a/drivers/video/Makefile
+++ b/drivers/video/Makefile
@@ -14,6 +14,7 @@ obj-$(CONFIG_VT)		  += console/
 obj-$(CONFIG_FB_STI)		  += console/
 obj-$(CONFIG_LOGO)		  += logo/
 obj-y				  += backlight/
+obj-y				  += rockchip/
 
 obj-y				  += fbdev/
 
diff --git a/drivers/video/rockchip/Kconfig b/drivers/video/rockchip/Kconfig
new file mode 100644
index 0000000000000..6548dd38b534d
--- /dev/null
+++ b/drivers/video/rockchip/Kconfig
@@ -0,0 +1,11 @@
+# SPDX-License-Identifier: GPL-2.0
+source "drivers/video/rockchip/rga/Kconfig"
+source "drivers/video/rockchip/rga2/Kconfig"
+source "drivers/video/rockchip/rga3/Kconfig"
+source "drivers/video/rockchip/rve/Kconfig"
+source "drivers/video/rockchip/iep/Kconfig"
+source "drivers/video/rockchip/mpp/Kconfig"
+source "drivers/video/rockchip/mpp_osal/Kconfig"
+source "drivers/video/rockchip/dvbm/Kconfig"
+source "drivers/video/rockchip/vehicle/Kconfig"
+source "drivers/video/rockchip/vtunnel/Kconfig"
diff --git a/drivers/video/rockchip/Makefile b/drivers/video/rockchip/Makefile
new file mode 100644
index 0000000000000..867ac56fe5b72
--- /dev/null
+++ b/drivers/video/rockchip/Makefile
@@ -0,0 +1,11 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_ROCKCHIP_RGA) += rga/
+obj-$(CONFIG_ROCKCHIP_RGA2) += rga2/
+obj-$(CONFIG_ROCKCHIP_MULTI_RGA) += rga3/
+obj-$(CONFIG_ROCKCHIP_RVE) += rve/
+obj-$(CONFIG_IEP) += iep/
+obj-$(CONFIG_ROCKCHIP_MPP_SERVICE) += mpp/
+obj-$(CONFIG_ROCKCHIP_MPP_OSAL) += mpp_osal/
+obj-$(CONFIG_ROCKCHIP_DVBM) += dvbm/
+obj-$(CONFIG_VIDEO_REVERSE_IMAGE) += vehicle/
+obj-$(CONFIG_ROCKCHIP_VIDEO_TUNNEL) += vtunnel/
diff --git a/drivers/video/rockchip/dvbm/Kconfig b/drivers/video/rockchip/dvbm/Kconfig
new file mode 100644
index 0000000000000..bfbd396b27066
--- /dev/null
+++ b/drivers/video/rockchip/dvbm/Kconfig
@@ -0,0 +1,18 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+menuconfig ROCKCHIP_DVBM
+	tristate "RK Direct Video Buffer Manager driver"
+	depends on ARCH_ROCKCHIP
+	help
+	  rockchip dvbm module.
+
+if ROCKCHIP_DVBM
+
+config ROCKCHIP_DVBM_PROC_FS
+	bool "enable dvbm procfs"
+	depends on PROC_FS
+	default y
+	help
+	  rockchip dvbm procfs.
+
+endif
diff --git a/drivers/video/rockchip/dvbm/Makefile b/drivers/video/rockchip/dvbm/Makefile
new file mode 100644
index 0000000000000..9096c34720ec1
--- /dev/null
+++ b/drivers/video/rockchip/dvbm/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+rk_dvbm-objs := rockchip_dvbm.o
+
+obj-$(CONFIG_ROCKCHIP_DVBM) += rk_dvbm.o
diff --git a/drivers/video/rockchip/dvbm/rockchip_dvbm.c b/drivers/video/rockchip/dvbm/rockchip_dvbm.c
new file mode 100644
index 0000000000000..2e40a7e78e78b
--- /dev/null
+++ b/drivers/video/rockchip/dvbm/rockchip_dvbm.c
@@ -0,0 +1,752 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2022 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Yandong Lin, yandong.lin@rock-chips.com
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/proc_fs.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <linux/interrupt.h>
+#include <soc/rockchip/rockchip_dvbm.h>
+
+#include "rockchip_dvbm.h"
+
+#define RK_DVBM		"rk_dvbm"
+
+unsigned int dvbm_debug;
+module_param(dvbm_debug, uint, 0644);
+MODULE_PARM_DESC(dvbm_debug, "bit switch for dvbm debug information");
+
+static struct dvbm_ctx *g_ctx;
+
+#define DVBM_DEBUG	0x00000001
+#define DVBM_DEBUG_IRQ	0x00000002
+#define DVBM_DEBUG_REG	0x00000004
+#define DVBM_DEBUG_DUMP	0x00000008
+#define DVBM_DEBUG_FRM	0x00000010
+
+#define dvbm_debug(fmt, args...)				\
+	do {							\
+		if (unlikely(dvbm_debug & (DVBM_DEBUG)))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define dvbm_debug_reg(fmt, args...)				\
+	do {							\
+		if (unlikely(dvbm_debug & (DVBM_DEBUG_REG)))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define dvbm_debug_irq(fmt, args...)				\
+	do {							\
+		if (unlikely(dvbm_debug & (DVBM_DEBUG_IRQ)))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define dvbm_debug_dump(fmt, args...)				\
+	do {							\
+		if (unlikely(dvbm_debug & (DVBM_DEBUG_DUMP)))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define dvbm_debug_frm(fmt, args...)				\
+	do {							\
+		if (unlikely(dvbm_debug & (DVBM_DEBUG_FRM)))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define dvbm_err(fmt, args...)	\
+	pr_err(fmt, ##args)
+
+enum dvbm_flow {
+	ISP_CFG		= 1,
+	ISP_CONNECT	= 2,
+	VEPU_CFG	= 3,
+	VEPU_CONNECT	= 4,
+};
+/* dvbm status reg bit value define */
+#define BUF_OVERFLOW		BIT(0)
+#define RESYNC_FINISH		BIT(1)
+#define ISP_CNCT_TIMEOUT	BIT(2)
+#define VEPU_CNCT_TIMEOUT	BIT(3)
+#define VEPU_HANDSHAKE_TIMEOUT	BIT(4)
+#define ISP_CNCT		BIT(5)
+#define ISP_DISCNCT		BIT(6)
+#define VEPU_CNCT		BIT(7)
+#define VEPU_DISCNCT		BIT(8)
+
+/* dvbm reg addr define */
+#define DVBM_VERSION	0x0
+#define DVBM_ISP_CNCT	0x4
+#define DVBM_VEPU_CNCT	0x8
+/* cfg regs */
+#define DVBM_CFG	0xC
+#define DVBM_WDG_CFG0	0x10
+#define DVBM_WDG_CFG1	0x14
+#define DVBM_WDG_CFG2	0x18
+/* interrupt regs */
+#define DVBM_INT_EN	0x1c
+#define DVBM_INT_MSK	0x20
+#define DVBM_INT_CLR	0x24
+#define DVBM_INT_ST	0x28
+/* addr regs */
+#define DVBM_YBUF_BOT	0x2c
+#define DVBM_YBUF_TOP	0x30
+#define DVBM_YBUF_SADR	0x34
+#define DVBM_YBUF_LSTD	0x38
+#define DVBM_YBUF_FSTD	0x3c
+#define DVBM_CBUF_BOT	0x40
+#define DVBM_CBUF_TOP	0x44
+#define DVBM_CBUF_SADR	0x48
+#define DVBM_CBUF_LSTD	0x4c
+#define DVBM_CBUF_FSTD	0x50
+#define DVBM_AFUL_THDY	0x54
+#define DVBM_AFUL_THDC	0x58
+#define DVBM_OVFL_THDY	0x5c
+#define DVBM_OVFL_THDC	0x60
+/* status regs */
+#define DVBM_ST		0x80
+#define DVBM_OVFL_ST	0x84
+
+#define DVBM_REG_OFFSET 0x2c
+
+#define SOFT_DVBM 1
+#define UPDATE_LINE_CNT 0
+
+static void rk_dvbm_set_reg(struct dvbm_ctx *ctx, u32 offset, u32 val)
+{
+	if (!SOFT_DVBM) {
+		dvbm_debug_reg("write reg[%d] 0x%x = 0x%08x\n", offset >> 2, offset, val);
+		writel(val, ctx->reg_base + offset);
+	}
+}
+
+static u32 rk_dvbm_read_reg(struct dvbm_ctx *ctx, u32 offset)
+{
+	u32 val = 0;
+
+	if (!SOFT_DVBM) {
+		val = readl(ctx->reg_base + offset);
+		dvbm_debug_reg("read reg[%d] 0x%x = 0x%08x\n", offset >> 2, offset, val);
+	}
+	return val;
+}
+
+static struct dvbm_ctx *port_to_ctx(struct dvbm_port *port)
+{
+	struct dvbm_ctx *ctx = NULL;
+
+	if (IS_ERR_OR_NULL(port))
+		return g_ctx;
+	if (port->dir == DVBM_ISP_PORT)
+		ctx = container_of(port, struct dvbm_ctx, port_isp);
+	else if (port->dir == DVBM_VEPU_PORT)
+		ctx = container_of(port, struct dvbm_ctx, port_vepu);
+
+	return ctx;
+}
+
+static void dvbm2enc_callback(struct dvbm_ctx *ctx, enum dvbm_cb_event event, void *arg)
+{
+	struct dvbm_cb *callback = &ctx->vepu_cb;
+	dvbm_callback cb = callback->cb;
+
+	if (!ctx->port_vepu.linked)
+		return;
+	if (cb)
+		cb(callback->ctx, event, arg);
+}
+
+static void rk_dvbm_dump_regs(struct dvbm_ctx *ctx)
+{
+	u32 start = ctx->dump_s;//0x80;
+	u32 end = ctx->dump_e;//0xb8;
+	u32 i;
+	dvbm_debug_dump("=== %s ===\n", __func__);
+	for (i = start; i <= end; i += 4)
+		dvbm_debug_dump("reg[0x%0x] = 0x%08x\n", i, readl(ctx->reg_base + i));
+	dvbm_debug_dump("=== %s ===\n", __func__);
+}
+
+static int rk_dvbm_clk_on(struct dvbm_ctx *ctx)
+{
+	int ret = 0;
+
+	if (ctx->clk)
+		ret = clk_prepare_enable(ctx->clk);
+	if (ret)
+		dev_err(ctx->dev, "clk on failed\n");
+	return ret;
+}
+
+static int rk_dvbm_clk_off(struct dvbm_ctx *ctx)
+{
+	if (ctx->clk)
+		clk_disable_unprepare(ctx->clk);
+	return 0;
+}
+
+static void init_isp_infos(struct dvbm_ctx *ctx)
+{
+	ctx->isp_frm_start = 0;
+	ctx->isp_frm_end = 0;
+	ctx->isp_frm_time = 0;
+}
+
+static void rk_dvbm_show_time(struct dvbm_ctx *ctx)
+{
+	ktime_t time = ktime_get();
+
+	if (ctx->isp_frm_time)
+		dvbm_debug("isp frame start[%d : %d] times %lld us\n",
+			   ctx->isp_frm_start, ctx->isp_frm_end,
+			   ktime_us_delta(time, ctx->isp_frm_time));
+	ctx->isp_frm_time = time;
+}
+
+static void rk_dvbm_update_isp_frm_info(struct dvbm_ctx *ctx, u32 line_cnt)
+{
+#if UPDATE_LINE_CNT
+	struct dvbm_isp_frm_info *frm_info = &ctx->isp_frm_info;
+
+	frm_info->line_cnt = ALIGN(line_cnt, 32);
+	dvbm_debug_frm("dvbm frame %d line %d\n", frm_info->frame_cnt, frm_info->line_cnt);
+	dvbm2enc_callback(ctx, DVBM_VEPU_NOTIFY_FRM_INFO, frm_info);
+#endif
+}
+
+static int rk_dvbm_setup_iobuf(struct dvbm_ctx *ctx)
+{
+	u32 *data;
+	u32 i;
+	struct rk_dvbm_base *addr_base = &ctx->regs.addr_base;
+	struct dvbm_isp_cfg_t *cfg = &ctx->isp_cfg;
+
+	addr_base->ybuf_bot = cfg->dma_addr + cfg->ybuf_bot;
+	addr_base->ybuf_top = cfg->dma_addr + cfg->ybuf_top;
+	addr_base->ybuf_sadr = cfg->dma_addr + cfg->ybuf_bot;
+	addr_base->ybuf_fstd = cfg->ybuf_fstd;
+	addr_base->ybuf_lstd = cfg->ybuf_lstd;
+
+	addr_base->cbuf_bot = cfg->dma_addr + cfg->cbuf_bot;
+	addr_base->cbuf_top = cfg->dma_addr + cfg->cbuf_top;
+	addr_base->cbuf_sadr = cfg->dma_addr + cfg->cbuf_bot;
+	addr_base->cbuf_fstd = cfg->cbuf_fstd;
+	addr_base->cbuf_lstd = cfg->cbuf_lstd;
+
+	addr_base->aful_thdy = cfg->ybuf_lstd;
+	addr_base->aful_thdc = cfg->ybuf_lstd;
+	addr_base->oful_thdy = cfg->ybuf_lstd;
+	addr_base->oful_thdc = cfg->ybuf_lstd;
+
+	ctx->isp_max_lcnt = cfg->ybuf_fstd / cfg->ybuf_lstd;
+	ctx->wrap_line = (cfg->ybuf_top - cfg->ybuf_bot) / cfg->ybuf_lstd;
+	ctx->isp_frm_info.frame_cnt = 0;
+	ctx->isp_frm_info.line_cnt = 0;
+	ctx->isp_frm_info.max_line_cnt = ALIGN(ctx->isp_max_lcnt, 32);
+	ctx->isp_frm_info.wrap_line = ctx->wrap_line;
+	dvbm_debug("dma_addr %pad y_lstd %d y_fstd %d\n",
+		   &cfg->dma_addr, cfg->ybuf_lstd, cfg->ybuf_fstd);
+	dvbm_debug("ybot 0x%x top 0x%x cbuf bot 0x%x top 0x%x\n",
+		   addr_base->ybuf_bot, addr_base->ybuf_top,
+		   addr_base->cbuf_bot, addr_base->cbuf_top);
+
+	data = (u32 *)addr_base;
+	for (i = 0; i < sizeof(struct rk_dvbm_base) / sizeof(u32); i++)
+		rk_dvbm_set_reg(ctx, i * sizeof(u32) + DVBM_REG_OFFSET, data[i]);
+
+	for (i = 1; i < 65536; i++)
+		if (!((addr_base->ybuf_fstd * i) % (cfg->ybuf_top - cfg->ybuf_bot)))
+			break;
+	ctx->loopcnt = i;
+	return 0;
+}
+
+static void rk_dvbm_reg_init(struct dvbm_ctx *ctx)
+{
+	struct rk_dvbm_regs *reg = &ctx->regs;
+	u32 *val = (u32 *)reg;
+
+	reg->int_en.buf_ovfl               = 1;
+	reg->int_en.isp_cnct               = 1;
+	reg->int_en.vepu_cnct              = 1;
+	reg->int_en.vepu_discnct           = 1;
+	reg->int_en.isp_discnct            = 1;
+	reg->int_en.resync_finish          = 1;
+	reg->int_en.isp_cnct_timeout       = 1;
+	reg->int_en.vepu_cnct_timeout      = 1;
+	reg->int_en.vepu_handshake_timeout = 1;
+
+	reg->dvbm_cfg.fmt                         = 0;
+	reg->dvbm_cfg.auto_resyn                  = 0;
+	reg->dvbm_cfg.ignore_vepu_cnct_ack        = 0;
+	reg->dvbm_cfg.start_point_after_vepu_cnct = 0;
+
+	reg->wdg_cfg0.wdg_isp_cnct_timeout       = 0xfffff;
+	reg->wdg_cfg1.wdg_vepu_cnct_timeout      = 0xfffff;
+	reg->wdg_cfg2.wdg_vepu_handshake_timeout = 0xfffff;
+
+	rk_dvbm_set_reg(ctx, DVBM_WDG_CFG0, val[DVBM_WDG_CFG0 >> 2]);
+	rk_dvbm_set_reg(ctx, DVBM_WDG_CFG1, val[DVBM_WDG_CFG1 >> 2]);
+	rk_dvbm_set_reg(ctx, DVBM_WDG_CFG2, val[DVBM_WDG_CFG2 >> 2]);
+	rk_dvbm_set_reg(ctx, DVBM_CFG, val[DVBM_CFG >> 2]);
+	rk_dvbm_set_reg(ctx, DVBM_INT_EN, val[DVBM_INT_EN >> 2]);
+}
+
+struct dvbm_port *rk_dvbm_get_port(struct platform_device *pdev,
+				   enum dvbm_port_dir dir)
+{
+	struct dvbm_ctx *ctx = NULL;
+	struct dvbm_port *port = NULL;
+
+	if (WARN_ON(!pdev))
+		return NULL;
+
+	ctx = (struct dvbm_ctx *)platform_get_drvdata(pdev);
+	WARN_ON(!ctx);
+	dvbm_debug("%s dir %d\n", __func__, dir);
+	if (dir == DVBM_ISP_PORT)
+		port = &ctx->port_isp;
+	else if (dir == DVBM_VEPU_PORT)
+		port = &ctx->port_vepu;
+
+	return port;
+}
+EXPORT_SYMBOL(rk_dvbm_get_port);
+
+int rk_dvbm_put(struct dvbm_port *port)
+{
+	struct dvbm_ctx *ctx = NULL;
+
+	if (WARN_ON(!port))
+		return -EINVAL;
+
+	ctx = port_to_ctx(port);
+
+	if (!ctx)
+		return -EINVAL;
+	return 0;
+}
+EXPORT_SYMBOL(rk_dvbm_put);
+
+int rk_dvbm_link(struct dvbm_port *port)
+{
+	struct dvbm_ctx *ctx;
+	enum dvbm_port_dir dir;
+	struct rk_dvbm_regs *reg;
+	int ret = 0;
+
+	if (WARN_ON(!port))
+		return -EINVAL;
+
+	ctx = port_to_ctx(port);
+	dir = port->dir;
+	reg = &ctx->regs;
+
+	if (dir == DVBM_ISP_PORT) {
+		if (port->linked) {
+			rk_dvbm_unlink(port);
+			udelay(5);
+		}
+		reg->isp_cnct.isp_cnct = 1;
+		rk_dvbm_set_reg(ctx, DVBM_ISP_CNCT, 0x1);
+	} else if (dir == DVBM_VEPU_PORT) {
+		if (!port->linked) {
+			reg->vepu_cnct.vepu_cnct = 1;
+			rk_dvbm_set_reg(ctx, DVBM_VEPU_CNCT, 0x1);
+		}
+		port->linked = 1;
+		dvbm_debug_dump("=== vepu link ===\n");
+		rk_dvbm_dump_regs(ctx);
+		dvbm_debug_dump("=== vepu link ===\n");
+	}
+
+	dvbm_debug("%s connect frm_cnt[%d : %d]\n",
+		   dir == DVBM_ISP_PORT ? "isp" : "vepu",
+		   ctx->isp_frm_start, ctx->isp_frm_end);
+
+	return ret;
+}
+EXPORT_SYMBOL(rk_dvbm_link);
+
+int rk_dvbm_unlink(struct dvbm_port *port)
+{
+	struct dvbm_ctx *ctx;
+	enum dvbm_port_dir dir;
+	struct rk_dvbm_regs *reg;
+
+	if (WARN_ON(!port))
+		return -EINVAL;
+
+	ctx = port_to_ctx(port);
+	dir = port->dir;
+	reg = &ctx->regs;
+
+	if (dir == DVBM_ISP_PORT) {
+		reg->isp_cnct.isp_cnct = 0;
+		rk_dvbm_set_reg(ctx, DVBM_ISP_CNCT, 0);
+	} else if (dir == DVBM_VEPU_PORT) {
+		reg->vepu_cnct.vepu_cnct = 0;
+		port->linked = 0;
+		rk_dvbm_set_reg(ctx, DVBM_VEPU_CNCT, 0);
+		if (!ctx->regs.dvbm_cfg.auto_resyn) {
+			u32 connect = 0;
+
+			dvbm2enc_callback(ctx, DVBM_VEPU_REQ_CONNECT, &connect);
+		}
+	}
+	dvbm_debug("%s disconnect\n", dir == DVBM_ISP_PORT ? "isp" : "vepu");
+
+	return 0;
+}
+EXPORT_SYMBOL(rk_dvbm_unlink);
+
+int rk_dvbm_set_cb(struct dvbm_port *port, struct dvbm_cb *cb)
+{
+	struct dvbm_ctx *ctx;
+	enum dvbm_port_dir dir;
+
+	if (WARN_ON(!port) || WARN_ON(!cb))
+		return -EINVAL;
+
+	ctx = port_to_ctx(port);
+	dir = port->dir;
+
+	if (dir == DVBM_ISP_PORT) {
+
+	} else if (dir == DVBM_VEPU_PORT) {
+		ctx->vepu_cb.cb = cb->cb;
+		ctx->vepu_cb.ctx = cb->ctx;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(rk_dvbm_set_cb);
+
+static void rk_dvbm_update_next_adr(struct dvbm_ctx *ctx)
+{
+	u32 frame_cnt = ctx->isp_frm_start;
+	struct dvbm_isp_cfg_t *isp_cfg = &ctx->isp_cfg;
+	struct dvbm_addr_cfg *vepu_cfg = &ctx->vepu_cfg;
+	u32 y_wrap_size = isp_cfg->ybuf_top - isp_cfg->ybuf_bot;
+	u32 c_wrap_size = isp_cfg->cbuf_top - isp_cfg->cbuf_bot;
+	u32 s_off;
+
+	frame_cnt = (frame_cnt + 1) % (ctx->loopcnt);
+	s_off = (frame_cnt * isp_cfg->ybuf_fstd) % y_wrap_size;
+	vepu_cfg->ybuf_sadr = isp_cfg->dma_addr + isp_cfg->ybuf_bot + s_off;
+
+	s_off = (frame_cnt * isp_cfg->cbuf_fstd) % c_wrap_size;
+	vepu_cfg->cbuf_sadr = isp_cfg->dma_addr + isp_cfg->cbuf_bot + s_off;
+}
+
+int rk_dvbm_ctrl(struct dvbm_port *port, enum dvbm_cmd cmd, void *arg)
+{
+	struct dvbm_ctx *ctx;
+	struct rk_dvbm_regs *reg;
+
+	if ((cmd < DVBM_ISP_CMD_BASE) || (cmd > DVBM_VEPU_CMD_BUTT)) {
+		dvbm_err("%s input cmd invalid\n", __func__);
+		return -EINVAL;
+	}
+
+	ctx = port_to_ctx(port);
+	reg = &ctx->regs;
+
+	switch (cmd) {
+	case DVBM_ISP_SET_CFG: {
+		struct dvbm_isp_cfg_t *cfg = (struct dvbm_isp_cfg_t *)arg;
+
+		memcpy(&ctx->isp_cfg, cfg, sizeof(struct dvbm_isp_cfg_t));
+		rk_dvbm_setup_iobuf(ctx);
+		init_isp_infos(ctx);
+		rk_dvbm_update_next_adr(ctx);
+	} break;
+	case DVBM_ISP_FRM_START: {
+		rk_dvbm_update_isp_frm_info(ctx, 0);
+		rk_dvbm_show_time(ctx);
+	} break;
+	case DVBM_ISP_FRM_END: {
+		u32 line_cnt = ctx->isp_max_lcnt;
+
+		ctx->isp_frm_end = *(u32 *)arg;
+		/* wrap frame_cnt 0 - 255 */
+		ctx->isp_frm_info.frame_cnt = (ctx->isp_frm_start + 1) % 256;
+		rk_dvbm_update_next_adr(ctx);
+		rk_dvbm_update_isp_frm_info(ctx, line_cnt);
+		ctx->isp_frm_start++;
+		dvbm_debug("isp frame end[%d : %d]\n", ctx->isp_frm_start, ctx->isp_frm_end);
+	} break;
+	case DVBM_ISP_FRM_QUARTER: {
+		u32 line_cnt;
+
+		line_cnt = ctx->isp_max_lcnt >> 2;
+		rk_dvbm_update_isp_frm_info(ctx, line_cnt);
+	} break;
+	case DVBM_ISP_FRM_HALF: {
+		u32 line_cnt;
+
+		line_cnt = ctx->isp_max_lcnt >> 1;
+		rk_dvbm_update_isp_frm_info(ctx, line_cnt);
+	} break;
+	case DVBM_ISP_FRM_THREE_QUARTERS: {
+		u32 line_cnt;
+
+		line_cnt = (ctx->isp_max_lcnt >> 2) * 3;
+		rk_dvbm_update_isp_frm_info(ctx, line_cnt);
+	} break;
+	case DVBM_VEPU_GET_ADR: {
+		struct dvbm_addr_cfg *dvbm_adr = (struct dvbm_addr_cfg *)arg;
+		struct rk_dvbm_base *addr_base = &reg->addr_base;
+
+		dvbm_adr->ybuf_top = addr_base->ybuf_top;
+		dvbm_adr->ybuf_bot = addr_base->ybuf_bot;
+		dvbm_adr->cbuf_top = addr_base->cbuf_top;
+		dvbm_adr->cbuf_bot = addr_base->cbuf_bot;
+		dvbm_adr->cbuf_sadr = ctx->vepu_cfg.cbuf_sadr;
+		dvbm_adr->ybuf_sadr = ctx->vepu_cfg.ybuf_sadr;
+		dvbm_adr->overflow = ctx->isp_frm_info.line_cnt >= ctx->wrap_line;
+		dvbm_adr->frame_id = ctx->isp_frm_info.frame_cnt;
+		dvbm_adr->line_cnt = ctx->isp_frm_info.line_cnt;
+	} break;
+	case DVBM_VEPU_GET_FRAME_INFO: {
+		memcpy(arg, &ctx->isp_frm_info, sizeof(struct dvbm_isp_frm_info));
+	} break;
+	case DVBM_VEPU_SET_RESYNC: {
+		reg->dvbm_cfg.auto_resyn = *(u32 *)arg;
+		dev_info(ctx->dev, "change resync %s\n",
+			 reg->dvbm_cfg.auto_resyn ? "auto" : "soft");
+		rk_dvbm_set_reg(ctx, DVBM_CFG, ((u32 *)&reg->dvbm_cfg)[0]);
+	} break;
+	case DVBM_VEPU_SET_CFG: {
+		struct dvbm_vepu_cfg *cfg = (struct dvbm_vepu_cfg *)arg;
+
+		reg->dvbm_cfg.auto_resyn = cfg->auto_resyn;
+		reg->dvbm_cfg.ignore_vepu_cnct_ack = cfg->ignore_vepu_cnct_ack;
+		reg->dvbm_cfg.start_point_after_vepu_cnct = cfg->start_point_after_vepu_cnct;
+
+		rk_dvbm_set_reg(ctx, DVBM_CFG, ((u32 *)&reg->dvbm_cfg)[0]);
+	} break;
+	case DVBM_VEPU_DUMP_REGS: {
+		rk_dvbm_dump_regs(ctx);
+	} break;
+	default: {
+	} break;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(rk_dvbm_ctrl);
+
+static void dvbm_check_irq(struct dvbm_ctx *ctx)
+{
+	u32 irq_st = ctx->irq_status;
+	u32 cur_st = ctx->dvbm_status;
+
+	if (irq_st & ISP_CNCT) {
+		dvbm_debug_irq("%s isp connect success! st 0x%08x\n",
+			       __func__, cur_st);
+		ctx->port_isp.linked = 1;
+	}
+	if (irq_st & ISP_DISCNCT) {
+		dvbm_debug_irq("%s isp disconnect success!\n", __func__);
+		ctx->port_isp.linked = 0;
+	}
+	if (irq_st & VEPU_CNCT) {
+		dvbm_debug_irq("%s vepu connect success! st 0x%08x\n",
+			       __func__, cur_st);
+		ctx->port_vepu.linked = 1;
+	}
+	if (irq_st & VEPU_DISCNCT) {
+		dvbm_debug_irq("%s vepu disconnect success! st 0x%08x\n", __func__, cur_st);
+		ctx->port_vepu.linked = 0;
+	}
+	if (irq_st & BUF_OVERFLOW) {
+		dvbm_debug_irq("%s buf overflow st 0x%08x auto_resync %d ignore %d\n",
+			       __func__, cur_st, ctx->regs.dvbm_cfg.auto_resyn, ctx->ignore_ovfl);
+
+		if (!ctx->regs.dvbm_cfg.auto_resyn && !ctx->ignore_ovfl)
+			rk_dvbm_unlink(&ctx->port_vepu);
+	}
+	if (irq_st & (ISP_CNCT_TIMEOUT | VEPU_CNCT_TIMEOUT))
+		rk_dvbm_dump_regs(ctx);
+}
+
+static irqreturn_t rk_dvbm_irq(int irq, void *param)
+{
+	struct dvbm_ctx *ctx = param;
+	u32 irq_st = 0;
+	u32 cur_st = 0;
+
+	if (ctx->reg_base) {
+		/* read irq st */
+		irq_st = rk_dvbm_read_reg(ctx, DVBM_INT_ST);
+		cur_st = rk_dvbm_read_reg(ctx, DVBM_ST);
+		if (irq_st & BUF_OVERFLOW) {
+			dvbm_debug_dump("=== dvbm overflow! dump reg st: 0x%08x===\n", irq_st);
+			rk_dvbm_dump_regs(ctx);
+			dvbm2enc_callback(ctx, DVBM_VEPU_NOTIFY_DUMP, NULL);
+			dvbm_debug_dump("=== dvbm overflow! dump reg end===\n");
+		}
+		/* clr irq */
+		rk_dvbm_set_reg(ctx, DVBM_INT_CLR, irq_st);
+		rk_dvbm_set_reg(ctx, DVBM_INT_ST, 0);
+	}
+	ctx->irq_status = irq_st;
+	ctx->dvbm_status = cur_st;
+
+	dvbm_debug_irq("%s irq status 0x%08x\n", __func__, irq_st);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static irqreturn_t rk_dvbm_isr(int irq, void *param)
+{
+	struct dvbm_ctx *ctx = param;
+
+	dvbm_check_irq(ctx);
+
+	return IRQ_HANDLED;
+}
+
+static int rk_dvbm_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct dvbm_ctx *ctx = NULL;
+	struct device *dev = &pdev->dev;
+	struct resource *res = NULL;
+
+	dev_info(dev, "probe start\n");
+	ctx = devm_kzalloc(dev, sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+	dev_info(dev, "dvbm ctx %p\n", ctx);
+
+	ctx->dev = dev;
+
+	atomic_set(&ctx->isp_ref, 0);
+	atomic_set(&ctx->vepu_ref, 0);
+	ctx->port_isp.dir = DVBM_ISP_PORT;
+	ctx->port_vepu.dir = DVBM_VEPU_PORT;
+
+	platform_set_drvdata(pdev, ctx);
+
+	pm_runtime_enable(dev);
+
+	/* get irq */
+	ctx->irq = platform_get_irq(pdev, 0);
+	if (ctx->irq < 0) {
+		dev_err(&pdev->dev, "no interrupt resource found\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+	/* get mem resource */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "no memory resource defined\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+
+	ctx->reg_base = devm_ioremap_resource(dev, res);
+	if (IS_ERR_OR_NULL(ctx->reg_base)) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		ret = -ENODEV;
+		goto failed;
+	}
+
+	ctx->clk = devm_clk_get(ctx->dev, "clk_core");
+	if (IS_ERR_OR_NULL(ctx->clk)) {
+		dev_err(dev, "clk_get failed for resource %pR\n", res);
+		ret = -ENODEV;
+		goto failed;
+	}
+	ctx->rst = devm_reset_control_get(ctx->dev, "dvbm_rst");
+	if (IS_ERR_OR_NULL(ctx->rst)) {
+		dev_err(dev, "clk_rst failed for resource %pR\n", res);
+		ret = -ENODEV;
+		goto failed;
+	}
+	if (!SOFT_DVBM) {
+		ret = pm_runtime_get_sync(dev);
+		if (ret)
+			dev_err(dev, "pm get failed!\n");
+		ret = rk_dvbm_clk_on(ctx);
+		if (ret)
+			goto failed;
+	}
+	g_ctx = ctx;
+	rk_dvbm_reg_init(ctx);
+	ctx->ignore_ovfl = 1;
+	ctx->dump_s = 0x80;
+	ctx->dump_e = 0xb8;
+	ret = devm_request_threaded_irq(dev, ctx->irq,
+					rk_dvbm_irq, rk_dvbm_isr,
+					IRQF_ONESHOT, dev_name(dev), ctx);
+	if (ret) {
+		dev_err(dev, "register interrupter failed\n");
+		goto failed;
+	}
+	dev_info(dev, "probe success\n");
+
+	return 0;
+
+failed:
+	pm_runtime_disable(dev);
+
+	return ret;
+}
+
+static int rk_dvbm_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	dev_info(dev, "remove device\n");
+	if (!SOFT_DVBM) {
+		rk_dvbm_clk_off(g_ctx);
+		pm_runtime_put(dev);
+	}
+	pm_runtime_disable(dev);
+
+	return 0;
+}
+
+static const struct of_device_id rk_dvbm_dt_ids[] = {
+	{
+		.compatible = "rockchip,rk-dvbm",
+	},
+	{ },
+};
+
+static struct platform_driver rk_dvbm_driver = {
+	.probe = rk_dvbm_probe,
+	.remove = rk_dvbm_remove,
+	.driver = {
+		.name = "rk_dvbm",
+		.of_match_table = of_match_ptr(rk_dvbm_dt_ids),
+	},
+};
+
+static int __init rk_dvbm_init(void)
+{
+	return platform_driver_register(&rk_dvbm_driver);
+}
+
+static __exit void rk_dvbm_exit(void)
+{
+	platform_driver_unregister(&rk_dvbm_driver);
+}
+
+subsys_initcall(rk_dvbm_init);
+module_exit(rk_dvbm_exit);
+
+MODULE_LICENSE("Dual MIT/GPL");
+MODULE_AUTHOR("Yandong Lin yandong.lin@rock-chips.com");
+MODULE_DESCRIPTION("Rockchip dvbm driver");
diff --git a/drivers/video/rockchip/dvbm/rockchip_dvbm.h b/drivers/video/rockchip/dvbm/rockchip_dvbm.h
new file mode 100644
index 0000000000000..9df38b8cbd582
--- /dev/null
+++ b/drivers/video/rockchip/dvbm/rockchip_dvbm.h
@@ -0,0 +1,214 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __ROCKCHIP_DVBM_H__
+#define __ROCKCHIP_DVBM_H__
+
+#include <linux/clk.h>
+#include <linux/reset.h>
+
+struct rk_dvbm_base {
+	/* 0x2c */
+	u32 ybuf_bot;
+	/* 0x30 */
+	u32 ybuf_top;
+	/* 0x34 */
+	u32 ybuf_sadr;
+	/* 0x38 */
+	u32 ybuf_lstd;
+	/* 0x3c */
+	u32 ybuf_fstd;
+	/* 0x40 */
+	u32 cbuf_bot;
+	/* 0x44 */
+	u32 cbuf_top;
+	/* 0x48 */
+	u32 cbuf_sadr;
+	/* 0x4c */
+	u32 cbuf_lstd;
+	/* 0x50 */
+	u32 cbuf_fstd;
+	/* 0x54 */
+	u32 aful_thdy;
+	/* 0x58 */
+	u32 aful_thdc;
+	/* 0x5c */
+	u32 oful_thdy;
+	/* 0x60 */
+	u32 oful_thdc;
+};
+
+struct rk_dvbm_regs {
+	/* 0x0 */
+	u32 version;
+
+	/* 0x4 */
+	struct {
+		u32 isp_cnct : 1;
+		u32 reserved : 31;
+	} isp_cnct;
+
+	/* 0x8 */
+	struct {
+		u32 vepu_cnct : 1;
+		u32 reserved : 31;
+	} vepu_cnct;
+
+	/* 0xc */
+	struct {
+		u32 auto_resyn                  : 1;
+		u32 ignore_vepu_cnct_ack        : 1;
+		/*
+		 * 1’b0                            : the current ISP frame
+		 * 1’b1                            : the next ISP frame
+		 */
+		u32 start_point_after_vepu_cnct : 1;
+		u32 reserved0                   : 5;
+		/* only support yuv420sp 4'h0 */
+		u32 fmt                         : 4;
+		u32 reserved1                   : 20;
+	} dvbm_cfg;
+
+	/* 0x10 */
+	struct {
+		u32 wdg_isp_cnct_timeout        : 22;
+		u32 reserved                    : 10;
+	} wdg_cfg0;
+
+	/* 0x14 */
+	struct {
+		u32 wdg_vepu_cnct_timeout       : 22;
+		u32 reserved                    : 10;
+	} wdg_cfg1;
+
+	/* 0x18 */
+	struct {
+		u32 wdg_vepu_handshake_timeout  : 22;
+		u32 reserved                    : 10;
+	} wdg_cfg2;
+
+	/* 0x1c */
+	struct {
+		u32 buf_ovfl               : 1;
+		u32 resync_finish          : 1;
+		u32 isp_cnct_timeout       : 1;
+		u32 vepu_cnct_timeout      : 1;
+
+		u32 vepu_handshake_timeout : 1;
+		u32 isp_cnct               : 1;
+		u32 isp_discnct            : 1;
+		u32 vepu_cnct              : 1;
+
+		u32 vepu_discnct           : 1;
+		u32 reserved               : 23;
+	} int_en;
+
+	/* 0x20 */
+	struct {
+		u32 buf_ovfl               : 1;
+		u32 resync_finish          : 1;
+		u32 isp_cnct_timeout       : 1;
+		u32 vepu_cnct_timeout      : 1;
+
+		u32 vepu_handshake_timeout : 1;
+		u32 isp_cnct               : 1;
+		u32 isp_discnct            : 1;
+		u32 vepu_cnct              : 1;
+
+		u32 vepu_discnct           : 1;
+		u32 reserved               : 23;
+	} int_msk;
+
+	/* 0x24 */
+	struct {
+		u32 buf_ovfl               : 1;
+		u32 resync_finish          : 1;
+		u32 isp_cnct_timeout       : 1;
+		u32 vepu_cnct_timeout      : 1;
+
+		u32 vepu_handshake_timeout : 1;
+		u32 isp_cnct               : 1;
+		u32 isp_discnct            : 1;
+		u32 vepu_cnct              : 1;
+
+		u32 vepu_discnct           : 1;
+		u32 reserved               : 23;
+	} int_clr;
+
+	/* 0x28 */
+	struct {
+		u32 buf_ovfl               : 1;
+		u32 resync_finish          : 1;
+		u32 isp_cnct_timeout       : 1;
+		u32 vepu_cnct_timeout      : 1;
+
+		u32 vepu_handshake_timeout : 1;
+		u32 isp_cnct               : 1;
+		u32 isp_discnct            : 1;
+		u32 vepu_cnct              : 1;
+
+		u32 vepu_discnct           : 1;
+		u32 reserved               : 23;
+	} int_st;
+	struct rk_dvbm_base addr_base;
+	/* 0x64 - 0x7c */
+	u32 reserved[7];
+
+	/* 0x80 */
+	struct {
+		u32 isp_connection       : 1;
+		u32 vepu_connection      : 1;
+		u32 resynchronization    : 1;
+		u32 y_buf_ovfl           : 1;
+
+		u32 c_buf_ovfl           : 1;
+		u32 reserved             : 27;
+	} dvbm_st;
+
+	/* 0x84 */
+	u32 ovfl_st;
+};
+
+struct dvbm_ctx {
+	struct clk *clk;
+	struct device *dev;
+	void __iomem *reg_base;
+	struct rk_dvbm_regs regs;
+	struct reset_control *rst;
+
+	u32 isp_connet;
+	u32 vepu_connet;
+	u32 buf_overflow;
+	u32 irq_status;
+	u32 dvbm_status;
+	int irq;
+
+	/* vepu infos */
+	struct dvbm_port port_vepu;
+	atomic_t vepu_ref;
+	atomic_t vepu_link;
+	struct dvbm_cb	vepu_cb;
+	struct dvbm_addr_cfg vepu_cfg;
+
+	/* isp infos */
+	struct dvbm_port port_isp;
+	struct dvbm_cb	isp_cb;
+	struct dvbm_isp_cfg_t isp_cfg;
+	struct dvbm_isp_frm_info isp_frm_info;
+	atomic_t isp_link;
+	atomic_t isp_ref;
+	u32 isp_max_lcnt;
+	u32 isp_frm_start;
+	u32 isp_frm_end;
+	ktime_t isp_frm_time;
+	u32 wrap_line;
+
+	/* debug infos */
+	u32 dump_s;
+	u32 dump_e;
+	u32 ignore_ovfl;
+	u32 loopcnt;
+};
+
+#endif
diff --git a/drivers/video/rockchip/iep/Kconfig b/drivers/video/rockchip/iep/Kconfig
new file mode 100644
index 0000000000000..cd8ba653f9eaa
--- /dev/null
+++ b/drivers/video/rockchip/iep/Kconfig
@@ -0,0 +1,10 @@
+# SPDX-License-Identifier: GPL-2.0
+menu "IEP"
+	depends on ARCH_ROCKCHIP
+
+config IEP
+	tristate "ROCKCHIP IEP driver"
+	help
+	  rockchip iep module.
+
+endmenu
diff --git a/drivers/video/rockchip/iep/Makefile b/drivers/video/rockchip/iep/Makefile
new file mode 100644
index 0000000000000..f4dba6e9379a2
--- /dev/null
+++ b/drivers/video/rockchip/iep/Makefile
@@ -0,0 +1,4 @@
+# SPDX-License-Identifier: GPL-2.0
+iep-y += hw_iep_reg.o iep_drv.o iep_iommu_ops.o
+iep-$(CONFIG_DRM) += iep_iommu_drm.o
+obj-$(CONFIG_IEP) += iep.o
diff --git a/drivers/video/rockchip/iep/hw_iep_config_addr.h b/drivers/video/rockchip/iep/hw_iep_config_addr.h
new file mode 100644
index 0000000000000..1f4a0706233fc
--- /dev/null
+++ b/drivers/video/rockchip/iep/hw_iep_config_addr.h
@@ -0,0 +1,99 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef HW_IEP_CONFIG_ADDR_H_
+#define HW_IEP_CONFIG_ADDR_H_
+
+#include <asm/io.h>
+
+/*ignore the IEP_BASE when program running in linux kernel */
+#define      IEP_BASE                      0x0
+
+#define      IEP_CONFIG0      		       0x0000
+#define      IEP_CONFIG1      		       0x0004
+
+#define      IEP_STATUS              	   0x0008
+#define      IEP_INT                 	   0x000C
+#define      IEP_FRM_START         		   0x0010
+#define      IEP_SOFT_RST           	   0x0014
+#define      IEP_CONF_DONE                 0x0018
+
+#define      IEP_VIR_IMG_WIDTH        	   0x0020
+
+#define      IEP_IMG_SCL_FCT         	   0x0024
+
+#define      IEP_SRC_IMG_SIZE         	   0x0028
+#define      IEP_DST_IMG_SIZE         	   0x002C
+
+#define      IEP_DST_IMG_WIDTH_TILE0  	   0x0030
+#define      IEP_DST_IMG_WIDTH_TILE1  	   0x0034
+#define      IEP_DST_IMG_WIDTH_TILE2  	   0x0038
+#define      IEP_DST_IMG_WIDTH_TILE3  	   0x003C
+
+#define      IEP_ENH_YUV_CNFG_0       	   0x0040
+#define      IEP_ENH_YUV_CNFG_1       	   0x0044
+#define      IEP_ENH_YUV_CNFG_2       	   0x0048
+#define      IEP_ENH_RGB_CNFG        	   0x004C
+#define      IEP_ENH_C_COE            	   0x0050
+
+#define      IEP_SRC_ADDR_YRGB        	   0x0080
+#define      IEP_SRC_ADDR_CBCR             0x0084
+#define      IEP_SRC_ADDR_CR               0x0088
+#define      IEP_SRC_ADDR_Y1               0x008C
+#define      IEP_SRC_ADDR_CBCR1            0x0090
+#define      IEP_SRC_ADDR_CR1              0x0094
+#define      IEP_SRC_ADDR_Y_ITEMP          0x0098
+#define      IEP_SRC_ADDR_CBCR_ITEMP       0x009C
+#define      IEP_SRC_ADDR_CR_ITEMP         0x00A0
+#define      IEP_SRC_ADDR_Y_FTEMP          0x00A4
+#define      IEP_SRC_ADDR_CBCR_FTEMP       0x00A8
+#define      IEP_SRC_ADDR_CR_FTEMP         0x00AC
+
+#define      IEP_DST_ADDR_YRGB        	   0x00B0
+#define      IEP_DST_ADDR_CBCR             0x00B4
+#define      IEP_DST_ADDR_CR               0x00B8
+#define      IEP_DST_ADDR_Y1               0x00BC
+#define      IEP_DST_ADDR_CBCR1            0x00C0
+#define      IEP_DST_ADDR_CR1              0x00C4
+#define      IEP_DST_ADDR_Y_ITEMP          0x00C8
+#define      IEP_DST_ADDR_CBCR_ITEMP       0x00CC
+#define      IEP_DST_ADDR_CR_ITEMP         0x00D0
+#define      IEP_DST_ADDR_Y_FTEMP          0x00D4
+#define      IEP_DST_ADDR_CBCR_FTEMP       0x00D8
+#define      IEP_DST_ADDR_CR_FTEMP         0x00DC
+
+#define      IEP_DIL_MTN_TAB0              0x00E0
+#define      IEP_DIL_MTN_TAB1              0x00E4
+#define      IEP_DIL_MTN_TAB2              0x00E8
+#define      IEP_DIL_MTN_TAB3              0x00EC
+#define      IEP_DIL_MTN_TAB4              0x00F0
+#define      IEP_DIL_MTN_TAB5              0x00F4
+#define      IEP_DIL_MTN_TAB6              0x00F8
+#define      IEP_DIL_MTN_TAB7              0x00FC
+
+#define      IEP_ENH_CG_TAB                0x0100
+
+#define      IEP_YUV_DNS_CRCT_TEMP         0x0400
+#define      IEP_YUV_DNS_CRCT_SPAT         0x0800
+
+#define      IEP_ENH_DDE_COE0              0x0C00
+#define      IEP_ENH_DDE_COE1              0x0E00
+
+#define      RAW_IEP_CONFIG0               0x0058
+#define      RAW_IEP_CONFIG1      		   0x005C
+#define      RAW_IEP_VIR_IMG_WIDTH         0x0060
+
+#define      RAW_IEP_IMG_SCL_FCT      	   0x0064
+
+#define      RAW_IEP_SRC_IMG_SIZE      	   0x0068
+#define      RAW_IEP_DST_IMG_SIZE      	   0x006C
+
+#define      RAW_IEP_ENH_YUV_CNFG_0        0x0070
+#define      RAW_IEP_ENH_YUV_CNFG_1        0x0074
+#define      RAW_IEP_ENH_YUV_CNFG_2        0x0078
+#define      RAW_IEP_ENH_RGB_CNFG          0x007C
+
+#define ReadReg32(base, raddr)	        (__raw_readl(base + raddr))
+#define WriteReg32(base, waddr, value)	(__raw_writel(value, base + waddr))
+#define ConfRegBits32(base, raddr, waddr, position, value)           WriteReg32(base, waddr, (ReadReg32(base, waddr)&~(position))|(value))
+#define MaskRegBits32(base, waddr, position, value)                  WriteReg32(base, waddr, (ReadReg32(base, waddr)&~(position))|(value))
+
+#endif
diff --git a/drivers/video/rockchip/iep/hw_iep_reg.c b/drivers/video/rockchip/iep/hw_iep_reg.c
new file mode 100644
index 0000000000000..fb6e79bb4800e
--- /dev/null
+++ b/drivers/video/rockchip/iep/hw_iep_reg.c
@@ -0,0 +1,1530 @@
+/* 
+ * Copyright (C) 2013 Rockchip Electronics Co., Ltd.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include "iep_iommu_ops.h"
+#include "hw_iep_reg.h"
+#include "iep.h"
+#include "hw_iep_config_addr.h"
+
+extern iep_service_info iep_service;
+static void iep_config_src_size(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_SRC_IMG_WIDTH(iep_msg->base, iep_msg->src.act_w - 1);
+	IEP_REGB_SRC_IMG_HEIGHT(iep_msg->base, iep_msg->src.act_h - 1);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==source image size config===================//\n\n");
+	IEP_DBG("sw_src_img_height          = %d;//source image height \n",
+		iep_msg->src.act_h - 1);
+	IEP_DBG("sw_src_img_width           = %d;//source image width \n\n",
+		iep_msg->src.act_w - 1);
+#endif
+}
+
+static void iep_config_dst_size(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_DST_IMG_WIDTH(iep_msg->base, iep_msg->dst.act_w - 1);
+	IEP_REGB_DST_IMG_HEIGHT(iep_msg->base, iep_msg->dst.act_h - 1);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==destination image size config===================//\n\n");
+	IEP_DBG("sw_dst_img_height          = %d;//source image height \n",
+		iep_msg->dst.act_h - 1);
+	IEP_DBG("sw_dst_img_width           = %d;//source image width \n",
+		iep_msg->dst.act_w - 1);
+#endif
+}
+
+static void iep_config_dst_width_tile(struct IEP_MSG *iep_msg)
+{
+	/*IEP_REGB_DST_IMG_WIDTH_TILE0();
+	IEP_REGB_DST_IMG_WIDTH_TILE1();
+	IEP_REGB_DST_IMG_WIDTH_TILE2();
+	IEP_REGB_DST_IMG_WIDTH_TILE3();*/
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_dst_width_tile0         = 0;\n");
+	IEP_DBG("sw_dst_width_tile1         = 0;\n");
+	IEP_DBG("sw_dst_width_tile2         = 0;\n");
+	IEP_DBG("sw_dst_width_tile3         = 0;\n\n");
+#endif
+}
+
+static void iep_config_dst_fmt(struct IEP_MSG *iep_msg)
+{
+	unsigned int dst_fmt = 0;
+	unsigned int dst_rgb_swap = 0;
+	unsigned int dst_yuv_swap = 0;
+	switch (iep_msg->dst.format) {
+	case IEP_FORMAT_ARGB_8888 :
+		IEP_REGB_DST_FMT(iep_msg->base, 0);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 0);
+		dst_fmt = 0;
+		dst_rgb_swap = 0;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_ABGR_8888 :
+		IEP_REGB_DST_FMT(iep_msg->base, 0);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 1);
+		dst_fmt = 0;
+		dst_rgb_swap = 1;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_RGBA_8888 :
+		IEP_REGB_DST_FMT(iep_msg->base, 0);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 2);
+		dst_fmt = 0;
+		dst_rgb_swap = 2;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_BGRA_8888 :
+		IEP_REGB_DST_FMT(iep_msg->base, 0);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 3);
+		dst_fmt = 0;
+		dst_rgb_swap = 3;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_RGB_565 :
+		IEP_REGB_DST_FMT(iep_msg->base, 1);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 0);
+		dst_fmt = 1;
+		dst_rgb_swap = 0;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_BGR_565 :
+		IEP_REGB_DST_FMT(iep_msg->base, 1);
+		IEP_REGB_DST_RGB_SWAP(iep_msg->base, 1);
+		dst_fmt = 1;
+		dst_rgb_swap = 1;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_YCbCr_422_SP :
+		IEP_REGB_DST_FMT(iep_msg->base, 2);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 0);
+		dst_fmt = 2;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		IEP_REGB_DST_FMT(iep_msg->base, 2);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 2);
+		dst_fmt = 2;
+		dst_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		IEP_REGB_DST_FMT(iep_msg->base, 3);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 0);
+		dst_fmt = 3;
+		dst_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		IEP_REGB_DST_FMT(iep_msg->base, 3);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 2);
+		dst_fmt = 3;
+		dst_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		IEP_REGB_DST_FMT(iep_msg->base, 2);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 1);
+		dst_fmt = 2;
+		dst_yuv_swap = 1;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		IEP_REGB_DST_FMT(iep_msg->base, 2);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 2);
+		dst_fmt = 2;
+		dst_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		IEP_REGB_DST_FMT(iep_msg->base, 3);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 1);
+		dst_fmt = 3;
+		dst_yuv_swap = 1;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		IEP_REGB_DST_FMT(iep_msg->base, 3);
+		IEP_REGB_DST_YUV_SWAP(iep_msg->base, 2);
+		dst_fmt = 3;
+		dst_yuv_swap = 2;
+		break;
+	default:
+		break;
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==destination data format config============//\n\n");
+	IEP_DBG("sw_dst_yuv_swap            = %d;//0:sp uv; 1:sp vu; 2:p ;"
+		" 3:p;\n",
+		dst_yuv_swap);
+	IEP_DBG("sw_dst_rgb_swap            = %d;//if ARGB 0:argb; "
+		"1,abgr; 2:rgba; 3:bgra; if rgb565: 0,2:rgb; 1,3:bgr;\n",
+		dst_rgb_swap);
+	IEP_DBG("sw_dst_fmt                 = %d;//0:argb; 1:rgb565; 2:yuv422;"
+		" 3:yuv420;\n\n", dst_fmt);
+#endif
+}
+
+static void iep_config_src_fmt(struct IEP_MSG *iep_msg)
+{
+	unsigned int src_fmt = 0;
+	unsigned int src_rgb_swap = 0;
+	unsigned int src_yuv_swap = 0;
+	switch (iep_msg->src.format) {
+	case IEP_FORMAT_ARGB_8888 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 0);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 0);
+		src_fmt = 0;
+		src_rgb_swap = 0;
+		break;
+	case IEP_FORMAT_ABGR_8888 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 0);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 1);
+		src_fmt = 0;
+		src_rgb_swap = 1;
+		break;
+	case IEP_FORMAT_RGBA_8888 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 0);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 2);
+		src_fmt = 0;
+		src_rgb_swap = 2;
+		break;
+	case IEP_FORMAT_BGRA_8888 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 0);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 3);
+		src_fmt = 0;
+		src_rgb_swap = 3;
+		break;
+	case IEP_FORMAT_RGB_565 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 1);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 0);
+		src_fmt = 1;
+		src_rgb_swap = 0;
+		break;
+	case IEP_FORMAT_BGR_565 :
+		IEP_REGB_SRC_FMT(iep_msg->base, 1);
+		IEP_REGB_SRC_RGB_SWAP(iep_msg->base, 1);
+		src_fmt = 1;
+		src_rgb_swap = 1;
+		break;
+	case IEP_FORMAT_YCbCr_422_SP :
+		IEP_REGB_SRC_FMT(iep_msg->base, 2);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 0);
+		src_fmt = 2;
+		src_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		IEP_REGB_SRC_FMT(iep_msg->base, 2);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 2);
+		src_fmt = 2;
+		src_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		IEP_REGB_SRC_FMT(iep_msg->base, 3);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 0);
+		src_fmt = 3;
+		src_yuv_swap = 0;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		IEP_REGB_SRC_FMT(iep_msg->base, 3);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 2);
+		src_fmt = 3;
+		src_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		IEP_REGB_SRC_FMT(iep_msg->base, 2);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 1);
+		src_fmt = 2;
+		src_yuv_swap = 1;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		IEP_REGB_SRC_FMT(iep_msg->base, 2);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 2);
+		src_fmt = 2;
+		src_yuv_swap = 2;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		IEP_REGB_SRC_FMT(iep_msg->base, 3);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 1);
+		src_fmt = 3;
+		src_yuv_swap = 1;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		IEP_REGB_SRC_FMT(iep_msg->base, 3);
+		IEP_REGB_SRC_YUV_SWAP(iep_msg->base, 2);
+		src_fmt = 3;
+		src_yuv_swap = 2;
+		break;
+	default:
+		break;
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==source data format config=================//\n\n");
+	IEP_DBG("sw_src_yuv_swap            = %d;//0:sp uv; 1:sp vu;"
+		" 2:p ; 3:p;\n", src_yuv_swap);
+	IEP_DBG("sw_src_rgb_swap            = %d;//if ARGB 0:argb; 1,abgr;"
+		" 2:rgba; 3:bgra; if rgb565: 0,2:rgb; 1,3:bgr;\n",
+		src_rgb_swap);
+	IEP_DBG("sw_src_fmt                 = %d;//0:argb; 1:rgb565;"
+		" 2:yuv422; 3:yuv420;\n\n", src_fmt);
+#endif
+}
+
+static void iep_config_scl(struct IEP_MSG *iep_msg)
+{
+	int scl_en;
+	int scl_sel;
+	//int vrt_fct;
+	//int hrz_fct;
+
+	unsigned int src_height, src_width, dst_height, dst_width;
+
+	int div_height_dst_src;
+	int div_width_dst_src;
+
+	src_height = iep_msg->src.act_h - 1;
+	src_width = iep_msg->src.act_w - 1;
+	dst_height = iep_msg->dst.act_h - 1;
+	dst_width = iep_msg->dst.act_w - 1;
+
+	if ((iep_msg->src.act_w == iep_msg->dst.act_w) &&
+	    (iep_msg->src.act_h == iep_msg->dst.act_h))
+		scl_en = 0;
+	else
+		scl_en = 1;
+
+	if ((iep_msg->src.act_w >= iep_msg->dst.act_w) &&
+	    (iep_msg->src.act_h >= iep_msg->dst.act_h))
+		scl_sel = 0;
+	else if ((iep_msg->src.act_w >= iep_msg->dst.act_w) &&
+		 (iep_msg->src.act_h <= iep_msg->dst.act_h))
+		scl_sel = 1;
+	else if ((iep_msg->src.act_w <= iep_msg->dst.act_w) &&
+		 (iep_msg->src.act_h >= iep_msg->dst.act_h))
+		scl_sel = 2;
+	else
+		scl_sel = 3;
+
+	//for vrt_fct
+	if ((scl_sel == 1) || (scl_sel == 3)) {
+		div_height_dst_src = src_height * 65536 / dst_height;
+	} else {
+		div_height_dst_src = (dst_height + 1) * 65536 /
+			(src_height + 1);
+		if ((div_height_dst_src * (src_height + 1)) <
+		    ((dst_height + 1) * 65536))
+			div_height_dst_src = div_height_dst_src + 1;
+	}
+
+	if (div_height_dst_src == 65536)
+		div_height_dst_src = 0;
+
+	//for hrz_fct
+	if ((scl_sel == 2) || (scl_sel == 3)) {
+		div_width_dst_src = src_width * 65536 / dst_width;
+	} else {
+		div_width_dst_src = (dst_width + 1) * 65536 / (src_width + 1);
+		if ((div_width_dst_src * (src_width + 1)) <
+		    ((dst_width + 1) * 65536))
+			div_width_dst_src = div_width_dst_src + 1;
+	}
+
+	if (div_width_dst_src == 65536)
+		div_width_dst_src = 0;
+
+
+	IEP_REGB_SCL_EN(iep_msg->base, scl_en);
+
+	if (scl_en == 1) {
+		IEP_REGB_SCL_SEL(iep_msg->base, scl_sel);
+		IEP_REGB_SCL_UP_COE_SEL(iep_msg->base, iep_msg->scale_up_mode);
+		IEP_REGB_SCL_VRT_FCT(iep_msg->base, div_height_dst_src);
+		IEP_REGB_SCL_HRZ_FCT(iep_msg->base, div_width_dst_src);
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==scaling config============================//\n\n");
+	IEP_DBG("sw_scl_en                  = %d;//0:disable; 1:enable;\n",
+		scl_en);
+	IEP_DBG("sw_scl_sel                 = %d;//0:hrz down & vrt down;"
+		"  1:hrz down & vrt up; 2:hrz up & vrt down;  3:hrz up &"
+		" vrt up;\n", scl_sel);
+	IEP_DBG("sw_scl_up_coe_sel          = %d;//select four groups of"
+		" up scaling coefficient\n", iep_msg->scale_up_mode);
+	IEP_DBG("sw_scl_vrt_fct             = %d;//if up-scaling,equal"
+		" to floor(src_img_height/dst_image_height)*2^16;"
+		" if down-scaling,equal to ceiling(dst_image_height/"
+		"src_image_height)*2^16;\n", div_height_dst_src);
+	IEP_DBG("sw_scl_hrz_fct             = %d;//if up-scaling,equal"
+		" to floor(src_img_widht/dst_image_width)*2^16;   if"
+		" down-scaling,equal to ceiling(dst_image_width/"
+		"src_image_width)*2^16  ; \n\n", div_width_dst_src);
+#endif
+}
+
+static void iep_config_cg_order(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_CON_GAM_ORDER(iep_msg->base,
+		iep_msg->rgb_contrast_enhance_mode);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG(" //==rgb enhancement & denoise config==========//\n\n");
+	IEP_DBG("sw_con_gam_order           = %d;//0:CG(contrast/gamma"
+		" operation)prior to DDE(de-noise/detail/edge enhance);"
+		"  1:DDE prior to CG;\n",
+		iep_msg->rgb_contrast_enhance_mode);
+#endif
+}
+
+static void iep_config_cg(struct IEP_MSG *iep_msg)
+{
+	unsigned i;
+	unsigned int cg_conf_addr;
+
+	IEP_REGB_RGB_CON_GAM_EN(iep_msg->base, iep_msg->rgb_cg_en);
+
+	if (iep_msg->rgb_cg_en) {
+		cg_conf_addr = rIEP_CG_TAB_ADDR;
+
+		for (i = 0; i < 192; i++) {
+			WriteReg32(iep_msg->base, cg_conf_addr,
+				iep_msg->cg_tab[i]);
+			cg_conf_addr += 0x04;
+		}
+	}
+
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_rgb_con_gam_en = 0;//0:contrast"
+		" & gamma disable; 1:enable;\n",
+		iep_msg->rgb_cg_en);
+#endif
+}
+
+static void iep_config_dde(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_RGB_ENH_SEL(iep_msg->base, iep_msg->rgb_enhance_mode);
+	IEP_REGB_ENH_THRESHOLD(iep_msg->base, iep_msg->enh_threshold);
+	IEP_REGB_ENH_ALPHA(iep_msg->base, iep_msg->enh_alpha);
+	IEP_REGB_ENH_RADIUS(iep_msg->base, iep_msg->enh_radius);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_rgb_enh_sel = %d;//0:no operation;"
+		" 1:de-noise; 2:detail enhance; 3:edge enhance;\n",
+		iep_msg->rgb_enhance_mode);
+#endif
+
+}
+
+static void iep_config_color_enh(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_RGB_COLOR_ENH_EN(iep_msg->base, iep_msg->rgb_color_enhance_en);
+	IEP_REGB_ENH_C_COE(iep_msg->base, iep_msg->rgb_enh_coe);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_rgb_color_enh_en = %d;//0:color enhance disable;"
+		" 1:enable;\n\n",
+		iep_msg->rgb_color_enhance_en);
+#endif
+}
+
+static void iep_config_yuv_dns(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_YUV_DNS_EN(iep_msg->base, iep_msg->yuv_3D_denoise_en);
+	IEP_REGB_YUV_DNS_LUMA_SPAT_SEL(iep_msg->base, 0);
+	IEP_REGB_YUV_DNS_LUMA_TEMP_SEL(iep_msg->base, 1);
+	IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL(iep_msg->base, 2);
+	IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL(iep_msg->base, 3);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==yuv denoise config========================// \n\n");
+	IEP_DBG("sw_yuv_dns_en              = %d;//0:yuv 3d denoise disable;"
+		" 1:enable\n\n", iep_msg->yuv_3D_denoise_en);
+#endif
+}
+
+
+static void iep_config_dil(struct IEP_MSG *iep_msg)
+{
+    int dein_mode;
+    switch (iep_msg->dein_mode) {
+    case IEP_DEINTERLACE_MODE_DISABLE:
+        dein_mode = dein_mode_bypass_dis;
+        break;
+    case IEP_DEINTERLACE_MODE_I2O1:
+        dein_mode = iep_msg->field_order == FIELD_ORDER_TOP_FIRST ? dein_mode_I2O1T : dein_mode_I2O1B;
+        break;
+    case IEP_DEINTERLACE_MODE_I4O1:
+#if 1
+        dein_mode = iep_msg->field_order == FIELD_ORDER_TOP_FIRST ? dein_mode_I4O1B : dein_mode_I4O1T;
+#else
+        dein_mode = iep_msg->field_order == FIELD_ORDER_TOP_FIRST ? dein_mode_I4O1T : dein_mode_I4O1B;
+#endif
+        break;
+    case IEP_DEINTERLACE_MODE_I4O2:
+        dein_mode = dein_mode_I4O2;
+        break;
+    case IEP_DEINTERLACE_MODE_BYPASS:
+        dein_mode = dein_mode_bypass;
+        break;
+    default:
+        IEP_ERR("unknown deinterlace mode, set deinterlace mode (bypass)\n");
+        dein_mode = dein_mode_bypass;
+    }
+
+    IEP_REGB_DIL_MODE(iep_msg->base, dein_mode);
+    //hf
+    IEP_REGB_DIL_HF_EN(iep_msg->base, iep_msg->dein_high_fre_en);
+    if (iep_msg->dein_high_fre_en == 1) IEP_REGB_DIL_HF_FCT(iep_msg->base, iep_msg->dein_high_fre_fct);
+    //ei
+    IEP_REGB_DIL_EI_MODE(iep_msg->base, iep_msg->dein_ei_mode);
+    IEP_REGB_DIL_EI_SMOOTH(iep_msg->base, iep_msg->dein_ei_smooth);
+    IEP_REGB_DIL_EI_SEL(iep_msg->base, iep_msg->dein_ei_sel);
+    if (iep_msg->dein_ei_sel == 0) IEP_REGB_DIL_EI_RADIUS(iep_msg->base, iep_msg->dein_ei_radius);
+	IEP_REGB_DIL_MTN_TAB0(iep_msg->base, 0x40404040);
+	IEP_REGB_DIL_MTN_TAB1(iep_msg->base, 0x3c3e3f3f);
+	IEP_REGB_DIL_MTN_TAB2(iep_msg->base, 0x3336393b);
+	IEP_REGB_DIL_MTN_TAB3(iep_msg->base, 0x272a2d31);
+	IEP_REGB_DIL_MTN_TAB4(iep_msg->base, 0x181c2023);
+	IEP_REGB_DIL_MTN_TAB5(iep_msg->base, 0x0c0e1215);
+	IEP_REGB_DIL_MTN_TAB6(iep_msg->base, 0x03040609);
+	IEP_REGB_DIL_MTN_TAB7(iep_msg->base, 0x00000001);
+}
+
+static void iep_config_yuv_enh(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_YUV_ENH_EN(iep_msg->base, iep_msg->yuv_enhance_en);
+	if (iep_msg->yuv_enhance_en == 1) {
+		IEP_REGB_VIDEO_MODE(iep_msg->base, iep_msg->video_mode);
+		if (iep_msg->video_mode == normal_mode) {
+			IEP_REGB_SAT_CON(iep_msg->base, iep_msg->sat_con_int);
+			IEP_REGB_CONTRAST(iep_msg->base,
+				iep_msg->contrast_int);
+			IEP_REGB_BRIGHTNESS(iep_msg->base,
+				iep_msg->yuv_enh_brightness);
+			IEP_REGB_COS_HUE(iep_msg->base, iep_msg->cos_hue_int);
+			IEP_REGB_SIN_HUE(iep_msg->base, iep_msg->sin_hue_int);
+		} else if (iep_msg->video_mode == color_bar) { //color bar
+			IEP_REGB_COLOR_BAR_Y(iep_msg->base,
+				iep_msg->color_bar_y);
+			IEP_REGB_COLOR_BAR_U(iep_msg->base,
+				iep_msg->color_bar_u);
+			IEP_REGB_COLOR_BAR_V(iep_msg->base,
+				iep_msg->color_bar_v);
+		}
+
+	}
+}
+
+static void iep_config_rgb2yuv(struct IEP_MSG *iep_msg)
+{
+	unsigned char cond1, cond2;
+	unsigned int rgb2yuv_en = 0;
+
+	//rgb in,yuv out
+	cond1 = ((iep_msg->src.format <= 5) && (iep_msg->dst.format > 5)) ?
+		1 : 0;
+
+	//rgb process,yuv out
+	cond2 = (((iep_msg->rgb_color_enhance_en == 1) ||
+		  (iep_msg->rgb_cg_en == 1) ||
+		  (iep_msg->rgb_enhance_mode != rgb_enhance_bypass)) &&
+		 (iep_msg->dst.format > 5)) ? 1 : 0;
+
+
+	if ((cond1 == 1) || (cond2 == 1)) {
+		IEP_REGB_RGB_TO_YUV_EN(iep_msg->base, 1);
+		rgb2yuv_en = 1;
+		IEP_REGB_RGB2YUV_COE_SEL(iep_msg->base, iep_msg->rgb2yuv_mode);
+		IEP_REGB_RGB2YUV_INPUT_CLIP(iep_msg->base,
+			iep_msg->rgb2yuv_clip_en);
+	} else
+		IEP_REGB_RGB_TO_YUV_EN(iep_msg->base, 0);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==color space conversion config============//\n\n");
+	IEP_DBG("sw_rgb_to_yuv_en = %d;\n", rgb2yuv_en);
+	IEP_DBG("sw_rgb2yuv_coe_sel = %d;\n", iep_msg->rgb2yuv_mode);
+	IEP_DBG("sw_rgb2yuv_input_clip = %d;\n\n", iep_msg->rgb2yuv_clip_en);
+#endif
+
+}
+
+static void iep_config_yuv2rgb(struct IEP_MSG *iep_msg)
+{
+	unsigned char cond1, cond2;
+	unsigned int yuv2rgb_en = 0;
+
+	//yuv in,rgb out
+	cond1 = ((iep_msg->src.format > 5) &&
+		 (iep_msg->dst.format <= 5)) ? 1 : 0;
+
+	//yuv in,rgb process
+	cond2 = (((iep_msg->rgb_color_enhance_en == 1) ||
+		  (iep_msg->rgb_cg_en == 1) ||
+		  (iep_msg->rgb_enhance_mode != rgb_enhance_bypass)) &&
+		 (iep_msg->src.format > 5)) ? 1 : 0;
+
+	if ((cond1 == 1) || (cond2 == 1)) {
+		IEP_REGB_YUV_TO_RGB_EN(iep_msg->base, 1);
+		yuv2rgb_en = 1;
+		IEP_REGB_YUV2RGB_COE_SEL(iep_msg->base,
+			iep_msg->yuv2rgb_mode);
+		IEP_REGB_YUV2RGB_INPUT_CLIP(iep_msg->base,
+			iep_msg->yuv2rgb_clip_en);
+	} else {
+		IEP_REGB_YUV_TO_RGB_EN(iep_msg->base, 0);
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_yuv_to_rgb_en           = %d;\n", yuv2rgb_en);
+	IEP_DBG("sw_yuv2rgb_coe_sel         = %d;\n", iep_msg->yuv2rgb_mode);
+	IEP_DBG("sw_yuv2rgb_input_clip = %d;\n\n", iep_msg->yuv2rgb_clip_en);
+#endif
+}
+
+static void iep_config_dither_up(struct IEP_MSG *iep_msg)
+{
+	unsigned int dither_up = 0;
+	if ((iep_msg->src.format == IEP_FORMAT_RGB_565) ||
+	    (iep_msg->src.format == IEP_FORMAT_BGR_565)) {
+		IEP_REGB_DITHER_UP_EN(iep_msg->base, iep_msg->dither_up_en);
+		dither_up = iep_msg->dither_up_en;
+	} else {
+		IEP_REGB_DITHER_UP_EN(iep_msg->base, 0);
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==dither config=============================//\n\n");
+	IEP_DBG("sw_dither_up_en            = %d;\n", dither_up);
+#endif
+}
+
+static void iep_config_dither_down(struct IEP_MSG *iep_msg)
+{
+	unsigned int dither_down = 0;
+	if ((iep_msg->dst.format == IEP_FORMAT_RGB_565) ||
+	    (iep_msg->dst.format == IEP_FORMAT_BGR_565)) {
+		IEP_REGB_DITHER_DOWN_EN(iep_msg->base, 1);
+		dither_down = 1;
+	} else {
+		IEP_REGB_DITHER_DOWN_EN(iep_msg->base, 0);
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("sw_dither_down_en = %d;\n\n", dither_down);
+#endif
+}
+
+static void iep_config_glb_alpha(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_GLB_ALPHA(iep_msg->base, iep_msg->global_alpha_value);
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==global alpha for ARGB config=============//\n\n");
+	IEP_DBG("sw_glb_alpha = %d;//global alpha value for output ARGB\n\n",
+		iep_msg->global_alpha_value);
+#endif
+}
+
+static void iep_config_vir_line(struct IEP_MSG *iep_msg)
+{
+	unsigned int src_vir_w;
+	unsigned int dst_vir_w;
+
+	switch (iep_msg->src.format) {
+	case IEP_FORMAT_ARGB_8888 :
+		src_vir_w = iep_msg->src.vir_w;
+		break;
+	case IEP_FORMAT_ABGR_8888 :
+		src_vir_w = iep_msg->src.vir_w;
+		break;
+	case IEP_FORMAT_RGBA_8888 :
+		src_vir_w = iep_msg->src.vir_w;
+		break;
+	case IEP_FORMAT_BGRA_8888 :
+		src_vir_w = iep_msg->src.vir_w;
+		break;
+	case IEP_FORMAT_RGB_565 :
+		if (iep_msg->src.vir_w % 2 == 1)
+			src_vir_w = (iep_msg->src.vir_w + 1) / 2;
+		else
+			src_vir_w = iep_msg->src.vir_w / 2;
+		break;
+	case IEP_FORMAT_BGR_565 :
+		if (iep_msg->src.vir_w % 2 == 1)
+			src_vir_w = iep_msg->src.vir_w / 2 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 2;
+		break;
+	case IEP_FORMAT_YCbCr_422_SP :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		if (iep_msg->src.vir_w % 4 != 0)
+			src_vir_w = iep_msg->src.vir_w / 4 + 1;
+		else
+			src_vir_w = iep_msg->src.vir_w / 4;
+		break;
+	default:
+		IEP_ERR("Unkown format,"
+			"set the source image virtual width 0\n");
+		src_vir_w = 0;
+		break;
+	}
+
+	switch (iep_msg->dst.format) {
+	case IEP_FORMAT_ARGB_8888 :
+		dst_vir_w = iep_msg->dst.vir_w;
+		break;
+	case IEP_FORMAT_ABGR_8888 :
+		dst_vir_w = iep_msg->dst.vir_w;
+		break;
+	case IEP_FORMAT_RGBA_8888 :
+		dst_vir_w = iep_msg->dst.vir_w;
+		break;
+	case IEP_FORMAT_BGRA_8888 :
+		dst_vir_w = iep_msg->dst.vir_w;
+		break;
+	case IEP_FORMAT_RGB_565 :
+		if (iep_msg->dst.vir_w % 2 == 1)
+			dst_vir_w = (iep_msg->dst.vir_w + 1) / 2;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 2;
+		break;
+	case IEP_FORMAT_BGR_565 :
+		if (iep_msg->dst.vir_w % 2 == 1)
+			dst_vir_w = iep_msg->dst.vir_w / 2 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 2;
+		break;
+	case IEP_FORMAT_YCbCr_422_SP :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		if (iep_msg->dst.vir_w % 4 != 0)
+			dst_vir_w = iep_msg->dst.vir_w / 4 + 1;
+		else
+			dst_vir_w = iep_msg->dst.vir_w / 4;
+		break;
+	default:
+		IEP_ERR("Unkown format, set the destination"
+			" image virtual width 0\n");
+		dst_vir_w = 0;
+		break;
+	}
+	IEP_REGB_DST_VIR_LINE_WIDTH(iep_msg->base, dst_vir_w);
+	IEP_REGB_SRC_VIR_LINE_WIDTH(iep_msg->base, src_vir_w);
+}
+
+static void iep_config_src_addr(struct IEP_MSG *iep_msg)
+{
+	u32 src_addr_yrgb;
+	u32 src_addr_cbcr;
+	u32 src_addr_cr;
+	u32 src_addr_y1;
+	u32 src_addr_cbcr1;
+	u32 src_addr_cr1;
+	u32 src_addr_y_itemp;
+	u32 src_addr_cbcr_itemp;
+	u32 src_addr_cr_itemp;
+	u32 src_addr_y_ftemp;
+	u32 src_addr_cbcr_ftemp;
+	u32 src_addr_cr_ftemp;
+	unsigned int offset_addr_y = 0;
+	unsigned int offset_addr_uv = 0;
+	unsigned int offset_addr_v = 0;
+	//unsigned int offset_addr_y_w = 0;
+	unsigned int offset_addr_uv_w = 0;
+	unsigned int offset_addr_v_w = 0;
+	//unsigned int offset_addr_y_h = 0;
+	unsigned int offset_addr_uv_h = 0;
+	unsigned int offset_addr_v_h = 0;
+
+	unsigned int offset_x_equ_uv;
+	unsigned int offset_x_u_byte;
+	unsigned int offset_x_v_byte;
+	unsigned int vir_w_euq_uv;
+	unsigned int line_u_byte;
+	unsigned int line_v_byte;
+	unsigned int offset_y_equ_420_uv = 0;
+
+	//**********************************************//
+	//***********y addr offset**********************//
+	//**********************************************//
+	if (iep_msg->src.format <= 3) {
+		offset_addr_y = iep_msg->src.y_off * 4 *
+			iep_msg->src.vir_w + iep_msg->src.x_off * 4;
+	} else if (iep_msg->src.format <= 5) {
+		offset_addr_y = iep_msg->src.y_off * 2 *
+			iep_msg->src.vir_w + iep_msg->src.x_off * 2;
+	} else {
+		offset_addr_y = iep_msg->src.y_off *
+			iep_msg->src.vir_w + iep_msg->src.x_off;
+	}
+
+	//**********************************************//
+	//***********uv addr offset*********************//
+	//**********************************************//
+	// note: image size align to even when image format is yuv
+
+	//----------offset_w--------//
+	if (iep_msg->src.x_off % 2 == 1)
+		offset_x_equ_uv = iep_msg->src.x_off + 1;
+	else
+		offset_x_equ_uv = iep_msg->src.x_off;
+
+	offset_x_u_byte = offset_x_equ_uv / 2;
+	offset_x_v_byte = offset_x_equ_uv / 2;
+
+	if ((iep_msg->src.format == IEP_FORMAT_YCbCr_422_SP) ||
+	    (iep_msg->src.format == IEP_FORMAT_YCbCr_420_SP)
+		|| (iep_msg->src.format == IEP_FORMAT_YCrCb_422_SP) ||
+	    (iep_msg->src.format == IEP_FORMAT_YCrCb_420_SP))
+		offset_addr_uv_w = offset_x_u_byte + offset_x_v_byte;
+	else {
+		offset_addr_uv_w = offset_x_u_byte;
+		offset_addr_v_w = offset_x_v_byte;
+	}
+
+	//----------offset_h--------//
+	if (iep_msg->src.vir_w % 2 == 1)
+		vir_w_euq_uv = iep_msg->src.vir_w + 1;
+	else
+		vir_w_euq_uv = iep_msg->src.vir_w;
+
+	line_u_byte = vir_w_euq_uv / 2;
+	line_v_byte = vir_w_euq_uv / 2;
+
+	if (iep_msg->src.y_off % 2 == 1)
+		offset_y_equ_420_uv = iep_msg->src.y_off + 1;
+	else
+		offset_y_equ_420_uv = iep_msg->src.y_off;
+
+	switch (iep_msg->src.format) {
+	case IEP_FORMAT_YCbCr_422_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			iep_msg->src.y_off;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		offset_addr_uv_h = line_u_byte * iep_msg->src.y_off;
+		offset_addr_v_h = line_v_byte * iep_msg->src.y_off;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		offset_addr_uv_h = line_u_byte * offset_y_equ_420_uv / 2;
+		offset_addr_v_h = line_v_byte * offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			iep_msg->src.y_off;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		offset_addr_uv_h = line_u_byte * iep_msg->src.y_off;
+		offset_addr_v_h = line_v_byte * iep_msg->src.y_off;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		offset_addr_uv_h = line_u_byte * offset_y_equ_420_uv / 2;
+		offset_addr_v_h = line_v_byte * offset_y_equ_420_uv / 2;
+		break;
+	default:
+		break;
+	}
+	//----------offset u/v addr--------//
+
+	offset_addr_uv = offset_addr_uv_w + offset_addr_uv_h;
+	offset_addr_v  = offset_addr_v_w + offset_addr_v_h;
+	//**********************************************//
+	//***********yuv address   *********************//
+	//**********************************************//
+	if (iep_service.iommu_dev == NULL) {
+		src_addr_yrgb = ((u32)iep_msg->src.mem_addr) + offset_addr_y;
+		src_addr_cbcr = ((u32)iep_msg->src.uv_addr) + offset_addr_uv;
+		src_addr_cr = ((u32)iep_msg->src.v_addr) + offset_addr_v;
+
+		src_addr_y1 = ((u32)iep_msg->src1.mem_addr) + offset_addr_y;
+		src_addr_cbcr1 = ((u32)iep_msg->src1.uv_addr) + offset_addr_uv;
+		src_addr_cr1 = ((u32)iep_msg->src1.v_addr) + offset_addr_v;
+
+		src_addr_y_itemp = ((u32)iep_msg->src_itemp.mem_addr) +
+			offset_addr_y;
+		src_addr_cbcr_itemp = ((u32)iep_msg->src_itemp.uv_addr) +
+			offset_addr_uv;
+		src_addr_cr_itemp = ((u32)iep_msg->src_itemp.v_addr) +
+			offset_addr_v;
+
+		src_addr_y_ftemp = ((u32)iep_msg->src_ftemp.mem_addr) +
+			offset_addr_y;
+		src_addr_cbcr_ftemp = ((u32)iep_msg->src_ftemp.uv_addr) +
+			offset_addr_uv;
+		src_addr_cr_ftemp = ((u32)iep_msg->src_ftemp.v_addr) +
+			offset_addr_v;
+	} else {
+		src_addr_yrgb = ((u32)iep_msg->src.mem_addr) + (offset_addr_y << 10);
+		src_addr_cbcr = ((u32)iep_msg->src.uv_addr) + (offset_addr_uv << 10);
+		src_addr_cr = ((u32)iep_msg->src.v_addr) + (offset_addr_v << 10);
+
+		src_addr_y1 = ((u32)iep_msg->src1.mem_addr) + (offset_addr_y << 10);
+		src_addr_cbcr1 = ((u32)iep_msg->src1.uv_addr) + (offset_addr_uv  << 10);
+		src_addr_cr1 = ((u32)iep_msg->src1.v_addr) + (offset_addr_v << 10);
+
+		src_addr_y_itemp = ((u32)iep_msg->src_itemp.mem_addr) +
+			(offset_addr_y << 10);
+		src_addr_cbcr_itemp = ((u32)iep_msg->src_itemp.uv_addr) +
+			(offset_addr_uv << 10);
+		src_addr_cr_itemp = ((u32)iep_msg->src_itemp.v_addr) +
+			(offset_addr_v << 10);
+
+		src_addr_y_ftemp = ((u32)iep_msg->src_ftemp.mem_addr) +
+			(offset_addr_y << 10);
+		src_addr_cbcr_ftemp = ((u32)iep_msg->src_ftemp.uv_addr) +
+			(offset_addr_uv << 10);
+		src_addr_cr_ftemp = ((u32)iep_msg->src_ftemp.v_addr) +
+			(offset_addr_v << 10);
+	}
+
+	if ((iep_msg->dein_mode == IEP_DEINTERLACE_MODE_I4O1 ||
+	     iep_msg->dein_mode == IEP_DEINTERLACE_MODE_I4O2) &&
+#if 1
+		iep_msg->field_order == FIELD_ORDER_BOTTOM_FIRST
+#else
+		iep_msg->field_order == FIELD_ORDER_TOP_FIRST
+#endif
+		) {
+		IEP_REGB_SRC_ADDR_YRGB(iep_msg->base, src_addr_y1);
+		IEP_REGB_SRC_ADDR_CBCR(iep_msg->base, src_addr_cbcr1);
+		IEP_REGB_SRC_ADDR_CR(iep_msg->base, src_addr_cr1);
+		IEP_REGB_SRC_ADDR_Y1(iep_msg->base, src_addr_yrgb);
+		IEP_REGB_SRC_ADDR_CBCR1(iep_msg->base, src_addr_cbcr);
+		IEP_REGB_SRC_ADDR_CR1(iep_msg->base, src_addr_cr);
+	} else {
+		IEP_REGB_SRC_ADDR_YRGB(iep_msg->base, src_addr_yrgb);
+		IEP_REGB_SRC_ADDR_CBCR(iep_msg->base, src_addr_cbcr);
+		IEP_REGB_SRC_ADDR_CR(iep_msg->base, src_addr_cr);
+		IEP_REGB_SRC_ADDR_Y1(iep_msg->base, src_addr_y1);
+		IEP_REGB_SRC_ADDR_CBCR1(iep_msg->base, src_addr_cbcr1);
+		IEP_REGB_SRC_ADDR_CR1(iep_msg->base, src_addr_cr1);
+	}
+
+	if (iep_msg->yuv_3D_denoise_en) {
+		IEP_REGB_SRC_ADDR_Y_ITEMP(iep_msg->base,
+			src_addr_y_itemp);
+		IEP_REGB_SRC_ADDR_CBCR_ITEMP(iep_msg->base,
+			src_addr_cbcr_itemp);
+		IEP_REGB_SRC_ADDR_Y_FTEMP(iep_msg->base,
+			src_addr_y_ftemp);
+		IEP_REGB_SRC_ADDR_CBCR_FTEMP(iep_msg->base,
+			src_addr_cbcr_ftemp);
+		if ((iep_msg->src.format == IEP_FORMAT_YCbCr_422_P) ||
+		    (iep_msg->src.format == IEP_FORMAT_YCbCr_420_P)
+			|| (iep_msg->src.format == IEP_FORMAT_YCrCb_422_P) ||
+		    (iep_msg->src.format == IEP_FORMAT_YCrCb_420_P)) {
+			IEP_REGB_SRC_ADDR_CR_ITEMP(iep_msg->base,
+				src_addr_cr_itemp);
+			IEP_REGB_SRC_ADDR_CR_FTEMP(iep_msg->base,
+				src_addr_cr_ftemp);
+		}
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//-------source address for image-------// \n\n");
+	IEP_DBG("sw_src_addr_yrgb           = 32'h%x;\n", src_addr_yrgb);
+	IEP_DBG("sw_src_addr_cbcr           = 32'h%x;\n", src_addr_cbcr);
+	IEP_DBG("sw_src_addr_cr             = 32'h%x;\n", src_addr_cr);
+	IEP_DBG("sw_src_addr_y1             = 32'h%x;\n", src_addr_y1);
+	IEP_DBG("sw_src_addr_cbcr0          = 32'h%x;\n", src_addr_cbcr1);
+	IEP_DBG("sw_src_addr_cr0            = 32'h%x;\n", src_addr_cr1);
+	IEP_DBG("sw_src_addr_y_itemp        = 32'h%x;\n", src_addr_y_itemp);
+	IEP_DBG("sw_src_addr_cbcr_itemp     = 32'h%x;\n", src_addr_cbcr_itemp);
+	IEP_DBG("sw_src_addr_cr_itemp       = 32'h%x;\n", src_addr_cr_itemp);
+	IEP_DBG("sw_src_addr_y_ftemp        = 32'h%x;\n", src_addr_y_ftemp);
+	IEP_DBG("sw_src_addr_cbcr_ftemp     = 32'h%x;\n", src_addr_cbcr_ftemp);
+	IEP_DBG("sw_src_addr_cr_ftemp       = 32'h%x;\n\n", src_addr_cr_ftemp);
+#endif
+}
+
+static void iep_config_dst_addr(struct IEP_MSG *iep_msg)
+{
+	u32 dst_addr_yrgb;
+	u32 dst_addr_cbcr;
+	u32 dst_addr_cr;
+	u32 dst_addr_y1;
+	u32 dst_addr_cbcr1;
+	u32 dst_addr_cr1;
+	u32 dst_addr_y_itemp;
+	u32 dst_addr_cbcr_itemp;
+	u32 dst_addr_cr_itemp;
+	u32 dst_addr_y_ftemp;
+	u32 dst_addr_cbcr_ftemp;
+	u32 dst_addr_cr_ftemp;
+	unsigned int offset_addr_y = 0;
+	unsigned int offset_addr_uv = 0;
+	unsigned int offset_addr_v = 0;
+	//unsigned int offset_addr_y_w = 0;
+	unsigned int offset_addr_uv_w = 0;
+	unsigned int offset_addr_v_w = 0;
+	//unsigned int offset_addr_y_h = 0;
+	unsigned int offset_addr_uv_h = 0;
+	unsigned int offset_addr_v_h = 0;
+
+	unsigned int offset_x_equ_uv;
+	unsigned int offset_x_u_byte;
+	unsigned int offset_x_v_byte;
+	unsigned int vir_w_euq_uv;
+	unsigned int line_u_byte;
+	unsigned int line_v_byte;
+	unsigned int offset_y_equ_420_uv = 0;
+
+	//**********************************************//
+	//***********y addr offset**********************//
+	//**********************************************//
+	if (iep_msg->dst.format <= 3) {
+		offset_addr_y = iep_msg->dst.y_off * 4 *
+			iep_msg->dst.vir_w + iep_msg->dst.x_off * 4;
+	} else if (iep_msg->dst.format <= 5) {
+		offset_addr_y = iep_msg->dst.y_off * 2 *
+			iep_msg->dst.vir_w + iep_msg->dst.x_off * 2;
+	} else {
+		offset_addr_y = iep_msg->dst.y_off *
+			iep_msg->dst.vir_w + iep_msg->dst.x_off;
+	}
+
+	//**********************************************//
+	//***********uv addr offset*********************//
+	//**********************************************//
+	// note: image size align to even when image format is yuv
+
+	//----------offset_w--------//
+	if (iep_msg->dst.x_off % 2 == 1)
+		offset_x_equ_uv = iep_msg->dst.x_off + 1;
+	else
+		offset_x_equ_uv = iep_msg->dst.x_off;
+
+	offset_x_u_byte = offset_x_equ_uv / 2;
+	offset_x_v_byte = offset_x_equ_uv / 2;
+
+	if ((iep_msg->dst.format == IEP_FORMAT_YCbCr_422_SP) ||
+	    (iep_msg->dst.format == IEP_FORMAT_YCbCr_420_SP)
+		|| (iep_msg->dst.format == IEP_FORMAT_YCrCb_422_SP) ||
+	    (iep_msg->dst.format == IEP_FORMAT_YCrCb_420_SP))
+		offset_addr_uv_w = offset_x_u_byte + offset_x_v_byte;
+	else {
+		offset_addr_uv_w = offset_x_u_byte;
+		offset_addr_v_w = offset_x_v_byte;
+	}
+
+	//----------offset_h--------//
+	if (iep_msg->dst.vir_w % 2 == 1)
+		vir_w_euq_uv = iep_msg->dst.vir_w + 1;
+	else
+		vir_w_euq_uv = iep_msg->dst.vir_w;
+
+	line_u_byte = vir_w_euq_uv / 2;
+	line_v_byte = vir_w_euq_uv / 2;
+
+	if (iep_msg->dst.y_off % 2 == 1)
+		offset_y_equ_420_uv = iep_msg->dst.y_off + 1;
+	else
+		offset_y_equ_420_uv = iep_msg->dst.y_off;
+
+	switch (iep_msg->dst.format) {
+	case IEP_FORMAT_YCbCr_422_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			iep_msg->dst.y_off;
+		break;
+	case IEP_FORMAT_YCbCr_422_P :
+		offset_addr_uv_h = line_u_byte * iep_msg->dst.y_off;
+		offset_addr_v_h = line_v_byte * iep_msg->dst.y_off;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCbCr_420_P :
+		offset_addr_uv_h = line_u_byte * offset_y_equ_420_uv / 2;
+		offset_addr_v_h = line_v_byte * offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCrCb_422_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			iep_msg->dst.y_off;
+		break;
+	case IEP_FORMAT_YCrCb_422_P :
+		offset_addr_uv_h = line_u_byte * iep_msg->dst.y_off;
+		offset_addr_v_h = line_v_byte * iep_msg->dst.y_off;
+		break;
+	case IEP_FORMAT_YCrCb_420_SP :
+		offset_addr_uv_h = (line_u_byte + line_v_byte) *
+			offset_y_equ_420_uv / 2;
+		break;
+	case IEP_FORMAT_YCrCb_420_P :
+		offset_addr_uv_h = line_u_byte * offset_y_equ_420_uv / 2;
+		offset_addr_v_h = line_v_byte * offset_y_equ_420_uv / 2;
+		break;
+	default:
+		break;
+	}
+	//----------offset u/v addr--------//
+
+	offset_addr_uv = offset_addr_uv_w + offset_addr_uv_h;
+	offset_addr_v  = offset_addr_v_w + offset_addr_v_h;
+	//**********************************************//
+	//***********yuv address   *********************//
+	//**********************************************//
+
+	if (iep_service.iommu_dev == NULL) {
+		dst_addr_yrgb = ((u32)iep_msg->dst.mem_addr) + offset_addr_y;
+		dst_addr_cbcr = ((u32)iep_msg->dst.uv_addr) + offset_addr_uv;
+		dst_addr_cr = ((u32)iep_msg->dst.v_addr) + offset_addr_v;
+
+		// former frame when processing deinterlace
+		dst_addr_y1 = ((u32)iep_msg->dst1.mem_addr) + offset_addr_y;
+		dst_addr_cbcr1 = ((u32)iep_msg->dst1.uv_addr) + offset_addr_uv;
+		dst_addr_cr1 = ((u32)iep_msg->dst1.v_addr) + offset_addr_v;
+
+		dst_addr_y_itemp = ((u32)iep_msg->dst_itemp.mem_addr) +
+			offset_addr_y;
+		dst_addr_cbcr_itemp = ((u32)iep_msg->dst_itemp.uv_addr) +
+			offset_addr_uv;
+		dst_addr_cr_itemp = ((u32)iep_msg->dst_itemp.v_addr) +
+			offset_addr_v;
+
+		dst_addr_y_ftemp = ((u32)iep_msg->dst_ftemp.mem_addr) +
+			offset_addr_y;
+		dst_addr_cbcr_ftemp = ((u32)iep_msg->dst_ftemp.uv_addr) +
+			offset_addr_uv;
+		dst_addr_cr_ftemp = ((u32)iep_msg->dst_ftemp.v_addr) +
+			offset_addr_v;
+	} else {
+		dst_addr_yrgb = ((u32)iep_msg->dst.mem_addr) + (offset_addr_y << 10);
+		dst_addr_cbcr = ((u32)iep_msg->dst.uv_addr) + (offset_addr_uv << 10);
+		dst_addr_cr = ((u32)iep_msg->dst.v_addr) + (offset_addr_v << 10);
+
+		// former frame when processing deinterlace
+		dst_addr_y1 = ((u32)iep_msg->dst1.mem_addr) + (offset_addr_y << 10);
+		dst_addr_cbcr1 = ((u32)iep_msg->dst1.uv_addr) + (offset_addr_uv << 10);
+		dst_addr_cr1 = ((u32)iep_msg->dst1.v_addr) + (offset_addr_v << 10);
+
+		dst_addr_y_itemp = ((u32)iep_msg->dst_itemp.mem_addr) +
+			(offset_addr_y << 10);
+		dst_addr_cbcr_itemp = ((u32)iep_msg->dst_itemp.uv_addr) +
+			(offset_addr_uv << 10);
+		dst_addr_cr_itemp = ((u32)iep_msg->dst_itemp.v_addr) +
+			(offset_addr_v << 10);
+
+		dst_addr_y_ftemp = ((u32)iep_msg->dst_ftemp.mem_addr) +
+			(offset_addr_y << 10);
+		dst_addr_cbcr_ftemp = ((u32)iep_msg->dst_ftemp.uv_addr) +
+			(offset_addr_uv << 10);
+		dst_addr_cr_ftemp = ((u32)iep_msg->dst_ftemp.v_addr) +
+			(offset_addr_v << 10);
+	}
+
+	IEP_REGB_DST_ADDR_YRGB(iep_msg->base, dst_addr_yrgb);
+	IEP_REGB_DST_ADDR_CBCR(iep_msg->base, dst_addr_cbcr);
+	IEP_REGB_DST_ADDR_Y1(iep_msg->base, dst_addr_y1);
+	IEP_REGB_DST_ADDR_CBCR1(iep_msg->base, dst_addr_cbcr1);
+	IEP_REGB_DST_ADDR_CR(iep_msg->base, dst_addr_cr);
+	IEP_REGB_DST_ADDR_CR1(iep_msg->base, dst_addr_cr1);
+
+	if (iep_msg->yuv_3D_denoise_en) {
+		IEP_REGB_DST_ADDR_Y_ITEMP(iep_msg->base,
+			dst_addr_y_itemp);
+		IEP_REGB_DST_ADDR_CBCR_ITEMP(iep_msg->base,
+			dst_addr_cbcr_itemp);
+		IEP_REGB_DST_ADDR_Y_FTEMP(iep_msg->base,
+			dst_addr_y_ftemp);
+		IEP_REGB_DST_ADDR_CBCR_FTEMP(iep_msg->base,
+			dst_addr_cbcr_ftemp);
+		if ((iep_msg->dst.format == IEP_FORMAT_YCbCr_422_P) ||
+		    (iep_msg->dst.format == IEP_FORMAT_YCbCr_420_P) ||
+		    (iep_msg->dst.format == IEP_FORMAT_YCrCb_422_P) ||
+		    (iep_msg->dst.format == IEP_FORMAT_YCrCb_420_P)) {
+			IEP_REGB_DST_ADDR_CR_ITEMP(iep_msg->base,
+				dst_addr_cr_itemp);
+			IEP_REGB_DST_ADDR_CR_FTEMP(iep_msg->base,
+				dst_addr_cr_ftemp);
+		}
+	}
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//-------destination address for image-------// \n\n");
+	IEP_DBG("sw_dst_addr_yrgb           = 32'h%x;\n",
+		(u32)iep_msg->dst.mem_addr);
+	IEP_DBG("sw_dst_addr_cbcr           = 32'h%x;\n",
+		(u32)iep_msg->dst.uv_addr);
+	IEP_DBG("sw_dst_addr_cr             = 32'h%x;\n",
+		(u32)iep_msg->dst.v_addr);
+	IEP_DBG("sw_dst_addr_y1             = 32'h%x;\n",
+		(u32)iep_msg->dst1.mem_addr);
+	IEP_DBG("sw_dst_addr_cbcr0          = 32'h%x;\n",
+		(u32)iep_msg->dst1.uv_addr);
+	IEP_DBG("sw_dst_addr_cr0            = 32'h%x;\n",
+		(u32)iep_msg->dst1.v_addr);
+	IEP_DBG("sw_dst_addr_y_itemp        = 32'h%x;\n",
+		(u32)iep_msg->dst_itemp.mem_addr);
+	IEP_DBG("sw_dst_addr_cbcr_itemp     = 32'h%x;\n",
+		(u32)iep_msg->dst_itemp.uv_addr);
+	IEP_DBG("sw_dst_addr_cr_itemp       = 32'h%x;\n",
+		(u32)iep_msg->dst_itemp.v_addr);
+	IEP_DBG("sw_dst_addr_y_ftemp        = 32'h%x;\n",
+		(u32)iep_msg->dst_ftemp.mem_addr);
+	IEP_DBG("sw_dst_addr_cbcr_ftemp     = 32'h%x;\n",
+		(u32)iep_msg->dst_ftemp.uv_addr);
+	IEP_DBG("sw_dst_addr_cr_ftemp       = 32'h%x;\n\n",
+		(u32)iep_msg->dst_ftemp.v_addr);
+#endif
+}
+
+void iep_config_lcdc_path(struct IEP_MSG *iep_msg)
+{
+	IEP_REGB_LCDC_PATH_EN(iep_msg->base, iep_msg->lcdc_path_en);
+
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==write back or lcdc direct path config=====// \n\n");
+	IEP_DBG("sw_lcdc_path_en = %d;//lcdc direct path enable,c"
+		" model don't care this value\n\n", iep_msg->lcdc_path_en);
+#endif
+}
+
+int iep_probe_int(void *base)
+{
+	return ReadReg32(base, rIEP_INT) & 1;
+}
+
+void iep_config_frame_end_int_clr(void *base)
+{
+	IEP_REGB_FRAME_END_INT_CLR(base, 1);
+}
+
+void iep_config_frame_end_int_en(void *base)
+{
+	IEP_REGB_FRAME_END_INT_CLR(base, 1);
+	IEP_REGB_FRAME_END_INT_EN(base, 1);
+}
+
+static void iep_config_misc(struct IEP_MSG *iep_msg)
+{
+//	IEP_REGB_V_REVERSE_DISP();
+//	IEP_REGB_H_REVERSE_DISP();
+#ifdef IEP_PRINT_INFO
+	IEP_DBG("//==misc config==========================//\n\n");
+	IEP_DBG("sw_v_reverse_disp          = 0;\n");
+	IEP_DBG("sw_u_reverse_disp          = 0;\n\n");
+#endif
+}
+
+#define IEP_RESET_TIMEOUT   1000
+void iep_soft_rst(void *base)
+{
+	unsigned int rst_state = 0;
+	int i = 0;
+	WriteReg32(base, rIEP_SOFT_RST, 2);
+	WriteReg32(base, rIEP_SOFT_RST, 1);
+	while (i++ < IEP_RESET_TIMEOUT) {
+		rst_state = ReadReg32(base, IEP_STATUS);
+		if ((rst_state & 0x200) == 0x200) {
+			break;
+		}
+
+		udelay(1);
+	}
+	WriteReg32(base, IEP_SOFT_RST, 2);
+
+	if (i == IEP_RESET_TIMEOUT)
+		IEP_DBG("soft reset timeout.\n");
+}
+
+void iep_config_done(void *base)
+{
+	WriteReg32(base, rIEP_CONF_DONE, 1);
+}
+
+void iep_config_frm_start(void *base)
+{
+	IEP_REGB_FRM_START(base, 1);
+}
+
+struct iep_status iep_get_status(void *base)
+{
+	uint32_t sts_int = IEP_REGB_STATUS(base);
+	struct iep_status sts;
+
+	memcpy(&sts, &sts_int, 4);
+
+	return sts;
+}
+
+int iep_get_deinterlace_mode(void *base)
+{
+	int cfg = ReadReg32(base, IEP_CONFIG0);
+	return (cfg >> 8) & 0x7;
+}
+
+void iep_set_deinterlace_mode(int mode, void *base)
+{
+	int cfg;
+
+	if (mode > dein_mode_bypass) {
+		IEP_ERR("invalid deinterlace mode\n");
+		return;
+	}
+
+	cfg = ReadReg32(base, RAW_IEP_CONFIG0);
+	cfg = (cfg & (~(7 << 8))) | (mode << 8);
+	WriteReg32(base, IEP_CONFIG0, cfg);
+
+	//IEP_REGB_DIL_MODE(base, mode);
+}
+
+void iep_switch_input_address(void *base)
+{
+	u32 src_addr_yrgb  = ReadReg32(base, IEP_SRC_ADDR_YRGB);
+	u32 src_addr_cbcr  = ReadReg32(base, IEP_SRC_ADDR_CBCR);
+	u32 src_addr_cr    = ReadReg32(base, IEP_SRC_ADDR_CR);
+
+	u32 src_addr_y1    = ReadReg32(base, IEP_SRC_ADDR_Y1);
+	u32 src_addr_cbcr1 = ReadReg32(base, IEP_SRC_ADDR_CBCR1);
+	u32 src_addr_cr1   = ReadReg32(base, IEP_SRC_ADDR_CR1);
+
+	IEP_REGB_SRC_ADDR_YRGB(base, src_addr_y1);
+	IEP_REGB_SRC_ADDR_CBCR(base, src_addr_cbcr1);
+	IEP_REGB_SRC_ADDR_CR(base, src_addr_cr1);
+	IEP_REGB_SRC_ADDR_Y1(base, src_addr_yrgb);
+	IEP_REGB_SRC_ADDR_CBCR1(base, src_addr_cbcr);
+	IEP_REGB_SRC_ADDR_CR1(base, src_addr_cr);
+}
+
+static int iep_bufid_to_iova(iep_service_info *pservice, u8 *tbl,
+	int size, struct iep_reg *reg)
+{
+	int i;
+	int usr_fd = 0;
+	int offset = 0;
+
+	if (tbl == NULL || size <= 0) {
+		dev_err(pservice->iommu_dev, "input arguments invalidate\n");
+		return -1;
+	}
+
+	for (i = 0; i < size; i++) {
+		usr_fd = reg->reg[tbl[i]] & 0x3FF;
+		offset = reg->reg[tbl[i]] >> 10;
+		if (usr_fd != 0) {
+			int hdl;
+			int ret;
+			struct iep_mem_region *mem_region;
+
+			hdl = iep_iommu_import(pservice->iommu_info,
+					       reg->session, usr_fd);
+
+			mem_region = kzalloc(sizeof(struct iep_mem_region),
+				GFP_KERNEL);
+
+			if (mem_region == NULL) {
+				dev_err(pservice->iommu_dev,
+					"allocate memory for"
+					" iommu memory region failed\n");
+				iep_iommu_free(pservice->iommu_info,
+					       reg->session, hdl);
+				return -ENOMEM;
+			}
+
+			mem_region->hdl = hdl;
+
+			ret = iep_iommu_map_iommu(pservice->iommu_info,
+				reg->session, mem_region->hdl,
+				&mem_region->iova, &mem_region->len);
+			if (ret < 0) {
+				dev_err(pservice->iommu_dev,
+					"ion map iommu failed\n");
+				kfree(mem_region);
+				iep_iommu_free(pservice->iommu_info,
+					       reg->session, hdl);
+				return ret;
+			}
+
+			reg->reg[tbl[i]] = mem_region->iova + offset;
+			INIT_LIST_HEAD(&mem_region->reg_lnk);
+			list_add_tail(&mem_region->reg_lnk,
+				&reg->mem_region_list);
+		}
+	}
+
+	return 0;
+}
+
+static u8 addr_tbl_iep[] = {
+	32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55
+};
+
+static int iep_reg_address_translate(iep_service_info *pservice, struct iep_reg *reg)
+{
+	return iep_bufid_to_iova(pservice, addr_tbl_iep, sizeof(addr_tbl_iep), reg);
+}
+
+/**
+ * generating a series of registers copy from iep message
+ */
+void iep_config(iep_session *session, struct IEP_MSG *iep_msg)
+{
+	struct iep_reg *reg = NULL;
+	int w;
+	int h;
+
+	reg = kzalloc(sizeof(*reg), GFP_KERNEL);
+	if (!reg)
+		return;
+	reg->session = session;
+	iep_msg->base = reg->reg;
+	atomic_set(&reg->session->done, 0);
+
+	INIT_LIST_HEAD(&reg->session_link);
+	INIT_LIST_HEAD(&reg->status_link);
+
+	INIT_LIST_HEAD(&reg->mem_region_list);
+
+	//write config
+	iep_config_src_size(iep_msg);
+	iep_config_dst_size(iep_msg);
+	iep_config_dst_width_tile(iep_msg); //not implement
+	iep_config_dst_fmt(iep_msg);
+	iep_config_src_fmt(iep_msg);
+	iep_config_scl(iep_msg);
+	iep_config_cg_order(iep_msg);
+
+	iep_config_cg(iep_msg);
+	iep_config_dde(iep_msg);            //not implement
+	iep_config_color_enh(iep_msg);      //not implement
+	iep_config_yuv_dns(iep_msg);
+	iep_config_dil(iep_msg);
+	iep_config_yuv_enh(iep_msg);
+	iep_config_rgb2yuv(iep_msg);
+	iep_config_yuv2rgb(iep_msg);
+	iep_config_dither_up(iep_msg);
+	iep_config_dither_down(iep_msg);
+	iep_config_glb_alpha(iep_msg);
+	iep_config_vir_line(iep_msg);
+	iep_config_src_addr(iep_msg);
+	iep_config_dst_addr(iep_msg);
+	iep_config_lcdc_path(iep_msg);
+	iep_config_misc(iep_msg);           //not implement
+
+	if (iep_msg->lcdc_path_en) {
+		reg->dpi_en     = true;
+		reg->act_width  = iep_msg->dst.act_w;
+		reg->act_height = iep_msg->dst.act_h;
+		reg->off_x      = iep_msg->off_x;
+		reg->off_y      = iep_msg->off_y;
+		reg->vir_width  = iep_msg->width;
+		reg->vir_height = iep_msg->height;
+		reg->layer      = iep_msg->layer;
+		reg->format     = iep_msg->dst.format;
+	} else {
+		reg->dpi_en     = false;
+	}
+
+	if (iep_service.iommu_dev) {
+		if (0 > iep_reg_address_translate(&iep_service, reg)) {
+			IEP_ERR("error: translate reg address failed\n");
+			kfree(reg);
+			return;
+		}
+	}
+
+	/* workaround for iommu enable case when 4k video input */
+	w = (iep_msg->src.act_w + 15) & (0xfffffff0);
+	h = (iep_msg->src.act_h + 15) & (0xfffffff0);
+	if (w > 1920 && iep_msg->src.format == IEP_FORMAT_YCbCr_420_SP)
+		reg->reg[33] = reg->reg[32] + w * h;
+
+	w = (iep_msg->dst.act_w + 15) & (0xfffffff0);
+	h = (iep_msg->dst.act_h + 15) & (0xfffffff0);
+	if (w > 1920 && iep_msg->dst.format == IEP_FORMAT_YCbCr_420_SP)
+		reg->reg[45] = reg->reg[44] + w * h;
+
+	mutex_lock(&iep_service.lock);
+
+	list_add_tail(&reg->status_link, &iep_service.waiting);
+	list_add_tail(&reg->session_link, &session->waiting);
+	mutex_unlock(&iep_service.lock);
+}
+
diff --git a/drivers/video/rockchip/iep/hw_iep_reg.h b/drivers/video/rockchip/iep/hw_iep_reg.h
new file mode 100644
index 0000000000000..03d9fe92ca231
--- /dev/null
+++ b/drivers/video/rockchip/iep/hw_iep_reg.h
@@ -0,0 +1,525 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef IEP_REGS_H
+#define IEP_REGS_H
+#include "hw_iep_config_addr.h"
+#include "iep.h"
+#include "iep_drv.h"
+
+struct iep_status {
+	uint32_t reserved0   : 1;
+	uint32_t scl_sts     : 1;
+	uint32_t dil_sts     : 1;
+	uint32_t reserved1   : 1;
+	uint32_t wyuv_sts    : 1;
+	uint32_t ryuv_sts    : 1;
+	uint32_t wrgb_sts    : 1;
+	uint32_t rrgb_sts    : 1;
+	uint32_t voi_sts     : 1;
+};
+
+#define      rIEP_CONFIG0      		         (IEP_BASE+IEP_CONFIG0)
+#define      rIEP_CONFIG1      		         (IEP_BASE+IEP_CONFIG1)
+
+#define      rIEP_STATUS              	     (IEP_BASE+IEP_STATUS)
+#define      rIEP_INT                 	     (IEP_BASE+IEP_INT)
+#define      rIEP_FRM_START         	     (IEP_BASE+IEP_FRM_START)
+#define      rIEP_SOFT_RST           	     (IEP_BASE+IEP_SOFT_RST)
+#define      rIEP_CONF_DONE                  (IEP_BASE+IEP_CONF_DONE)
+
+#define      rIEP_VIR_IMG_WIDTH        	     (IEP_BASE+IEP_VIR_IMG_WIDTH)
+
+#define      rIEP_IMG_SCL_FCT         	     (IEP_BASE+IEP_IMG_SCL_FCT)
+
+#define      rIEP_SRC_IMG_SIZE         	     (IEP_BASE+IEP_SRC_IMG_SIZE)
+#define      rIEP_DST_IMG_SIZE         	     (IEP_BASE+IEP_DST_IMG_SIZE)
+
+#define      rIEP_DST_IMG_WIDTH_TILE0  	     (IEP_BASE+IEP_DST_IMG_WIDTH_TILE0)
+#define      rIEP_DST_IMG_WIDTH_TILE1  	     (IEP_BASE+IEP_DST_IMG_WIDTH_TILE1)
+#define      rIEP_DST_IMG_WIDTH_TILE2  	     (IEP_BASE+IEP_DST_IMG_WIDTH_TILE2)
+#define      rIEP_DST_IMG_WIDTH_TILE3  	     (IEP_BASE+IEP_DST_IMG_WIDTH_TILE3)
+
+#define      rIEP_ENH_YUV_CNFG_0       	     (IEP_BASE+IEP_ENH_YUV_CNFG_0)
+#define      rIEP_ENH_YUV_CNFG_1       	     (IEP_BASE+IEP_ENH_YUV_CNFG_1)
+#define      rIEP_ENH_YUV_CNFG_2       	     (IEP_BASE+IEP_ENH_YUV_CNFG_2)
+#define      rIEP_ENH_RGB_CNFG        	     (IEP_BASE+IEP_ENH_RGB_CNFG)
+#define      rIEP_ENH_C_COE            	     (IEP_BASE+IEP_ENH_C_COE)
+
+#define      rIEP_SRC_ADDR_YRGB        	     (IEP_BASE+IEP_SRC_ADDR_YRGB)
+#define      rIEP_SRC_ADDR_CBCR              (IEP_BASE+IEP_SRC_ADDR_CBCR)
+#define      rIEP_SRC_ADDR_CR                (IEP_BASE+IEP_SRC_ADDR_CR)
+#define      rIEP_SRC_ADDR_Y1                (IEP_BASE+IEP_SRC_ADDR_Y1)
+#define      rIEP_SRC_ADDR_CBCR1             (IEP_BASE+IEP_SRC_ADDR_CBCR1)
+#define      rIEP_SRC_ADDR_CR1               (IEP_BASE+IEP_SRC_ADDR_CR1)
+#define      rIEP_SRC_ADDR_Y_ITEMP           (IEP_BASE+IEP_SRC_ADDR_Y_ITEMP)
+#define      rIEP_SRC_ADDR_CBCR_ITEMP        (IEP_BASE+IEP_SRC_ADDR_CBCR_ITEMP)
+#define      rIEP_SRC_ADDR_CR_ITEMP          (IEP_BASE+IEP_SRC_ADDR_CR_ITEMP)
+#define      rIEP_SRC_ADDR_Y_FTEMP           (IEP_BASE+IEP_SRC_ADDR_Y_FTEMP)
+#define      rIEP_SRC_ADDR_CBCR_FTEMP        (IEP_BASE+IEP_SRC_ADDR_CBCR_FTEMP)
+#define      rIEP_SRC_ADDR_CR_FTEMP          (IEP_BASE+IEP_SRC_ADDR_CR_FTEMP)
+
+#define      rIEP_DST_ADDR_YRGB        	     (IEP_BASE+IEP_DST_ADDR_YRGB)
+#define      rIEP_DST_ADDR_CBCR              (IEP_BASE+IEP_DST_ADDR_CBCR)
+#define      rIEP_DST_ADDR_CR                (IEP_BASE+IEP_DST_ADDR_CR)
+#define      rIEP_DST_ADDR_Y1                (IEP_BASE+IEP_DST_ADDR_Y1)
+#define      rIEP_DST_ADDR_CBCR1             (IEP_BASE+IEP_DST_ADDR_CBCR1)
+#define      rIEP_DST_ADDR_CR1               (IEP_BASE+IEP_DST_ADDR_CR1)
+#define      rIEP_DST_ADDR_Y_ITEMP           (IEP_BASE+IEP_DST_ADDR_Y_ITEMP)
+#define      rIEP_DST_ADDR_CBCR_ITEMP        (IEP_BASE+IEP_DST_ADDR_CBCR_ITEMP)
+#define      rIEP_DST_ADDR_CR_ITEMP          (IEP_BASE+IEP_DST_ADDR_CR_ITEMP)
+#define      rIEP_DST_ADDR_Y_FTEMP           (IEP_BASE+IEP_DST_ADDR_Y_FTEMP)
+#define      rIEP_DST_ADDR_CBCR_FTEMP        (IEP_BASE+IEP_DST_ADDR_CBCR_FTEMP)
+#define      rIEP_DST_ADDR_CR_FTEMP          (IEP_BASE+IEP_DST_ADDR_CR_FTEMP)
+
+#define      rIEP_DIL_MTN_TAB0               (IEP_BASE+IEP_DIL_MTN_TAB0)
+#define      rIEP_DIL_MTN_TAB1               (IEP_BASE+IEP_DIL_MTN_TAB1)
+#define      rIEP_DIL_MTN_TAB2               (IEP_BASE+IEP_DIL_MTN_TAB2)
+#define      rIEP_DIL_MTN_TAB3               (IEP_BASE+IEP_DIL_MTN_TAB3)
+#define      rIEP_DIL_MTN_TAB4               (IEP_BASE+IEP_DIL_MTN_TAB4)
+#define      rIEP_DIL_MTN_TAB5               (IEP_BASE+IEP_DIL_MTN_TAB5)
+#define      rIEP_DIL_MTN_TAB6               (IEP_BASE+IEP_DIL_MTN_TAB6)
+#define      rIEP_DIL_MTN_TAB7               (IEP_BASE+IEP_DIL_MTN_TAB7)
+
+#define      rIEP_ENH_CG_TAB                 (IEP_BASE+IEP_ENH_CG_TAB)
+
+#define      rIEP_YUV_DNS_CRCT_TEMP          (IEP_BASE+IEP_YUV_DNS_CRCT_TEMP)
+#define      rIEP_YUV_DNS_CRCT_SPAT          (IEP_BASE+IEP_YUV_DNS_CRCT_SPAT)
+
+#define      rIEP_ENH_DDE_COE0               (IEP_BASE+IEP_ENH_DDE_COE0)
+#define      rIEP_ENH_DDE_COE1               (IEP_BASE+IEP_ENH_DDE_COE1)
+
+#define      RAW_rIEP_CONFIG0                (IEP_BASE+RAW_IEP_CONFIG0)
+#define      RAW_rIEP_CONFIG1      		     (IEP_BASE+RAW_IEP_CONFIG1)
+#define      RAW_rIEP_VIR_IMG_WIDTH          (IEP_BASE+RAW_IEP_VIR_IMG_WIDTH)
+
+#define      RAW_rIEP_IMG_SCL_FCT      	     (IEP_BASE+RAW_IEP_IMG_SCL_FCT)
+
+#define      RAW_rIEP_SRC_IMG_SIZE      	 (IEP_BASE+RAW_IEP_SRC_IMG_SIZE)
+#define      RAW_rIEP_DST_IMG_SIZE      	 (IEP_BASE+RAW_IEP_DST_IMG_SIZE)
+
+#define      RAW_rIEP_ENH_YUV_CNFG_0         (IEP_BASE+RAW_IEP_ENH_YUV_CNFG_0)
+#define      RAW_rIEP_ENH_YUV_CNFG_1         (IEP_BASE+RAW_IEP_ENH_YUV_CNFG_1)
+#define      RAW_rIEP_ENH_YUV_CNFG_2         (IEP_BASE+RAW_IEP_ENH_YUV_CNFG_2)
+#define      RAW_rIEP_ENH_RGB_CNFG           (IEP_BASE+RAW_IEP_ENH_RGB_CNFG)
+
+#define      rIEP_CG_TAB_ADDR                 (IEP_BASE+0x0100)
+
+/*-----------------------------------------------------------------
+//reg bit operation definition
+-----------------------------------------------------------------*/
+/*iep_config0*/
+#define     IEP_REGB_V_REVERSE_DISP_Z(x)      (((x)&0x1 ) << 31 )
+#define     IEP_REGB_H_REVERSE_DISP_Z(x)      (((x)&0x1 ) << 30 )
+#define     IEP_REGB_SCL_EN_Z(x)              (((x)&0x1 ) << 28 )
+#define     IEP_REGB_SCL_SEL_Z(x)             (((x)&0x3 ) << 26 )
+#define     IEP_REGB_SCL_UP_COE_SEL_Z(x)      (((x)&0x3 ) << 24 )
+#define     IEP_REGB_DIL_EI_SEL_Z(x)          (((x)&0x1 ) << 23 )
+#define     IEP_REGB_DIL_EI_RADIUS_Z(x)       (((x)&0x3 ) << 21 )
+#define     IEP_REGB_CON_GAM_ORDER_Z(x)       (((x)&0x1 ) << 20 )
+#define     IEP_REGB_RGB_ENH_SEL_Z(x)         (((x)&0x3 ) << 18 )
+#define     IEP_REGB_RGB_CON_GAM_EN_Z(x)      (((x)&0x1 ) << 17 )
+#define     IEP_REGB_RGB_COLOR_ENH_EN_Z(x)    (((x)&0x1 ) << 16 )
+#define     IEP_REGB_DIL_EI_SMOOTH_Z(x)       (((x)&0x1 ) << 15 )
+#define     IEP_REGB_YUV_ENH_EN_Z(x)          (((x)&0x1 ) << 14 )
+#define     IEP_REGB_YUV_DNS_EN_Z(x)          (((x)&0x1 ) << 13 )
+#define     IEP_REGB_DIL_EI_MODE_Z(x)         (((x)&0x1 ) << 12 )
+#define     IEP_REGB_DIL_HF_EN_Z(x)           (((x)&0x1 ) << 11 )
+#define     IEP_REGB_DIL_MODE_Z(x)            (((x)&0x7 ) << 8  )
+#define     IEP_REGB_DIL_HF_FCT_Z(x)          (((x)&0x7F) << 1  )
+#define     IEP_REGB_LCDC_PATH_EN_Z(x)        (((x)&0x1 ) << 0  )
+
+/*iep_conig1*/
+#define     IEP_REGB_GLB_ALPHA_Z(x)           (((x)&0xff) << 24 )
+#define     IEP_REGB_RGB2YUV_INPUT_CLIP_Z(x)  (((x)&0x1 ) << 23 )
+#define     IEP_REGB_YUV2RGB_INPUT_CLIP_Z(x)  (((x)&0x1 ) << 22 )
+#define     IEP_REGB_RGB_TO_YUV_EN_Z(x)       (((x)&0x1 ) << 21 )
+#define     IEP_REGB_YUV_TO_RGB_EN_Z(x)       (((x)&0x1 ) << 20 )
+#define     IEP_REGB_RGB2YUV_COE_SEL_Z(x)     (((x)&0x3 ) << 18 )
+#define     IEP_REGB_YUV2RGB_COE_SEL_Z(x)     (((x)&0x3 ) << 16 )
+#define     IEP_REGB_DITHER_DOWN_EN_Z(x)      (((x)&0x1 ) << 15 )
+#define     IEP_REGB_DITHER_UP_EN_Z(x)        (((x)&0x1 ) << 14 )
+#define     IEP_REGB_DST_YUV_SWAP_Z(x)        (((x)&0x3 ) << 12 )
+#define     IEP_REGB_DST_RGB_SWAP_Z(x)        (((x)&0x3 ) << 10 )
+#define     IEP_REGB_DST_FMT_Z(x)             (((x)&0x3 ) << 8  )
+#define     IEP_REGB_SRC_YUV_SWAP_Z(x)        (((x)&0x3 ) << 4  )
+#define     IEP_REGB_SRC_RGB_SWAP_Z(x)        (((x)&0x3 ) << 2  )
+#define     IEP_REGB_SRC_FMT_Z(x)             (((x)&0x3 ) << 0  )
+
+/*iep_int*/
+#define     IEP_REGB_FRAME_END_INT_CLR_Z(x)   (((x)&0x1 ) << 16 )
+#define     IEP_REGB_FRAME_END_INT_EN_Z(x)    (((x)&0x1 ) << 8  )
+
+/*frm_start*/
+#define     IEP_REGB_FRM_START_Z(x)           (((x)&0x01 ) << 0 )
+
+/*soft_rst*/
+#define     IEP_REGB_SOFT_RST_Z(x)            (((x)&0x01 ) << 0 )
+
+/*iep_vir_img_width*/
+#define     IEP_REGB_DST_VIR_LINE_WIDTH_Z(x)  (((x)&0xffff) << 16 )
+#define     IEP_REGB_SRC_VIR_LINE_WIDTH_Z(x)  (((x)&0xffff) << 0  )
+
+/*iep_img_scl_fct*/
+#define     IEP_REGB_SCL_VRT_FCT_Z(x)         (((x)&0xffff) << 16 )
+#define     IEP_REGB_SCL_HRZ_FCT_Z(x)         (((x)&0xffff) << 0  )
+
+/*iep_src_img_size*/
+#define     IEP_REGB_SRC_IMG_HEIGHT_Z(x)      (((x)&0x1fff) << 16 )
+#define     IEP_REGB_SRC_IMG_WIDTH_Z(x)       (((x)&0x1fff) << 0  )
+/*iep_dst_img_size*/
+#define     IEP_REGB_DST_IMG_HEIGHT_Z(x)      (((x)&0x1fff) << 16 )
+#define     IEP_REGB_DST_IMG_WIDTH_Z(x)       (((x)&0x1fff) << 0  )
+
+/*dst_img_width_tile0/1/2/3*/
+#define     IEP_REGB_DST_IMG_WIDTH_TILE0_Z(x) (((x)&0x3ff ) << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE1_Z(x) (((x)&0x3ff ) << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE2_Z(x) (((x)&0x3ff ) << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE3_Z(x) (((x)&0x3ff ) << 0  )
+
+/*iep_enh_yuv_cnfg0*/
+#define     IEP_REGB_SAT_CON_Z(x)             (((x)&0x1ff ) << 16 )
+#define     IEP_REGB_CONTRAST_Z(x)            (((x)&0xff ) <<  8  )
+#define     IEP_REGB_BRIGHTNESS_Z(x)          (((x)&0x3f ) <<  0  )
+/*iep_enh_yuv_cnfg1*/
+#define     IEP_REGB_COS_HUE_Z(x)             (((x)&0xff ) <<  8  )
+#define     IEP_REGB_SIN_HUE_Z(x)             (((x)&0xff ) <<  0  )
+/*iep_enh_yuv_cnfg2*/
+#define     IEP_REGB_VIDEO_MODE_Z(x)          (((x)&0x3  ) <<  24 )
+#define     IEP_REGB_COLOR_BAR_V_Z(x)         (((x)&0xff ) <<  16 )
+#define     IEP_REGB_COLOR_BAR_U_Z(x)         (((x)&0xff ) <<  8  )
+#define     IEP_REGB_COLOR_BAR_Y_Z(x)         (((x)&0xff ) <<  0  )
+/*iep_enh_rgb_cnfg*/
+#define     IEP_REGB_YUV_DNS_LUMA_SPAT_SEL_Z(x)   (((x)&0x3  ) <<  30 )
+#define     IEP_REGB_YUV_DNS_LUMA_TEMP_SEL_Z(x)   (((x)&0x3  ) <<  28 )
+#define     IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL_Z(x) (((x)&0x3  ) <<  26 )
+#define     IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL_Z(x) (((x)&0x3  ) <<  24 )
+#define     IEP_REGB_ENH_THRESHOLD_Z(x)       (((x)&0xff ) <<  16 )
+#define     IEP_REGB_ENH_ALPHA_Z(x)           (((x)&0x3f ) <<  8  )
+#define     IEP_REGB_ENH_RADIUS_Z(x)          (((x)&0x3  ) <<  0  )
+/*iep_enh_c_coe*/
+#define     IEP_REGB_ENH_C_COE_Z(x)           (((x)&0x7f ) <<  0  )
+/*dil_mtn_tab*/
+#define     IEP_REGB_DIL_MTN_TAB0_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB0_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB0_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB0_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB1_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB1_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB1_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB1_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB2_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB2_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB2_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB2_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB3_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB3_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB3_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB3_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB4_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB4_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB4_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB4_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB5_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB5_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB5_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB5_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB6_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB6_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB6_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB6_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB7_0_Z(x)      (((x)&0x7f ) <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB7_1_Z(x)      (((x)&0x7f ) <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB7_2_Z(x)      (((x)&0x7f ) <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB7_3_Z(x)      (((x)&0x7f ) <<  24 )
+
+/*iep_config0*/
+#define     IEP_REGB_V_REVERSE_DISP_Y      (0x1  << 31 )
+#define     IEP_REGB_H_REVERSE_DISP_Y      (0x1  << 30 )
+#define     IEP_REGB_SCL_EN_Y              (0x1  << 28 )
+#define     IEP_REGB_SCL_SEL_Y             (0x3  << 26 )
+#define     IEP_REGB_SCL_UP_COE_SEL_Y      (0x3  << 24 )
+#define     IEP_REGB_DIL_EI_SEL_Y          (0x1  << 23 )
+#define     IEP_REGB_DIL_EI_RADIUS_Y       (0x3  << 21 )
+#define     IEP_REGB_CON_GAM_ORDER_Y       (0x1  << 20 )
+#define     IEP_REGB_RGB_ENH_SEL_Y         (0x3  << 18 )
+#define     IEP_REGB_RGB_CON_GAM_EN_Y      (0x1  << 17 )
+#define     IEP_REGB_RGB_COLOR_ENH_EN_Y    (0x1  << 16 )
+#define     IEP_REGB_DIL_EI_SMOOTH_Y       (0x1  << 15 )
+#define     IEP_REGB_YUV_ENH_EN_Y          (0x1  << 14 )
+#define     IEP_REGB_YUV_DNS_EN_Y          (0x1  << 13 )
+#define     IEP_REGB_DIL_EI_MODE_Y         (0x1  << 12 )
+#define     IEP_REGB_DIL_HF_EN_Y           (0x1  << 11 )
+#define     IEP_REGB_DIL_MODE_Y            (0x7  << 8  )
+#define     IEP_REGB_DIL_HF_FCT_Y          (0x7F << 1  )
+#define     IEP_REGB_LCDC_PATH_EN_Y        (0x1  << 0  )
+
+/*iep_conig1*/
+#define     IEP_REGB_GLB_ALPHA_Y           (0xff << 24 )
+#define     IEP_REGB_RGB2YUV_INPUT_CLIP_Y  (0x1  << 23 )
+#define     IEP_REGB_YUV2RGB_INPUT_CLIP_Y  (0x1  << 22 )
+#define     IEP_REGB_RGB_TO_YUV_EN_Y       (0x1  << 21 )
+#define     IEP_REGB_YUV_TO_RGB_EN_Y       (0x1  << 20 )
+#define     IEP_REGB_RGB2YUV_COE_SEL_Y     (0x3  << 18 )
+#define     IEP_REGB_YUV2RGB_COE_SEL_Y     (0x3  << 16 )
+#define     IEP_REGB_DITHER_DOWN_EN_Y      (0x1  << 15 )
+#define     IEP_REGB_DITHER_UP_EN_Y        (0x1  << 14 )
+#define     IEP_REGB_DST_YUV_SWAP_Y        (0x3  << 12 )
+#define     IEP_REGB_DST_RGB_SWAP_Y        (0x3  << 10 )
+#define     IEP_REGB_DST_FMT_Y             (0x3  << 8  )
+#define     IEP_REGB_SRC_YUV_SWAP_Y        (0x3  << 4  )
+#define     IEP_REGB_SRC_RGB_SWAP_Y        (0x3  << 2  )
+#define     IEP_REGB_SRC_FMT_Y             (0x3  << 0  )
+
+/*iep_int*/
+#define     IEP_REGB_FRAME_END_INT_CLR_Y   (0x1  << 16 )
+#define     IEP_REGB_FRAME_END_INT_EN_Y    (0x1  << 8  )
+
+/*frm_start*/
+#define     IEP_REGB_FRM_START_Y           (0x1  << 0  )
+
+/*soft_rst*/
+#define     IEP_REGB_SOFT_RST_Y            (0x1  << 0  )
+
+/*iep_vir_img_width*/
+#define     IEP_REGB_DST_VIR_LINE_WIDTH_Y  (0xffff << 16 )
+#define     IEP_REGB_SRC_VIR_LINE_WIDTH_Y  (0xffff << 0  )
+
+/*iep_img_scl_fct*/
+#define     IEP_REGB_SCL_VRT_FCT_Y         (0xffff << 16 )
+#define     IEP_REGB_SCL_HRZ_FCT_Y         (0xffff << 0  )
+
+/*iep_src_img_size*/
+#define     IEP_REGB_SRC_IMG_HEIGHT_Y      (0x1fff << 16 )
+#define     IEP_REGB_SRC_IMG_WIDTH_Y       (0x1fff << 0  )
+/*iep_dst_img_size*/
+#define     IEP_REGB_DST_IMG_HEIGHT_Y      (0x1fff << 16 )
+#define     IEP_REGB_DST_IMG_WIDTH_Y       (0x1fff << 0  )
+
+/*dst_img_width_tile0/1/2/3*/
+#define     IEP_REGB_DST_IMG_WIDTH_TILE0_Y (0x3ff  << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE1_Y (0x3ff  << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE2_Y (0x3ff  << 0  )
+#define     IEP_REGB_DST_IMG_WIDTH_TILE3_Y (0x3ff  << 0  )
+
+/*iep_enh_yuv_cnfg0*/
+#define     IEP_REGB_SAT_CON_Y             (0x1ff  <<  16)
+#define     IEP_REGB_CONTRAST_Y            (0xff  <<  8 )
+#define     IEP_REGB_BRIGHTNESS_Y          (0x3f  <<  0 )
+/*iep_enh_yuv_cnfg1*/
+#define     IEP_REGB_COS_HUE_Y             (0xff  <<  8 )
+#define     IEP_REGB_SIN_HUE_Y             (0xff  <<  0 )
+/*iep_enh_yuv_cnfg2*/
+#define     IEP_REGB_VIDEO_MODE_Y          (0x3   <<  24)
+#define     IEP_REGB_COLOR_BAR_V_Y         (0xff  <<  16)
+#define     IEP_REGB_COLOR_BAR_U_Y         (0xff  <<  8 )
+#define     IEP_REGB_COLOR_BAR_Y_Y         (0xff  <<  0 )
+/*iep_enh_rgb_cnfg*/
+#define     IEP_REGB_YUV_DNS_LUMA_SPAT_SEL_Y (0x3   <<  30)
+#define     IEP_REGB_YUV_DNS_LUMA_TEMP_SEL_Y (0x3   <<  28)
+#define     IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL_Y (0x3  <<  26)
+#define     IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL_Y (0x3  <<  24)
+#define     IEP_REGB_ENH_THRESHOLD_Y       (0xff  <<  16)
+#define     IEP_REGB_ENH_ALPHA_Y           (0x3f  <<  8 )
+#define     IEP_REGB_ENH_RADIUS_Y          (0x3   <<  0 )
+/*iep_enh_c_coe*/
+#define     IEP_REGB_ENH_C_COE_Y           (0x7f  <<  0 )
+/*dil_mtn_tab*/
+#define     IEP_REGB_DIL_MTN_TAB0_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB0_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB0_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB0_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB1_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB1_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB1_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB1_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB2_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB2_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB2_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB2_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB3_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB3_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB3_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB3_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB4_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB4_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB4_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB4_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB5_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB5_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB5_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB5_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB6_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB6_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB6_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB6_3_Y      (0x7f  <<  24 )
+
+#define     IEP_REGB_DIL_MTN_TAB7_0_Y      (0x7f  <<  0  )
+#define     IEP_REGB_DIL_MTN_TAB7_1_Y      (0x7f  <<  8  )
+#define     IEP_REGB_DIL_MTN_TAB7_2_Y      (0x7f  <<  16 )
+#define     IEP_REGB_DIL_MTN_TAB7_3_Y      (0x7f  <<  24 )
+
+/*-----------------------------------------------------------------
+MaskRegBits32(addr, y, z),Register configure
+-----------------------------------------------------------------*/
+/*iep_config0*/
+#define     IEP_REGB_V_REVERSE_DISP(base, x)      ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_V_REVERSE_DISP_Y,IEP_REGB_V_REVERSE_DISP_Z(x))
+#define     IEP_REGB_H_REVERSE_DISP(base, x)      ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_H_REVERSE_DISP_Y,IEP_REGB_H_REVERSE_DISP_Z(x))
+#define     IEP_REGB_SCL_EN(base, x)              ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_SCL_EN_Y,IEP_REGB_SCL_EN_Z(x))
+#define     IEP_REGB_SCL_SEL(base, x)             ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_SCL_SEL_Y,IEP_REGB_SCL_SEL_Z(x))
+#define     IEP_REGB_SCL_UP_COE_SEL(base, x)      ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_SCL_UP_COE_SEL_Y,IEP_REGB_SCL_UP_COE_SEL_Z(x))
+#define     IEP_REGB_DIL_EI_SEL(base, x)          ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_EI_SEL_Y,IEP_REGB_DIL_EI_SEL_Z(x))
+#define     IEP_REGB_DIL_EI_RADIUS(base, x)       ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_EI_RADIUS_Y,IEP_REGB_DIL_EI_RADIUS_Z(x))
+#define     IEP_REGB_CON_GAM_ORDER(base, x)       ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_CON_GAM_ORDER_Y,IEP_REGB_CON_GAM_ORDER_Z(x))
+#define     IEP_REGB_RGB_ENH_SEL(base, x)         ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_RGB_ENH_SEL_Y,IEP_REGB_RGB_ENH_SEL_Z(x))
+#define     IEP_REGB_RGB_CON_GAM_EN(base, x)      ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_RGB_CON_GAM_EN_Y,IEP_REGB_RGB_CON_GAM_EN_Z(x))
+#define     IEP_REGB_RGB_COLOR_ENH_EN(base, x)    ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_RGB_COLOR_ENH_EN_Y,IEP_REGB_RGB_COLOR_ENH_EN_Z(x))
+#define     IEP_REGB_DIL_EI_SMOOTH(base, x)       ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_EI_SMOOTH_Y,IEP_REGB_DIL_EI_SMOOTH_Z(x))
+#define     IEP_REGB_YUV_ENH_EN(base, x)          ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_YUV_ENH_EN_Y,IEP_REGB_YUV_ENH_EN_Z(x))
+#define     IEP_REGB_YUV_DNS_EN(base, x)          ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_YUV_DNS_EN_Y,IEP_REGB_YUV_DNS_EN_Z(x))
+#define     IEP_REGB_DIL_EI_MODE(base, x)         ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_EI_MODE_Y,IEP_REGB_DIL_EI_MODE_Z(x))
+#define     IEP_REGB_DIL_HF_EN(base, x)           ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_HF_EN_Y,IEP_REGB_DIL_HF_EN_Z(x))
+#define     IEP_REGB_DIL_MODE(base, x)            ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_MODE_Y,IEP_REGB_DIL_MODE_Z(x))
+#define     IEP_REGB_DIL_HF_FCT(base, x)          ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_DIL_HF_FCT_Y,IEP_REGB_DIL_HF_FCT_Z(x))
+#define     IEP_REGB_LCDC_PATH_EN(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG0,rIEP_CONFIG0,IEP_REGB_LCDC_PATH_EN_Y,IEP_REGB_LCDC_PATH_EN_Z(x))
+
+/*iep_conig1*/
+#define     IEP_REGB_GLB_ALPHA(base, x)           ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_GLB_ALPHA_Y,IEP_REGB_GLB_ALPHA_Z(x))
+#define     IEP_REGB_RGB2YUV_INPUT_CLIP(base, x)  ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_RGB2YUV_INPUT_CLIP_Y,IEP_REGB_RGB2YUV_INPUT_CLIP_Z(x))
+#define     IEP_REGB_YUV2RGB_INPUT_CLIP(base, x)  ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_YUV2RGB_INPUT_CLIP_Y,IEP_REGB_YUV2RGB_INPUT_CLIP_Z(x))
+#define     IEP_REGB_RGB_TO_YUV_EN(base, x)       ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_RGB_TO_YUV_EN_Y,IEP_REGB_RGB_TO_YUV_EN_Z(x))
+#define     IEP_REGB_YUV_TO_RGB_EN(base, x)       ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_YUV_TO_RGB_EN_Y,IEP_REGB_YUV_TO_RGB_EN_Z(x))
+#define     IEP_REGB_RGB2YUV_COE_SEL(base, x)     ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_RGB2YUV_COE_SEL_Y,IEP_REGB_RGB2YUV_COE_SEL_Z(x))
+#define     IEP_REGB_YUV2RGB_COE_SEL(base, x)     ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_YUV2RGB_COE_SEL_Y,IEP_REGB_YUV2RGB_COE_SEL_Z(x))
+#define     IEP_REGB_DITHER_DOWN_EN(base, x)      ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_DITHER_DOWN_EN_Y,IEP_REGB_DITHER_DOWN_EN_Z(x))
+#define     IEP_REGB_DITHER_UP_EN(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_DITHER_UP_EN_Y,IEP_REGB_DITHER_UP_EN_Z(x))
+#define     IEP_REGB_DST_YUV_SWAP(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_DST_YUV_SWAP_Y,IEP_REGB_DST_YUV_SWAP_Z(x))
+#define     IEP_REGB_DST_RGB_SWAP(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_DST_RGB_SWAP_Y,IEP_REGB_DST_RGB_SWAP_Z(x))
+#define     IEP_REGB_DST_FMT(base, x)             ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_DST_FMT_Y,IEP_REGB_DST_FMT_Z(x))
+#define     IEP_REGB_SRC_YUV_SWAP(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_SRC_YUV_SWAP_Y,IEP_REGB_SRC_YUV_SWAP_Z(x))
+#define     IEP_REGB_SRC_RGB_SWAP(base, x)        ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_SRC_RGB_SWAP_Y,IEP_REGB_SRC_RGB_SWAP_Z(x))
+#define     IEP_REGB_SRC_FMT(base, x)             ConfRegBits32(base, RAW_rIEP_CONFIG1,rIEP_CONFIG1,IEP_REGB_SRC_FMT_Y,IEP_REGB_SRC_FMT_Z(x))
+
+/*iep_int*/
+#define     IEP_REGB_FRAME_END_INT_CLR(base, x)   MaskRegBits32(base, rIEP_INT,IEP_REGB_FRAME_END_INT_CLR_Y,IEP_REGB_FRAME_END_INT_CLR_Z(x))
+#define     IEP_REGB_FRAME_END_INT_EN(base, x)    MaskRegBits32(base, rIEP_INT,IEP_REGB_FRAME_END_INT_EN_Y,IEP_REGB_FRAME_END_INT_EN_Z(x))
+
+/*frm_start*/
+#define     IEP_REGB_FRM_START(base, x)           WriteReg32(base, rIEP_FRM_START,x)
+
+/*soft_rst*/
+#define     IEP_REGB_SOFT_RST(base, x)            WriteReg32(base, rIEP_SOFT_RST,x)
+
+/*iep_vir_img_width*/
+#define     IEP_REGB_DST_VIR_LINE_WIDTH(base, x)  ConfRegBits32(base, RAW_rIEP_VIR_IMG_WIDTH,rIEP_VIR_IMG_WIDTH,IEP_REGB_DST_VIR_LINE_WIDTH_Y,IEP_REGB_DST_VIR_LINE_WIDTH_Z(x))
+#define     IEP_REGB_SRC_VIR_LINE_WIDTH(base, x)  ConfRegBits32(base, RAW_rIEP_VIR_IMG_WIDTH,rIEP_VIR_IMG_WIDTH,IEP_REGB_SRC_VIR_LINE_WIDTH_Y,IEP_REGB_SRC_VIR_LINE_WIDTH_Z(x))
+
+/*iep_img_scl_fct*/
+#define     IEP_REGB_SCL_VRT_FCT(base, x)         ConfRegBits32(base, RAW_rIEP_IMG_SCL_FCT,rIEP_IMG_SCL_FCT,IEP_REGB_SCL_VRT_FCT_Y,IEP_REGB_SCL_VRT_FCT_Z(x))
+#define     IEP_REGB_SCL_HRZ_FCT(base, x)         ConfRegBits32(base, RAW_rIEP_IMG_SCL_FCT,rIEP_IMG_SCL_FCT,IEP_REGB_SCL_HRZ_FCT_Y,IEP_REGB_SCL_HRZ_FCT_Z(x))
+
+/*iep_src_img_size*/
+#define     IEP_REGB_SRC_IMG_HEIGHT(base, x)      ConfRegBits32(base, RAW_rIEP_SRC_IMG_SIZE,rIEP_SRC_IMG_SIZE,IEP_REGB_SRC_IMG_HEIGHT_Y,IEP_REGB_SRC_IMG_HEIGHT_Z(x))
+#define     IEP_REGB_SRC_IMG_WIDTH(base, x)       ConfRegBits32(base, RAW_rIEP_SRC_IMG_SIZE,rIEP_SRC_IMG_SIZE,IEP_REGB_SRC_IMG_WIDTH_Y,IEP_REGB_SRC_IMG_WIDTH_Z(x))
+//iep_dst_img_size
+#define     IEP_REGB_DST_IMG_HEIGHT(base, x)      ConfRegBits32(base, RAW_rIEP_DST_IMG_SIZE,rIEP_DST_IMG_SIZE,IEP_REGB_DST_IMG_HEIGHT_Y,IEP_REGB_DST_IMG_HEIGHT_Z(x))
+#define     IEP_REGB_DST_IMG_WIDTH(base, x)       ConfRegBits32(base, RAW_rIEP_DST_IMG_SIZE,rIEP_DST_IMG_SIZE,IEP_REGB_DST_IMG_WIDTH_Y,IEP_REGB_DST_IMG_WIDTH_Z(x))
+
+/*dst_img_width_tile0/1/2/3*/
+#define     IEP_REGB_DST_IMG_WIDTH_TILE0(base, x) WriteReg32(base, rIEP_DST_IMG_WIDTH_TILE0,x)
+#define     IEP_REGB_DST_IMG_WIDTH_TILE1(base, x) WriteReg32(base, rIEP_DST_IMG_WIDTH_TILE1,x)
+#define     IEP_REGB_DST_IMG_WIDTH_TILE2(base, x) WriteReg32(base, rIEP_DST_IMG_WIDTH_TILE2,x)
+#define     IEP_REGB_DST_IMG_WIDTH_TILE3(base, x) WriteReg32(base, rIEP_DST_IMG_WIDTH_TILE3,x)
+
+/*iep_enh_yuv_cnfg0*/
+#define     IEP_REGB_SAT_CON(base, x)             ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_0,rIEP_ENH_YUV_CNFG_0,IEP_REGB_SAT_CON_Y,IEP_REGB_SAT_CON_Z(x))
+#define     IEP_REGB_CONTRAST(base, x)            ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_0,rIEP_ENH_YUV_CNFG_0,IEP_REGB_CONTRAST_Y,IEP_REGB_CONTRAST_Z(x))
+#define     IEP_REGB_BRIGHTNESS(base, x)          ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_0,rIEP_ENH_YUV_CNFG_0,IEP_REGB_BRIGHTNESS_Y,IEP_REGB_BRIGHTNESS_Z(x))
+/*iep_enh_yuv_cnfg1*/
+#define     IEP_REGB_COS_HUE(base, x)             ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_1,rIEP_ENH_YUV_CNFG_1,IEP_REGB_COS_HUE_Y,IEP_REGB_COS_HUE_Z(x))
+#define     IEP_REGB_SIN_HUE(base, x)             ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_1,rIEP_ENH_YUV_CNFG_1,IEP_REGB_SIN_HUE_Y,IEP_REGB_SIN_HUE_Z(x))
+/*iep_enh_yuv_cnfg2*/
+#define     IEP_REGB_VIDEO_MODE(base, x)          ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_2,rIEP_ENH_YUV_CNFG_2,IEP_REGB_VIDEO_MODE_Y,IEP_REGB_VIDEO_MODE_Z(x))
+#define     IEP_REGB_COLOR_BAR_V(base, x)         ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_2,rIEP_ENH_YUV_CNFG_2,IEP_REGB_COLOR_BAR_V_Y,IEP_REGB_COLOR_BAR_V_Z(x))
+#define     IEP_REGB_COLOR_BAR_U(base, x)         ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_2,rIEP_ENH_YUV_CNFG_2,IEP_REGB_COLOR_BAR_U_Y,IEP_REGB_COLOR_BAR_U_Z(x))
+#define     IEP_REGB_COLOR_BAR_Y(base, x)         ConfRegBits32(base, RAW_rIEP_ENH_YUV_CNFG_2,rIEP_ENH_YUV_CNFG_2,IEP_REGB_COLOR_BAR_Y_Y,IEP_REGB_COLOR_BAR_Y_Z(x))
+/*iep_enh_rgb_cnfg*/
+#define     IEP_REGB_YUV_DNS_LUMA_SPAT_SEL(base, x) ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_YUV_DNS_LUMA_SPAT_SEL_Y,IEP_REGB_YUV_DNS_LUMA_SPAT_SEL_Z(x))
+#define     IEP_REGB_YUV_DNS_LUMA_TEMP_SEL(base, x) ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_YUV_DNS_LUMA_TEMP_SEL_Y,IEP_REGB_YUV_DNS_LUMA_TEMP_SEL_Z(x))
+#define     IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL(base, x) ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL_Y,IEP_REGB_YUV_DNS_CHROMA_SPAT_SEL_Z(x))
+#define     IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL(base, x) ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL_Y,IEP_REGB_YUV_DNS_CHROMA_TEMP_SEL_Z(x))
+#define     IEP_REGB_ENH_THRESHOLD(base, x)       ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_ENH_THRESHOLD_Y,IEP_REGB_ENH_THRESHOLD_Z(x))
+#define     IEP_REGB_ENH_ALPHA(base, x)           ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_ENH_ALPHA_Y,IEP_REGB_ENH_ALPHA_Z(x))
+#define     IEP_REGB_ENH_RADIUS(base, x)          ConfRegBits32(base, RAW_rIEP_ENH_RGB_CNFG,rIEP_ENH_RGB_CNFG,IEP_REGB_ENH_RADIUS_Y,IEP_REGB_ENH_RADIUS_Z(x))
+/*iep_enh_c_coe*/
+#define     IEP_REGB_ENH_C_COE(base, x)           WriteReg32(base, rIEP_ENH_C_COE,x)
+/*src_addr*/
+#define     IEP_REGB_SRC_ADDR_YRGB(base, x)       WriteReg32(base, rIEP_SRC_ADDR_YRGB, x)
+#define     IEP_REGB_SRC_ADDR_CBCR(base, x)       WriteReg32(base, rIEP_SRC_ADDR_CBCR, x)
+#define     IEP_REGB_SRC_ADDR_CR(base, x)         WriteReg32(base, rIEP_SRC_ADDR_CR, x)
+#define     IEP_REGB_SRC_ADDR_Y1(base, x)         WriteReg32(base, rIEP_SRC_ADDR_Y1, x)
+#define     IEP_REGB_SRC_ADDR_CBCR1(base, x)      WriteReg32(base, rIEP_SRC_ADDR_CBCR1, x)
+#define     IEP_REGB_SRC_ADDR_CR1(base, x)        WriteReg32(base, rIEP_SRC_ADDR_CR1, x)
+#define     IEP_REGB_SRC_ADDR_Y_ITEMP(base, x)    WriteReg32(base, rIEP_SRC_ADDR_Y_ITEMP, x)
+#define     IEP_REGB_SRC_ADDR_CBCR_ITEMP(base, x) WriteReg32(base, rIEP_SRC_ADDR_CBCR_ITEMP, x)
+#define     IEP_REGB_SRC_ADDR_CR_ITEMP(base, x)   WriteReg32(base, rIEP_SRC_ADDR_CR_ITEMP, x)
+#define     IEP_REGB_SRC_ADDR_Y_FTEMP(base, x)    WriteReg32(base, rIEP_SRC_ADDR_Y_FTEMP, x)
+#define     IEP_REGB_SRC_ADDR_CBCR_FTEMP(base, x) WriteReg32(base, rIEP_SRC_ADDR_CBCR_FTEMP, x)
+#define     IEP_REGB_SRC_ADDR_CR_FTEMP(base, x)   WriteReg32(base, rIEP_SRC_ADDR_CR_FTEMP, x)
+/*dst_addr*/
+#define     IEP_REGB_DST_ADDR_YRGB(base, x)       WriteReg32(base, rIEP_DST_ADDR_YRGB,x)
+#define     IEP_REGB_DST_ADDR_CBCR(base, x)       WriteReg32(base, rIEP_DST_ADDR_CBCR, x)
+#define     IEP_REGB_DST_ADDR_CR(base, x)         WriteReg32(base, rIEP_DST_ADDR_CR, x)
+#define     IEP_REGB_DST_ADDR_Y1(base, x)         WriteReg32(base, rIEP_DST_ADDR_Y1, x)
+#define     IEP_REGB_DST_ADDR_CBCR1(base, x)      WriteReg32(base, rIEP_DST_ADDR_CBCR1, x)
+#define     IEP_REGB_DST_ADDR_CR1(base, x)        WriteReg32(base, rIEP_DST_ADDR_CR1, x)
+#define     IEP_REGB_DST_ADDR_Y_ITEMP(base, x)    WriteReg32(base, rIEP_DST_ADDR_Y_ITEMP, x)
+#define     IEP_REGB_DST_ADDR_CBCR_ITEMP(base, x) WriteReg32(base, rIEP_DST_ADDR_CBCR_ITEMP, x)
+#define     IEP_REGB_DST_ADDR_CR_ITEMP(base, x)   WriteReg32(base, rIEP_DST_ADDR_CR_ITEMP, x)
+#define     IEP_REGB_DST_ADDR_Y_FTEMP(base, x)    WriteReg32(base, rIEP_DST_ADDR_Y_FTEMP, x)
+#define     IEP_REGB_DST_ADDR_CBCR_FTEMP(base, x) WriteReg32(base, rIEP_DST_ADDR_CBCR_FTEMP, x)
+#define     IEP_REGB_DST_ADDR_CR_FTEMP(base, x)   WriteReg32(base, rIEP_DST_ADDR_CR_FTEMP, x)
+
+/*dil_mtn_tab*/
+#define     IEP_REGB_DIL_MTN_TAB0(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB0,x)
+#define     IEP_REGB_DIL_MTN_TAB1(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB1,x)
+#define     IEP_REGB_DIL_MTN_TAB2(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB2,x)
+#define     IEP_REGB_DIL_MTN_TAB3(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB3,x)
+#define     IEP_REGB_DIL_MTN_TAB4(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB4,x)
+#define     IEP_REGB_DIL_MTN_TAB5(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB5,x)
+#define     IEP_REGB_DIL_MTN_TAB6(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB6,x)
+#define     IEP_REGB_DIL_MTN_TAB7(base, x)      WriteReg32(base, rIEP_DIL_MTN_TAB7,x)
+
+#define     IEP_REGB_STATUS(base)               ReadReg32(base, rIEP_STATUS)
+
+void iep_config_lcdc_path(struct IEP_MSG *iep_msg);
+
+/* system control, directly operating the device registers.*/
+/* parameter @base need to be set to device base address. */
+void iep_soft_rst(void *base);
+void iep_config_done(void *base);
+void iep_config_frm_start(void *base);
+int iep_probe_int(void *base);
+void iep_config_frame_end_int_clr(void *base);
+void iep_config_frame_end_int_en(void *base);
+struct iep_status iep_get_status(void *base);
+int iep_get_deinterlace_mode(void *base);
+void iep_set_deinterlace_mode(int mode, void *base);
+void iep_switch_input_address(void *base);
+
+/* generating a series of iep registers copy to the session private buffer */
+void iep_config(iep_session *session, struct IEP_MSG *iep_msg);
+
+/*#define IEP_PRINT_INFO*/
+#endif
diff --git a/drivers/video/rockchip/iep/iep.h b/drivers/video/rockchip/iep/iep.h
new file mode 100644
index 0000000000000..94e372e6a471d
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep.h
@@ -0,0 +1,276 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _IEP_H_
+#define _IEP_H_
+
+/* Capability for current iep version
+using by userspace to determine iep features */
+struct IEP_CAP {
+	u8 scaling_supported;
+	u8 i4_deinterlace_supported;
+	u8 i2_deinterlace_supported;
+	u8 compression_noise_reduction_supported;
+	u8 sampling_noise_reduction_supported;
+	u8 hsb_enhancement_supported;
+	u8 cg_enhancement_supported;
+	u8 direct_path_supported;
+	u16 max_dynamic_width;
+	u16 max_dynamic_height;
+	u16 max_static_width;
+	u16 max_static_height;
+	u8 max_enhance_radius;
+};
+
+#define IEP_IOC_MAGIC 'i'
+
+#define IEP_SET_PARAMETER_REQ		_IOW(IEP_IOC_MAGIC, 1, unsigned long)
+#define IEP_SET_PARAMETER_DEINTERLACE	_IOW(IEP_IOC_MAGIC, 2, unsigned long)
+#define IEP_SET_PARAMETER_ENHANCE	_IOW(IEP_IOC_MAGIC, 3, unsigned long)
+#define IEP_SET_PARAMETER_CONVERT	_IOW(IEP_IOC_MAGIC, 4, unsigned long)
+#define IEP_SET_PARAMETER_SCALE		_IOW(IEP_IOC_MAGIC, 5, unsigned long)
+#define IEP_GET_RESULT_SYNC		_IOW(IEP_IOC_MAGIC, 6, unsigned long)
+#define IEP_GET_RESULT_ASYNC		_IOW(IEP_IOC_MAGIC, 7, unsigned long)
+#define IEP_SET_PARAMETER		_IOW(IEP_IOC_MAGIC, 8, unsigned long)
+#define IEP_RELEASE_CURRENT_TASK	_IOW(IEP_IOC_MAGIC, 9, unsigned long)
+#define IEP_GET_IOMMU_STATE		_IOR(IEP_IOC_MAGIC,10, unsigned long)
+#define IEP_QUERY_CAP			_IOR(IEP_IOC_MAGIC,11, struct IEP_CAP)
+
+#ifdef CONFIG_COMPAT
+#define COMPAT_IEP_SET_PARAMETER_REQ		_IOW(IEP_IOC_MAGIC, 1, u32)
+#define COMPAT_IEP_SET_PARAMETER_DEINTERLACE	_IOW(IEP_IOC_MAGIC, 2, u32)
+#define COMPAT_IEP_SET_PARAMETER_ENHANCE	_IOW(IEP_IOC_MAGIC, 3, u32)
+#define COMPAT_IEP_SET_PARAMETER_CONVERT	_IOW(IEP_IOC_MAGIC, 4, u32)
+#define COMPAT_IEP_SET_PARAMETER_SCALE		_IOW(IEP_IOC_MAGIC, 5, u32)
+#define COMPAT_IEP_GET_RESULT_SYNC		_IOW(IEP_IOC_MAGIC, 6, u32)
+#define COMPAT_IEP_GET_RESULT_ASYNC		_IOW(IEP_IOC_MAGIC, 7, u32)
+#define COMPAT_IEP_SET_PARAMETER		_IOW(IEP_IOC_MAGIC, 8, u32)
+#define COMPAT_IEP_RELEASE_CURRENT_TASK		_IOW(IEP_IOC_MAGIC, 9, u32)
+#define COMPAT_IEP_GET_IOMMU_STATE		_IOR(IEP_IOC_MAGIC,10, u32)
+#define COMPAT_IEP_QUERY_CAP			_IOR(IEP_IOC_MAGIC,11, struct IEP_CAP)
+#endif
+
+/* Driver information */
+#define DRIVER_DESC		"IEP Device Driver"
+#define DRIVER_NAME		"iep"
+
+#define DEBUG
+#ifdef DEBUG
+#define iep_debug(level, fmt, args...)				\
+	do {							\
+		if (debug >= level)				\
+			pr_info("%s:%d: " fmt,			\
+				 __func__, __LINE__, ##args);	\
+	} while (0)
+#else
+#define iep_debug(level, fmt, args...)
+#endif
+
+#define iep_debug_enter() vpu_debug(4, "enter\n")
+#define iep_debug_leave() vpu_debug(4, "leave\n")
+
+#define iep_err(fmt, args...)				\
+		pr_err("%s:%d: " fmt, __func__, __LINE__, ##args)
+
+/* Logging */
+#define IEP_DEBUG 0
+#if IEP_DEBUG
+#define IEP_DBG(format, args...)	printk("%s: " format, DRIVER_NAME, ## args)
+#else
+#define IEP_DBG(format, args...)
+#endif
+
+#define IEP_INFORMATION 1
+#if IEP_INFORMATION
+#define IEP_INFO(format, args...)			\
+		printk(KERN_INFO "%s: " format, DRIVER_NAME, ## args)
+#else
+#define IEP_INFO(format, args...)
+#endif
+
+#define IEP_ERR(format, args...)	printk(KERN_ERR "%s: " format, DRIVER_NAME, ## args)
+#define IEP_WARNING(format, args...)	printk(KERN_WARNING "%s: " format, DRIVER_NAME, ## args)
+
+enum {
+	yuv2rgb_BT_601_l = 0x0,     /* BT.601_1 */
+	yuv2rgb_BT_601_f = 0x1,     /* BT.601_f */
+	yuv2rgb_BT_709_l = 0x2,     /* BT.709_1 */
+	yuv2rgb_BT_709_f = 0x3,     /* BT.709_f */
+};
+
+enum {
+	rgb2yuv_BT_601_l = 0x0,     /* BT.601_1 */
+	rgb2yuv_BT_601_f = 0x1,     /* BT.601_f */
+	rgb2yuv_BT_709_l = 0x2,     /* BT.709_1 */
+	rgb2yuv_BT_709_f = 0x3,     /* BT.709_f */
+};
+
+enum {
+	dein_mode_bypass_dis         = 0x0,
+	dein_mode_I4O2               = 0x1,
+	dein_mode_I4O1B              = 0x2,
+	dein_mode_I4O1T              = 0x3,
+	dein_mode_I2O1B              = 0x4,
+	dein_mode_I2O1T              = 0x5,
+	dein_mode_bypass             = 0x6,
+};
+
+enum IEP_FIELD_ORDER {
+	FIELD_ORDER_TOP_FIRST,
+	FIELD_ORDER_BOTTOM_FIRST
+};
+
+enum IEP_YUV_DEINTERLACE_MODE {
+	IEP_DEINTERLACE_MODE_DISABLE,
+	IEP_DEINTERLACE_MODE_I2O1,
+	IEP_DEINTERLACE_MODE_I4O1,
+	IEP_DEINTERLACE_MODE_I4O2,
+	IEP_DEINTERLACE_MODE_BYPASS
+};
+
+enum {
+	rgb_enhance_bypass          = 0x0,
+	rgb_enhance_denoise         = 0x1,
+	rgb_enhance_detail          = 0x2,
+	rgb_enhance_edge            = 0x3,
+};/* for rgb_enhance_mode */
+
+enum {
+	rgb_contrast_CC_P_DDE          = 0x0, /* cg prior to dde */
+	rgb_contrast_DDE_P_CC          = 0x1, /* dde prior to cg */
+}; /* for rgb_contrast_enhance_mode */
+
+enum {
+	black_screen                   = 0x0,
+	blue_screen                    = 0x1,
+	color_bar                      = 0x2,
+	normal_mode                    = 0x3,
+}; /* for video mode */
+
+/*
+          Alpha    Red     Green   Blue  
+{  4, 32, {{32,24,   24,16,  16, 8,  8, 0 }}, GGL_RGBA },    IEP_FORMAT_ARGB_8888
+{  4, 32, {{32,24,   8, 0,  16, 8,  24,16 }}, GGL_RGB  },    IEP_FORMAT_ABGR_8888
+{  4, 32, {{ 8, 0,  32,24,  24,16,  16, 8 }}, GGL_RGB  },    IEP_FORMAT_RGBA_8888
+{  4, 32, {{ 8, 0,  16, 8,  24,16,  32,24 }}, GGL_BGRA },    IEP_FORMAT_BGRA_8888
+{  2, 16, {{ 0, 0,  16,11,  11, 5,   5, 0 }}, GGL_RGB  },    IEP_FORMAT_RGB_565
+{  2, 16, {{ 0, 0,   5, 0,  11, 5,  16,11 }}, GGL_RGB  },    IEP_FORMAT_RGB_565
+*/
+enum {
+	IEP_FORMAT_ARGB_8888    = 0x0,
+	IEP_FORMAT_ABGR_8888    = 0x1,
+	IEP_FORMAT_RGBA_8888    = 0x2,
+	IEP_FORMAT_BGRA_8888    = 0x3,
+	IEP_FORMAT_RGB_565      = 0x4,
+	IEP_FORMAT_BGR_565      = 0x5,
+
+	IEP_FORMAT_YCbCr_422_SP = 0x10,
+	IEP_FORMAT_YCbCr_422_P  = 0x11,
+	IEP_FORMAT_YCbCr_420_SP = 0x12,
+	IEP_FORMAT_YCbCr_420_P  = 0x13,
+	IEP_FORMAT_YCrCb_422_SP = 0x14,
+	IEP_FORMAT_YCrCb_422_P  = 0x15,/* same as IEP_FORMAT_YCbCr_422_P */
+	IEP_FORMAT_YCrCb_420_SP = 0x16,
+	IEP_FORMAT_YCrCb_420_P  = 0x17,/* same as IEP_FORMAT_YCbCr_420_P */
+}; /* for format */
+
+struct iep_img
+{
+	u16 act_w;	/* act_width */
+	u16 act_h;	/* act_height */
+	s16 x_off;	/* x offset for the vir,word unit */
+	s16 y_off;	/* y offset for the vir,word unit */
+
+	u16 vir_w;	/* unit :pix */
+	u16 vir_h;	/* unit :pix */
+	u32 format;
+	u32 mem_addr;
+	u32 uv_addr;
+	u32 v_addr;
+
+	u8 rb_swap;	/* not be used */
+	u8 uv_swap;	/* not be used */
+
+	u8 alpha_swap;	/* not be used */
+};
+
+struct IEP_MSG {
+	struct iep_img src;
+	struct iep_img dst;
+
+	struct iep_img src1;
+	struct iep_img dst1;
+
+	struct iep_img src_itemp;
+	struct iep_img src_ftemp;
+
+	struct iep_img dst_itemp;
+	struct iep_img dst_ftemp;
+
+	u8 dither_up_en;
+	u8 dither_down_en;/* not to be used */
+
+	u8 yuv2rgb_mode;
+	u8 rgb2yuv_mode;
+
+	u8 global_alpha_value;
+
+	u8 rgb2yuv_clip_en;
+	u8 yuv2rgb_clip_en;
+
+	u8 lcdc_path_en;
+	s32 off_x;
+	s32 off_y;
+	s32 width;
+	s32 height;
+	s32 layer;
+
+	u8 yuv_3D_denoise_en;
+
+	/* yuv color enhance */
+	u8 yuv_enhance_en;
+	s32 sat_con_int;
+	s32 contrast_int;
+	s32 cos_hue_int;
+	s32 sin_hue_int;
+	s8 yuv_enh_brightness;	/*-32<brightness<31*/
+	u8 video_mode;		/*0-3*/
+	u8 color_bar_y;	/*0-127*/
+	u8 color_bar_u;	/*0-127*/
+	u8 color_bar_v;	/*0-127*/
+
+
+	u8 rgb_enhance_en;/*i don't konw what is used*/
+
+	u8 rgb_color_enhance_en;/*sw_rgb_color_enh_en*/
+	u32 rgb_enh_coe;
+
+	u8 rgb_enhance_mode;/*sw_rgb_enh_sel,dde sel*/
+
+	u8 rgb_cg_en;/*sw_rgb_con_gam_en*/
+	u32 cg_tab[192];
+
+	/*sw_con_gam_order;0 cg prior to dde,1 dde prior to cg*/
+	u8 rgb_contrast_enhance_mode;
+
+	s32 enh_threshold;
+	s32 enh_alpha;
+	s32 enh_radius;
+
+	u8 scale_up_mode;
+
+	u8 field_order;
+	u8 dein_mode;
+	/*DIL HF*/
+	u8 dein_high_fre_en;
+	u8 dein_high_fre_fct;
+	/*DIL EI*/
+	u8 dein_ei_mode;
+	u8 dein_ei_smooth;
+	u8 dein_ei_sel;
+	u8 dein_ei_radius;/*when dein_ei_sel=0 will be used*/
+
+	u8 vir_addr_enable;
+
+	void *base;
+};
+
+#endif
diff --git a/drivers/video/rockchip/iep/iep_drv.c b/drivers/video/rockchip/iep/iep_drv.c
new file mode 100644
index 0000000000000..783cdee17112c
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep_drv.c
@@ -0,0 +1,1321 @@
+/*
+ * Copyright (C) 2013 Rockchip Electronics Co., Ltd.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/kthread.h>
+#include <linux/poll.h>
+#include <linux/dma-mapping.h>
+#include <linux/fb.h>
+#include <linux/wakelock.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/rockchip/cpu.h>
+#include <linux/iommu.h>
+#include <asm/cacheflush.h>
+#include "iep_drv.h"
+#include "hw_iep_reg.h"
+#include "iep_iommu_ops.h"
+
+#define IEP_MAJOR		255
+#define IEP_CLK_ENABLE
+/*#define IEP_TEST_CASE*/
+
+static int debug;
+module_param(debug, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(debug,
+		 "Debug level - higher value produces more verbose messages");
+
+#define RK_IEP_SIZE		0x1000
+#define IEP_TIMEOUT_DELAY	2*HZ
+#define IEP_POWER_OFF_DELAY	4*HZ
+
+struct iep_drvdata {
+	struct miscdevice miscdev;
+	void *iep_base;
+	int irq0;
+
+	struct clk *aclk_iep;
+	struct clk *hclk_iep;
+	struct clk *pd_iep;
+	struct clk *aclk_vio1;
+
+	struct mutex mutex;
+
+	/* direct path interface mode. true: enable, false: disable */
+	bool dpi_mode;
+
+	struct delayed_work power_off_work;
+
+	/* clk enable or disable */
+	bool enable;
+	struct wake_lock wake_lock;
+
+	atomic_t iep_int;
+	atomic_t mmu_page_fault;
+	atomic_t mmu_bus_error;
+
+	/* capability for this iep device */
+	struct IEP_CAP cap;
+	struct device *dev;
+};
+
+struct iep_drvdata *iep_drvdata1 = NULL;
+iep_service_info iep_service;
+
+static void iep_reg_deinit(struct iep_reg *reg)
+{
+	struct iep_mem_region *mem_region = NULL, *n;
+	/* release memory region attach to this registers table.*/
+	if (iep_service.iommu_dev) {
+		list_for_each_entry_safe(mem_region, n, &reg->mem_region_list,
+					 reg_lnk) {
+			iep_iommu_unmap_iommu(iep_service.iommu_info,
+					      reg->session, mem_region->hdl);
+			iep_iommu_free(iep_service.iommu_info,
+				       reg->session, mem_region->hdl);
+			list_del_init(&mem_region->reg_lnk);
+			kfree(mem_region);
+		}
+	}
+
+	list_del_init(&reg->session_link);
+	list_del_init(&reg->status_link);
+	kfree(reg);
+}
+
+static void iep_reg_from_wait_to_ready(struct iep_reg *reg)
+{
+	list_del_init(&reg->status_link);
+	list_add_tail(&reg->status_link, &iep_service.ready);
+
+	list_del_init(&reg->session_link);
+	list_add_tail(&reg->session_link, &reg->session->ready);
+}
+
+static void iep_reg_from_ready_to_running(struct iep_reg *reg)
+{
+	list_del_init(&reg->status_link);
+	list_add_tail(&reg->status_link, &iep_service.running);
+
+	list_del_init(&reg->session_link);
+	list_add_tail(&reg->session_link, &reg->session->running);
+}
+
+static void iep_del_running_list(void)
+{
+	struct iep_reg *reg;
+	int cnt = 0;
+
+	mutex_lock(&iep_service.lock);
+
+	while (!list_empty(&iep_service.running)) {
+		BUG_ON(cnt != 0);
+		reg = list_entry(iep_service.running.next,
+				 struct iep_reg, status_link);
+
+		atomic_dec(&reg->session->task_running);
+		atomic_dec(&iep_service.total_running);
+
+		if (list_empty(&reg->session->waiting)) {
+			atomic_set(&reg->session->done, 1);
+			atomic_inc(&reg->session->num_done);
+			wake_up(&reg->session->wait);
+		}
+
+		iep_reg_deinit(reg);
+		cnt++;
+	}
+
+	mutex_unlock(&iep_service.lock);
+}
+
+static void iep_dump(void)
+{
+	struct iep_status sts;
+
+	sts = iep_get_status(iep_drvdata1->iep_base);
+
+	IEP_INFO("scl_sts: %u, dil_sts %u, wyuv_sts %u, ryuv_sts %u, wrgb_sts %u, rrgb_sts %u, voi_sts %u\n",
+		sts.scl_sts, sts.dil_sts, sts.wyuv_sts, sts.ryuv_sts, sts.wrgb_sts, sts.rrgb_sts, sts.voi_sts); {
+		int *reg = (int *)iep_drvdata1->iep_base;
+		int i;
+
+		/* could not read validate data from address after base+0x40 */
+		for (i = 0; i < 0x40; i++) {
+			IEP_INFO("%08x ", reg[i]);
+
+			if ((i + 1) % 4 == 0) {
+				IEP_INFO("\n");
+			}
+		}
+
+		IEP_INFO("\n");
+	}
+}
+
+/* Caller must hold iep_service.lock */
+static void iep_del_running_list_timeout(void)
+{
+	struct iep_reg *reg;
+
+	mutex_lock(&iep_service.lock);
+
+	while (!list_empty(&iep_service.running)) {
+		reg = list_entry(iep_service.running.next, struct iep_reg, status_link);
+
+		atomic_dec(&reg->session->task_running);
+		atomic_dec(&iep_service.total_running);
+
+		/* iep_soft_rst(iep_drvdata1->iep_base); */
+
+		iep_dump();
+
+		if (list_empty(&reg->session->waiting)) {
+			atomic_set(&reg->session->done, 1);
+			wake_up(&reg->session->wait);
+		}
+
+		iep_reg_deinit(reg);
+	}
+
+	mutex_unlock(&iep_service.lock);
+}
+
+static inline void iep_queue_power_off_work(void)
+{
+	queue_delayed_work(system_wq, &iep_drvdata1->power_off_work, IEP_POWER_OFF_DELAY);
+}
+
+static void iep_power_on(void)
+{
+	static ktime_t last;
+	ktime_t now = ktime_get();
+	if (ktime_to_ns(ktime_sub(now, last)) > NSEC_PER_SEC) {
+		cancel_delayed_work_sync(&iep_drvdata1->power_off_work);
+		iep_queue_power_off_work();
+		last = now;
+	}
+
+	if (iep_service.enable)
+		return;
+
+	IEP_INFO("IEP Power ON\n");
+
+	/* iep_soft_rst(iep_drvdata1->iep_base); */
+
+#ifdef IEP_CLK_ENABLE
+	pm_runtime_get_sync(iep_drvdata1->dev);
+	if (iep_drvdata1->pd_iep)
+		clk_prepare_enable(iep_drvdata1->pd_iep);
+	clk_prepare_enable(iep_drvdata1->aclk_iep);
+	clk_prepare_enable(iep_drvdata1->hclk_iep);
+#endif
+
+	wake_lock(&iep_drvdata1->wake_lock);
+
+	iep_iommu_attach(iep_service.iommu_info);
+
+	iep_service.enable = true;
+}
+
+static void iep_power_off(void)
+{
+	int total_running;
+
+	if (!iep_service.enable) {
+		return;
+	}
+
+	IEP_INFO("IEP Power OFF\n");
+
+	total_running = atomic_read(&iep_service.total_running);
+	if (total_running) {
+		IEP_WARNING("power off when %d task running!!\n", total_running);
+		mdelay(50);
+		IEP_WARNING("delay 50 ms for running task\n");
+		iep_dump();
+	}
+
+	if (iep_service.iommu_dev) {
+		iep_iommu_detach(iep_service.iommu_info);
+	}
+
+#ifdef IEP_CLK_ENABLE
+	clk_disable_unprepare(iep_drvdata1->aclk_iep);
+	clk_disable_unprepare(iep_drvdata1->hclk_iep);
+	if (iep_drvdata1->pd_iep)
+		clk_disable_unprepare(iep_drvdata1->pd_iep);
+	pm_runtime_put(iep_drvdata1->dev);
+#endif
+
+	wake_unlock(&iep_drvdata1->wake_lock);
+	iep_service.enable = false;
+}
+
+static void iep_power_off_work(struct work_struct *work)
+{
+	if (mutex_trylock(&iep_service.lock)) {
+		if (!iep_drvdata1->dpi_mode) {
+			IEP_INFO("iep dpi mode inactivity\n");
+			iep_power_off();
+		}
+		mutex_unlock(&iep_service.lock);
+	} else {
+		/* Come back later if the device is busy... */
+		iep_queue_power_off_work();
+	}
+}
+
+#ifdef CONFIG_FB_ROCKCHIP
+extern void rk_direct_fb_show(struct fb_info *fbi);
+extern struct fb_info* rk_get_fb(int fb_id);
+extern bool rk_fb_poll_wait_frame_complete(void);
+extern int rk_fb_dpi_open(bool open);
+extern int rk_fb_dpi_win_sel(int layer_id);
+
+static void iep_config_lcdc(struct iep_reg *reg)
+{
+	struct fb_info *fb;
+	int fbi = 0;
+	int fmt = 0;
+
+	fbi = reg->layer == 0 ? 0 : 1;
+
+	rk_fb_dpi_win_sel(fbi);
+
+	fb = rk_get_fb(fbi);
+#if 1
+	switch (reg->format) {
+	case IEP_FORMAT_ARGB_8888:
+	case IEP_FORMAT_ABGR_8888:
+		fmt = HAL_PIXEL_FORMAT_RGBA_8888;
+		fb->var.bits_per_pixel = 32;
+
+		fb->var.red.length = 8;
+		fb->var.red.offset = 16;
+		fb->var.red.msb_right = 0;
+
+		fb->var.green.length = 8;
+		fb->var.green.offset = 8;
+		fb->var.green.msb_right = 0;
+
+		fb->var.blue.length = 8;
+		fb->var.blue.offset = 0;
+		fb->var.blue.msb_right = 0;
+
+		fb->var.transp.length = 8;
+		fb->var.transp.offset = 24;
+		fb->var.transp.msb_right = 0;
+
+		break;
+	case IEP_FORMAT_BGRA_8888:
+		fmt = HAL_PIXEL_FORMAT_BGRA_8888;
+		fb->var.bits_per_pixel = 32;
+		break;
+	case IEP_FORMAT_RGB_565:
+		fmt = HAL_PIXEL_FORMAT_RGB_565;
+		fb->var.bits_per_pixel = 16;
+
+		fb->var.red.length = 5;
+		fb->var.red.offset = 11;
+		fb->var.red.msb_right = 0;
+
+		fb->var.green.length = 6;
+		fb->var.green.offset = 5;
+		fb->var.green.msb_right = 0;
+
+		fb->var.blue.length = 5;
+		fb->var.blue.offset = 0;
+		fb->var.blue.msb_right = 0;
+
+		break;
+	case IEP_FORMAT_YCbCr_422_SP:
+		fmt = HAL_PIXEL_FORMAT_YCbCr_422_SP;
+		fb->var.bits_per_pixel = 16;
+		break;
+	case IEP_FORMAT_YCbCr_420_SP:
+		fmt = HAL_PIXEL_FORMAT_YCrCb_NV12;
+		fb->var.bits_per_pixel = 16;
+		break;
+	case IEP_FORMAT_YCbCr_422_P:
+	case IEP_FORMAT_YCrCb_422_SP:
+	case IEP_FORMAT_YCrCb_422_P:
+	case IEP_FORMAT_YCrCb_420_SP:
+	case IEP_FORMAT_YCbCr_420_P:
+	case IEP_FORMAT_YCrCb_420_P:
+	case IEP_FORMAT_RGBA_8888:
+	case IEP_FORMAT_BGR_565:
+		/* unsupported format */
+		IEP_ERR("unsupported format %d\n", reg->format);
+		break;
+	default:
+		;
+	}
+
+	fb->var.xoffset = 0;
+	fb->var.yoffset = 0;
+	fb->var.xres = reg->act_width;
+	fb->var.yres = reg->act_height;
+	fb->var.xres_virtual = reg->act_width;
+	fb->var.yres_virtual = reg->act_height;
+	fb->var.nonstd = ((reg->off_y & 0xFFF) << 20) +
+		((reg->off_x & 0xFFF) << 8) + (fmt & 0xFF);
+	fb->var.grayscale =
+		((reg->vir_height & 0xFFF) << 20) +
+		((reg->vir_width & 0xFFF) << 8) + 0;/*win0 xsize & ysize*/
+#endif
+	rk_direct_fb_show(fb);
+}
+
+static int iep_switch_dpi(struct iep_reg *reg)
+{
+	if (reg->dpi_en) {
+		if (!iep_drvdata1->dpi_mode) {
+			/* Turn on dpi */
+			rk_fb_dpi_open(true);
+			iep_drvdata1->dpi_mode = true;
+		}
+		iep_config_lcdc(reg);
+	} else {
+		if (iep_drvdata1->dpi_mode) {
+			/* Turn off dpi */
+			/* wait_lcdc_dpi_close(); */
+			bool status;
+			rk_fb_dpi_open(false);
+			status = rk_fb_poll_wait_frame_complete();
+			iep_drvdata1->dpi_mode = false;
+			IEP_INFO("%s %d, iep dpi inactivated\n",
+				 __func__, __LINE__);
+		}
+	}
+
+	return 0;
+}
+#endif
+
+static void iep_reg_copy_to_hw(struct iep_reg *reg)
+{
+	int i;
+
+	u32 *pbase = (u32 *)iep_drvdata1->iep_base;
+
+	/* config registers */
+	for (i = 0; i < IEP_CNF_REG_LEN; i++)
+		pbase[IEP_CNF_REG_BASE + i] = reg->reg[IEP_CNF_REG_BASE + i];
+
+	/* command registers */
+	for (i = 0; i < IEP_CMD_REG_LEN; i++)
+		pbase[IEP_CMD_REG_BASE + i] = reg->reg[IEP_CMD_REG_BASE + i];
+
+	/* address registers */
+	for (i = 0; i < IEP_ADD_REG_LEN; i++)
+		pbase[IEP_ADD_REG_BASE + i] = reg->reg[IEP_ADD_REG_BASE + i];
+
+	/* dmac_flush_range(&pbase[0], &pbase[IEP_REG_LEN]); */
+	/* outer_flush_range(virt_to_phys(&pbase[0]),virt_to_phys(&pbase[IEP_REG_LEN])); */
+
+	dsb(sy);
+}
+
+/** switch fields order before the next lcdc frame start
+ *  coming */
+static void iep_switch_fields_order(void)
+{
+	void *pbase = (void *)iep_drvdata1->iep_base;
+	int mode = iep_get_deinterlace_mode(pbase);
+#ifdef CONFIG_FB_ROCKCHIP
+	struct fb_info *fb;
+#endif
+	switch (mode) {
+	case dein_mode_I4O1B:
+		iep_set_deinterlace_mode(dein_mode_I4O1T, pbase);
+		break;
+	case dein_mode_I4O1T:
+		iep_set_deinterlace_mode(dein_mode_I4O1B, pbase);
+		break;
+	case dein_mode_I2O1B:
+		iep_set_deinterlace_mode(dein_mode_I2O1T, pbase);
+		break;
+	case dein_mode_I2O1T:
+		iep_set_deinterlace_mode(dein_mode_I2O1B, pbase);
+		break;
+	default:
+		;
+	}
+#ifdef CONFIG_FB_ROCKCHIP
+	fb = rk_get_fb(1);
+	rk_direct_fb_show(fb);
+#endif
+	/*iep_switch_input_address(pbase);*/
+}
+
+/* Caller must hold iep_service.lock */
+static void iep_try_set_reg(void)
+{
+	struct iep_reg *reg;
+
+	mutex_lock(&iep_service.lock);
+
+	if (list_empty(&iep_service.ready)) {
+		if (!list_empty(&iep_service.waiting)) {
+			reg = list_entry(iep_service.waiting.next, struct iep_reg, status_link);
+
+			iep_power_on();
+			udelay(1);
+
+			iep_reg_from_wait_to_ready(reg);
+			atomic_dec(&iep_service.waitcnt);
+
+			/*iep_soft_rst(iep_drvdata1->iep_base);*/
+
+			iep_reg_copy_to_hw(reg);
+		}
+	} else {
+		if (iep_drvdata1->dpi_mode)
+			iep_switch_fields_order();
+	}
+
+	mutex_unlock(&iep_service.lock);
+}
+
+static void iep_try_start_frm(void)
+{
+	struct iep_reg *reg;
+
+	mutex_lock(&iep_service.lock);
+
+	if (list_empty(&iep_service.running)) {
+		if (!list_empty(&iep_service.ready)) {
+			reg = list_entry(iep_service.ready.next, struct iep_reg, status_link);
+#ifdef CONFIG_FB_ROCKCHIP
+			iep_switch_dpi(reg);
+#endif
+			iep_reg_from_ready_to_running(reg);
+			iep_config_frame_end_int_en(iep_drvdata1->iep_base);
+			iep_config_done(iep_drvdata1->iep_base);
+
+			/* Start proc */
+			atomic_inc(&reg->session->task_running);
+			atomic_inc(&iep_service.total_running);
+			iep_config_frm_start(iep_drvdata1->iep_base);
+		}
+	}
+
+	mutex_unlock(&iep_service.lock);
+}
+
+static irqreturn_t iep_isr(int irq, void *dev_id)
+{
+	if (atomic_read(&iep_drvdata1->iep_int) > 0) {
+		if (iep_service.enable) {
+			if (list_empty(&iep_service.waiting)) {
+				if (iep_drvdata1->dpi_mode) {
+					iep_switch_fields_order();
+				}
+			}
+			iep_del_running_list();
+		}
+
+		iep_try_set_reg();
+		iep_try_start_frm();
+
+		atomic_dec(&iep_drvdata1->iep_int);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t iep_irq(int irq,  void *dev_id)
+{
+	/*clear INT */
+	void *pbase = (void *)iep_drvdata1->iep_base;
+
+	if (iep_probe_int(pbase)) {
+		iep_config_frame_end_int_clr(pbase);
+		atomic_inc(&iep_drvdata1->iep_int);
+	}
+
+	return IRQ_WAKE_THREAD;
+}
+
+static void iep_service_session_clear(iep_session *session)
+{
+	struct iep_reg *reg, *n;
+
+	list_for_each_entry_safe(reg, n, &session->waiting, session_link) {
+		iep_reg_deinit(reg);
+	}
+
+	list_for_each_entry_safe(reg, n, &session->ready, session_link) {
+		iep_reg_deinit(reg);
+	}
+
+	list_for_each_entry_safe(reg, n, &session->running, session_link) {
+		iep_reg_deinit(reg);
+	}
+}
+
+static int iep_open(struct inode *inode, struct file *filp)
+{
+	//DECLARE_WAITQUEUE(wait, current);
+	iep_session *session = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (NULL == session) {
+		IEP_ERR("unable to allocate memory for iep_session.\n");
+		return -ENOMEM;
+	}
+
+	session->pid = current->pid;
+	INIT_LIST_HEAD(&session->waiting);
+	INIT_LIST_HEAD(&session->ready);
+	INIT_LIST_HEAD(&session->running);
+	INIT_LIST_HEAD(&session->list_session);
+	init_waitqueue_head(&session->wait);
+	/*add_wait_queue(&session->wait, wait);*/
+	/* no need to protect */
+	mutex_lock(&iep_service.lock);
+	list_add_tail(&session->list_session, &iep_service.session);
+	mutex_unlock(&iep_service.lock);
+	atomic_set(&session->task_running, 0);
+	atomic_set(&session->num_done, 0);
+
+	filp->private_data = (void *)session;
+
+	return nonseekable_open(inode, filp);
+}
+
+static int iep_release(struct inode *inode, struct file *filp)
+{
+	int task_running;
+	iep_session *session = (iep_session *)filp->private_data;
+
+	if (NULL == session)
+		return -EINVAL;
+
+	task_running = atomic_read(&session->task_running);
+
+	if (task_running) {
+		IEP_ERR("iep_service session %d still "
+			"has %d task running when closing\n",
+			session->pid, task_running);
+		msleep(100);
+		/*synchronization*/
+	}
+
+	wake_up(&session->wait);
+	iep_power_on();
+	mutex_lock(&iep_service.lock);
+	list_del(&session->list_session);
+	iep_service_session_clear(session);
+	iep_iommu_clear(iep_service.iommu_info, session);
+	kfree(session);
+	mutex_unlock(&iep_service.lock);
+
+	return 0;
+}
+
+static unsigned int iep_poll(struct file *filp, poll_table *wait)
+{
+	int mask = 0;
+	iep_session *session = (iep_session *)filp->private_data;
+	if (NULL == session)
+		return POLL_ERR;
+	poll_wait(filp, &session->wait, wait);
+	if (atomic_read(&session->done))
+		mask |= POLL_IN | POLLRDNORM;
+
+	return mask;
+}
+
+static int iep_get_result_sync(iep_session *session)
+{
+	int ret = 0;
+
+	iep_try_start_frm();
+
+	ret = wait_event_timeout(session->wait,
+		atomic_read(&session->done), IEP_TIMEOUT_DELAY);
+
+	if (unlikely(ret < 0)) {
+		IEP_ERR("sync pid %d wait task ret %d\n", session->pid, ret);
+		iep_del_running_list();
+	} else if (0 == ret) {
+		IEP_ERR("sync pid %d wait %d task done timeout\n",
+			session->pid, atomic_read(&session->task_running));
+		iep_del_running_list_timeout();
+		iep_try_set_reg();
+		iep_try_start_frm();
+		ret = -ETIMEDOUT;
+	}
+
+	return ret;
+}
+
+static void iep_get_result_async(iep_session *session)
+{
+	iep_try_start_frm();
+	return;
+}
+
+static long iep_ioctl(struct file *filp, uint32_t cmd, unsigned long arg)
+{
+	int ret = 0;
+	iep_session *session = (iep_session *)filp->private_data;
+
+	if (NULL == session) {
+		IEP_ERR("%s [%d] iep thread session is null\n",
+			__FUNCTION__, __LINE__);
+		return -EINVAL;
+	}
+
+	mutex_lock(&iep_service.mutex);
+
+	switch (cmd) {
+	case IEP_SET_PARAMETER:
+		{
+			struct IEP_MSG *msg;
+			msg = kzalloc(sizeof(*msg), GFP_KERNEL);
+			if (msg) {
+				if (copy_from_user(msg, (struct IEP_MSG *)arg,
+						sizeof(struct IEP_MSG))) {
+					IEP_ERR("copy_from_user failure\n");
+					ret = -EFAULT;
+				}
+			}
+
+			if (ret == 0) {
+				if (atomic_read(&iep_service.waitcnt) < 10) {
+					iep_power_on();
+					iep_config(session, msg);
+					atomic_inc(&iep_service.waitcnt);
+				} else {
+					IEP_ERR("iep task queue full\n");
+					ret = -EFAULT;
+				}
+			}
+
+			/** REGISTER CONFIG must accord to Timing When DPI mode
+			 *  enable */
+			if (!iep_drvdata1->dpi_mode)
+				iep_try_set_reg();
+			kfree(msg);
+		}
+		break;
+	case IEP_GET_RESULT_SYNC:
+		if (0 > iep_get_result_sync(session)) {
+			ret = -ETIMEDOUT;
+		}
+		break;
+	case IEP_GET_RESULT_ASYNC:
+		iep_get_result_async(session);
+		break;
+	case IEP_RELEASE_CURRENT_TASK:
+		iep_del_running_list_timeout();
+		iep_try_set_reg();
+		iep_try_start_frm();
+		break;
+	case IEP_GET_IOMMU_STATE:
+		{
+			int iommu_enable = 0;
+
+			iommu_enable = iep_service.iommu_dev ? 1 : 0;
+
+			if (copy_to_user((void __user *)arg, &iommu_enable,
+				sizeof(int))) {
+				IEP_ERR("error: copy_to_user failed\n");
+				ret = -EFAULT;
+			}
+		}
+		break;
+	case IEP_QUERY_CAP:
+		if (copy_to_user((void __user *)arg, &iep_drvdata1->cap,
+			sizeof(struct IEP_CAP))) {
+			IEP_ERR("error: copy_to_user failed\n");
+			ret = -EFAULT;
+		}
+		break;
+	default:
+		IEP_ERR("unknown ioctl cmd!\n");
+		ret = -EINVAL;
+	}
+	mutex_unlock(&iep_service.mutex);
+
+	return ret;
+}
+
+#ifdef CONFIG_COMPAT
+static long compat_iep_ioctl(struct file *filp, uint32_t cmd,
+			     unsigned long arg)
+{
+	int ret = 0;
+	iep_session *session = (iep_session *)filp->private_data;
+
+	if (NULL == session) {
+		IEP_ERR("%s [%d] iep thread session is null\n",
+			__func__, __LINE__);
+		return -EINVAL;
+	}
+
+	mutex_lock(&iep_service.mutex);
+
+	switch (cmd) {
+	case COMPAT_IEP_SET_PARAMETER:
+		{
+			struct IEP_MSG *msg;
+
+			msg = kzalloc(sizeof(*msg), GFP_KERNEL);
+
+			if (msg) {
+				if (copy_from_user
+				    (msg, compat_ptr((compat_uptr_t)arg),
+				     sizeof(struct IEP_MSG))) {
+					IEP_ERR("copy_from_user failure\n");
+					ret = -EFAULT;
+				}
+			}
+
+			if (ret == 0) {
+				if (atomic_read(&iep_service.waitcnt) < 10) {
+					iep_power_on();
+					iep_config(session, msg);
+					atomic_inc(&iep_service.waitcnt);
+				} else {
+					IEP_ERR("iep task queue full\n");
+					ret = -EFAULT;
+				}
+			}
+
+			/** REGISTER CONFIG must accord to Timing When DPI mode
+			 *  enable */
+			if (!iep_drvdata1->dpi_mode)
+				iep_try_set_reg();
+			kfree(msg);
+		}
+		break;
+	case COMPAT_IEP_GET_RESULT_SYNC:
+		if (0 > iep_get_result_sync(session))
+			ret = -ETIMEDOUT;
+		break;
+	case COMPAT_IEP_GET_RESULT_ASYNC:
+		iep_get_result_async(session);
+		break;
+	case COMPAT_IEP_RELEASE_CURRENT_TASK:
+		iep_del_running_list_timeout();
+		iep_try_set_reg();
+		iep_try_start_frm();
+		break;
+	case COMPAT_IEP_GET_IOMMU_STATE:
+		{
+			int iommu_enable = 0;
+
+			iommu_enable = iep_service.iommu_dev ? 1 : 0;
+
+			if (copy_to_user((void __user *)arg, &iommu_enable,
+				sizeof(int))) {
+				IEP_ERR("error: copy_to_user failed\n");
+				ret = -EFAULT;
+			}
+		}
+		break;
+	case COMPAT_IEP_QUERY_CAP:
+		if (copy_to_user((void __user *)arg, &iep_drvdata1->cap,
+			sizeof(struct IEP_CAP))) {
+			IEP_ERR("error: copy_to_user failed\n");
+			ret = -EFAULT;
+		}
+		break;
+	default:
+		IEP_ERR("unknown ioctl cmd!\n");
+		ret = -EINVAL;
+	}
+	mutex_unlock(&iep_service.mutex);
+
+	return ret;
+}
+#endif
+
+struct file_operations iep_fops = {
+	.owner		= THIS_MODULE,
+	.open		= iep_open,
+	.release	= iep_release,
+	.poll		= iep_poll,
+	.unlocked_ioctl	= iep_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= compat_iep_ioctl,
+#endif
+};
+
+static struct miscdevice iep_dev = {
+	.minor = IEP_MAJOR,
+	.name  = "iep",
+	.fops  = &iep_fops,
+};
+
+static int iep_sysmmu_fault_handler(struct iommu_domain *domain,
+				    struct device *iommu_dev,
+				    unsigned long iova, int status, void *arg)
+{
+	struct iep_reg *reg = list_entry(iep_service.running.next,
+		struct iep_reg, status_link);
+	if (reg != NULL) {
+		struct iep_mem_region *mem, *n;
+		int i = 0;
+		pr_info("iep, fault addr 0x%08x\n", (u32)iova);
+		list_for_each_entry_safe(mem, n,
+			&reg->mem_region_list,
+			reg_lnk) {
+			pr_info("iep, mem region [%02d] 0x%08x %ld\n",
+				i, (u32)mem->iova, mem->len);
+			i++;
+		}
+
+		pr_alert("iep, page fault occur\n");
+
+		iep_del_running_list();
+	}
+
+	return 0;
+}
+
+static int iep_drv_probe(struct platform_device *pdev)
+{
+	struct iep_drvdata *data;
+	int ret = 0;
+	struct resource *res = NULL;
+	u32 version;
+	struct device_node *np = pdev->dev.of_node;
+	struct platform_device *sub_dev = NULL;
+	struct device_node *sub_np = NULL;
+	u32 iommu_en = 0;
+	struct iommu_domain *domain;
+
+	of_property_read_u32(np, "iommu_enabled", &iommu_en);
+
+	data = devm_kzalloc(&pdev->dev, sizeof(*data),
+			    GFP_KERNEL);
+	if (NULL == data) {
+		IEP_ERR("failed to allocate driver data.\n");
+		return  -ENOMEM;
+	}
+
+	iep_drvdata1 = data;
+
+	INIT_LIST_HEAD(&iep_service.waiting);
+	INIT_LIST_HEAD(&iep_service.ready);
+	INIT_LIST_HEAD(&iep_service.running);
+	INIT_LIST_HEAD(&iep_service.done);
+	INIT_LIST_HEAD(&iep_service.session);
+	atomic_set(&iep_service.waitcnt, 0);
+	mutex_init(&iep_service.lock);
+	atomic_set(&iep_service.total_running, 0);
+	iep_service.enable = false;
+
+#ifdef IEP_CLK_ENABLE
+	data->pd_iep = devm_clk_get(&pdev->dev, "pd_iep");
+	if (IS_ERR(data->pd_iep)) {
+		IEP_ERR("failed to find iep power down clock source.\n");
+		data->pd_iep = NULL;
+	}
+
+	data->aclk_iep = devm_clk_get(&pdev->dev, "aclk_iep");
+	if (IS_ERR(data->aclk_iep)) {
+		IEP_ERR("failed to find iep axi clock source.\n");
+		ret = -ENOENT;
+		goto err_clock;
+	}
+
+	data->hclk_iep = devm_clk_get(&pdev->dev, "hclk_iep");
+	if (IS_ERR(data->hclk_iep)) {
+		IEP_ERR("failed to find iep ahb clock source.\n");
+		ret = -ENOENT;
+		goto err_clock;
+	}
+#endif
+
+	iep_service.enable = false;
+	INIT_DELAYED_WORK(&data->power_off_work, iep_power_off_work);
+	wake_lock_init(&data->wake_lock, WAKE_LOCK_SUSPEND, "iep");
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+
+	data->iep_base = (void *)devm_ioremap_resource(&pdev->dev, res);
+	if (data->iep_base == NULL) {
+		IEP_ERR("iep ioremap failed\n");
+		ret = -ENOENT;
+		goto err_ioremap;
+	}
+
+	atomic_set(&data->iep_int, 0);
+	atomic_set(&data->mmu_page_fault, 0);
+	atomic_set(&data->mmu_bus_error, 0);
+
+	/* get the IRQ */
+	data->irq0 = platform_get_irq(pdev, 0);
+	if (data->irq0 <= 0) {
+		IEP_ERR("failed to get iep irq resource (%d).\n", data->irq0);
+		ret = data->irq0;
+		goto err_irq;
+	}
+
+	/* request the IRQ */
+	ret = devm_request_threaded_irq(&pdev->dev, data->irq0, iep_irq,
+		iep_isr, IRQF_SHARED, dev_name(&pdev->dev), pdev);
+	if (ret) {
+		IEP_ERR("iep request_irq failed (%d).\n", ret);
+		goto err_irq;
+	}
+
+	mutex_init(&iep_service.mutex);
+
+	if (of_property_read_u32(np, "version", &version)) {
+		version = 0;
+	}
+
+	data->cap.scaling_supported = 0;
+	data->cap.i4_deinterlace_supported = 1;
+	data->cap.i2_deinterlace_supported = 1;
+	data->cap.compression_noise_reduction_supported = 1;
+	data->cap.sampling_noise_reduction_supported = 1;
+	data->cap.hsb_enhancement_supported = 1;
+	data->cap.cg_enhancement_supported = 1;
+	data->cap.direct_path_supported = 1;
+	data->cap.max_dynamic_width = 1920;
+	data->cap.max_dynamic_height = 1088;
+	data->cap.max_static_width = 8192;
+	data->cap.max_static_height = 8192;
+	data->cap.max_enhance_radius = 3;
+
+	switch (version) {
+	case 0:
+		data->cap.scaling_supported = 1;
+		break;
+	case 1:
+		data->cap.compression_noise_reduction_supported = 0;
+		data->cap.sampling_noise_reduction_supported = 0;
+		if (soc_is_rk3126b() || soc_is_rk3126c()) {
+			data->cap.i4_deinterlace_supported = 0;
+			data->cap.hsb_enhancement_supported = 0;
+			data->cap.cg_enhancement_supported = 0;
+		}
+		break;
+	case 2:
+		data->cap.max_dynamic_width = 4096;
+		data->cap.max_dynamic_height = 2340;
+		data->cap.max_enhance_radius = 2;
+		break;
+	default:
+		;
+	}
+
+	platform_set_drvdata(pdev, data);
+
+	ret = misc_register(&iep_dev);
+	if (ret) {
+		IEP_ERR("cannot register miscdev (%d)\n", ret);
+		goto err_misc_register;
+	}
+
+	data->dev = &pdev->dev;
+#ifdef IEP_CLK_ENABLE
+	pm_runtime_enable(data->dev);
+#endif
+
+	iep_service.iommu_dev = NULL;
+	sub_np = of_parse_phandle(np, "iommus", 0);
+	if (sub_np) {
+		sub_dev = of_find_device_by_node(sub_np);
+		iep_service.iommu_dev = &sub_dev->dev;
+		domain = iommu_get_domain_for_dev(&pdev->dev);
+		iommu_set_fault_handler(domain, iep_sysmmu_fault_handler, data);
+	}
+
+	of_property_read_u32(np, "allocator", (u32 *)&iep_service.alloc_type);
+	iep_power_on();
+	iep_service.iommu_info = iep_iommu_info_create(data->dev,
+						       iep_service.iommu_dev,
+						       iep_service.alloc_type);
+	iep_power_off();
+
+	IEP_INFO("IEP Driver loaded succesfully\n");
+
+	return 0;
+
+err_misc_register:
+	free_irq(data->irq0, pdev);
+err_irq:
+err_ioremap:
+	wake_lock_destroy(&data->wake_lock);
+#ifdef IEP_CLK_ENABLE
+err_clock:
+#endif
+	return ret;
+}
+
+static int iep_drv_remove(struct platform_device *pdev)
+{
+	struct iep_drvdata *data = platform_get_drvdata(pdev);
+
+	iep_iommu_info_destroy(iep_service.iommu_info);
+	iep_service.iommu_info = NULL;
+
+	wake_lock_destroy(&data->wake_lock);
+
+	misc_deregister(&(data->miscdev));
+	free_irq(data->irq0, &data->miscdev);
+
+#ifdef IEP_CLK_ENABLE
+	pm_runtime_disable(data->dev);
+#endif
+
+	return 0;
+}
+
+#if defined(CONFIG_OF)
+static const struct of_device_id iep_dt_ids[] = {
+	{ .compatible = "rockchip,iep", },
+	{ },
+};
+#endif
+
+static struct platform_driver iep_driver = {
+	.probe		= iep_drv_probe,
+	.remove		= iep_drv_remove,
+	.driver		= {
+		.name	= "iep",
+#if defined(CONFIG_OF)
+		.of_match_table = of_match_ptr(iep_dt_ids),
+#endif
+	},
+};
+
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+static int proc_iep_show(struct seq_file *s, void *v)
+{
+	struct iep_status sts;
+	//mutex_lock(&iep_service.mutex);
+	iep_power_on();
+	seq_printf(s, "\nIEP Modules Status:\n");
+	sts = iep_get_status(iep_drvdata1->iep_base);
+	seq_printf(s, "scl_sts: %u, dil_sts %u, wyuv_sts %u, "
+		      "ryuv_sts %u, wrgb_sts %u, rrgb_sts %u, voi_sts %u\n",
+		sts.scl_sts, sts.dil_sts, sts.wyuv_sts, sts.ryuv_sts,
+		sts.wrgb_sts, sts.rrgb_sts, sts.voi_sts); {
+		int *reg = (int *)iep_drvdata1->iep_base;
+		int i;
+
+		/* could not read validate data from address after base+0x40 */
+		for (i = 0; i < 0x40; i++) {
+			seq_printf(s, "%08x ", reg[i]);
+
+			if ((i + 1) % 4 == 0)
+				seq_printf(s, "\n");
+		}
+
+		seq_printf(s, "\n");
+	}
+
+	//mutex_unlock(&iep_service.mutex);
+
+	return 0;
+}
+
+static int proc_iep_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, proc_iep_show, NULL);
+}
+
+static const struct proc_ops proc_iep_fops = {
+	.proc_open	= proc_iep_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
+};
+
+static int __init iep_proc_init(void)
+{
+	proc_create("iep", 0, NULL, &proc_iep_fops);
+	return 0;
+}
+
+static void __exit iep_proc_release(void)
+{
+	remove_proc_entry("iep", NULL);
+}
+#endif
+
+#ifdef IEP_TEST_CASE
+void iep_test_case0(void);
+#endif
+
+static int __init iep_init(void)
+{
+	int ret;
+
+	if ((ret = platform_driver_register(&iep_driver)) != 0) {
+		IEP_ERR("Platform device register failed (%d).\n", ret);
+		return ret;
+	}
+
+#ifdef CONFIG_PROC_FS
+	iep_proc_init();
+#endif
+
+	IEP_INFO("Module initialized.\n");
+
+#ifdef IEP_TEST_CASE
+	iep_test_case0();
+#endif
+
+	return 0;
+}
+
+static void __exit iep_exit(void)
+{
+	IEP_ERR("%s IN\n", __func__);
+#ifdef CONFIG_PROC_FS
+	iep_proc_release();
+#endif
+
+	iep_power_off();
+	platform_driver_unregister(&iep_driver);
+}
+
+module_init(iep_init);
+module_exit(iep_exit);
+
+/* Module information */
+MODULE_AUTHOR("ljf@rock-chips.com");
+MODULE_DESCRIPTION("Driver for iep device");
+MODULE_LICENSE("GPL");
+MODULE_IMPORT_NS(DMA_BUF);
+
+#ifdef IEP_TEST_CASE
+
+/*this test just test for iep , not test iep's iommu
+ *so dts need cancel iommus handle
+ */
+
+#include "yuv420sp_480x480_interlaced.h"
+#include "yuv420sp_480x480_deinterlaced_i2o1.h"
+
+//unsigned char tmp_buf[480*480*3/2];
+
+void iep_test_case0(void)
+{
+	struct IEP_MSG msg;
+	iep_session session;
+	unsigned int phy_src, phy_tmp;
+	int i;
+	int ret = 0;
+	unsigned char *tmp_buf;
+
+	tmp_buf = kmalloc(480 * 480 * 3 / 2, GFP_KERNEL);
+
+	session.pid	= current->pid;
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.ready);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	list_add_tail(&session.list_session, &iep_service.session);
+	atomic_set(&session.task_running, 0);
+	atomic_set(&session.num_done, 0);
+
+	memset(&msg, 0, sizeof(struct IEP_MSG));
+	memset(tmp_buf, 0xCC, 480 * 480 * 3 / 2);
+
+#ifdef CONFIG_ARM
+	dmac_flush_range(&yuv420sp_480x480_interlaced[0],
+			 &yuv420sp_480x480_interlaced[480 * 480 * 3 / 2]);
+	outer_flush_range(virt_to_phys(&yuv420sp_480x480_interlaced[0]),
+		virt_to_phys(&yuv420sp_480x480_interlaced[480 * 480 * 3 / 2]));
+
+	dmac_flush_range(&tmp_buf[0], &tmp_buf[480 * 480 * 3 / 2]);
+	outer_flush_range(virt_to_phys(&tmp_buf[0]), virt_to_phys(&tmp_buf[480 * 480 * 3 / 2]));
+#elif defined(CONFIG_ARM64)
+	__dma_flush_area(&yuv420sp_480x480_interlaced[0], 480 * 480 * 3 / 2);
+	__dma_flush_area(&tmp_buf[0], 480 * 480 * 3 / 2);
+#endif
+
+	phy_src = virt_to_phys(&yuv420sp_480x480_interlaced[0]);
+	phy_tmp = virt_to_phys(&tmp_buf[0]);
+
+	IEP_INFO("*********** IEP MSG GENARATE ************\n");
+
+	msg.src.act_w = 480;
+	msg.src.act_h = 480;
+	msg.src.x_off = 0;
+	msg.src.y_off = 0;
+	msg.src.vir_w = 480;
+	msg.src.vir_h = 480;
+	msg.src.format = IEP_FORMAT_YCbCr_420_SP;
+	msg.src.mem_addr = phy_src;
+	msg.src.uv_addr  = (phy_src + 480 * 480);
+	msg.src.v_addr = 0;
+
+	msg.dst.act_w = 480;
+	msg.dst.act_h = 480;
+	msg.dst.x_off = 0;
+	msg.dst.y_off = 0;
+	msg.dst.vir_w = 480;
+	msg.dst.vir_h = 480;
+	msg.dst.format = IEP_FORMAT_YCbCr_420_SP;
+	msg.dst.mem_addr = phy_tmp;
+	msg.dst.uv_addr = (phy_tmp + 480 * 480);
+	msg.dst.v_addr = 0;
+
+	msg.dein_mode = IEP_DEINTERLACE_MODE_I2O1;
+	msg.field_order = FIELD_ORDER_BOTTOM_FIRST;
+
+	IEP_INFO("*********** IEP TEST CASE 0  ************\n");
+
+	iep_config(&session, &msg);
+	iep_try_set_reg();
+	if (0 > iep_get_result_sync(&session)) {
+		IEP_INFO("%s failed, timeout\n", __func__);
+		ret = -ETIMEDOUT;
+	}
+
+	mdelay(10);
+
+	IEP_INFO("*********** RESULT CHECKING  ************\n");
+
+	for (i = 0; i < 480 * 480 * 3 / 2; i++) {
+		if (tmp_buf[i] != yuv420sp_480x480_deinterlaced_i2o1[i]) {
+			IEP_INFO("diff occur position %d, 0x%02x 0x%02x\n", i, tmp_buf[i], yuv420sp_480x480_deinterlaced_i2o1[i]);
+
+			if (i > 10) {
+				iep_dump();
+				break;
+			}
+		}
+	}
+
+	if (i == 480 * 480 * 3 / 2)
+		IEP_INFO("IEP pass the checking\n");
+}
+
+#endif
diff --git a/drivers/video/rockchip/iep/iep_drv.h b/drivers/video/rockchip/iep/iep_drv.h
new file mode 100644
index 0000000000000..19160c6ffccde
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep_drv.h
@@ -0,0 +1,159 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef IEP_DRV_H_
+#define IEP_DRV_H_
+
+#include <linux/device.h>
+#include <linux/miscdevice.h>
+#include <linux/mutex.h>
+
+#include "iep.h"
+
+#define IEP_REG_LEN         0x100
+#define IEP_CMD_REG_LEN     0xE
+#define IEP_ADD_REG_LEN     0xE0
+#define IEP_RAW_REG_LEN     0xA
+#define IEP_SYS_REG_LEN     0x6
+#define IEP_CNF_REG_LEN     0x2
+
+#define IEP_CNF_REG_BASE    0x0
+#define IEP_SYS_REG_BASE    0x2
+#define IEP_CMD_REG_BASE    0x8
+#define IEP_ADD_REG_BASE    0x20
+#define IEP_RAW_REG_BASE    0x16
+
+struct iep_parameter_req {
+	struct iep_img src;
+	struct iep_img dst;
+};
+
+struct iep_parameter_deinterlace {
+	struct iep_img src1;
+	struct iep_img dst1;
+
+	struct iep_img src_itemp;
+	struct iep_img src_ftemp;
+
+	struct iep_img dst_itemp;
+	struct iep_img dst_ftemp;
+
+	u8 dein_mode;
+
+	// deinterlace high frequency
+	u8 dein_high_fre_en;
+	u8 dein_high_fre_fct;
+
+	// deinterlace edge interpolation
+	u8 dein_ei_mode;
+	u8 dein_ei_smooth;
+	u8 dein_ei_sel;
+	u8 dein_ei_radius;
+};
+
+struct iep_parameter_enhance {
+	u8 yuv_3D_denoise_en;
+
+	u8 yuv_enhance_en;
+	float yuv_enh_saturation; //0-1.992
+	float yuv_enh_contrast; //0-1.992
+	s8 yuv_enh_brightness; //-32<brightness<31
+	s8 yuv_enh_hue_angle; //0-30,value is 0 - 30
+
+	u8 video_mode; //0-3
+	u8 color_bar_y; //0-127
+	u8 color_bar_u; //0-127
+	u8 color_bar_v; //0-127
+
+	u8 rgb_enhance_en;
+
+	u8 rgb_cg_en; //sw_rgb_con_gam_en
+	double cg_rr;
+	double cg_rg;
+	double cg_rb;
+	u8 rgb_color_enhance_en; //sw_rgb_color_enh_en
+	float rgb_enh_coe; //0-3.96875
+};
+
+struct iep_parameter_scale {
+	u8 scale_up_mode;
+};
+
+struct iep_parameter_convert {
+	u8 dither_up_en;
+	u8 dither_down_en; //not to be used
+
+	u8 yuv2rgb_mode;
+	u8 rgb2yuv_mode;
+
+	u8 global_alpha_value;
+
+	u8 rgb2yuv_clip_en;
+	u8 yuv2rgb_clip_en;
+};
+
+typedef struct iep_session {
+	/* a linked list of data so we can access them for debugging */
+	struct list_head    list_session;
+	/* a linked list of register data waiting for process */
+	struct list_head    waiting;
+	/* a linked list of register data in ready */
+	struct list_head    ready;
+	/* a linked list of register data in processing */
+	struct list_head    running;
+	/* all coommand this thread done */
+	atomic_t            done;
+	wait_queue_head_t   wait;
+	pid_t               pid;
+	atomic_t            task_running;
+	atomic_t            num_done;
+} iep_session;
+
+typedef struct iep_service_info {
+	struct mutex        lock;
+	struct timer_list	timer;          /* timer for power off */
+	struct list_head	waiting;        /* link to link_reg in struct iep_reg */
+	atomic_t            waitcnt;
+	struct list_head    ready;          /* link to link_reg in struct iep_reg */
+	struct list_head	running;        /* link to link_reg in struct iep_reg */
+	struct list_head	done;           /* link to link_reg in struct iep_reg */
+	struct list_head	session;        /* link to list_session in struct vpu_session */
+	atomic_t		    total_running;
+
+	struct iep_reg      *reg;
+	bool                enable;
+
+	struct mutex	    mutex;  // mutex
+
+	struct iep_iommu_info *iommu_info;
+
+	struct device *iommu_dev;
+	u32 alloc_type;
+} iep_service_info;
+
+struct iep_reg {
+	iep_session *session;
+	struct list_head 	session_link;      /* link to rga service session */
+	struct list_head 	status_link;       /* link to register set list */
+	uint32_t 			reg[0x300];
+	bool                dpi_en;
+	int                 off_x;
+	int                 off_y;
+	int                 act_width;
+	int                 act_height;
+	int                 vir_width;
+	int                 vir_height;
+	int                 layer;
+	unsigned int        format;
+	struct list_head    mem_region_list;
+};
+
+struct iep_mem_region {
+	struct list_head srv_lnk;
+	struct list_head reg_lnk;
+	struct list_head session_lnk;
+	unsigned long iova;              /* virtual address for iommu */
+	unsigned long len;
+	int hdl;
+};
+
+#endif
+
diff --git a/drivers/video/rockchip/iep/iep_iommu_drm.c b/drivers/video/rockchip/iep/iep_iommu_drm.c
new file mode 100644
index 0000000000000..654459a908ff6
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep_iommu_drm.c
@@ -0,0 +1,464 @@
+/*
+ * Copyright (C) 2016 Fuzhou Rockchip Electronics Co., Ltd
+ * author: Jung Zhao jung.zhao@rock-chips.com
+ *         Randy Li, randy.li@rock-chips.com
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#include <drm/drm_device.h>
+#include <linux/dma-buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/iommu.h>
+#include <linux/kref.h>
+#include <linux/slab.h>
+
+#include "iep_iommu_ops.h"
+
+struct iep_drm_buffer {
+	struct list_head list;
+	struct dma_buf *dma_buf;
+	union {
+		unsigned long iova;
+		unsigned long phys;
+	};
+	unsigned long size;
+	int index;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	struct page **pages;
+	struct kref ref;
+	struct iep_iommu_session_info *session_info;
+};
+
+struct iep_iommu_drm_info {
+	struct iommu_domain *domain;
+	bool attached;
+};
+
+static struct iep_drm_buffer *
+iep_drm_get_buffer_no_lock(struct iep_iommu_session_info *session_info,
+			   int idx)
+{
+	struct iep_drm_buffer *drm_buffer = NULL, *n;
+
+	list_for_each_entry_safe(drm_buffer, n, &session_info->buffer_list,
+				 list) {
+		if (drm_buffer->index == idx)
+			return drm_buffer;
+	}
+
+	return NULL;
+}
+
+static struct iep_drm_buffer *
+iep_drm_get_buffer_fd_no_lock(struct iep_iommu_session_info *session_info,
+			      int fd)
+{
+	struct iep_drm_buffer *drm_buffer = NULL, *n;
+	struct dma_buf *dma_buf = NULL;
+
+	dma_buf = dma_buf_get(fd);
+
+	list_for_each_entry_safe(drm_buffer, n, &session_info->buffer_list,
+				 list) {
+		if (drm_buffer->dma_buf == dma_buf) {
+			dma_buf_put(dma_buf);
+			return drm_buffer;
+		}
+	}
+
+	dma_buf_put(dma_buf);
+
+	return NULL;
+}
+
+static void iep_drm_detach(struct iep_iommu_info *iommu_info)
+{
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+	struct device *dev = iommu_info->dev;
+	struct iommu_domain *domain = drm_info->domain;
+
+	mutex_lock(&iommu_info->iommu_mutex);
+
+	if (!drm_info->attached) {
+		mutex_unlock(&iommu_info->iommu_mutex);
+		return;
+	}
+
+	iommu_detach_device(domain, dev);
+	drm_info->attached = false;
+
+	mutex_unlock(&iommu_info->iommu_mutex);
+}
+
+static int iep_drm_attach_unlock(struct iep_iommu_info *iommu_info)
+{
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+	struct device *dev = iommu_info->dev;
+	struct iommu_domain *domain = drm_info->domain;
+	int ret = 0;
+
+	ret = dma_set_coherent_mask(dev, DMA_BIT_MASK(32));
+	if (ret)
+		return ret;
+
+	dma_set_max_seg_size(dev, DMA_BIT_MASK(32));
+	ret = iommu_attach_device(domain, dev);
+	if (ret) {
+		dev_err(dev, "Failed to attach iommu device\n");
+		return ret;
+	}
+
+	return ret;
+}
+
+static int iep_drm_attach(struct iep_iommu_info *iommu_info)
+{
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+	int ret;
+
+	mutex_lock(&iommu_info->iommu_mutex);
+
+	if (drm_info->attached) {
+		mutex_unlock(&iommu_info->iommu_mutex);
+		return 0;
+	}
+
+	ret = iep_drm_attach_unlock(iommu_info);
+	if (ret) {
+		mutex_unlock(&iommu_info->iommu_mutex);
+		return ret;
+	}
+
+	drm_info->attached = true;
+
+	mutex_unlock(&iommu_info->iommu_mutex);
+
+	return ret;
+}
+
+static void iep_drm_clear_map(struct kref *ref)
+{
+	struct iep_drm_buffer *drm_buffer =
+		container_of(ref, struct iep_drm_buffer, ref);
+	struct iep_iommu_session_info *session_info =
+		drm_buffer->session_info;
+	struct iep_iommu_info *iommu_info = session_info->iommu_info;
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+	struct device *dev = session_info->dev;
+	struct iommu_domain *domain = drm_info->domain;
+
+	mutex_lock(&iommu_info->iommu_mutex);
+	drm_info = session_info->iommu_info->private;
+	if (!drm_info->attached) {
+		if (iep_drm_attach_unlock(session_info->iommu_info))
+			dev_err(dev, "can't clea map, attach iommu failed.\n");
+	}
+
+	if (drm_buffer->attach) {
+		dma_buf_unmap_attachment(drm_buffer->attach, drm_buffer->sgt,
+					 DMA_BIDIRECTIONAL);
+		dma_buf_detach(drm_buffer->dma_buf, drm_buffer->attach);
+		dma_buf_put(drm_buffer->dma_buf);
+		drm_buffer->attach = NULL;
+	}
+
+	if (!drm_info->attached)
+		iommu_detach_device(domain, dev);
+
+	mutex_unlock(&iommu_info->iommu_mutex);
+}
+
+static void vcdoec_drm_dump_info(struct iep_iommu_session_info *session_info)
+{
+	struct iep_drm_buffer *drm_buffer = NULL, *n;
+
+	vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_OPS_DUMP,
+			"still there are below buffers stored in list\n");
+	list_for_each_entry_safe(drm_buffer, n, &session_info->buffer_list,
+				 list) {
+		vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_OPS_DUMP,
+				"index %d drm_buffer dma_buf %p\n",
+				drm_buffer->index,
+				drm_buffer->dma_buf);
+	}
+}
+
+static int iep_drm_free(struct iep_iommu_session_info *session_info,
+			int idx)
+{
+	struct device *dev = session_info->dev;
+	/* please double-check all maps have been release */
+	struct iep_drm_buffer *drm_buffer;
+
+	mutex_lock(&session_info->list_mutex);
+	drm_buffer = iep_drm_get_buffer_no_lock(session_info, idx);
+
+	if (!drm_buffer) {
+		dev_err(dev, "can not find %d buffer in list\n", idx);
+		mutex_unlock(&session_info->list_mutex);
+
+		return -EINVAL;
+	}
+
+	if (kref_read(&drm_buffer->ref) == 0) {
+		dma_buf_put(drm_buffer->dma_buf);
+		list_del_init(&drm_buffer->list);
+		kfree(drm_buffer);
+		session_info->buffer_nums--;
+		vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_NORMAL,
+			"buffer nums %d\n", session_info->buffer_nums);
+	}
+	mutex_unlock(&session_info->list_mutex);
+
+	return 0;
+}
+
+static int
+iep_drm_unmap_iommu(struct iep_iommu_session_info *session_info,
+		    int idx)
+{
+	struct device *dev = session_info->dev;
+	struct iep_drm_buffer *drm_buffer;
+
+	mutex_lock(&session_info->list_mutex);
+	drm_buffer = iep_drm_get_buffer_no_lock(session_info, idx);
+	mutex_unlock(&session_info->list_mutex);
+
+	if (!drm_buffer) {
+		dev_err(dev, "can not find %d buffer in list\n", idx);
+		return -EINVAL;
+	}
+
+	kref_put(&drm_buffer->ref, iep_drm_clear_map);
+
+	return 0;
+}
+
+static int iep_drm_map_iommu(struct iep_iommu_session_info *session_info,
+			     int idx,
+			     unsigned long *iova,
+			     unsigned long *size)
+{
+	struct device *dev = session_info->dev;
+	struct iep_drm_buffer *drm_buffer;
+
+	mutex_lock(&session_info->list_mutex);
+	drm_buffer = iep_drm_get_buffer_no_lock(session_info, idx);
+	mutex_unlock(&session_info->list_mutex);
+
+	if (!drm_buffer) {
+		dev_err(dev, "can not find %d buffer in list\n", idx);
+		return -EINVAL;
+	}
+
+	kref_get(&drm_buffer->ref);
+	if (iova)
+		*iova = drm_buffer->iova;
+	if (size)
+		*size = drm_buffer->size;
+	return 0;
+}
+
+static int
+iep_drm_free_fd(struct iep_iommu_session_info *session_info, int fd)
+{
+	/* please double-check all maps have been release */
+	struct iep_drm_buffer *drm_buffer = NULL;
+
+	mutex_lock(&session_info->list_mutex);
+	drm_buffer = iep_drm_get_buffer_fd_no_lock(session_info, fd);
+
+	if (!drm_buffer) {
+		vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_NORMAL,
+				"can not find %d buffer in list\n", fd);
+		mutex_unlock(&session_info->list_mutex);
+
+		return -EINVAL;
+	}
+	mutex_unlock(&session_info->list_mutex);
+
+	iep_drm_unmap_iommu(session_info, drm_buffer->index);
+
+	mutex_lock(&session_info->list_mutex);
+	if (kref_read(&drm_buffer->ref) == 0) {
+		dma_buf_put(drm_buffer->dma_buf);
+		list_del_init(&drm_buffer->list);
+		kfree(drm_buffer);
+		session_info->buffer_nums--;
+		vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_NORMAL,
+				"buffer nums %d\n", session_info->buffer_nums);
+	}
+	mutex_unlock(&session_info->list_mutex);
+
+	return 0;
+}
+
+static void
+iep_drm_clear_session(struct iep_iommu_session_info *session_info)
+{
+	struct iep_drm_buffer *drm_buffer = NULL, *n;
+
+	list_for_each_entry_safe(drm_buffer, n, &session_info->buffer_list,
+				 list) {
+		kref_put(&drm_buffer->ref, iep_drm_clear_map);
+		iep_drm_free(session_info, drm_buffer->index);
+	}
+}
+
+static int iep_drm_import(struct iep_iommu_session_info *session_info,
+			  int fd)
+{
+	struct iep_drm_buffer *drm_buffer = NULL, *n;
+	struct iep_iommu_info *iommu_info = session_info->iommu_info;
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+	struct iommu_domain *domain = drm_info->domain;
+	struct device *dev = session_info->dev;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	struct dma_buf *dma_buf;
+	int ret = 0;
+
+	dma_buf = dma_buf_get(fd);
+	if (IS_ERR(dma_buf)) {
+		ret = PTR_ERR(dma_buf);
+		return ret;
+	}
+
+	list_for_each_entry_safe(drm_buffer, n,
+				 &session_info->buffer_list, list) {
+		if (drm_buffer->dma_buf == dma_buf) {
+			dma_buf_put(dma_buf);
+			return drm_buffer->index;
+		}
+	}
+
+	drm_buffer = kzalloc(sizeof(*drm_buffer), GFP_KERNEL);
+	if (!drm_buffer) {
+		ret = -ENOMEM;
+		return ret;
+	}
+
+	drm_buffer->dma_buf = dma_buf;
+	drm_buffer->session_info = session_info;
+
+	kref_init(&drm_buffer->ref);
+
+	mutex_lock(&iommu_info->iommu_mutex);
+	drm_info = session_info->iommu_info->private;
+	if (!drm_info->attached) {
+		ret = iep_drm_attach_unlock(session_info->iommu_info);
+		if (ret)
+			goto fail_out;
+	}
+
+	attach = dma_buf_attach(drm_buffer->dma_buf, dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		goto fail_out;
+	}
+
+	get_dma_buf(drm_buffer->dma_buf);
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		goto fail_detach;
+	}
+
+	drm_buffer->iova = sg_dma_address(sgt->sgl);
+	drm_buffer->size = drm_buffer->dma_buf->size;
+
+	drm_buffer->attach = attach;
+	drm_buffer->sgt = sgt;
+
+	if (!drm_info->attached)
+		iommu_detach_device(domain, dev);
+
+	mutex_unlock(&iommu_info->iommu_mutex);
+
+	INIT_LIST_HEAD(&drm_buffer->list);
+	mutex_lock(&session_info->list_mutex);
+	session_info->buffer_nums++;
+	vpu_iommu_debug(session_info->debug_level, DEBUG_IOMMU_NORMAL,
+			"buffer nums %d\n", session_info->buffer_nums);
+	drm_buffer->index = session_info->max_idx;
+	list_add_tail(&drm_buffer->list, &session_info->buffer_list);
+	session_info->max_idx++;
+	if ((session_info->max_idx & 0xfffffff) == 0)
+		session_info->max_idx = 0;
+	mutex_unlock(&session_info->list_mutex);
+
+	return drm_buffer->index;
+
+fail_detach:
+	dev_err(dev, "dmabuf map attach failed\n");
+	dma_buf_detach(drm_buffer->dma_buf, attach);
+	dma_buf_put(drm_buffer->dma_buf);
+fail_out:
+	kfree(drm_buffer);
+	mutex_unlock(&iommu_info->iommu_mutex);
+
+	return ret;
+}
+
+static int iep_drm_create(struct iep_iommu_info *iommu_info)
+{
+	struct iep_iommu_drm_info *drm_info;
+
+	iommu_info->private = kzalloc(sizeof(*drm_info),
+				      GFP_KERNEL);
+	drm_info = iommu_info->private;
+	if (!drm_info)
+		return -ENOMEM;
+
+	drm_info->domain = iommu_get_domain_for_dev(iommu_info->dev);
+	drm_info->attached = false;
+	if (!drm_info->domain) {
+		kfree(iommu_info->private);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int iep_drm_destroy(struct iep_iommu_info *iommu_info)
+{
+	struct iep_iommu_drm_info *drm_info = iommu_info->private;
+
+	iep_drm_detach(iommu_info);
+
+	kfree(drm_info);
+	iommu_info->private = NULL;
+
+	return 0;
+}
+
+static struct iep_iommu_ops drm_ops = {
+	.create = iep_drm_create,
+	.import = iep_drm_import,
+	.free = iep_drm_free,
+	.free_fd = iep_drm_free_fd,
+	.map_iommu = iep_drm_map_iommu,
+	.unmap_iommu = iep_drm_unmap_iommu,
+	.destroy = iep_drm_destroy,
+	.dump = vcdoec_drm_dump_info,
+	.attach = iep_drm_attach,
+	.detach = iep_drm_detach,
+	.clear = iep_drm_clear_session,
+};
+
+void iep_iommu_drm_set_ops(struct iep_iommu_info *iommu_info)
+{
+	if (!iommu_info)
+		return;
+	iommu_info->ops = &drm_ops;
+}
diff --git a/drivers/video/rockchip/iep/iep_iommu_ops.c b/drivers/video/rockchip/iep/iep_iommu_ops.c
new file mode 100644
index 0000000000000..e84772237b898
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep_iommu_ops.c
@@ -0,0 +1,244 @@
+/**
+ * Copyright (C) 2016 Fuzhou Rockchip Electronics Co., Ltd
+ * author: Jung Zhao jung.zhao@rock-chips.com
+ *         Randy Li, randy.li@rock-chips.com
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/slab.h>
+
+#include "iep_iommu_ops.h"
+
+static
+struct iep_iommu_session_info *iep_iommu_get_session_info
+	(struct iep_iommu_info *iommu_info, struct iep_session *session)
+{
+	struct iep_iommu_session_info *session_info = NULL, *n;
+
+	list_for_each_entry_safe(session_info, n, &iommu_info->session_list,
+				 head) {
+		if (session_info->session == session)
+			return session_info;
+	}
+
+	return NULL;
+}
+
+int iep_iommu_create(struct iep_iommu_info *iommu_info)
+{
+	if (!iommu_info || !iommu_info->ops || !iommu_info->ops->create)
+		return -EINVAL;
+
+	return iommu_info->ops->create(iommu_info);
+}
+
+int iep_iommu_import(struct iep_iommu_info *iommu_info,
+		     struct iep_session *session, int fd)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info || !iommu_info->ops ||
+	    !iommu_info->ops->import || !session)
+		return -EINVAL;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+	if (!session_info) {
+		session_info = kzalloc(sizeof(*session_info), GFP_KERNEL);
+		if (!session_info)
+			return -ENOMEM;
+
+		INIT_LIST_HEAD(&session_info->head);
+		INIT_LIST_HEAD(&session_info->buffer_list);
+		mutex_init(&session_info->list_mutex);
+		session_info->max_idx = 0;
+		session_info->session = session;
+		session_info->mmu_dev = iommu_info->mmu_dev;
+		session_info->dev = iommu_info->dev;
+		session_info->iommu_info = iommu_info;
+		session_info->buffer_nums = 0;
+		mutex_lock(&iommu_info->list_mutex);
+		list_add_tail(&session_info->head, &iommu_info->session_list);
+		mutex_unlock(&iommu_info->list_mutex);
+	}
+
+	session_info->debug_level = iommu_info->debug_level;
+
+	return iommu_info->ops->import(session_info, fd);
+}
+
+int iep_iommu_free(struct iep_iommu_info *iommu_info,
+		   struct iep_session *session, int idx)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return -EINVAL;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->free || !session_info)
+		return -EINVAL;
+
+	return iommu_info->ops->free(session_info, idx);
+}
+
+int iep_iommu_free_fd(struct iep_iommu_info *iommu_info,
+		      struct iep_session *session, int fd)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return -EINVAL;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->free_fd || !session_info)
+		return -EINVAL;
+
+	return iommu_info->ops->free_fd(session_info, fd);
+}
+
+int iep_iommu_map_iommu(struct iep_iommu_info *iommu_info,
+			struct iep_session *session,
+			int idx, unsigned long *iova,
+			unsigned long *size)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return -EINVAL;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->map_iommu || !session_info)
+		return -EINVAL;
+
+	return iommu_info->ops->map_iommu(session_info, idx, iova, size);
+}
+
+int iep_iommu_unmap_iommu(struct iep_iommu_info *iommu_info,
+			  struct iep_session *session, int idx)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return -EINVAL;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->unmap_iommu || !session_info)
+		return -EINVAL;
+
+	return iommu_info->ops->unmap_iommu(session_info, idx);
+}
+
+int iep_iommu_destroy(struct iep_iommu_info *iommu_info)
+{
+	if (!iommu_info || !iommu_info->ops || !iommu_info->ops->destroy)
+		return -EINVAL;
+
+	return iommu_info->ops->destroy(iommu_info);
+}
+
+void iep_iommu_dump(struct iep_iommu_info *iommu_info,
+		    struct iep_session *session)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->dump || !session_info)
+		return;
+
+	iommu_info->ops->dump(session_info);
+}
+
+void iep_iommu_clear(struct iep_iommu_info *iommu_info,
+		     struct iep_session *session)
+{
+	struct iep_iommu_session_info *session_info = NULL;
+
+	if (!iommu_info)
+		return;
+
+	session_info = iep_iommu_get_session_info(iommu_info, session);
+
+	if (!iommu_info->ops || !iommu_info->ops->clear || !session_info)
+		return;
+
+	iommu_info->ops->clear(session_info);
+
+	mutex_lock(&iommu_info->list_mutex);
+	list_del_init(&session_info->head);
+	kfree(session_info);
+	mutex_unlock(&iommu_info->list_mutex);
+}
+
+int iep_iommu_attach(struct iep_iommu_info *iommu_info)
+{
+	if (!iommu_info || !iommu_info->ops || !iommu_info->ops->attach)
+		return 0;
+
+	return iommu_info->ops->attach(iommu_info);
+}
+
+void iep_iommu_detach(struct iep_iommu_info *iommu_info)
+{
+	if (!iommu_info || !iommu_info->ops || !iommu_info->ops->detach)
+		return;
+
+	return iommu_info->ops->detach(iommu_info);
+}
+
+struct iep_iommu_info *
+iep_iommu_info_create(struct device *dev,
+		      struct device *mmu_dev,
+		      int alloc_type)
+{
+	struct iep_iommu_info *iommu_info = NULL;
+
+	iommu_info = kzalloc(sizeof(*iommu_info), GFP_KERNEL);
+	if (!iommu_info)
+		return NULL;
+
+	iommu_info->dev = dev;
+	INIT_LIST_HEAD(&iommu_info->session_list);
+	mutex_init(&iommu_info->list_mutex);
+	mutex_init(&iommu_info->iommu_mutex);
+	switch (alloc_type) {
+#ifdef CONFIG_DRM
+	case ALLOCATOR_USE_DRM:
+		iep_iommu_drm_set_ops(iommu_info);
+		break;
+#endif
+	default:
+		iommu_info->ops = NULL;
+		break;
+	}
+
+	iommu_info->mmu_dev = mmu_dev;
+
+	iep_iommu_create(iommu_info);
+
+	return iommu_info;
+}
+
+int iep_iommu_info_destroy(struct iep_iommu_info *iommu_info)
+{
+	iep_iommu_destroy(iommu_info);
+	kfree(iommu_info);
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/iep/iep_iommu_ops.h b/drivers/video/rockchip/iep/iep_iommu_ops.h
new file mode 100644
index 0000000000000..9b71d53a51ebe
--- /dev/null
+++ b/drivers/video/rockchip/iep/iep_iommu_ops.h
@@ -0,0 +1,121 @@
+/**
+ * Copyright (C) 2016 Fuzhou Rockchip Electronics Co., Ltd
+ * author: Jung Zhao jung.zhao@rock-chips.com
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef __IEP_IOMMU_OPS_H__
+#define __IEP_IOMMU_OPS_H__
+
+#include <linux/platform_device.h>
+#include "iep_drv.h"
+
+#define BUFFER_LIST_MAX_NUMS	30
+
+#define ALLOCATOR_USE_ION		0x00000000
+#define ALLOCATOR_USE_DRM		0x00000001
+
+#define DEBUG_IOMMU_OPS_DUMP	0x00020000
+#define DEBUG_IOMMU_NORMAL	0x00040000
+
+#define vpu_iommu_debug_func(debug_level, type, fmt, args...)	\
+	do {							\
+		if (unlikely(debug_level & type)) {		\
+			pr_info("%s:%d: " fmt,			\
+				 __func__, __LINE__, ##args);	\
+		}						\
+	} while (0)
+#define vpu_iommu_debug(debug_level, type, fmt, args...)	\
+	do {							\
+		if (unlikely(debug_level & type)) {		\
+			pr_info(fmt, ##args);			\
+		}						\
+	} while (0)
+
+struct iep_iommu_info;
+struct iep_iommu_session_info;
+
+struct iep_iommu_ops {
+	int (*create)(struct iep_iommu_info *iommu_info);
+	int (*import)(struct iep_iommu_session_info *session_info, int fd);
+	int (*free)(struct iep_iommu_session_info *session_info, int idx);
+	int (*free_fd)(struct iep_iommu_session_info *session_info, int fd);
+	int (*map_iommu)(struct iep_iommu_session_info *session_info,
+			 int idx,
+			 unsigned long *iova, unsigned long *size);
+	int (*unmap_iommu)(struct iep_iommu_session_info *session_info,
+			   int idx);
+	int (*destroy)(struct iep_iommu_info *iommu_info);
+	void (*dump)(struct iep_iommu_session_info *session_info);
+	int (*attach)(struct iep_iommu_info *iommu_info);
+	void (*detach)(struct iep_iommu_info *iommu_info);
+	void (*clear)(struct iep_iommu_session_info *session_info);
+};
+
+struct iep_iommu_session_info {
+	struct list_head head;
+	struct iep_session *session;
+	int buffer_nums;
+	struct list_head buffer_list;
+	struct mutex list_mutex;
+	int max_idx;
+	struct device *dev;
+	struct device *mmu_dev;
+	struct iep_iommu_info *iommu_info;
+	int debug_level;
+};
+
+struct iep_iommu_info {
+	struct list_head session_list;
+	struct mutex list_mutex;
+	struct mutex iommu_mutex;
+	struct device *dev;
+	struct device *mmu_dev;
+	struct iep_iommu_ops *ops;
+	int debug_level;
+	void *private;
+};
+
+#ifdef CONFIG_DRM
+void iep_iommu_drm_set_ops(struct iep_iommu_info *iommu_info);
+#endif
+
+struct iep_iommu_info *iep_iommu_info_create(struct device *dev,
+					     struct device *mmu_dev,
+					     int alloc_type);
+int iep_iommu_info_destroy(struct iep_iommu_info *iommu_info);
+
+int iep_iommu_create(struct iep_iommu_info *iommu_info);
+int iep_iommu_import(struct iep_iommu_info *iommu_info,
+		     struct iep_session *session, int fd);
+int iep_iommu_free(struct iep_iommu_info *iommu_info,
+		   struct iep_session *session, int idx);
+int iep_iommu_free_fd(struct iep_iommu_info *iommu_info,
+		      struct iep_session *session, int fd);
+int iep_iommu_map_iommu(struct iep_iommu_info *iommu_info,
+			struct iep_session *session,
+			int idx,
+			unsigned long *iova,
+			unsigned long *size);
+int iep_iommu_unmap_iommu(struct iep_iommu_info *iommu_info,
+			  struct iep_session *session,
+			  int idx);
+int iep_iommu_destroy(struct iep_iommu_info *iommu_info);
+void iep_iommu_dump(struct iep_iommu_info *iommu_info,
+		    struct iep_session *session);
+void iep_iommu_clear(struct iep_iommu_info *iommu_info,
+		     struct iep_session *session);
+
+int iep_iommu_attach(struct iep_iommu_info *iommu_info);
+void iep_iommu_detach(struct iep_iommu_info *iommu_info);
+
+#endif
diff --git a/drivers/video/rockchip/mpp/Kconfig b/drivers/video/rockchip/mpp/Kconfig
new file mode 100644
index 0000000000000..92dfe93e7eafd
--- /dev/null
+++ b/drivers/video/rockchip/mpp/Kconfig
@@ -0,0 +1,83 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+menuconfig ROCKCHIP_MPP_SERVICE
+	tristate "mpp service framework"
+	depends on ARCH_ROCKCHIP
+	help
+	  rockchip mpp service framework.
+
+if ROCKCHIP_MPP_SERVICE
+
+config ROCKCHIP_MPP_PROC_FS
+	bool "mpp service procfs"
+	depends on PROC_FS
+	default y
+	help
+	  rockchip mpp service procfs.
+
+config ROCKCHIP_MPP_RKVDEC
+	bool "RKV decoder device driver"
+	help
+	  rockchip mpp rkv combo decoder and hevc decoder.
+
+config ROCKCHIP_MPP_RKVDEC2
+	bool "RKV decoder v2 device driver"
+	help
+	  rockchip mpp rkv combo decoder v2.
+
+config ROCKCHIP_MPP_RKVENC
+	bool "RKV encoder device driver"
+	help
+	  rockchip mpp rkv combo encoder.
+
+config ROCKCHIP_MPP_RKVENC2
+	bool "RKV encoder v2 device driver"
+	help
+	  rockchip mpp rkv combo encoder v2.
+
+config ROCKCHIP_MPP_VDPU1
+	bool "VPU decoder v1 device driver"
+	help
+	  rockchip mpp vpu decoder v1.
+
+config ROCKCHIP_MPP_VEPU1
+	bool "VPU encoder v1 device driver"
+	help
+	  rockchip mpp vpu encoder v1.
+
+config ROCKCHIP_MPP_VDPU2
+	bool "VPU decoder v2 device driver"
+	help
+	  rockchip mpp vpu decoder v2.
+
+config ROCKCHIP_MPP_VEPU2
+	bool "VPU encoder v2 device driver"
+	help
+	  rockchip mpp vpu encoder v2.
+
+config ROCKCHIP_MPP_IEP2
+	bool "IEP v2 device driver"
+	help
+	  rockchip iep v2.
+
+config ROCKCHIP_MPP_JPGDEC
+	bool "RKV jpeg decoder v1 device driver"
+	help
+	  rockchip mpp rkv jpeg decoder.
+
+config ROCKCHIP_MPP_JPGENC
+	bool "RKV jpeg encoder v1 device driver"
+	help
+	  rockchip mpp rkv jpeg encoder.
+
+config ROCKCHIP_MPP_AV1DEC
+	bool "AV1 decoder device driver"
+	help
+	  rockchip mpp av1 decoder.
+
+config ROCKCHIP_MPP_VDPP
+	bool "VDPP device driver"
+	help
+	  rockchip vdpp.
+
+endif
diff --git a/drivers/video/rockchip/mpp/Makefile b/drivers/video/rockchip/mpp/Makefile
new file mode 100644
index 0000000000000..0be859eb1ae03
--- /dev/null
+++ b/drivers/video/rockchip/mpp/Makefile
@@ -0,0 +1,32 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+MPP_GIT_REVISION := \
+	$(shell git log -1 --no-decorate --date=short \
+	--pretty=format:"%h author: %<|(30)%an %cd %s" -- $(src) || \
+	echo -n "unknown mpp version for missing VCS info")
+
+MPP_REVISION_0 := $(subst \,\\\,$(MPP_GIT_REVISION))
+MPP_REVISION   := $(subst ",\\\",$(MPP_REVISION_0))
+
+rk_vcodec-objs := mpp_service.o mpp_common.o mpp_iommu.o
+CFLAGS_mpp_service.o += -DMPP_VERSION="\"$(MPP_REVISION)\""
+
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_RKVDEC) += mpp_rkvdec.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_RKVDEC2) += mpp_rkvdec2.o mpp_rkvdec2_link.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_RKVENC) += mpp_rkvenc.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_RKVENC2) += mpp_rkvenc2.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_VDPU1)  += mpp_vdpu1.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_VEPU1)  += mpp_vepu1.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_VDPU2)  += mpp_vdpu2.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_VEPU2)  += mpp_vepu2.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_IEP2)   += mpp_iep2.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_JPGDEC) += mpp_jpgdec.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_JPGENC) += mpp_jpgenc.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_AV1DEC) += mpp_av1dec.o
+rk_vcodec-$(CONFIG_ROCKCHIP_MPP_VDPP)   += mpp_vdpp.o
+
+# hack for workaround
+rk_vcodec-$(CONFIG_CPU_PX30) += hack/mpp_hack_px30.o
+rk_vcodec-$(CONFIG_CPU_RK3576) += hack/mpp_hack_rk3576.o
+
+obj-$(CONFIG_ROCKCHIP_MPP_SERVICE) += rk_vcodec.o
diff --git a/drivers/video/rockchip/mpp/hack/mpp_hack_px30.c b/drivers/video/rockchip/mpp/hack/mpp_hack_px30.c
new file mode 100644
index 0000000000000..a8e1a457bf975
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_hack_px30.c
@@ -0,0 +1,245 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/regmap.h>
+
+#include <soc/rockchip/pm_domains.h>
+
+#include "../mpp_debug.h"
+#include "../mpp_common.h"
+#include "../mpp_iommu.h"
+#include "mpp_hack_px30.h"
+#include <soc/rockchip/rockchip_iommu.h>
+
+#define RK_MMU_DTE_ADDR			0x00 /* Directory table address */
+#define RK_MMU_STATUS			0x04
+#define RK_MMU_COMMAND			0x08
+#define RK_MMU_INT_MASK			0x1C /* IRQ enable */
+
+/* RK_MMU_COMMAND command values */
+#define RK_MMU_CMD_ENABLE_PAGING	0 /* Enable memory translation */
+#define RK_MMU_CMD_DISABLE_PAGING	1 /* Disable memory translation */
+#define RK_MMU_CMD_ENABLE_STALL		2 /* Stall paging to allow other cmds */
+#define RK_MMU_CMD_DISABLE_STALL	3 /* Stop stall re-enables paging */
+#define RK_MMU_CMD_ZAP_CACHE		4 /* Shoot down entire IOTLB */
+#define RK_MMU_CMD_PAGE_FAULT_DONE	5 /* Clear page fault */
+#define RK_MMU_CMD_FORCE_RESET		6 /* Reset all registers */
+
+/* RK_MMU_INT_* register fields */
+#define RK_MMU_IRQ_MASK			0x03
+/* RK_MMU_STATUS fields */
+#define RK_MMU_STATUS_PAGING_ENABLED	BIT(0)
+#define RK_MMU_STATUS_STALL_ACTIVE	BIT(2)
+
+static bool mpp_iommu_is_paged(struct mpp_rk_iommu *iommu)
+{
+	int i;
+	u32 status;
+	bool active = true;
+
+	for (i = 0; i < iommu->mmu_num; i++) {
+		status = readl(iommu->bases[i] + RK_MMU_STATUS);
+		active &= !!(status & RK_MMU_STATUS_PAGING_ENABLED);
+	}
+
+	return active;
+}
+
+static u32 mpp_iommu_get_dte_addr(struct mpp_rk_iommu *iommu)
+{
+	return readl(iommu->bases[0] + RK_MMU_DTE_ADDR);
+}
+
+static int mpp_iommu_enable(struct mpp_rk_iommu *iommu)
+{
+	int i;
+
+	/* check iommu whether is paged */
+	iommu->is_paged = mpp_iommu_is_paged(iommu);
+	if (iommu->is_paged)
+		return 0;
+
+	/* enable stall */
+	for (i = 0; i < iommu->mmu_num; i++)
+		writel(RK_MMU_CMD_ENABLE_STALL,
+		       iommu->bases[i] + RK_MMU_COMMAND);
+	udelay(2);
+	/* force reset */
+	for (i = 0; i < iommu->mmu_num; i++)
+		writel(RK_MMU_CMD_FORCE_RESET,
+		       iommu->bases[i] + RK_MMU_COMMAND);
+	udelay(2);
+
+	for (i = 0; i < iommu->mmu_num; i++) {
+		/* restore dte and status */
+		writel(iommu->dte_addr,
+		       iommu->bases[i] + RK_MMU_DTE_ADDR);
+		/* zap cache */
+		writel(RK_MMU_CMD_ZAP_CACHE,
+		       iommu->bases[i] + RK_MMU_COMMAND);
+		/* irq mask */
+		writel(RK_MMU_IRQ_MASK,
+		       iommu->bases[i] + RK_MMU_INT_MASK);
+	}
+	udelay(2);
+	/* enable paging */
+	for (i = 0; i < iommu->mmu_num; i++)
+		writel(RK_MMU_CMD_ENABLE_PAGING,
+		       iommu->bases[i] + RK_MMU_COMMAND);
+	udelay(2);
+	/* disable stall */
+	for (i = 0; i < iommu->mmu_num; i++)
+		writel(RK_MMU_CMD_DISABLE_STALL,
+		       iommu->bases[i] + RK_MMU_COMMAND);
+	udelay(2);
+
+	/* iommu should be paging enable */
+	iommu->is_paged = mpp_iommu_is_paged(iommu);
+	if (!iommu->is_paged) {
+		mpp_err("iommu->base_addr=%08x enable failed\n",
+			iommu->base_addr[0]);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int mpp_iommu_disable(struct mpp_rk_iommu *iommu)
+{
+	int i;
+	u32 dte;
+
+	if (iommu->is_paged) {
+		dte = readl(iommu->bases[0] + RK_MMU_DTE_ADDR);
+		if (!dte)
+			return -EINVAL;
+		udelay(2);
+		/* enable stall */
+		for (i = 0; i < iommu->mmu_num; i++)
+			writel(RK_MMU_CMD_ENABLE_STALL,
+			       iommu->bases[i] + RK_MMU_COMMAND);
+		udelay(2);
+		/* disable paging */
+		for (i = 0; i < iommu->mmu_num; i++)
+			writel(RK_MMU_CMD_DISABLE_PAGING,
+			       iommu->bases[i] + RK_MMU_COMMAND);
+		udelay(2);
+		/* disable stall */
+		for (i = 0; i < iommu->mmu_num; i++)
+			writel(RK_MMU_CMD_DISABLE_STALL,
+			       iommu->bases[i] + RK_MMU_COMMAND);
+		udelay(2);
+	}
+
+	return 0;
+}
+
+int px30_workaround_combo_init(struct mpp_dev *mpp)
+{
+	struct mpp_rk_iommu *iommu = NULL, *loop = NULL, *n;
+	struct platform_device *pdev = mpp->iommu_info->pdev;
+
+	/* find whether exist in iommu link */
+	list_for_each_entry_safe(loop, n, &mpp->queue->mmu_list, link) {
+		if (loop->base_addr[0] == pdev->resource[0].start) {
+			iommu = loop;
+			break;
+		}
+	}
+	/* if not exist, add it */
+	if (!iommu) {
+		int i;
+		struct resource *res;
+		void __iomem *base;
+
+		iommu = devm_kzalloc(mpp->srv->dev, sizeof(*iommu), GFP_KERNEL);
+		for (i = 0; i < pdev->num_resources; i++) {
+			res = platform_get_resource(pdev, IORESOURCE_MEM, i);
+			if (!res)
+				continue;
+			base = devm_ioremap(&pdev->dev,
+					    res->start, resource_size(res));
+			if (IS_ERR(base))
+				continue;
+			iommu->base_addr[i] = res->start;
+			iommu->bases[i] = base;
+			iommu->mmu_num++;
+		}
+		iommu->grf_val = mpp->grf_info->val & MPP_GRF_VAL_MASK;
+		/*
+		 * switch grf ctrl bit to ensure working in current hardware
+		 */
+		mpp_set_grf(mpp->grf_info);
+		if (mpp->hw_ops->clk_on)
+			mpp->hw_ops->clk_on(mpp);
+		/*
+		 * ensure that iommu is enable, so that read valid dte value
+		 */
+		if (rockchip_iommu_is_enabled(mpp->dev))
+			iommu->dte_addr = mpp_iommu_get_dte_addr(iommu);
+		else {
+			rockchip_iommu_enable(mpp->dev);
+			iommu->dte_addr = mpp_iommu_get_dte_addr(iommu);
+			rockchip_iommu_disable(mpp->dev);
+		}
+		dev_err(mpp->dev, "%s dte_addr %08x\n", __func__, iommu->dte_addr);
+		if (mpp->hw_ops->clk_off)
+			mpp->hw_ops->clk_off(mpp);
+		INIT_LIST_HEAD(&iommu->link);
+		mutex_lock(&mpp->queue->mmu_lock);
+		list_add_tail(&iommu->link, &mpp->queue->mmu_list);
+		mutex_unlock(&mpp->queue->mmu_lock);
+	}
+	mpp->iommu_info->iommu = iommu;
+
+	return 0;
+}
+
+int px30_workaround_combo_switch_grf(struct mpp_dev *mpp)
+{
+	int ret = 0;
+	u32 curr_val;
+	u32 next_val;
+	bool pd_is_on;
+	struct mpp_rk_iommu *loop = NULL, *n;
+
+	if (!mpp->grf_info->grf || !mpp->grf_info->val)
+		return 0;
+
+	curr_val = mpp_get_grf(mpp->grf_info);
+	next_val = mpp->grf_info->val & MPP_GRF_VAL_MASK;
+	if (curr_val == next_val)
+		return 0;
+
+	pd_is_on = rockchip_pmu_pd_is_on(mpp->dev);
+	if (!pd_is_on)
+		rockchip_pmu_pd_on(mpp->dev);
+	mpp->hw_ops->clk_on(mpp);
+
+	list_for_each_entry_safe(loop, n, &mpp->queue->mmu_list, link) {
+		/* update iommu parameters */
+		if (loop->grf_val == curr_val)
+			loop->is_paged = mpp_iommu_is_paged(loop);
+		/* disable all iommu */
+		mpp_iommu_disable(loop);
+	}
+	mpp_set_grf(mpp->grf_info);
+	/* enable current iommu */
+	ret = mpp_iommu_enable(mpp->iommu_info->iommu);
+
+	mpp->hw_ops->clk_off(mpp);
+	if (!pd_is_on)
+		rockchip_pmu_pd_off(mpp->dev);
+
+	return ret;
+}
diff --git a/drivers/video/rockchip/mpp/hack/mpp_hack_px30.h b/drivers/video/rockchip/mpp/hack/mpp_hack_px30.h
new file mode 100644
index 0000000000000..c8402e30d810a
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_hack_px30.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#ifndef __ROCKCHIP_MPP_HACK_PX30_H__
+#define __ROCKCHIP_MPP_HACK_PX30_H__
+
+#ifdef CONFIG_CPU_PX30
+int px30_workaround_combo_init(struct mpp_dev *mpp);
+int px30_workaround_combo_switch_grf(struct mpp_dev *mpp);
+#else
+static inline int px30_workaround_combo_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int px30_workaround_combo_switch_grf(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+#endif
diff --git a/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.c b/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.c
new file mode 100644
index 0000000000000..aade189bd3d5a
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.c
@@ -0,0 +1,193 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2024 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei <leo.ding@rock-chips.com>
+ */
+
+#include <linux/delay.h>
+
+#include "../mpp_rkvdec2.h"
+#include "../mpp_rkvdec2_link.h"
+
+#include "mpp_hack_rk3576.h"
+
+#define RK3576_HACK_DATA_RLC_OFFSET		(0)
+#define RK3576_HACK_DATA_PPS_OFFSET		(RK3576_HACK_DATA_RLC_OFFSET + 64)
+#define RK3576_HACK_DATA_RPS_OFFSET		(RK3576_HACK_DATA_PPS_OFFSET + 256)
+#define RK3576_HACK_DATA_RCB_STRMD_OFFSET	(RK3576_HACK_DATA_RPS_OFFSET + 256)
+#define RK3576_HACK_DATA_RCB_INTRA_OFFSET	(RK3576_HACK_DATA_RCB_STRMD_OFFSET + 256)
+#define RK3576_HACK_DATA_OUT_OFFSET		(RK3576_HACK_DATA_RCB_INTRA_OFFSET + 512)
+#define RK3576_HACK_DATA_COLMV_OFFSET		(RK3576_HACK_DATA_OUT_OFFSET + 1536)
+
+#define RK3576_HACK_REGS_OFFSET			(4096)
+#define RK3576_HACK_REG_NODE_OFFSET		(RK3576_HACK_REGS_OFFSET)
+#define RK3576_HACK_REG_SEG0_OFFSET		(RK3576_HACK_REGS_OFFSET + 32)
+#define RK3576_HACK_REG_SEG1_OFFSET		(RK3576_HACK_REGS_OFFSET + 256)
+#define RK3576_HACK_REG_SEG2_OFFSET		(RK3576_HACK_REGS_OFFSET + 512)
+#define RK3576_HACK_REG_DEBUG_OFFSET		(RK3576_HACK_REGS_OFFSET + 1024)
+
+
+static const char rk3576_hack_h264_rlc_data[] = {
+	0x00, 0x00, 0x01, 0x65,
+	0x88, 0x82, 0x0b, 0x01,
+	0x2f, 0x08, 0xc5, 0x00,
+	0x01, 0x51, 0x78, 0xe0,
+	0x00, 0x24, 0xf7, 0x1c,
+	0x00, 0x04, 0xcc, 0xeb,
+	0x89, 0xd7, 0x80, 0x00,
+	0x00, 0x00, 0x00, 0x00,
+};
+
+static const char rk3576_hack_h264_pps_data[] = {
+	0x40, 0x26, 0x00, 0x10,
+	0x04, 0x08, 0x00, 0x08,
+	0x80, 0x01, 0x00, 0x00,
+	0x00, 0x40, 0x01, 0xd8,
+	0x07, 0x7c, 0x7a, 0x00,
+	0x00, 0x00, 0x00, 0x04,
+	0x00, 0x00, 0x00, 0x00,
+};
+
+static bool rk3576_hack_flag;
+
+static bool is_rk3576(struct device *dev)
+{
+	int ret;
+
+	ret = device_property_match_string(dev, "compatible", "rockchip,rkv-decoder-rk3576");
+
+	return (ret < 0) ? false : true;
+}
+
+int rk3576_workaround_init(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	u32 *reg;
+
+	/* check current soc is rk3576 */
+	rk3576_hack_flag = is_rk3576(mpp->dev);
+	if (!rk3576_hack_flag)
+		return 0;
+
+	/* alloc buffer for node */
+	dec->fix = mpp_dma_alloc(mpp->dev, 2 * PAGE_SIZE);
+	if (!dec->fix) {
+		dev_err(mpp->dev, "failed to create buffer for hack\n");
+		return -ENOMEM;
+	}
+	memset(dec->fix->vaddr, 0, dec->fix->size);
+
+	/* set rlc_in */
+	memcpy(dec->fix->vaddr + RK3576_HACK_DATA_RLC_OFFSET,
+		rk3576_hack_h264_rlc_data, sizeof(rk3576_hack_h264_rlc_data));
+
+	/* set sps_pps */
+	memcpy(dec->fix->vaddr + RK3576_HACK_DATA_PPS_OFFSET,
+		rk3576_hack_h264_pps_data, sizeof(rk3576_hack_h264_pps_data));
+
+	/* set core registers */
+	reg = dec->fix->vaddr + RK3576_HACK_REGS_OFFSET;
+	reg[8]  = 0x00000001; // dec_mode
+	reg[13] = 0x0000ffff; // core time out
+	reg[16] = 0x00000101; // disable error proc
+	reg[20] = 0xffffffff; // cabac error low bits
+	reg[21] = 0x3ff3ffff; // cabac error high bits
+
+	reg[64] = RK3576_HACK_MAGIC; // magic number
+	reg[65] = 0x00000000; // stream start bit
+	reg[66] = 0x00000020; // stream_len
+	reg[67] = 0x000000a8; // global_len
+	reg[68] = 0x00000002; // hor_virstride
+	reg[69] = 0x00000002; // ver_virstride
+	reg[70] = 0x00000040; // y_virstride
+
+	reg[128] = dec->fix->iova + RK3576_HACK_DATA_RLC_OFFSET; // rlc_base
+	reg[129] = dec->fix->iova + RK3576_HACK_DATA_RPS_OFFSET; // rps_base
+	reg[131] = dec->fix->iova + RK3576_HACK_DATA_PPS_OFFSET; // pps_base
+	reg[140] = dec->fix->iova + RK3576_HACK_DATA_RCB_STRMD_OFFSET; // streamd_rcb
+	reg[141] = 0x000000c0;
+	reg[148] = dec->fix->iova + RK3576_HACK_DATA_RCB_INTRA_OFFSET; // intra_rcb
+	reg[149] = 0x00000200;
+	reg[168] = dec->fix->iova + RK3576_HACK_DATA_OUT_OFFSET; // decout_base
+	reg[216] = dec->fix->iova + RK3576_HACK_DATA_COLMV_OFFSET; // colmv_base
+
+	/* set link node */
+	reg = dec->fix->vaddr + RK3576_HACK_REG_NODE_OFFSET;
+	reg[0] = dec->fix->iova + RK3576_HACK_DATA_OUT_OFFSET; // next link node
+	reg[1] = 0x76543212; // table info
+	reg[2] = dec->fix->iova + RK3576_HACK_REG_DEBUG_OFFSET; // debug regs
+	reg[3] = dec->fix->iova + RK3576_HACK_REG_SEG0_OFFSET; // seg0 regs
+	reg[4] = dec->fix->iova + RK3576_HACK_REG_SEG1_OFFSET; // seg1 regs
+	reg[5] = dec->fix->iova + RK3576_HACK_REG_SEG2_OFFSET; // seg2 regs
+
+	return 0;
+}
+
+int rk3576_workaround_exit(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	/* check current soc is rk3576 */
+	if (!rk3576_hack_flag)
+		return 0;
+
+	if (dec->fix) {
+		mpp_dma_free(dec->fix);
+		dec->fix = NULL;
+	}
+
+	return 0;
+}
+
+int rk3576_workaround_run(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link = dec->link_dec;
+	int ret;
+	u32 irq_val, status;
+
+	/* check current soc is rk3576 */
+	if (!rk3576_hack_flag || !dec->fix || !link)
+		return 0;
+
+	/* disable hardware irq */
+	writel_relaxed(0x00008000, link->reg_base + 0x58);
+
+	/* set link registers */
+	writel_relaxed(0x0007ffff, link->reg_base + 0x54); // set ip time out
+	writel_relaxed(0x00010001, link->reg_base + 0x00); // ccu mode
+	writel_relaxed(dec->fix->iova + RK3576_HACK_REG_NODE_OFFSET, link->reg_base + 0x4); // addr
+	writel_relaxed(0x00000001, link->reg_base + 0x08); // link mode
+	writel_relaxed(0x00000001, link->reg_base + 0x18); // link en
+
+	/* enable hardware */
+	wmb();
+	writel(0x01, link->reg_base + 0x0c); // config link
+
+	/* wait hardware */
+	ret = readl_relaxed_poll_timeout_atomic(link->reg_base + 0x48, irq_val, irq_val, 1, 500);
+	if (ret == -ETIMEDOUT) {
+		pr_err("%s timeout.\n", __func__);
+		// return ret;
+	} else {
+		status = readl(link->reg_base + 0x4c);
+		if (status & 0x3fe)
+			pr_err("%s not ready, status %08x.\n", __func__, status);
+	}
+	/* clear irq and status */
+	writel_relaxed(0xffff0000, link->reg_base + 0x48);
+	writel_relaxed(0xffff0000, link->reg_base + 0x4c);
+
+	/* reset register */
+	writel_relaxed(0x00000000, link->reg_base + 0x00); // ccu mode
+	writel_relaxed(0x00000000, link->reg_base + 0x08); // link mode
+	writel_relaxed(0x00000000, link->reg_base + 0x18); // link en
+
+	/* enable irq */
+	writel(0x00000000, link->reg_base + 0x58);
+	udelay(5);
+
+	return ret;
+}
diff --git a/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.h b/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.h
new file mode 100644
index 0000000000000..f386c184b89fc
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_hack_rk3576.h
@@ -0,0 +1,35 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2024 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei <leo.ding@rock-chips.com>
+ */
+
+#ifndef __ROCKCHIP_MPP_HACK_RK3576_H__
+#define __ROCKCHIP_MPP_HACK_RK3576_H__
+
+#define RK3576_HACK_MAGIC	(0x76543210)
+
+#ifdef CONFIG_CPU_RK3576
+int rk3576_workaround_init(struct mpp_dev *mpp);
+int rk3576_workaround_exit(struct mpp_dev *mpp);
+int rk3576_workaround_run(struct mpp_dev *mpp);
+#else
+static inline int rk3576_workaround_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rk3576_workaround_exit(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rk3576_workaround_run(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+#endif
diff --git a/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_hack_rk3568.c b/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_hack_rk3568.c
new file mode 100644
index 0000000000000..67f211f3d2537
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_hack_rk3568.c
@@ -0,0 +1,732 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Herman Chen <herman.chen@rock-chips.com>
+ */
+
+#include <linux/printk.h>
+
+#define FIX_RK3568_BUF_SIZE	(2 * PAGE_SIZE)
+#define RKDEC_HACK_DATA_RPS_OFFSET (128 * 1)
+#define RKDEC_HACK_DATA_PPS_OFFSET (128 * 2)
+#define RKDEC_HACK_DATA_RLC_OFFSET (128 * 3)
+#define RKDEC_HACK_DATA_OUT_OFFSET (128 * 4)
+#define RKDEC_HACK_DATA_COLMV_OFFSET (128 * 5)
+
+struct hack_info {
+	u32 data;
+	u32 offset;
+};
+
+static const char h264_cabac_tbl[] = {
+	0x14, 0xf1, 0x02, 0x36, 0x03, 0x4a, 0x14, 0xf1,
+	0x02, 0x36, 0x03, 0x4a, 0xe4, 0x7f, 0xe9, 0x68,
+	0xfa, 0x35, 0xff, 0x36, 0x07, 0x33, 0x17, 0x21,
+	0x17, 0x02, 0x15, 0x00, 0x01, 0x09, 0x00, 0x31,
+	0xdb, 0x76, 0x05, 0x39, 0xf3, 0x4e, 0xf5, 0x41,
+	0x01, 0x3e, 0x0c, 0x31, 0xfc, 0x49, 0x11, 0x32,
+	0x12, 0x40, 0x09, 0x2b, 0x1d, 0x00, 0x1a, 0x43,
+	0x10, 0x5a, 0x09, 0x68, 0xd2, 0x7f, 0xec, 0x68,
+	0x01, 0x43, 0xf3, 0x4e, 0xf5, 0x41, 0x01, 0x3e,
+	0xfa, 0x56, 0xef, 0x5f, 0xfa, 0x3d, 0x09, 0x2d,
+	0xfd, 0x45, 0xfa, 0x51, 0xf5, 0x60, 0x06, 0x37,
+	0x07, 0x43, 0xfb, 0x56, 0x02, 0x58, 0x00, 0x3a,
+	0xfd, 0x4c, 0xf6, 0x5e, 0x05, 0x36, 0x04, 0x45,
+	0xfd, 0x51, 0x00, 0x58, 0xf9, 0x43, 0xfb, 0x4a,
+	0xfc, 0x4a, 0xfb, 0x50, 0xf9, 0x48, 0x01, 0x3a,
+	0x00, 0x29, 0x00, 0x3f, 0x00, 0x3f, 0x00, 0x3f,
+	0xf7, 0x53, 0x04, 0x56, 0x00, 0x61, 0xf9, 0x48,
+	0x0d, 0x29, 0x03, 0x3e, 0x00, 0x2d, 0xfc, 0x4e,
+	0xfd, 0x60, 0xe5, 0x7e, 0xe4, 0x62, 0xe7, 0x65,
+	0xe9, 0x43, 0xe4, 0x52, 0xec, 0x5e, 0xf0, 0x53,
+	0xea, 0x6e, 0xeb, 0x5b, 0xee, 0x66, 0xf3, 0x5d,
+	0xe3, 0x7f, 0xf9, 0x5c, 0xfb, 0x59, 0xf9, 0x60,
+	0xf3, 0x6c, 0xfd, 0x2e, 0xff, 0x41, 0xff, 0x39,
+	0xf7, 0x5d, 0xfd, 0x4a, 0xf7, 0x5c, 0xf8, 0x57,
+	0xe9, 0x7e, 0x05, 0x36, 0x06, 0x3c, 0x06, 0x3b,
+	0x06, 0x45, 0xff, 0x30, 0x00, 0x44, 0xfc, 0x45,
+	0xf8, 0x58, 0xfe, 0x55, 0xfa, 0x4e, 0xff, 0x4b,
+	0xf9, 0x4d, 0x02, 0x36, 0x05, 0x32, 0xfd, 0x44,
+	0x01, 0x32, 0x06, 0x2a, 0xfc, 0x51, 0x01, 0x3f,
+	0xfc, 0x46, 0x00, 0x43, 0x02, 0x39, 0xfe, 0x4c,
+	0x0b, 0x23, 0x04, 0x40, 0x01, 0x3d, 0x0b, 0x23,
+	0x12, 0x19, 0x0c, 0x18, 0x0d, 0x1d, 0x0d, 0x24,
+	0xf6, 0x5d, 0xf9, 0x49, 0xfe, 0x49, 0x0d, 0x2e,
+	0x09, 0x31, 0xf9, 0x64, 0x09, 0x35, 0x02, 0x35,
+	0x05, 0x35, 0xfe, 0x3d, 0x00, 0x38, 0x00, 0x38,
+	0xf3, 0x3f, 0xfb, 0x3c, 0xff, 0x3e, 0x04, 0x39,
+	0xfa, 0x45, 0x04, 0x39, 0x0e, 0x27, 0x04, 0x33,
+	0x0d, 0x44, 0x03, 0x40, 0x01, 0x3d, 0x09, 0x3f,
+	0x07, 0x32, 0x10, 0x27, 0x05, 0x2c, 0x04, 0x34,
+	0x0b, 0x30, 0xfb, 0x3c, 0xff, 0x3b, 0x00, 0x3b,
+	0x16, 0x21, 0x05, 0x2c, 0x0e, 0x2b, 0xff, 0x4e,
+	0x00, 0x3c, 0x09, 0x45, 0x0b, 0x1c, 0x02, 0x28,
+	0x03, 0x2c, 0x00, 0x31, 0x00, 0x2e, 0x02, 0x2c,
+	0x02, 0x33, 0x00, 0x2f, 0x04, 0x27, 0x02, 0x3e,
+	0x06, 0x2e, 0x00, 0x36, 0x03, 0x36, 0x02, 0x3a,
+	0x04, 0x3f, 0x06, 0x33, 0x06, 0x39, 0x07, 0x35,
+	0x06, 0x34, 0x06, 0x37, 0x0b, 0x2d, 0x0e, 0x24,
+	0x08, 0x35, 0xff, 0x52, 0x07, 0x37, 0xfd, 0x4e,
+	0x0f, 0x2e, 0x16, 0x1f, 0xff, 0x54, 0x19, 0x07,
+	0x1e, 0xf9, 0x1c, 0x03, 0x1c, 0x04, 0x20, 0x00,
+	0x22, 0xff, 0x1e, 0x06, 0x1e, 0x06, 0x20, 0x09,
+	0x1f, 0x13, 0x1a, 0x1b, 0x1a, 0x1e, 0x25, 0x14,
+	0x1c, 0x22, 0x11, 0x46, 0x01, 0x43, 0x05, 0x3b,
+	0x09, 0x43, 0x10, 0x1e, 0x12, 0x20, 0x12, 0x23,
+	0x16, 0x1d, 0x18, 0x1f, 0x17, 0x26, 0x12, 0x2b,
+	0x14, 0x29, 0x0b, 0x3f, 0x09, 0x3b, 0x09, 0x40,
+	0xff, 0x5e, 0xfe, 0x59, 0xf7, 0x6c, 0xfa, 0x4c,
+	0xfe, 0x2c, 0x00, 0x2d, 0x00, 0x34, 0xfd, 0x40,
+	0xfe, 0x3b, 0xfc, 0x46, 0xfc, 0x4b, 0xf8, 0x52,
+	0xef, 0x66, 0xf7, 0x4d, 0x03, 0x18, 0x00, 0x2a,
+	0x00, 0x30, 0x00, 0x37, 0xfa, 0x3b, 0xf9, 0x47,
+	0xf4, 0x53, 0xf5, 0x57, 0xe2, 0x77, 0x01, 0x3a,
+	0xfd, 0x1d, 0xff, 0x24, 0x01, 0x26, 0x02, 0x2b,
+	0xfa, 0x37, 0x00, 0x3a, 0x00, 0x40, 0xfd, 0x4a,
+	0xf6, 0x5a, 0x00, 0x46, 0xfc, 0x1d, 0x05, 0x1f,
+	0x07, 0x2a, 0x01, 0x3b, 0xfe, 0x3a, 0xfd, 0x48,
+	0xfd, 0x51, 0xf5, 0x61, 0x00, 0x3a, 0x08, 0x05,
+	0x0a, 0x0e, 0x0e, 0x12, 0x0d, 0x1b, 0x02, 0x28,
+	0x00, 0x3a, 0xfd, 0x46, 0xfa, 0x4f, 0xf8, 0x55,
+	0x00, 0x00, 0xf3, 0x6a, 0xf0, 0x6a, 0xf6, 0x57,
+	0xeb, 0x72, 0xee, 0x6e, 0xf2, 0x62, 0xea, 0x6e,
+	0xeb, 0x6a, 0xee, 0x67, 0xeb, 0x6b, 0xe9, 0x6c,
+	0xe6, 0x70, 0xf6, 0x60, 0xf4, 0x5f, 0xfb, 0x5b,
+	0xf7, 0x5d, 0xea, 0x5e, 0xfb, 0x56, 0x09, 0x43,
+	0xfc, 0x50, 0xf6, 0x55, 0xff, 0x46, 0x07, 0x3c,
+	0x09, 0x3a, 0x05, 0x3d, 0x0c, 0x32, 0x0f, 0x32,
+	0x12, 0x31, 0x11, 0x36, 0x0a, 0x29, 0x07, 0x2e,
+	0xff, 0x33, 0x07, 0x31, 0x08, 0x34, 0x09, 0x29,
+	0x06, 0x2f, 0x02, 0x37, 0x0d, 0x29, 0x0a, 0x2c,
+	0x06, 0x32, 0x05, 0x35, 0x0d, 0x31, 0x04, 0x3f,
+	0x06, 0x40, 0xfe, 0x45, 0xfe, 0x3b, 0x06, 0x46,
+	0x0a, 0x2c, 0x09, 0x1f, 0x0c, 0x2b, 0x03, 0x35,
+	0x0e, 0x22, 0x0a, 0x26, 0xfd, 0x34, 0x0d, 0x28,
+	0x11, 0x20, 0x07, 0x2c, 0x07, 0x26, 0x0d, 0x32,
+	0x0a, 0x39, 0x1a, 0x2b, 0x0e, 0x0b, 0x0b, 0x0e,
+	0x09, 0x0b, 0x12, 0x0b, 0x15, 0x09, 0x17, 0xfe,
+	0x20, 0xf1, 0x20, 0xf1, 0x22, 0xeb, 0x27, 0xe9,
+	0x2a, 0xdf, 0x29, 0xe1, 0x2e, 0xe4, 0x26, 0xf4,
+	0x15, 0x1d, 0x2d, 0xe8, 0x35, 0xd3, 0x30, 0xe6,
+	0x41, 0xd5, 0x2b, 0xed, 0x27, 0xf6, 0x1e, 0x09,
+	0x12, 0x1a, 0x14, 0x1b, 0x00, 0x39, 0xf2, 0x52,
+	0xfb, 0x4b, 0xed, 0x61, 0xdd, 0x7d, 0x1b, 0x00,
+	0x1c, 0x00, 0x1f, 0xfc, 0x1b, 0x06, 0x22, 0x08,
+	0x1e, 0x0a, 0x18, 0x16, 0x21, 0x13, 0x16, 0x20,
+	0x1a, 0x1f, 0x15, 0x29, 0x1a, 0x2c, 0x17, 0x2f,
+	0x10, 0x41, 0x0e, 0x47, 0x08, 0x3c, 0x06, 0x3f,
+	0x11, 0x41, 0x15, 0x18, 0x17, 0x14, 0x1a, 0x17,
+	0x1b, 0x20, 0x1c, 0x17, 0x1c, 0x18, 0x17, 0x28,
+	0x18, 0x20, 0x1c, 0x1d, 0x17, 0x2a, 0x13, 0x39,
+	0x16, 0x35, 0x16, 0x3d, 0x0b, 0x56, 0x0c, 0x28,
+	0x0b, 0x33, 0x0e, 0x3b, 0xfc, 0x4f, 0xf9, 0x47,
+	0xfb, 0x45, 0xf7, 0x46, 0xf8, 0x42, 0xf6, 0x44,
+	0xed, 0x49, 0xf4, 0x45, 0xf0, 0x46, 0xf1, 0x43,
+	0xec, 0x3e, 0xed, 0x46, 0xf0, 0x42, 0xea, 0x41,
+	0xec, 0x3f, 0x09, 0xfe, 0x1a, 0xf7, 0x21, 0xf7,
+	0x27, 0xf9, 0x29, 0xfe, 0x2d, 0x03, 0x31, 0x09,
+	0x2d, 0x1b, 0x24, 0x3b, 0xfa, 0x42, 0xf9, 0x23,
+	0xf9, 0x2a, 0xf8, 0x2d, 0xfb, 0x30, 0xf4, 0x38,
+	0xfa, 0x3c, 0xfb, 0x3e, 0xf8, 0x42, 0xf8, 0x4c,
+	0xfb, 0x55, 0xfa, 0x51, 0xf6, 0x4d, 0xf9, 0x51,
+	0xef, 0x50, 0xee, 0x49, 0xfc, 0x4a, 0xf6, 0x53,
+	0xf7, 0x47, 0xf7, 0x43, 0xff, 0x3d, 0xf8, 0x42,
+	0xf2, 0x42, 0x00, 0x3b, 0x02, 0x3b, 0x15, 0xf3,
+	0x21, 0xf2, 0x27, 0xf9, 0x2e, 0xfe, 0x33, 0x02,
+	0x3c, 0x06, 0x3d, 0x11, 0x37, 0x22, 0x2a, 0x3e,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x14, 0xf1, 0x02, 0x36, 0x03, 0x4a, 0x14, 0xf1,
+	0x02, 0x36, 0x03, 0x4a, 0xe4, 0x7f, 0xe9, 0x68,
+	0xfa, 0x35, 0xff, 0x36, 0x07, 0x33, 0x16, 0x19,
+	0x22, 0x00, 0x10, 0x00, 0xfe, 0x09, 0x04, 0x29,
+	0xe3, 0x76, 0x02, 0x41, 0xfa, 0x47, 0xf3, 0x4f,
+	0x05, 0x34, 0x09, 0x32, 0xfd, 0x46, 0x0a, 0x36,
+	0x1a, 0x22, 0x13, 0x16, 0x28, 0x00, 0x39, 0x02,
+	0x29, 0x24, 0x1a, 0x45, 0xd3, 0x7f, 0xf1, 0x65,
+	0xfc, 0x4c, 0xfa, 0x47, 0xf3, 0x4f, 0x05, 0x34,
+	0x06, 0x45, 0xf3, 0x5a, 0x00, 0x34, 0x08, 0x2b,
+	0xfe, 0x45, 0xfb, 0x52, 0xf6, 0x60, 0x02, 0x3b,
+	0x02, 0x4b, 0xfd, 0x57, 0xfd, 0x64, 0x01, 0x38,
+	0xfd, 0x4a, 0xfa, 0x55, 0x00, 0x3b, 0xfd, 0x51,
+	0xf9, 0x56, 0xfb, 0x5f, 0xff, 0x42, 0xff, 0x4d,
+	0x01, 0x46, 0xfe, 0x56, 0xfb, 0x48, 0x00, 0x3d,
+	0x00, 0x29, 0x00, 0x3f, 0x00, 0x3f, 0x00, 0x3f,
+	0xf7, 0x53, 0x04, 0x56, 0x00, 0x61, 0xf9, 0x48,
+	0x0d, 0x29, 0x03, 0x3e, 0x0d, 0x0f, 0x07, 0x33,
+	0x02, 0x50, 0xd9, 0x7f, 0xee, 0x5b, 0xef, 0x60,
+	0xe6, 0x51, 0xdd, 0x62, 0xe8, 0x66, 0xe9, 0x61,
+	0xe5, 0x77, 0xe8, 0x63, 0xeb, 0x6e, 0xee, 0x66,
+	0xdc, 0x7f, 0x00, 0x50, 0xfb, 0x59, 0xf9, 0x5e,
+	0xfc, 0x5c, 0x00, 0x27, 0x00, 0x41, 0xf1, 0x54,
+	0xdd, 0x7f, 0xfe, 0x49, 0xf4, 0x68, 0xf7, 0x5b,
+	0xe1, 0x7f, 0x03, 0x37, 0x07, 0x38, 0x07, 0x37,
+	0x08, 0x3d, 0xfd, 0x35, 0x00, 0x44, 0xf9, 0x4a,
+	0xf7, 0x58, 0xf3, 0x67, 0xf3, 0x5b, 0xf7, 0x59,
+	0xf2, 0x5c, 0xf8, 0x4c, 0xf4, 0x57, 0xe9, 0x6e,
+	0xe8, 0x69, 0xf6, 0x4e, 0xec, 0x70, 0xef, 0x63,
+	0xb2, 0x7f, 0xba, 0x7f, 0xce, 0x7f, 0xd2, 0x7f,
+	0xfc, 0x42, 0xfb, 0x4e, 0xfc, 0x47, 0xf8, 0x48,
+	0x02, 0x3b, 0xff, 0x37, 0xf9, 0x46, 0xfa, 0x4b,
+	0xf8, 0x59, 0xde, 0x77, 0xfd, 0x4b, 0x20, 0x14,
+	0x1e, 0x16, 0xd4, 0x7f, 0x00, 0x36, 0xfb, 0x3d,
+	0x00, 0x3a, 0xff, 0x3c, 0xfd, 0x3d, 0xf8, 0x43,
+	0xe7, 0x54, 0xf2, 0x4a, 0xfb, 0x41, 0x05, 0x34,
+	0x02, 0x39, 0x00, 0x3d, 0xf7, 0x45, 0xf5, 0x46,
+	0x12, 0x37, 0xfc, 0x47, 0x00, 0x3a, 0x07, 0x3d,
+	0x09, 0x29, 0x12, 0x19, 0x09, 0x20, 0x05, 0x2b,
+	0x09, 0x2f, 0x00, 0x2c, 0x00, 0x33, 0x02, 0x2e,
+	0x13, 0x26, 0xfc, 0x42, 0x0f, 0x26, 0x0c, 0x2a,
+	0x09, 0x22, 0x00, 0x59, 0x04, 0x2d, 0x0a, 0x1c,
+	0x0a, 0x1f, 0x21, 0xf5, 0x34, 0xd5, 0x12, 0x0f,
+	0x1c, 0x00, 0x23, 0xea, 0x26, 0xe7, 0x22, 0x00,
+	0x27, 0xee, 0x20, 0xf4, 0x66, 0xa2, 0x00, 0x00,
+	0x38, 0xf1, 0x21, 0xfc, 0x1d, 0x0a, 0x25, 0xfb,
+	0x33, 0xe3, 0x27, 0xf7, 0x34, 0xde, 0x45, 0xc6,
+	0x43, 0xc1, 0x2c, 0xfb, 0x20, 0x07, 0x37, 0xe3,
+	0x20, 0x01, 0x00, 0x00, 0x1b, 0x24, 0x21, 0xe7,
+	0x22, 0xe2, 0x24, 0xe4, 0x26, 0xe4, 0x26, 0xe5,
+	0x22, 0xee, 0x23, 0xf0, 0x22, 0xf2, 0x20, 0xf8,
+	0x25, 0xfa, 0x23, 0x00, 0x1e, 0x0a, 0x1c, 0x12,
+	0x1a, 0x19, 0x1d, 0x29, 0x00, 0x4b, 0x02, 0x48,
+	0x08, 0x4d, 0x0e, 0x23, 0x12, 0x1f, 0x11, 0x23,
+	0x15, 0x1e, 0x11, 0x2d, 0x14, 0x2a, 0x12, 0x2d,
+	0x1b, 0x1a, 0x10, 0x36, 0x07, 0x42, 0x10, 0x38,
+	0x0b, 0x49, 0x0a, 0x43, 0xf6, 0x74, 0xe9, 0x70,
+	0xf1, 0x47, 0xf9, 0x3d, 0x00, 0x35, 0xfb, 0x42,
+	0xf5, 0x4d, 0xf7, 0x50, 0xf7, 0x54, 0xf6, 0x57,
+	0xde, 0x7f, 0xeb, 0x65, 0xfd, 0x27, 0xfb, 0x35,
+	0xf9, 0x3d, 0xf5, 0x4b, 0xf1, 0x4d, 0xef, 0x5b,
+	0xe7, 0x6b, 0xe7, 0x6f, 0xe4, 0x7a, 0xf5, 0x4c,
+	0xf6, 0x2c, 0xf6, 0x34, 0xf6, 0x39, 0xf7, 0x3a,
+	0xf0, 0x48, 0xf9, 0x45, 0xfc, 0x45, 0xfb, 0x4a,
+	0xf7, 0x56, 0x02, 0x42, 0xf7, 0x22, 0x01, 0x20,
+	0x0b, 0x1f, 0x05, 0x34, 0xfe, 0x37, 0xfe, 0x43,
+	0x00, 0x49, 0xf8, 0x59, 0x03, 0x34, 0x07, 0x04,
+	0x0a, 0x08, 0x11, 0x08, 0x10, 0x13, 0x03, 0x25,
+	0xff, 0x3d, 0xfb, 0x49, 0xff, 0x46, 0xfc, 0x4e,
+	0x00, 0x00, 0xeb, 0x7e, 0xe9, 0x7c, 0xec, 0x6e,
+	0xe6, 0x7e, 0xe7, 0x7c, 0xef, 0x69, 0xe5, 0x79,
+	0xe5, 0x75, 0xef, 0x66, 0xe6, 0x75, 0xe5, 0x74,
+	0xdf, 0x7a, 0xf6, 0x5f, 0xf2, 0x64, 0xf8, 0x5f,
+	0xef, 0x6f, 0xe4, 0x72, 0xfa, 0x59, 0xfe, 0x50,
+	0xfc, 0x52, 0xf7, 0x55, 0xf8, 0x51, 0xff, 0x48,
+	0x05, 0x40, 0x01, 0x43, 0x09, 0x38, 0x00, 0x45,
+	0x01, 0x45, 0x07, 0x45, 0xf9, 0x45, 0xfa, 0x43,
+	0xf0, 0x4d, 0xfe, 0x40, 0x02, 0x3d, 0xfa, 0x43,
+	0xfd, 0x40, 0x02, 0x39, 0xfd, 0x41, 0xfd, 0x42,
+	0x00, 0x3e, 0x09, 0x33, 0xff, 0x42, 0xfe, 0x47,
+	0xfe, 0x4b, 0xff, 0x46, 0xf7, 0x48, 0x0e, 0x3c,
+	0x10, 0x25, 0x00, 0x2f, 0x12, 0x23, 0x0b, 0x25,
+	0x0c, 0x29, 0x0a, 0x29, 0x02, 0x30, 0x0c, 0x29,
+	0x0d, 0x29, 0x00, 0x3b, 0x03, 0x32, 0x13, 0x28,
+	0x03, 0x42, 0x12, 0x32, 0x13, 0xfa, 0x12, 0xfa,
+	0x0e, 0x00, 0x1a, 0xf4, 0x1f, 0xf0, 0x21, 0xe7,
+	0x21, 0xea, 0x25, 0xe4, 0x27, 0xe2, 0x2a, 0xe2,
+	0x2f, 0xd6, 0x2d, 0xdc, 0x31, 0xde, 0x29, 0xef,
+	0x20, 0x09, 0x45, 0xb9, 0x3f, 0xc1, 0x42, 0xc0,
+	0x4d, 0xb6, 0x36, 0xd9, 0x34, 0xdd, 0x29, 0xf6,
+	0x24, 0x00, 0x28, 0xff, 0x1e, 0x0e, 0x1c, 0x1a,
+	0x17, 0x25, 0x0c, 0x37, 0x0b, 0x41, 0x25, 0xdf,
+	0x27, 0xdc, 0x28, 0xdb, 0x26, 0xe2, 0x2e, 0xdf,
+	0x2a, 0xe2, 0x28, 0xe8, 0x31, 0xe3, 0x26, 0xf4,
+	0x28, 0xf6, 0x26, 0xfd, 0x2e, 0xfb, 0x1f, 0x14,
+	0x1d, 0x1e, 0x19, 0x2c, 0x0c, 0x30, 0x0b, 0x31,
+	0x1a, 0x2d, 0x16, 0x16, 0x17, 0x16, 0x1b, 0x15,
+	0x21, 0x14, 0x1a, 0x1c, 0x1e, 0x18, 0x1b, 0x22,
+	0x12, 0x2a, 0x19, 0x27, 0x12, 0x32, 0x0c, 0x46,
+	0x15, 0x36, 0x0e, 0x47, 0x0b, 0x53, 0x19, 0x20,
+	0x15, 0x31, 0x15, 0x36, 0xfb, 0x55, 0xfa, 0x51,
+	0xf6, 0x4d, 0xf9, 0x51, 0xef, 0x50, 0xee, 0x49,
+	0xfc, 0x4a, 0xf6, 0x53, 0xf7, 0x47, 0xf7, 0x43,
+	0xff, 0x3d, 0xf8, 0x42, 0xf2, 0x42, 0x00, 0x3b,
+	0x02, 0x3b, 0x11, 0xf6, 0x20, 0xf3, 0x2a, 0xf7,
+	0x31, 0xfb, 0x35, 0x00, 0x40, 0x03, 0x44, 0x0a,
+	0x42, 0x1b, 0x2f, 0x39, 0xfb, 0x47, 0x00, 0x18,
+	0xff, 0x24, 0xfe, 0x2a, 0xfe, 0x34, 0xf7, 0x39,
+	0xfa, 0x3f, 0xfc, 0x41, 0xfc, 0x43, 0xf9, 0x52,
+	0xfd, 0x51, 0xfd, 0x4c, 0xf9, 0x48, 0xfa, 0x4e,
+	0xf4, 0x48, 0xf2, 0x44, 0xfd, 0x46, 0xfa, 0x4c,
+	0xfb, 0x42, 0xfb, 0x3e, 0x00, 0x39, 0xfc, 0x3d,
+	0xf7, 0x3c, 0x01, 0x36, 0x02, 0x3a, 0x11, 0xf6,
+	0x20, 0xf3, 0x2a, 0xf7, 0x31, 0xfb, 0x35, 0x00,
+	0x40, 0x03, 0x44, 0x0a, 0x42, 0x1b, 0x2f, 0x39,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x14, 0xf1, 0x02, 0x36, 0x03, 0x4a, 0x14, 0xf1,
+	0x02, 0x36, 0x03, 0x4a, 0xe4, 0x7f, 0xe9, 0x68,
+	0xfa, 0x35, 0xff, 0x36, 0x07, 0x33, 0x1d, 0x10,
+	0x19, 0x00, 0x0e, 0x00, 0xf6, 0x33, 0xfd, 0x3e,
+	0xe5, 0x63, 0x1a, 0x10, 0xfc, 0x55, 0xe8, 0x66,
+	0x05, 0x39, 0x06, 0x39, 0xef, 0x49, 0x0e, 0x39,
+	0x14, 0x28, 0x14, 0x0a, 0x1d, 0x00, 0x36, 0x00,
+	0x25, 0x2a, 0x0c, 0x61, 0xe0, 0x7f, 0xea, 0x75,
+	0xfe, 0x4a, 0xfc, 0x55, 0xe8, 0x66, 0x05, 0x39,
+	0xfa, 0x5d, 0xf2, 0x58, 0xfa, 0x2c, 0x04, 0x37,
+	0xf5, 0x59, 0xf1, 0x67, 0xeb, 0x74, 0x13, 0x39,
+	0x14, 0x3a, 0x04, 0x54, 0x06, 0x60, 0x01, 0x3f,
+	0xfb, 0x55, 0xf3, 0x6a, 0x05, 0x3f, 0x06, 0x4b,
+	0xfd, 0x5a, 0xff, 0x65, 0x03, 0x37, 0xfc, 0x4f,
+	0xfe, 0x4b, 0xf4, 0x61, 0xf9, 0x32, 0x01, 0x3c,
+	0x00, 0x29, 0x00, 0x3f, 0x00, 0x3f, 0x00, 0x3f,
+	0xf7, 0x53, 0x04, 0x56, 0x00, 0x61, 0xf9, 0x48,
+	0x0d, 0x29, 0x03, 0x3e, 0x07, 0x22, 0xf7, 0x58,
+	0xec, 0x7f, 0xdc, 0x7f, 0xef, 0x5b, 0xf2, 0x5f,
+	0xe7, 0x54, 0xe7, 0x56, 0xf4, 0x59, 0xef, 0x5b,
+	0xe1, 0x7f, 0xf2, 0x4c, 0xee, 0x67, 0xf3, 0x5a,
+	0xdb, 0x7f, 0x0b, 0x50, 0x05, 0x4c, 0x02, 0x54,
+	0x05, 0x4e, 0xfa, 0x37, 0x04, 0x3d, 0xf2, 0x53,
+	0xdb, 0x7f, 0xfb, 0x4f, 0xf5, 0x68, 0xf5, 0x5b,
+	0xe2, 0x7f, 0x00, 0x41, 0xfe, 0x4f, 0x00, 0x48,
+	0xfc, 0x5c, 0xfa, 0x38, 0x03, 0x44, 0xf8, 0x47,
+	0xf3, 0x62, 0xfc, 0x56, 0xf4, 0x58, 0xfb, 0x52,
+	0xfd, 0x48, 0xfc, 0x43, 0xf8, 0x48, 0xf0, 0x59,
+	0xf7, 0x45, 0xff, 0x3b, 0x05, 0x42, 0x04, 0x39,
+	0xfc, 0x47, 0xfe, 0x47, 0x02, 0x3a, 0xff, 0x4a,
+	0xfc, 0x2c, 0xff, 0x45, 0x00, 0x3e, 0xf9, 0x33,
+	0xfc, 0x2f, 0xfa, 0x2a, 0xfd, 0x29, 0xfa, 0x35,
+	0x08, 0x4c, 0xf7, 0x4e, 0xf5, 0x53, 0x09, 0x34,
+	0x00, 0x43, 0xfb, 0x5a, 0x01, 0x43, 0xf1, 0x48,
+	0xfb, 0x4b, 0xf8, 0x50, 0xeb, 0x53, 0xeb, 0x40,
+	0xf3, 0x1f, 0xe7, 0x40, 0xe3, 0x5e, 0x09, 0x4b,
+	0x11, 0x3f, 0xf8, 0x4a, 0xfb, 0x23, 0xfe, 0x1b,
+	0x0d, 0x5b, 0x03, 0x41, 0xf9, 0x45, 0x08, 0x4d,
+	0xf6, 0x42, 0x03, 0x3e, 0xfd, 0x44, 0xec, 0x51,
+	0x00, 0x1e, 0x01, 0x07, 0xfd, 0x17, 0xeb, 0x4a,
+	0x10, 0x42, 0xe9, 0x7c, 0x11, 0x25, 0x2c, 0xee,
+	0x32, 0xde, 0xea, 0x7f, 0x04, 0x27, 0x00, 0x2a,
+	0x07, 0x22, 0x0b, 0x1d, 0x08, 0x1f, 0x06, 0x25,
+	0x07, 0x2a, 0x03, 0x28, 0x08, 0x21, 0x0d, 0x2b,
+	0x0d, 0x24, 0x04, 0x2f, 0x03, 0x37, 0x02, 0x3a,
+	0x06, 0x3c, 0x08, 0x2c, 0x0b, 0x2c, 0x0e, 0x2a,
+	0x07, 0x30, 0x04, 0x38, 0x04, 0x34, 0x0d, 0x25,
+	0x09, 0x31, 0x13, 0x3a, 0x0a, 0x30, 0x0c, 0x2d,
+	0x00, 0x45, 0x14, 0x21, 0x08, 0x3f, 0x23, 0xee,
+	0x21, 0xe7, 0x1c, 0xfd, 0x18, 0x0a, 0x1b, 0x00,
+	0x22, 0xf2, 0x34, 0xd4, 0x27, 0xe8, 0x13, 0x11,
+	0x1f, 0x19, 0x24, 0x1d, 0x18, 0x21, 0x22, 0x0f,
+	0x1e, 0x14, 0x16, 0x49, 0x14, 0x22, 0x13, 0x1f,
+	0x1b, 0x2c, 0x13, 0x10, 0x0f, 0x24, 0x0f, 0x24,
+	0x15, 0x1c, 0x19, 0x15, 0x1e, 0x14, 0x1f, 0x0c,
+	0x1b, 0x10, 0x18, 0x2a, 0x00, 0x5d, 0x0e, 0x38,
+	0x0f, 0x39, 0x1a, 0x26, 0xe8, 0x7f, 0xe8, 0x73,
+	0xea, 0x52, 0xf7, 0x3e, 0x00, 0x35, 0x00, 0x3b,
+	0xf2, 0x55, 0xf3, 0x59, 0xf3, 0x5e, 0xf5, 0x5c,
+	0xe3, 0x7f, 0xeb, 0x64, 0xf2, 0x39, 0xf4, 0x43,
+	0xf5, 0x47, 0xf6, 0x4d, 0xeb, 0x55, 0xf0, 0x58,
+	0xe9, 0x68, 0xf1, 0x62, 0xdb, 0x7f, 0xf6, 0x52,
+	0xf8, 0x30, 0xf8, 0x3d, 0xf8, 0x42, 0xf9, 0x46,
+	0xf2, 0x4b, 0xf6, 0x4f, 0xf7, 0x53, 0xf4, 0x5c,
+	0xee, 0x6c, 0xfc, 0x4f, 0xea, 0x45, 0xf0, 0x4b,
+	0xfe, 0x3a, 0x01, 0x3a, 0xf3, 0x4e, 0xf7, 0x53,
+	0xfc, 0x51, 0xf3, 0x63, 0xf3, 0x51, 0xfa, 0x26,
+	0xf3, 0x3e, 0xfa, 0x3a, 0xfe, 0x3b, 0xf0, 0x49,
+	0xf6, 0x4c, 0xf3, 0x56, 0xf7, 0x53, 0xf6, 0x57,
+	0x00, 0x00, 0xea, 0x7f, 0xe7, 0x7f, 0xe7, 0x78,
+	0xe5, 0x7f, 0xed, 0x72, 0xe9, 0x75, 0xe7, 0x76,
+	0xe6, 0x75, 0xe8, 0x71, 0xe4, 0x76, 0xe1, 0x78,
+	0xdb, 0x7c, 0xf6, 0x5e, 0xf1, 0x66, 0xf6, 0x63,
+	0xf3, 0x6a, 0xce, 0x7f, 0xfb, 0x5c, 0x11, 0x39,
+	0xfb, 0x56, 0xf3, 0x5e, 0xf4, 0x5b, 0xfe, 0x4d,
+	0x00, 0x47, 0xff, 0x49, 0x04, 0x40, 0xf9, 0x51,
+	0x05, 0x40, 0x0f, 0x39, 0x01, 0x43, 0x00, 0x44,
+	0xf6, 0x43, 0x01, 0x44, 0x00, 0x4d, 0x02, 0x40,
+	0x00, 0x44, 0xfb, 0x4e, 0x07, 0x37, 0x05, 0x3b,
+	0x02, 0x41, 0x0e, 0x36, 0x0f, 0x2c, 0x05, 0x3c,
+	0x02, 0x46, 0xfe, 0x4c, 0xee, 0x56, 0x0c, 0x46,
+	0x05, 0x40, 0xf4, 0x46, 0x0b, 0x37, 0x05, 0x38,
+	0x00, 0x45, 0x02, 0x41, 0xfa, 0x4a, 0x05, 0x36,
+	0x07, 0x36, 0xfa, 0x4c, 0xf5, 0x52, 0xfe, 0x4d,
+	0xfe, 0x4d, 0x19, 0x2a, 0x11, 0xf3, 0x10, 0xf7,
+	0x11, 0xf4, 0x1b, 0xeb, 0x25, 0xe2, 0x29, 0xd8,
+	0x2a, 0xd7, 0x30, 0xd1, 0x27, 0xe0, 0x2e, 0xd8,
+	0x34, 0xcd, 0x2e, 0xd7, 0x34, 0xd9, 0x2b, 0xed,
+	0x20, 0x0b, 0x3d, 0xc9, 0x38, 0xd2, 0x3e, 0xce,
+	0x51, 0xbd, 0x2d, 0xec, 0x23, 0xfe, 0x1c, 0x0f,
+	0x22, 0x01, 0x27, 0x01, 0x1e, 0x11, 0x14, 0x26,
+	0x12, 0x2d, 0x0f, 0x36, 0x00, 0x4f, 0x24, 0xf0,
+	0x25, 0xf2, 0x25, 0xef, 0x20, 0x01, 0x22, 0x0f,
+	0x1d, 0x0f, 0x18, 0x19, 0x22, 0x16, 0x1f, 0x10,
+	0x23, 0x12, 0x1f, 0x1c, 0x21, 0x29, 0x24, 0x1c,
+	0x1b, 0x2f, 0x15, 0x3e, 0x12, 0x1f, 0x13, 0x1a,
+	0x24, 0x18, 0x18, 0x17, 0x1b, 0x10, 0x18, 0x1e,
+	0x1f, 0x1d, 0x16, 0x29, 0x16, 0x2a, 0x10, 0x3c,
+	0x0f, 0x34, 0x0e, 0x3c, 0x03, 0x4e, 0xf0, 0x7b,
+	0x15, 0x35, 0x16, 0x38, 0x19, 0x3d, 0x15, 0x21,
+	0x13, 0x32, 0x11, 0x3d, 0xfd, 0x4e, 0xf8, 0x4a,
+	0xf7, 0x48, 0xf6, 0x48, 0xee, 0x4b, 0xf4, 0x47,
+	0xf5, 0x3f, 0xfb, 0x46, 0xef, 0x4b, 0xf2, 0x48,
+	0xf0, 0x43, 0xf8, 0x35, 0xf2, 0x3b, 0xf7, 0x34,
+	0xf5, 0x44, 0x09, 0xfe, 0x1e, 0xf6, 0x1f, 0xfc,
+	0x21, 0xff, 0x21, 0x07, 0x1f, 0x0c, 0x25, 0x17,
+	0x1f, 0x26, 0x14, 0x40, 0xf7, 0x47, 0xf9, 0x25,
+	0xf8, 0x2c, 0xf5, 0x31, 0xf6, 0x38, 0xf4, 0x3b,
+	0xf8, 0x3f, 0xf7, 0x43, 0xfa, 0x44, 0xf6, 0x4f,
+	0xfd, 0x4e, 0xf8, 0x4a, 0xf7, 0x48, 0xf6, 0x48,
+	0xee, 0x4b, 0xf4, 0x47, 0xf5, 0x3f, 0xfb, 0x46,
+	0xef, 0x4b, 0xf2, 0x48, 0xf0, 0x43, 0xf8, 0x35,
+	0xf2, 0x3b, 0xf7, 0x34, 0xf5, 0x44, 0x09, 0xfe,
+	0x1e, 0xf6, 0x1f, 0xfc, 0x21, 0xff, 0x21, 0x07,
+	0x1f, 0x0c, 0x25, 0x17, 0x1f, 0x26, 0x14, 0x40,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x14, 0xf1, 0x02, 0x36, 0x03, 0x4a, 0x14, 0xf1,
+	0x02, 0x36, 0x03, 0x4a, 0xe4, 0x7f, 0xe9, 0x68,
+	0xfa, 0x35, 0xff, 0x36, 0x07, 0x33, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x29, 0x00, 0x3f, 0x00, 0x3f, 0x00, 0x3f,
+	0xf7, 0x53, 0x04, 0x56, 0x00, 0x61, 0xf9, 0x48,
+	0x0d, 0x29, 0x03, 0x3e, 0x00, 0x0b, 0x01, 0x37,
+	0x00, 0x45, 0xef, 0x7f, 0xf3, 0x66, 0x00, 0x52,
+	0xf9, 0x4a, 0xeb, 0x6b, 0xe5, 0x7f, 0xe1, 0x7f,
+	0xe8, 0x7f, 0xee, 0x5f, 0xe5, 0x7f, 0xeb, 0x72,
+	0xe2, 0x7f, 0xef, 0x7b, 0xf4, 0x73, 0xf0, 0x7a,
+	0xf5, 0x73, 0xf4, 0x3f, 0xfe, 0x44, 0xf1, 0x54,
+	0xf3, 0x68, 0xfd, 0x46, 0xf8, 0x5d, 0xf6, 0x5a,
+	0xe2, 0x7f, 0xff, 0x4a, 0xfa, 0x61, 0xf9, 0x5b,
+	0xec, 0x7f, 0xfc, 0x38, 0xfb, 0x52, 0xf9, 0x4c,
+	0xea, 0x7d, 0xf9, 0x5d, 0xf5, 0x57, 0xfd, 0x4d,
+	0xfb, 0x47, 0xfc, 0x3f, 0xfc, 0x44, 0xf4, 0x54,
+	0xf9, 0x3e, 0xf9, 0x41, 0x08, 0x3d, 0x05, 0x38,
+	0xfe, 0x42, 0x01, 0x40, 0x00, 0x3d, 0xfe, 0x4e,
+	0x01, 0x32, 0x07, 0x34, 0x0a, 0x23, 0x00, 0x2c,
+	0x0b, 0x26, 0x01, 0x2d, 0x00, 0x2e, 0x05, 0x2c,
+	0x1f, 0x11, 0x01, 0x33, 0x07, 0x32, 0x1c, 0x13,
+	0x10, 0x21, 0x0e, 0x3e, 0xf3, 0x6c, 0xf1, 0x64,
+	0xf3, 0x65, 0xf3, 0x5b, 0xf4, 0x5e, 0xf6, 0x58,
+	0xf0, 0x54, 0xf6, 0x56, 0xf9, 0x53, 0xf3, 0x57,
+	0xed, 0x5e, 0x01, 0x46, 0x00, 0x48, 0xfb, 0x4a,
+	0x12, 0x3b, 0xf8, 0x66, 0xf1, 0x64, 0x00, 0x5f,
+	0xfc, 0x4b, 0x02, 0x48, 0xf5, 0x4b, 0xfd, 0x47,
+	0x0f, 0x2e, 0xf3, 0x45, 0x00, 0x3e, 0x00, 0x41,
+	0x15, 0x25, 0xf1, 0x48, 0x09, 0x39, 0x10, 0x36,
+	0x00, 0x3e, 0x0c, 0x48, 0x18, 0x00, 0x0f, 0x09,
+	0x08, 0x19, 0x0d, 0x12, 0x0f, 0x09, 0x0d, 0x13,
+	0x0a, 0x25, 0x0c, 0x12, 0x06, 0x1d, 0x14, 0x21,
+	0x0f, 0x1e, 0x04, 0x2d, 0x01, 0x3a, 0x00, 0x3e,
+	0x07, 0x3d, 0x0c, 0x26, 0x0b, 0x2d, 0x0f, 0x27,
+	0x0b, 0x2a, 0x0d, 0x2c, 0x10, 0x2d, 0x0c, 0x29,
+	0x0a, 0x31, 0x1e, 0x22, 0x12, 0x2a, 0x0a, 0x37,
+	0x11, 0x33, 0x11, 0x2e, 0x00, 0x59, 0x1a, 0xed,
+	0x16, 0xef, 0x1a, 0xef, 0x1e, 0xe7, 0x1c, 0xec,
+	0x21, 0xe9, 0x25, 0xe5, 0x21, 0xe9, 0x28, 0xe4,
+	0x26, 0xef, 0x21, 0xf5, 0x28, 0xf1, 0x29, 0xfa,
+	0x26, 0x01, 0x29, 0x11, 0x1e, 0xfa, 0x1b, 0x03,
+	0x1a, 0x16, 0x25, 0xf0, 0x23, 0xfc, 0x26, 0xf8,
+	0x26, 0xfd, 0x25, 0x03, 0x26, 0x05, 0x2a, 0x00,
+	0x23, 0x10, 0x27, 0x16, 0x0e, 0x30, 0x1b, 0x25,
+	0x15, 0x3c, 0x0c, 0x44, 0x02, 0x61, 0xfd, 0x47,
+	0xfa, 0x2a, 0xfb, 0x32, 0xfd, 0x36, 0xfe, 0x3e,
+	0x00, 0x3a, 0x01, 0x3f, 0xfe, 0x48, 0xff, 0x4a,
+	0xf7, 0x5b, 0xfb, 0x43, 0xfb, 0x1b, 0xfd, 0x27,
+	0xfe, 0x2c, 0x00, 0x2e, 0xf0, 0x40, 0xf8, 0x44,
+	0xf6, 0x4e, 0xfa, 0x4d, 0xf6, 0x56, 0xf4, 0x5c,
+	0xf1, 0x37, 0xf6, 0x3c, 0xfa, 0x3e, 0xfc, 0x41,
+	0xf4, 0x49, 0xf8, 0x4c, 0xf9, 0x50, 0xf7, 0x58,
+	0xef, 0x6e, 0xf5, 0x61, 0xec, 0x54, 0xf5, 0x4f,
+	0xfa, 0x49, 0xfc, 0x4a, 0xf3, 0x56, 0xf3, 0x60,
+	0xf5, 0x61, 0xed, 0x75, 0xf8, 0x4e, 0xfb, 0x21,
+	0xfc, 0x30, 0xfe, 0x35, 0xfd, 0x3e, 0xf3, 0x47,
+	0xf6, 0x4f, 0xf4, 0x56, 0xf3, 0x5a, 0xf2, 0x61,
+	0x00, 0x00, 0xfa, 0x5d, 0xfa, 0x54, 0xf8, 0x4f,
+	0x00, 0x42, 0xff, 0x47, 0x00, 0x3e, 0xfe, 0x3c,
+	0xfe, 0x3b, 0xfb, 0x4b, 0xfd, 0x3e, 0xfc, 0x3a,
+	0xf7, 0x42, 0xff, 0x4f, 0x00, 0x47, 0x03, 0x44,
+	0x0a, 0x2c, 0xf9, 0x3e, 0x0f, 0x24, 0x0e, 0x28,
+	0x10, 0x1b, 0x0c, 0x1d, 0x01, 0x2c, 0x14, 0x24,
+	0x12, 0x20, 0x05, 0x2a, 0x01, 0x30, 0x0a, 0x3e,
+	0x11, 0x2e, 0x09, 0x40, 0xf4, 0x68, 0xf5, 0x61,
+	0xf0, 0x60, 0xf9, 0x58, 0xf8, 0x55, 0xf9, 0x55,
+	0xf7, 0x55, 0xf3, 0x58, 0x04, 0x42, 0xfd, 0x4d,
+	0xfd, 0x4c, 0xfa, 0x4c, 0x0a, 0x3a, 0xff, 0x4c,
+	0xff, 0x53, 0xf9, 0x63, 0xf2, 0x5f, 0x02, 0x5f,
+	0x00, 0x4c, 0xfb, 0x4a, 0x00, 0x46, 0xf5, 0x4b,
+	0x01, 0x44, 0x00, 0x41, 0xf2, 0x49, 0x03, 0x3e,
+	0x04, 0x3e, 0xff, 0x44, 0xf3, 0x4b, 0x0b, 0x37,
+	0x05, 0x40, 0x0c, 0x46, 0x0f, 0x06, 0x06, 0x13,
+	0x07, 0x10, 0x0c, 0x0e, 0x12, 0x0d, 0x0d, 0x0b,
+	0x0d, 0x0f, 0x0f, 0x10, 0x0c, 0x17, 0x0d, 0x17,
+	0x0f, 0x14, 0x0e, 0x1a, 0x0e, 0x2c, 0x11, 0x28,
+	0x11, 0x2f, 0x18, 0x11, 0x15, 0x15, 0x19, 0x16,
+	0x1f, 0x1b, 0x16, 0x1d, 0x13, 0x23, 0x0e, 0x32,
+	0x0a, 0x39, 0x07, 0x3f, 0xfe, 0x4d, 0xfc, 0x52,
+	0xfd, 0x5e, 0x09, 0x45, 0xf4, 0x6d, 0x24, 0xdd,
+	0x24, 0xde, 0x20, 0xe6, 0x25, 0xe2, 0x2c, 0xe0,
+	0x22, 0xee, 0x22, 0xf1, 0x28, 0xf1, 0x21, 0xf9,
+	0x23, 0xfb, 0x21, 0x00, 0x26, 0x02, 0x21, 0x0d,
+	0x17, 0x23, 0x0d, 0x3a, 0x1d, 0xfd, 0x1a, 0x00,
+	0x16, 0x1e, 0x1f, 0xf9, 0x23, 0xf1, 0x22, 0xfd,
+	0x22, 0x03, 0x24, 0xff, 0x22, 0x05, 0x20, 0x0b,
+	0x23, 0x05, 0x22, 0x0c, 0x27, 0x0b, 0x1e, 0x1d,
+	0x22, 0x1a, 0x1d, 0x27, 0x13, 0x42, 0x1f, 0x15,
+	0x1f, 0x1f, 0x19, 0x32, 0xef, 0x78, 0xec, 0x70,
+	0xee, 0x72, 0xf5, 0x55, 0xf1, 0x5c, 0xf2, 0x59,
+	0xe6, 0x47, 0xf1, 0x51, 0xf2, 0x50, 0x00, 0x44,
+	0xf2, 0x46, 0xe8, 0x38, 0xe9, 0x44, 0xe8, 0x32,
+	0xf5, 0x4a, 0x17, 0xf3, 0x1a, 0xf3, 0x28, 0xf1,
+	0x31, 0xf2, 0x2c, 0x03, 0x2d, 0x06, 0x2c, 0x22,
+	0x21, 0x36, 0x13, 0x52, 0xfd, 0x4b, 0xff, 0x17,
+	0x01, 0x22, 0x01, 0x2b, 0x00, 0x36, 0xfe, 0x37,
+	0x00, 0x3d, 0x01, 0x40, 0x00, 0x44, 0xf7, 0x5c,
+	0xf2, 0x6a, 0xf3, 0x61, 0xf1, 0x5a, 0xf4, 0x5a,
+	0xee, 0x58, 0xf6, 0x49, 0xf7, 0x4f, 0xf2, 0x56,
+	0xf6, 0x49, 0xf6, 0x46, 0xf6, 0x45, 0xfb, 0x42,
+	0xf7, 0x40, 0xfb, 0x3a, 0x02, 0x3b, 0x15, 0xf6,
+	0x18, 0xf5, 0x1c, 0xf8, 0x1c, 0xff, 0x1d, 0x03,
+	0x1d, 0x09, 0x23, 0x14, 0x1d, 0x24, 0x0e, 0x43,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+};
+
+static const char h264_fix_data[] = {
+	/* 128 * 0 stream in */
+	0x00, 0x00, 0x01, 0x65, 0x88, 0x81, 0x00, 0x9F,
+	0xFE, 0x6F, 0x5F, 0x32, 0xC5, 0x42, 0x54, 0x26,
+	0x81, 0xD5, 0xE9, 0x71, 0x10, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	/* stuff */
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+
+	/* 128 * 1 rps in */
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+
+	/* 128 * 2 pps in */
+	0xff, 0x3f, 0x80, 0x14, 0x40, 0x00, 0x04, 0x40,
+	0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0xff, 0x3f, 0x42, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	/* stuff */
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	/* rlc out */
+};
+
+static struct hack_info rkvdec2_3568_hack[] = {
+	{0x00000001, 0x0510},
+	{0x00000001, 0x0550},
+	{0x00000001, 0x0590},
+	{0x00000000, 0x0020},
+	{0x00000001, 0x0024},
+	{0x00000072, 0x002c},
+	{0x00000102, 0x0030},
+	{0x01048201, 0x0034},
+	{0x00000000, 0x0038},
+	{0x00000001, 0x003c},
+	{0x00000030, 0x0040},
+	{0x00003fff, 0x0044},
+	{0x00000001, 0x0048},
+	{0x00000001, 0x004c},
+	{0x00000010, 0x0050},
+	{0x00000006, 0x0054},
+	{0x00000000, 0x0058},
+	{0x00000000, 0x005c},
+	{0xffffdfff, 0x0060},
+	{0x3ffbfbff, 0x0064},
+	{0x800fffff, 0x0068},
+	{0x000000ff, 0x0080},
+	{0x00000000, 0x0100},
+	{0x00000000, 0x0104},
+	{0x00000000, 0x0108},
+	{0x00000000, 0x010c},
+	{0x00000000, 0x0110},
+	{0x00000000, 0x0114},
+	{0x00000000, 0x0118},
+	{0x00000000, 0x011c},
+	{0x00000000, 0x0120},
+	{0x00000000, 0x0124},
+	{0x00000000, 0x0128},
+	{0x00000000, 0x012c},
+	{0x00000000, 0x0130},
+	{0x00000000, 0x0134},
+	{0x00000000, 0x0138},
+	{0x00000000, 0x013c},
+	{0x00000000, 0x0140},
+	{0x00000000, 0x0144},
+	{0x00000000, 0x0148},
+	{0x00000000, 0x014c},
+	{0x00000000, 0x0150},
+	{0x00000000, 0x0154},
+	{0x00000000, 0x0158},
+	{0x00000000, 0x015c},
+	{0x00000000, 0x0160},
+	{0x00000000, 0x0164},
+	{0x00000000, 0x0168},
+	{0x00000000, 0x016c},
+	{0x00000000, 0x0170},
+	{0x00000000, 0x0174},
+	{0x00000000, 0x0178},
+	{0x00000000, 0x017c},
+	{0x00000000, 0x0180},
+	{0x00000000, 0x0184},
+	{0x00000000, 0x0188},
+	{0x00000000, 0x018c},
+	{0x00000000, 0x0190},
+	{0x00000000, 0x0194},
+	{0x00000000, 0x0198},
+	{0x00000000, 0x019c},
+	{0x00000000, 0x01a0},
+	{0x00000000, 0x01a4},
+	{0x00000000, 0x01a8},
+	{0x00000000, 0x01ac},
+	{0x00000000, 0x01b0},
+	{0x00000000, 0x01b4},
+	{0x00000000, 0x01b8},
+	{0x00000000, 0x01bc},
+	{0x00000000, 0x01c0},
+	{0x00000000, 0x0200},
+	{0x00000000, 0x0204},
+	{0x00000000, 0x0208},
+	{0x00000000, 0x020c},
+	{0x00000000, 0x0210},
+	{0x100001c0, 0x0214},
+	{0x100001c0, 0x0218},
+	{0x100001c0, 0x021c},
+	{0x100001c0, 0x0220},
+	{0x10000340, 0x0224},
+	{0x10000340, 0x0228},
+	{0x10000000, 0x022c},
+	{0x10000000, 0x0230},
+	{0x10000000, 0x0234},
+	{0x10000000, 0x0238},
+	{0x00000000, 0x0280},
+	{0x00000000, 0x0284},
+	{0x00000000, 0x0288},
+	{0x00000000, 0x028c},
+	{0x00000000, 0x0290},
+	{0x00000000, 0x0294},
+	{0x00000000, 0x0298},
+	{0x00000000, 0x029c},
+	{0x00000000, 0x02a0},
+	{0x00000000, 0x02a4},
+	{0x00000000, 0x02a8},
+	{0x00000000, 0x02ac},
+	{0x00000000, 0x02b0},
+	{0x00000000, 0x02b4},
+	{0x00000000, 0x02b8},
+	{0x00000000, 0x02bc},
+	{0x00000000, 0x02c0},
+	{0x00000000, 0x02c4},
+	{0x00000000, 0x02c8},
+	{0x00000000, 0x02cc},
+	{0x00000000, 0x02d0},
+	{0x00000000, 0x02d4},
+	{0x00000000, 0x02d8},
+	{0x00000000, 0x02dc},
+	{0x00000000, 0x02e0},
+	{0x00000000, 0x02e4},
+	{0x00000000, 0x02e8},
+	{0x00000000, 0x02ec},
+	{0x00000000, 0x02f0},
+	{0x00000000, 0x02f4},
+	{0x00000000, 0x02f8},
+	{0x00000000, 0x02fc},
+	{0x00000000, 0x0300},
+	{0x00000000, 0x0304},
+	{0x00000000, 0x0308},
+	{0x00000000, 0x030c},
+	{0x00000000, 0x0310},
+	{0x00000000, 0x0314},
+	{0x00000000, 0x0380},
+};
+
+void rkvdec2_3568_hack_data_setup(struct mpp_dma_buffer *fix)
+{
+	u32 iova = fix->iova;
+	u32 i;
+
+	memcpy(fix->vaddr, h264_fix_data, sizeof(h264_fix_data));
+	memcpy(fix->vaddr + PAGE_SIZE, h264_cabac_tbl, sizeof(h264_cabac_tbl));
+
+	/* input stream 0x0200*/
+	rkvdec2_3568_hack[71].data = iova;
+	/* rlc */
+	rkvdec2_3568_hack[72].data = iova + RKDEC_HACK_DATA_RLC_OFFSET;
+	/* output frame 0x0208*/
+	rkvdec2_3568_hack[73].data = iova + RKDEC_HACK_DATA_OUT_OFFSET;
+	/* colmv out 0x020c*/
+	rkvdec2_3568_hack[74].data = iova + RKDEC_HACK_DATA_COLMV_OFFSET;
+
+	/* pps in */
+	rkvdec2_3568_hack[87].data = iova + RKDEC_HACK_DATA_PPS_OFFSET;
+	/* rps in */
+	rkvdec2_3568_hack[89].data = iova + RKDEC_HACK_DATA_RPS_OFFSET;
+	for (i = 0; i < 33; i++)
+		rkvdec2_3568_hack[90 + i].data = iova + RKDEC_HACK_DATA_COLMV_OFFSET;
+	rkvdec2_3568_hack[123].data = iova + PAGE_SIZE;
+}
+
+void rkvdec2_3568_hack_fix(struct mpp_dev *mpp)
+{
+	void __iomem *reg_base = mpp->reg_base;
+	unsigned long flags;
+	u32 reg;
+	u32 cnt = 0;
+	u32 i = 0;
+
+	for (i = 0; i < ARRAY_SIZE(rkvdec2_3568_hack); i++)
+		writel_relaxed(rkvdec2_3568_hack[i].data, reg_base + rkvdec2_3568_hack[i].offset);
+
+	local_irq_save(flags);
+	/* write all data to register before start hardware */
+	wmb();
+	writel(0x00000001, reg_base + 0x0028);
+
+	udelay(5);
+
+	reg = readl(mpp->reg_base + 0x0380);
+	while ((reg & 0x106) != 0x106) {
+		udelay(2);
+		reg = readl(mpp->reg_base + 0x0380);
+		cnt++;
+		if (cnt > 25)
+			break;
+	}
+	/* clear irq */
+	writel(0x00000000, reg_base + 0x0380);
+	local_irq_restore(flags);
+}
diff --git a/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_link_hack_rk3568.c b/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_link_hack_rk3568.c
new file mode 100644
index 0000000000000..6574729f690a7
--- /dev/null
+++ b/drivers/video/rockchip/mpp/hack/mpp_rkvdec2_link_hack_rk3568.c
@@ -0,0 +1,213 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Herman Chen <herman.chen@rock-chips.com>
+ */
+
+#define FIX_RK3568_BUF_SIZE		(2 * PAGE_SIZE)
+#define RKDEC_HACK_DATA_RPS_OFFSET	(128 * 1)
+#define RKDEC_HACK_DATA_PPS_OFFSET	(128 * 2)
+#define RKDEC_HACK_DATA_RLC_OFFSET	(128 * 3)
+#define RKDEC_HACK_DATA_OUT_OFFSET	(128 * 4)
+#define RKDEC_HACK_DATA_COLMV_OFFSET	(128 * 5)
+
+static u32 rkvdec2_3568_hack_link[] = {
+	0x00000000, /* 0x0020 */
+	0x00000001, /* 0x0024 */
+	0x00000001, /* 0x0028 */
+	0x00000072, /* 0x002c */
+	0x00000182, /* 0x0030 */
+	0x01040201, /* 0x0034 */
+	0x00000000, /* 0x0038 */
+	0x00000001, /* 0x003c */
+	0x00000030, /* 0x0040 */
+	0x00003fff, /* 0x0044 */
+	0x00000001, /* 0x0048 */
+	0x00000001, /* 0x004c */
+	0x00000010, /* 0x0050 */
+	0x00000006, /* 0x0054 */
+	0x00000000, /* 0x0058 */
+	0x00000000, /* 0x005c */
+	0xffffdfff, /* 0x0060 */
+	0x3ffbfbff, /* 0x0064 */
+	0x800fffff, /* 0x0068 */
+	0x00000000, /* 0x006C */
+
+	0x00000000, /* 0x0100 */
+	0x00000000, /* 0x0104 */
+	0x00000000, /* 0x0108 */
+	0x00000000, /* 0x010c */
+	0x00000000, /* 0x0110 */
+	0x00000000, /* 0x0114 */
+	0x00000000, /* 0x0118 */
+	0x00000000, /* 0x011c */
+	0x00000000, /* 0x0120 */
+	0x00000000, /* 0x0124 */
+	0x00000000, /* 0x0128 */
+	0x00000000, /* 0x012c */
+	0x00000000, /* 0x0130 */
+	0x00000000, /* 0x0134 */
+	0x00000000, /* 0x0138 */
+	0x00000000, /* 0x013c */
+	0x00000000, /* 0x0140 */
+	0x00000000, /* 0x0144 */
+	0x00000000, /* 0x0148 */
+	0x00000000, /* 0x014c */
+	0x00000000, /* 0x0150 */
+	0x00000000, /* 0x0154 */
+	0x00000000, /* 0x0158 */
+	0x00000000, /* 0x015c */
+	0x00000000, /* 0x0160 */
+	0x00000000, /* 0x0164 */
+	0x00000000, /* 0x0168 */
+	0x00000000, /* 0x016c */
+	0x00000000, /* 0x0170 */
+	0x00000000, /* 0x0174 */
+	0x00000000, /* 0x0178 */
+	0x00000000, /* 0x017c */
+	0x00000000, /* 0x0180 */
+	0x00000000, /* 0x0184 */
+	0x00000000, /* 0x0188 */
+	0x00000000, /* 0x018c */
+	0x00000000, /* 0x0190 */
+	0x00000000, /* 0x0194 */
+	0x00000000, /* 0x0198 */
+	0x00000000, /* 0x019c */
+	0x00000000, /* 0x01a0 */
+	0x00000000, /* 0x01a4 */
+	0x00000000, /* 0x01a8 */
+	0x00000000, /* 0x01ac */
+	0x00000000, /* 0x01b0 */
+	0x00000000, /* 0x01b4 */
+	0x00000000, /* 0x01b8 */
+	0x00000000, /* 0x01bc */
+	0x00000000, /* 0x01c0 */
+	0x00000000, /* 0x01c4 */
+	0x00000000, /* 0x01c8 */
+	0x00000000, /* 0x01cc */
+
+	0x00000000, /* 0x0200 */
+	0x00000000, /* 0x0204 */
+	0x00000000, /* 0x0208 */
+	0x00000000, /* 0x020c */
+	0x00000000, /* 0x0210 */
+	0x100001c0, /* 0x0214 */
+	0x100001c0, /* 0x0218 */
+	0x100001c0, /* 0x021c */
+	0x100001c0, /* 0x0220 */
+	0x10000340, /* 0x0224 */
+	0x10000340, /* 0x0228 */
+	0x10000000, /* 0x022c */
+	0x10000000, /* 0x0230 */
+	0x10000000, /* 0x0234 */
+	0x10000000, /* 0x0238 */
+	0x10000000, /* 0x023c */
+
+	0x00000000, /* 0x0280 */
+	0x00000000, /* 0x0284 */
+	0x00000000, /* 0x0288 */
+	0x00000000, /* 0x028c */
+	0x00000000, /* 0x0290 */
+	0x00000000, /* 0x0294 */
+	0x00000000, /* 0x0298 */
+	0x00000000, /* 0x029c */
+	0x00000000, /* 0x02a0 */
+	0x00000000, /* 0x02a4 */
+	0x00000000, /* 0x02a8 */
+	0x00000000, /* 0x02ac */
+	0x00000000, /* 0x02b0 */
+	0x00000000, /* 0x02b4 */
+	0x00000000, /* 0x02b8 */
+	0x00000000, /* 0x02bc */
+	0x00000000, /* 0x02c0 */
+	0x00000000, /* 0x02c4 */
+	0x00000000, /* 0x02c8 */
+	0x00000000, /* 0x02cc */
+	0x00000000, /* 0x02d0 */
+	0x00000000, /* 0x02d4 */
+	0x00000000, /* 0x02d8 */
+	0x00000000, /* 0x02dc */
+	0x00000000, /* 0x02e0 */
+	0x00000000, /* 0x02e4 */
+	0x00000000, /* 0x02e8 */
+	0x00000000, /* 0x02ec */
+	0x00000000, /* 0x02f0 */
+	0x00000000, /* 0x02f4 */
+	0x00000000, /* 0x02f8 */
+	0x00000000, /* 0x02fc */
+	0x00000000, /* 0x0300 */
+	0x00000000, /* 0x0304 */
+	0x00000000, /* 0x0308 */
+	0x00000000, /* 0x030c */
+	0x00000000, /* 0x0310 */
+	0x00000000, /* 0x0314 */
+	0x00000000, /* 0x0318 */
+	0x00000000, /* 0x031c */
+
+	0x00000000, /* 0x0380 */
+	0x00000000, /* 0x0384 */
+	0x00000000, /* 0x0388 */
+	0x00000000, /* 0x038c */
+	0x00000000, /* 0x0390 */
+	0x00000000, /* 0x0394 */
+	0x00000000, /* 0x0398 */
+	0x00000000, /* 0x039c */
+	0x00000000, /* 0x03a0 */
+	0x00000000, /* 0x03a4 */
+	0x00000000, /* 0x03a8 */
+	0x00000000, /* 0x03ac */
+	0x00000000, /* 0x03b0 */
+	0x00000000, /* 0x03b4 */
+	0x00000000, /* 0x03b8 */
+	0x00000000, /* 0x03bc */
+
+	0x00000000, /* 0x0400 */
+	0x00000000, /* 0x0404 */
+	0x00000000, /* 0x0408 */
+	0x00000000, /* 0x040c */
+	0x00000000, /* 0x0410 */
+	0x00000000, /* 0x0414 */
+	0x00000000, /* 0x0418 */
+	0x00000000, /* 0x041c */
+	0x00000000, /* 0x0420 */
+	0x00000000, /* 0x0424 */
+	0x00000000, /* 0x0428 */
+	0x00000000, /* 0x042c */
+	0x00000000, /* 0x0430 */
+	0x00000000, /* 0x0434 */
+	0x00000000, /* 0x0438 */
+	0x00000000, /* 0x043c */
+};
+
+void rkvdec2_3568_hack_fix_link(void *buf)
+{
+	memcpy(buf, rkvdec2_3568_hack_link, sizeof(rkvdec2_3568_hack_link));
+}
+
+void rkvdec2_link_hack_data_setup(struct mpp_dma_buffer *fix)
+{
+	u32 iova = fix->iova;
+	u32 i;
+
+	/* input stream */
+	rkvdec2_3568_hack_link[72] = iova;
+	/* error info */
+	rkvdec2_3568_hack_link[73] = iova + RKDEC_HACK_DATA_RLC_OFFSET;
+	/* output frame */
+	rkvdec2_3568_hack_link[74] = iova + RKDEC_HACK_DATA_OUT_OFFSET;
+	/* colmv out */
+	rkvdec2_3568_hack_link[75] = iova + 128 * 6;
+	/* error ref */
+	rkvdec2_3568_hack_link[76] = iova + 128 * 4;
+
+	/* rps in */
+	rkvdec2_3568_hack_link[89] = iova + RKDEC_HACK_DATA_PPS_OFFSET;
+	/* pps in */
+	rkvdec2_3568_hack_link[91] = iova + RKDEC_HACK_DATA_RPS_OFFSET;
+	for (i = 0; i < 33; i++)
+		rkvdec2_3568_hack_link[92 + i] = iova + RKDEC_HACK_DATA_COLMV_OFFSET;
+
+	rkvdec2_3568_hack_link[125] = iova + PAGE_SIZE;
+}
diff --git a/drivers/video/rockchip/mpp/mpp_av1dec.c b/drivers/video/rockchip/mpp/mpp_av1dec.c
new file mode 100644
index 0000000000000..33fdc776be302
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_av1dec.c
@@ -0,0 +1,1062 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+
+#define pr_fmt(fmt) "mpp_av1dec: " fmt
+
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/clk/clk-conf.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/clk/clk-conf.h>
+#include <linux/pm_runtime.h>
+#include <linux/pm_domain.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define AV1DEC_DRIVER_NAME		"mpp_av1dec"
+
+#define	AV1DEC_SESSION_MAX_BUFFERS		40
+
+/* REG_DEC_INT, bits for interrupt */
+#define	AV1DEC_INT_PIC_INF		BIT(24)
+#define	AV1DEC_INT_TIMEOUT		BIT(18)
+#define	AV1DEC_INT_SLICE		BIT(17)
+#define	AV1DEC_INT_STRM_ERROR		BIT(16)
+#define	AV1DEC_INT_ASO_ERROR		BIT(15)
+#define	AV1DEC_INT_BUF_EMPTY		BIT(14)
+#define	AV1DEC_INT_BUS_ERROR		BIT(13)
+#define	AV1DEC_DEC_INT			BIT(12)
+#define	AV1DEC_DEC_INT_RAW		BIT(8)
+#define	AV1DEC_DEC_IRQ_DIS		BIT(4)
+#define	AV1DEC_DEC_START		BIT(0)
+
+#define MPP_ALIGN(x, a)         (((x)+(a)-1)&~((a)-1))
+/* REG_DEC_EN, bit for gate */
+#define	AV1DEC_CLOCK_GATE_EN		BIT(10)
+
+#define to_av1dec_info(info)		\
+		container_of(info, struct av1dec_hw_info, hw)
+#define to_av1dec_task(ctx)		\
+		container_of(ctx, struct av1dec_task, mpp_task)
+#define to_av1dec_dev(dev)		\
+		container_of(dev, struct av1dec_dev, mpp)
+
+/* define functions */
+#define MPP_GET_BITS(v, p, b)	(((v) >> (p)) & ((1 << (b)) - 1))
+#define MPP_BASE_TO_IDX(a)	((a) / sizeof(u32))
+
+enum AV1DEC_CLASS_TYPE {
+	AV1DEC_CLASS_VCD	= 0,
+	AV1DEC_CLASS_CACHE	= 1,
+	AV1DEC_CLASS_AFBC	= 2,
+	AV1DEC_CLASS_BUTT,
+};
+
+enum av1dec_trans_type {
+	AV1DEC_TRANS_BASE	= 0x0000,
+
+	AV1DEC_TRANS_VCD	= AV1DEC_TRANS_BASE + 0,
+	AV1DEC_TRANS_CACHE	= AV1DEC_TRANS_BASE + 1,
+	AV1DEC_TRANS_AFBC	= AV1DEC_TRANS_BASE + 2,
+	AV1DEC_TRANS_BUTT,
+};
+
+struct av1dec_hw_info {
+	struct mpp_hw_info hw;
+	/* register range by class */
+	u32 reg_class_num;
+	struct {
+		u32 base_s;
+		u32 base_e;
+	} reg_class[AV1DEC_CLASS_BUTT];
+	/* fd translate for class */
+	u32 trans_class_num;
+	struct {
+		u32 class;
+		u32 trans_fmt;
+	} trans_class[AV1DEC_TRANS_BUTT];
+
+	/* interrupt config register */
+	int int_base;
+	/* enable hardware register */
+	int en_base;
+	/* status register */
+	int sta_base;
+	/* clear irq register */
+	int clr_base;
+	/* stream register */
+	int strm_base;
+
+	u32 err_mask;
+};
+
+struct av1dec_task {
+	struct mpp_task mpp_task;
+
+	struct av1dec_hw_info *hw_info;
+	/* for malloc register data buffer */
+	u32 *reg_data;
+	/* class register */
+	struct {
+		u32 valid;
+		u32 base;
+		u32 *data;
+		/* offset base reg_data */
+		u32 off;
+		/* length for class */
+		u32 len;
+	} reg_class[AV1DEC_CLASS_BUTT];
+	/* register offset info */
+	struct reg_offset_info off_inf;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct av1dec_dev {
+	struct mpp_dev mpp;
+	struct av1dec_hw_info *hw_info;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	u32 default_max_load;
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+
+	void __iomem *reg_base[AV1DEC_CLASS_BUTT];
+	int irq[AV1DEC_CLASS_BUTT];
+};
+
+static struct av1dec_hw_info av1dec_hw_info = {
+	.hw = {
+		.reg_num = 512,
+		.reg_id = 0,
+		.reg_en = 1,
+		.reg_start = 1,
+		.reg_end = 319,
+	},
+	.reg_class_num = 3,
+	.reg_class[AV1DEC_CLASS_VCD] = {
+		.base_s = 0x0000,
+		.base_e = 0x07fc,
+	},
+	.reg_class[AV1DEC_CLASS_CACHE] = {
+		.base_s = 0x10000,
+		.base_e = 0x10294,
+	},
+	.reg_class[AV1DEC_CLASS_AFBC] = {
+		.base_s = 0x20000,
+		.base_e = 0x2034c,
+	},
+	.trans_class_num = AV1DEC_TRANS_BUTT,
+	.trans_class[AV1DEC_CLASS_VCD] = {
+		.class = AV1DEC_CLASS_VCD,
+		.trans_fmt = AV1DEC_TRANS_VCD,
+	},
+	.trans_class[AV1DEC_CLASS_CACHE] = {
+		.class = AV1DEC_CLASS_CACHE,
+		.trans_fmt = AV1DEC_TRANS_CACHE,
+	},
+	.trans_class[AV1DEC_CLASS_AFBC] = {
+		.class = AV1DEC_CLASS_AFBC,
+		.trans_fmt = AV1DEC_TRANS_AFBC,
+	},
+	.int_base = 0x0004,
+	.en_base = 0x0004,
+	.sta_base = 0x0004,
+	.clr_base = 0x0004,
+	.strm_base = 0x02a4,
+	.err_mask = 0x7e000,
+};
+
+/*
+ * file handle translate information for v2
+ */
+static const u16 trans_tbl_av1_vcd[] = {
+	65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91,
+	93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115,
+	117, 133, 135, 137, 139, 141, 143, 145, 147,
+	167, 169, 171, 173, 175, 177, 179, 183, 190, 192, 194,
+	196, 198, 200, 202, 204, 224, 226, 228, 230, 232, 234,
+	236, 238, 326, 328, 339, 341, 348, 350, 505, 507
+};
+
+static const u16 trans_tbl_av1_cache[] = {
+	13, 18, 23, 28, 33, 38, 43, 48, 53, 58, 63, 68, 73, 78, 83, 88,
+	134, 135, 138, 139, 142, 143, 146, 147,
+};
+
+static const u16 trans_tbl_av1_afbc[] = {
+	32, 33, 34, 35, 48, 49, 50, 51, 96, 97, 98, 99
+};
+
+static struct mpp_trans_info trans_av1dec[] = {
+	[AV1DEC_TRANS_VCD] = {
+		.count = ARRAY_SIZE(trans_tbl_av1_vcd),
+		.table = trans_tbl_av1_vcd,
+	},
+	[AV1DEC_TRANS_CACHE] = {
+		.count = ARRAY_SIZE(trans_tbl_av1_cache),
+		.table = trans_tbl_av1_cache,
+	},
+	[AV1DEC_TRANS_AFBC] = {
+		.count = ARRAY_SIZE(trans_tbl_av1_afbc),
+		.table = trans_tbl_av1_afbc,
+	},
+};
+
+static bool req_over_class(struct mpp_request *req,
+			   struct av1dec_task *task, int class)
+{
+	bool ret;
+	u32 base_s, base_e, req_e;
+	struct av1dec_hw_info *hw = task->hw_info;
+
+	if (class > hw->reg_class_num)
+		return false;
+
+	base_s = hw->reg_class[class].base_s;
+	base_e = hw->reg_class[class].base_e;
+	req_e = req->offset + req->size - sizeof(u32);
+
+	ret = (req->offset <= base_e && req_e >= base_s) ? true : false;
+
+	return ret;
+}
+
+static int av1dec_alloc_reg_class(struct av1dec_task *task)
+{
+	int i;
+	u32 data_size;
+	struct av1dec_hw_info *hw = task->hw_info;
+
+	data_size = 0;
+	for (i = 0; i < hw->reg_class_num; i++) {
+		u32 base_s = hw->reg_class[i].base_s;
+		u32 base_e = hw->reg_class[i].base_e;
+
+		task->reg_class[i].base = base_s;
+		task->reg_class[i].off = data_size;
+		task->reg_class[i].len = base_e - base_s + sizeof(u32);
+		data_size += task->reg_class[i].len;
+	}
+
+	task->reg_data = kzalloc(data_size, GFP_KERNEL);
+	if (!task->reg_data)
+		return -ENOMEM;
+
+	for (i = 0; i < hw->reg_class_num; i++)
+		task->reg_class[i].data = task->reg_data + (task->reg_class[i].off / sizeof(u32));
+
+	return 0;
+}
+
+static int av1dec_update_req(struct av1dec_task *task, int class,
+			     struct mpp_request *req_in,
+			     struct mpp_request *req_out)
+{
+	u32 base_s, base_e, req_e, s, e;
+	struct av1dec_hw_info *hw = task->hw_info;
+
+	if (class > hw->reg_class_num)
+		return -EINVAL;
+
+	base_s = hw->reg_class[class].base_s;
+	base_e = hw->reg_class[class].base_e;
+	req_e = req_in->offset + req_in->size - sizeof(u32);
+	s = max(req_in->offset, base_s);
+	e = min(req_e, base_e);
+
+	req_out->offset = s;
+	req_out->size = e - s + sizeof(u32);
+	req_out->data = (u8 *)req_in->data + (s - req_in->offset);
+	mpp_debug(DEBUG_TASK_INFO, "req_out->offset=%08x, req_out->size=%d\n",
+		  req_out->offset, req_out->size);
+
+	return 0;
+}
+
+static int av1dec_extract_task_msg(struct av1dec_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	int ret;
+	u32 i;
+	struct mpp_request *req;
+	struct av1dec_hw_info *hw = task->hw_info;
+
+	mpp_debug_enter();
+
+	mpp_debug(DEBUG_TASK_INFO, "req_cnt=%d, set_cnt=%d, poll_cnt=%d, reg_class=%d\n",
+		msgs->req_cnt, msgs->set_cnt, msgs->poll_cnt, hw->reg_class_num);
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		mpp_debug(DEBUG_TASK_INFO, "msg: cmd %08x, offset %08x, size %d\n",
+			req->cmd, req->offset, req->size);
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			u32 class;
+			u32 base, *regs;
+			struct mpp_request *wreq;
+
+			for (class = 0; class < hw->reg_class_num; class++) {
+				if (!req_over_class(req, task, class))
+					continue;
+				mpp_debug(DEBUG_TASK_INFO, "found write_calss %d\n", class);
+				wreq = &task->w_reqs[task->w_req_cnt];
+				av1dec_update_req(task, class, req, wreq);
+
+				base = task->reg_class[class].base;
+				regs = (u32 *)task->reg_class[class].data;
+				regs += MPP_BASE_TO_IDX(req->offset - base);
+				if (copy_from_user(regs, wreq->data, wreq->size)) {
+					mpp_err("copy_from_user fail, offset %08x\n", wreq->offset);
+					ret = -EIO;
+					goto fail;
+				}
+				task->w_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			u32 class;
+			struct mpp_request *rreq;
+
+			for (class = 0; class < hw->reg_class_num; class++) {
+				if (!req_over_class(req, task, class))
+					continue;
+				mpp_debug(DEBUG_TASK_INFO, "found read_calss %d\n", class);
+				rreq = &task->r_reqs[task->r_req_cnt];
+				av1dec_update_req(task, class, req, rreq);
+				task->r_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt=%d, r_req_cnt=%d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	mpp_debug_leave();
+	return 0;
+
+fail:
+	mpp_debug_leave();
+	return ret;
+}
+
+static void *av1dec_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	u32 i, j;
+	struct mpp_task *mpp_task = NULL;
+	struct av1dec_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	task->hw_info = to_av1dec_info(mpp_task->hw_info);
+
+	/* alloc reg data for task */
+	ret = av1dec_alloc_reg_class(task);
+	if (ret)
+		goto free_task;
+	mpp_task->reg = task->reg_class[0].data;
+	/* extract reqs for current task */
+	ret = av1dec_extract_task_msg(task, msgs);
+	if (ret)
+		goto free_reg_class;
+
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		int cnt;
+		const u16 *tbl;
+		u32 offset;
+		struct av1dec_hw_info *hw = task->hw_info;
+
+		for (i = 0; i < task->w_req_cnt; i++) {
+			struct mpp_request *req = &task->w_reqs[i];
+
+			for (i = 0; i < hw->trans_class_num; i++) {
+				u32 class = hw->trans_class[i].class;
+				u32 fmt = hw->trans_class[i].trans_fmt;
+				u32 *reg = task->reg_class[class].data;
+				u32 base_idx = MPP_BASE_TO_IDX(task->reg_class[class].base);
+
+				if (!req_over_class(req, task, i))
+					continue;
+				mpp_debug(DEBUG_TASK_INFO, "class=%d, base_idx=%d\n",
+					  class, base_idx);
+				if (!reg)
+					continue;
+
+				ret = mpp_translate_reg_address(session, mpp_task, fmt, reg, NULL);
+				if (ret)
+					goto fail;
+
+				cnt = mpp->var->trans_info[fmt].count;
+				tbl = mpp->var->trans_info[fmt].table;
+				for (j = 0; j < cnt; j++) {
+					offset = mpp_query_reg_offset_info(&task->off_inf,
+									tbl[j] + base_idx);
+					mpp_debug(DEBUG_IOMMU,
+						"reg[%d] + offset %d\n", tbl[j] + base_idx, offset);
+					reg[tbl[j]] += offset;
+				}
+			}
+		}
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+free_reg_class:
+	kfree(task->reg_data);
+free_task:
+	kfree(task);
+
+	return NULL;
+}
+#define AV1_PP_CONFIG_INDEX	321
+#define AV1_PP_TILE_SIZE	GENMASK_ULL(10, 9)
+#define AV1_PP_TILE_16X16	BIT(10)
+
+#define REG_CONTROL		0x20
+#define REG_INTRENBL		0x34
+#define REG_ACKNOWLEDGE		0x38
+#define REG_FORMAT		0x100
+#define REG_COMPRESSENABLE	0x340
+#define REG_HEADERBASE		0x80
+#define REG_PAYLOADBASE		0xC0
+#define REG_INPUTBUFBASE	0x180
+#define REG_INPUTBUFSTRIDE	0x200
+#define REG_INPUTBUFSIZE	0x140
+
+static int av1dec_set_afbc(struct av1dec_dev *dec, struct av1dec_task *task)
+{
+	u32 *regs = (u32 *)task->reg_class[0].data;
+	u32 width = (regs[4] >> 19) * 8;
+	u32 height = ((regs[4] >> 6) & 0x1fff) * 8;
+	u32 pixel_width_y, pixel_width_c, pixel_width = 8;
+	u32 vir_top  =  (((regs[503]) >> 16) & 0xf);
+	u32 vir_left  =  (((regs[503]) >> 20) & 0xf);
+	u32 vir_bottom = (((regs[503]) >> 24) & 0xf);
+	u32 vir_right  =  (((regs[503]) >> 28) & 0xf);
+	u32 fbc_format = 0;
+	u32 fbc_stream_number = 0;
+	u32 fbc_comp_en[2] = {0, 0};
+	u32 pp_width_final[2] = {0, 0};
+	u32 pp_height_final[2] = {0, 0};
+	u32 pp_hdr_base[2] = {0, 0};
+	u32 pp_payload_base[2] = {0, 0};
+	u32 pp_input_base[2] = {0, 0};
+	u32 pp_input_stride[2] = {0, 0};
+	u32 bus_address;
+	u32 i = 0;
+
+	pixel_width_y = ((regs[8] >> 6) & 0x3) + 8;
+	pixel_width_c = ((regs[8] >> 4) & 0x3) + 8;
+	pixel_width = (pixel_width_y == 8 && pixel_width_c == 8) ? 8 : 10;
+
+	if ((regs[AV1_PP_CONFIG_INDEX] & AV1_PP_TILE_SIZE) == AV1_PP_TILE_16X16) {
+		u32 offset = MPP_ALIGN((vir_left + width + vir_right) *
+			     (height + 28) / 16, 64);
+
+		bus_address = regs[505];
+		fbc_stream_number++;
+		if (pixel_width == 10)
+			fbc_format = 3;
+		else
+			fbc_format = 9;
+		fbc_comp_en[0] = 1;
+		fbc_comp_en[1] = 1;
+
+		pp_width_final[0] = pp_width_final[1] = vir_left + width + vir_right;
+		pp_height_final[0] = pp_height_final[1] = vir_top + height + vir_bottom;
+
+		if (pixel_width == 10)
+			pp_input_stride[0] = pp_input_stride[1] = 2 * pp_width_final[0];
+		else
+			pp_input_stride[0] = pp_input_stride[1] = pp_width_final[0];
+
+		pp_hdr_base[0] = pp_hdr_base[1] = bus_address;
+		pp_payload_base[0] = pp_payload_base[1] = bus_address + offset;
+		pp_input_base[0] = pp_input_base[1] = bus_address;
+
+		writel_relaxed((fbc_stream_number << 9),
+			       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_CONTROL);
+		writel_relaxed(0x1, dec->reg_base[AV1DEC_CLASS_AFBC] + REG_INTRENBL);
+
+		for (i = 0; i < 2; i++) {
+			writel_relaxed(fbc_format,
+				       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_FORMAT + i * 4);
+			writel_relaxed(fbc_comp_en[i], dec->reg_base[AV1DEC_CLASS_AFBC] +
+				       REG_COMPRESSENABLE + i * 4);
+			/* hdr base */
+			writel_relaxed(pp_hdr_base[i],
+				       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_HEADERBASE + i * 4);
+			/* payload */
+			writel_relaxed(pp_payload_base[i],
+				       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_PAYLOADBASE + i * 4);
+			/* bufsize */
+			writel_relaxed(((pp_height_final[i] << 15) | pp_width_final[i]),
+				       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_INPUTBUFSIZE + i * 4);
+			/* buf */
+			writel_relaxed(pp_input_base[i],
+				       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_INPUTBUFBASE + i * 4);
+			/* stride */
+			writel_relaxed(pp_input_stride[i], dec->reg_base[AV1DEC_CLASS_AFBC] +
+				       REG_INPUTBUFSTRIDE + i * 4);
+		}
+		/* wmb */
+		wmb();
+		writel(((fbc_stream_number << 9) | (1 << 7)),
+		       dec->reg_base[AV1DEC_CLASS_AFBC] + REG_CONTROL); /* update */
+		writel((fbc_stream_number << 9), dec->reg_base[AV1DEC_CLASS_AFBC] + REG_CONTROL);
+
+	}
+	return 0;
+}
+
+static int av1dec_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	int i;
+	u32 en_val = 0;
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_hw_info *hw = dec->hw_info;
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	av1dec_set_afbc(dec, task);
+
+	for (i = 0; i < task->w_req_cnt; i++) {
+		int class;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		for (class = 0; class < hw->reg_class_num; class++) {
+			int j, s, e;
+			u32 base, *regs;
+
+			if (!req_over_class(req, task, class))
+				continue;
+			base = task->reg_class[class].base;
+			s = MPP_BASE_TO_IDX(req->offset - base);
+			e = s + req->size / sizeof(u32);
+			regs = (u32 *)task->reg_class[class].data;
+
+			mpp_debug(DEBUG_TASK_INFO, "found rd_class %d, base=%08x, s=%d, e=%d\n",
+				  class, base, s, e);
+			for (j = s; j < e; j++) {
+				if (class == 0 && j == hw->hw.reg_en) {
+					en_val = regs[j];
+					continue;
+				}
+				writel_relaxed(regs[j], dec->reg_base[class] + j * sizeof(u32));
+			}
+		}
+	}
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, hw->en_base, en_val);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int av1dec_vcd_irq(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_hw_info *hw = dec->hw_info;
+
+	mpp_debug_enter();
+
+	mpp->irq_status = mpp_read(mpp, hw->sta_base);
+	if (!mpp->irq_status)
+		return IRQ_NONE;
+
+	mpp_write(mpp, hw->clr_base, 0);
+
+	mpp_debug_leave();
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int av1dec_isr(struct mpp_dev *mpp)
+{
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+	u32 *regs = (u32 *)task->reg_class[0].data;
+
+	mpp_debug_enter();
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+
+	/* clear l2 cache status */
+	writel_relaxed(0x0, dec->reg_base[AV1DEC_CLASS_CACHE] + 0x020);
+	writel_relaxed(0x0, dec->reg_base[AV1DEC_CLASS_CACHE] + 0x204);
+	/* multi id enable bit */
+	writel_relaxed(0x00000000, dec->reg_base[AV1DEC_CLASS_CACHE] + 0x208);
+
+	if (((regs[321] >> 9) & 0x3) == 0x2) {
+		u32 ack_status = readl(dec->reg_base[AV1DEC_CLASS_AFBC] + REG_ACKNOWLEDGE);
+
+		if ((ack_status & 0x1) == 0x1) {
+			u32 ctl_val = readl(dec->reg_base[AV1DEC_CLASS_AFBC] + REG_CONTROL);
+
+			ctl_val |= 1;
+			writel_relaxed(ctl_val, dec->reg_base[AV1DEC_CLASS_AFBC] + REG_CONTROL);
+		}
+	}
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+	if (task->irq_status & dec->hw_info->err_mask) {
+		atomic_inc(&mpp->reset_request);
+		/* dump register */
+		if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+			mpp_debug(DEBUG_DUMP_ERR_REG, "irq_status: %08x\n",
+				  task->irq_status);
+			mpp_task_dump_hw_reg(mpp);
+		}
+	}
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int av1dec_finish(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	u32 i;
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_hw_info *hw = dec->hw_info;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		int class;
+		struct mpp_request *req = &task->r_reqs[i];
+
+		for (class = 0; class < hw->reg_class_num; class++) {
+			int j, s, e;
+			u32 base, *regs;
+
+			if (!req_over_class(req, task, class))
+				continue;
+			base = task->reg_class[class].base;
+			s = MPP_BASE_TO_IDX(req->offset - base);
+			e = s + req->size / sizeof(u32);
+			regs = (u32 *)task->reg_class[class].data;
+
+			mpp_debug(DEBUG_TASK_INFO, "found rd_class %d, base=%08x, s=%d, e=%d\n",
+				  class, base, s, e);
+			for (j = s; j < e; j++) {
+				/* revert hack for irq status */
+				if (class == 0 && j == MPP_BASE_TO_IDX(hw->sta_base)) {
+					regs[j] = task->irq_status;
+					continue;
+				}
+				regs[j] = readl_relaxed(dec->reg_base[class] + j * sizeof(u32));
+			}
+		}
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int av1dec_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_hw_info *hw = dec->hw_info;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		int class;
+		struct mpp_request *req = &task->r_reqs[i];
+
+		for (class = 0; class < hw->reg_class_num; class++) {
+			u32 base, *regs;
+
+			if (!req_over_class(req, task, class))
+				continue;
+			base = task->reg_class[class].base;
+			regs = (u32 *)task->reg_class[class].data;
+			regs += MPP_BASE_TO_IDX(req->offset - base);
+
+			if (copy_to_user(req->data, regs, req->size)) {
+				mpp_err("copy_to_user reg fail\n");
+				return -EIO;
+			}
+		}
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int av1dec_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task->reg_data);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_PROC_FS
+static int av1dec_procfs_remove(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int av1dec_procfs_init(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+
+	dec->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	/* for debug */
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int av1dec_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int av1dec_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int av1dec_init(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	int ret = 0;
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load",
+			     &dec->default_max_load);
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int av1dec_reset(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+
+	mpp_debug_enter();
+
+	if (dec->rst_a && dec->rst_h) {
+		rockchip_pmu_idle_request(mpp->dev, true);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		rockchip_pmu_idle_request(mpp->dev, false);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int av1dec_clk_on(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int av1dec_clk_off(struct mpp_dev *mpp)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+
+	clk_disable_unprepare(dec->aclk_info.clk);
+	clk_disable_unprepare(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int av1dec_set_freq(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	struct av1dec_dev *dec = to_av1dec_dev(mpp);
+	struct av1dec_task *task = to_av1dec_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static struct mpp_hw_ops av1dec_hw_ops = {
+	.init = av1dec_init,
+	.clk_on = av1dec_clk_on,
+	.clk_off = av1dec_clk_off,
+	.set_freq = av1dec_set_freq,
+	.reset = av1dec_reset,
+};
+
+static struct mpp_dev_ops av1dec_dev_ops = {
+	.alloc_task = av1dec_alloc_task,
+	.run = av1dec_run,
+	.irq = av1dec_vcd_irq,
+	.isr = av1dec_isr,
+	.finish = av1dec_finish,
+	.result = av1dec_result,
+	.free_task = av1dec_free_task,
+};
+static const struct mpp_dev_var av1dec_data = {
+	.device_type = MPP_DEVICE_AV1DEC,
+	.hw_info = &av1dec_hw_info.hw,
+	.trans_info = trans_av1dec,
+	.hw_ops = &av1dec_hw_ops,
+	.dev_ops = &av1dec_dev_ops,
+};
+
+static const struct of_device_id mpp_av1dec_dt_match[] = {
+	{
+		.compatible = "rockchip,av1-decoder",
+		.data = &av1dec_data,
+	},
+	{},
+};
+
+static int av1dec_cache_init(struct platform_device *pdev, struct av1dec_dev *dec)
+{
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "cache");
+	if (!res)
+		return -ENOMEM;
+
+	dec->reg_base[AV1DEC_CLASS_CACHE] = devm_ioremap(dev, res->start, resource_size(res));
+	if (!dec->reg_base[AV1DEC_CLASS_CACHE]) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int av1dec_afbc_init(struct platform_device *pdev, struct av1dec_dev *dec)
+{
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "afbc");
+	if (!res)
+		return -ENOMEM;
+
+	dec->reg_base[AV1DEC_CLASS_AFBC] = devm_ioremap(dev, res->start, resource_size(res));
+	if (!dec->reg_base[AV1DEC_CLASS_AFBC]) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		return -EINVAL;
+	}
+	dec->irq[AV1DEC_CLASS_AFBC] = platform_get_irq(pdev, 2);
+
+	return 0;
+}
+
+static int av1dec_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct av1dec_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+
+	dev_info(dev, "probing start\n");
+
+	dec = devm_kzalloc(dev, sizeof(*dec), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_av1dec_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+	/* get vcd resource */
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret)
+		return ret;
+
+	dec->reg_base[AV1DEC_CLASS_VCD] = mpp->reg_base;
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		goto failed_get_irq;
+	}
+	dec->irq[AV1DEC_CLASS_VCD] = mpp->irq;
+	/* get cache resource */
+	ret = av1dec_cache_init(pdev, dec);
+	if (ret)
+		goto failed_get_irq;
+	/* get afbc resource */
+	ret = av1dec_afbc_init(pdev, dec);
+	if (ret)
+		goto failed_get_irq;
+	mpp->session_max_buffers = AV1DEC_SESSION_MAX_BUFFERS;
+	dec->hw_info = to_av1dec_info(mpp->var->hw_info);
+	av1dec_procfs_init(mpp);
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+
+failed_get_irq:
+	mpp_dev_remove(mpp);
+
+	return ret;
+}
+
+static int av1dec_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = platform_get_drvdata(pdev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	av1dec_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_av1dec_driver = {
+	.probe = av1dec_probe,
+	.remove = av1dec_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = AV1DEC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_av1dec_dt_match),
+	},
+};
diff --git a/drivers/video/rockchip/mpp/mpp_common.c b/drivers/video/rockchip/mpp/mpp_common.c
new file mode 100644
index 0000000000000..ab02cea09f170
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_common.c
@@ -0,0 +1,2688 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/proc_fs.h>
+#include <linux/pm_runtime.h>
+#include <linux/poll.h>
+#include <linux/regmap.h>
+#include <linux/rwsem.h>
+#include <linux/mfd/syscon.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/nospec.h>
+
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+/* input parmater structure for version 1 */
+struct mpp_msg_v1 {
+	__u32 cmd;
+	__u32 flags;
+	__u32 size;
+	__u32 offset;
+	__u64 data_ptr;
+};
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+const char *mpp_device_name[MPP_DEVICE_BUTT] = {
+	[MPP_DEVICE_VDPU1]		= "VDPU1",
+	[MPP_DEVICE_VDPU2]		= "VDPU2",
+	[MPP_DEVICE_VDPU1_PP]		= "VDPU1_PP",
+	[MPP_DEVICE_VDPU2_PP]		= "VDPU2_PP",
+	[MPP_DEVICE_AV1DEC]		= "AV1DEC",
+	[MPP_DEVICE_HEVC_DEC]		= "HEVC_DEC",
+	[MPP_DEVICE_RKVDEC]		= "RKVDEC",
+	[MPP_DEVICE_AVSPLUS_DEC]	= "AVSPLUS_DEC",
+	[MPP_DEVICE_RKJPEGD]		= "RKJPEGD",
+	[MPP_DEVICE_RKVENC]		= "RKVENC",
+	[MPP_DEVICE_VEPU1]		= "VEPU1",
+	[MPP_DEVICE_VEPU2]		= "VEPU2",
+	[MPP_DEVICE_VEPU2_JPEG]		= "VEPU2",
+	[MPP_DEVICE_RKJPEGE]		= "RKJPEGE",
+	[MPP_DEVICE_VEPU22]		= "VEPU22",
+	[MPP_DEVICE_IEP2]		= "IEP2",
+	[MPP_DEVICE_VDPP]		= "VDPP",
+};
+
+const char *enc_info_item_name[ENC_INFO_BUTT] = {
+	[ENC_INFO_BASE]		= "null",
+	[ENC_INFO_WIDTH]	= "width",
+	[ENC_INFO_HEIGHT]	= "height",
+	[ENC_INFO_FORMAT]	= "format",
+	[ENC_INFO_FPS_IN]	= "fps_in",
+	[ENC_INFO_FPS_OUT]	= "fps_out",
+	[ENC_INFO_RC_MODE]	= "rc_mode",
+	[ENC_INFO_BITRATE]	= "bitrate",
+	[ENC_INFO_GOP_SIZE]	= "gop_size",
+	[ENC_INFO_FPS_CALC]	= "fps_calc",
+	[ENC_INFO_PROFILE]	= "profile",
+};
+
+#endif
+
+static void mpp_attach_workqueue(struct mpp_dev *mpp,
+				 struct mpp_taskqueue *queue);
+
+static int
+mpp_taskqueue_pop_pending(struct mpp_taskqueue *queue,
+			  struct mpp_task *task)
+{
+	if (!task->session || !task->session->mpp)
+		return -EINVAL;
+
+	mutex_lock(&queue->pending_lock);
+	list_del_init(&task->queue_link);
+	mutex_unlock(&queue->pending_lock);
+	kref_put(&task->ref, mpp_free_task);
+
+	return 0;
+}
+
+static struct mpp_task *
+mpp_taskqueue_get_pending_task(struct mpp_taskqueue *queue)
+{
+	struct mpp_task *task = NULL;
+
+	mutex_lock(&queue->pending_lock);
+	task = list_first_entry_or_null(&queue->pending_list,
+					struct mpp_task,
+					queue_link);
+	mutex_unlock(&queue->pending_lock);
+
+	return task;
+}
+
+static bool
+mpp_taskqueue_is_running(struct mpp_taskqueue *queue)
+{
+	unsigned long flags;
+	bool flag;
+
+	spin_lock_irqsave(&queue->running_lock, flags);
+	flag = !list_empty(&queue->running_list);
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+
+	return flag;
+}
+
+int mpp_taskqueue_pending_to_run(struct mpp_taskqueue *queue, struct mpp_task *task)
+{
+	unsigned long flags;
+
+	mutex_lock(&queue->pending_lock);
+	spin_lock_irqsave(&queue->running_lock, flags);
+	list_move_tail(&task->queue_link, &queue->running_list);
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+
+	mutex_unlock(&queue->pending_lock);
+
+	return 0;
+}
+
+static struct mpp_task *
+mpp_taskqueue_get_running_task(struct mpp_taskqueue *queue)
+{
+	unsigned long flags;
+	struct mpp_task *task = NULL;
+
+	spin_lock_irqsave(&queue->running_lock, flags);
+	task = list_first_entry_or_null(&queue->running_list,
+					struct mpp_task,
+					queue_link);
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+
+	return task;
+}
+
+static int
+mpp_taskqueue_pop_running(struct mpp_taskqueue *queue,
+			  struct mpp_task *task)
+{
+	unsigned long flags;
+
+	if (!task->session || !task->session->mpp)
+		return -EINVAL;
+
+	spin_lock_irqsave(&queue->running_lock, flags);
+	list_del_init(&task->queue_link);
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+	kref_put(&task->ref, mpp_free_task);
+
+	return 0;
+}
+
+static void
+mpp_taskqueue_trigger_work(struct mpp_dev *mpp)
+{
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+}
+
+int mpp_power_on(struct mpp_dev *mpp)
+{
+	pm_runtime_get_sync(mpp->dev);
+	pm_stay_awake(mpp->dev);
+
+	if (mpp->hw_ops->clk_on)
+		mpp->hw_ops->clk_on(mpp);
+
+	return 0;
+}
+
+int mpp_power_off(struct mpp_dev *mpp)
+{
+	if (mpp->hw_ops->clk_off)
+		mpp->hw_ops->clk_off(mpp);
+
+	pm_relax(mpp->dev);
+	if (mpp_taskqueue_get_pending_task(mpp->queue) ||
+	    mpp_taskqueue_get_running_task(mpp->queue)) {
+		pm_runtime_mark_last_busy(mpp->dev);
+		pm_runtime_put_autosuspend(mpp->dev);
+	} else {
+		pm_runtime_put_sync_suspend(mpp->dev);
+	}
+
+	return 0;
+}
+
+static void task_msgs_reset(struct mpp_task_msgs *msgs)
+{
+	list_del_init(&msgs->list);
+
+	msgs->flags = 0;
+	msgs->req_cnt = 0;
+	msgs->set_cnt = 0;
+	msgs->poll_cnt = 0;
+}
+
+static void task_msgs_init(struct mpp_task_msgs *msgs, struct mpp_session *session)
+{
+	INIT_LIST_HEAD(&msgs->list);
+
+	msgs->session = session;
+	msgs->queue = NULL;
+	msgs->task = NULL;
+	msgs->mpp = NULL;
+
+	msgs->ext_fd = -1;
+
+	task_msgs_reset(msgs);
+}
+
+static struct mpp_task_msgs *get_task_msgs(struct mpp_session *session)
+{
+	unsigned long flags;
+	struct mpp_task_msgs *msgs;
+
+	spin_lock_irqsave(&session->lock_msgs, flags);
+	msgs = list_first_entry_or_null(&session->list_msgs_idle,
+					struct mpp_task_msgs, list_session);
+	if (msgs) {
+		list_move_tail(&msgs->list_session, &session->list_msgs);
+		spin_unlock_irqrestore(&session->lock_msgs, flags);
+
+		return msgs;
+	}
+	spin_unlock_irqrestore(&session->lock_msgs, flags);
+
+	msgs = kzalloc(sizeof(*msgs), GFP_KERNEL);
+	task_msgs_init(msgs, session);
+	INIT_LIST_HEAD(&msgs->list_session);
+
+	spin_lock_irqsave(&session->lock_msgs, flags);
+	list_move_tail(&msgs->list_session, &session->list_msgs);
+	session->msgs_cnt++;
+	spin_unlock_irqrestore(&session->lock_msgs, flags);
+
+	mpp_debug_func(DEBUG_TASK_INFO, "session %d:%d msgs cnt %d\n",
+		       session->pid, session->index, session->msgs_cnt);
+
+	return msgs;
+}
+
+static void put_task_msgs(struct mpp_task_msgs *msgs)
+{
+	struct mpp_session *session = msgs->session;
+	unsigned long flags;
+
+	if (!session) {
+		pr_err("invalid msgs without session\n");
+		return;
+	}
+
+	if (msgs->ext_fd >= 0) {
+		fdput(msgs->f);
+		msgs->ext_fd = -1;
+	}
+
+	task_msgs_reset(msgs);
+
+	spin_lock_irqsave(&session->lock_msgs, flags);
+	list_move_tail(&msgs->list_session, &session->list_msgs_idle);
+	spin_unlock_irqrestore(&session->lock_msgs, flags);
+}
+
+static void clear_task_msgs(struct mpp_session *session)
+{
+	struct mpp_task_msgs *msgs, *n;
+	LIST_HEAD(list_to_free);
+	unsigned long flags;
+
+	spin_lock_irqsave(&session->lock_msgs, flags);
+
+	list_for_each_entry_safe(msgs, n, &session->list_msgs, list_session)
+		list_move_tail(&msgs->list_session, &list_to_free);
+
+	list_for_each_entry_safe(msgs, n, &session->list_msgs_idle, list_session)
+		list_move_tail(&msgs->list_session, &list_to_free);
+
+	spin_unlock_irqrestore(&session->lock_msgs, flags);
+
+	list_for_each_entry_safe(msgs, n, &list_to_free, list_session)
+		kfree(msgs);
+}
+
+static void mpp_session_clear_pending(struct mpp_session *session)
+{
+	struct mpp_task *task = NULL, *n;
+
+	/* clear session pending list */
+	mutex_lock(&session->pending_lock);
+	list_for_each_entry_safe(task, n,
+				 &session->pending_list,
+				 pending_link) {
+		/* abort task in taskqueue */
+		atomic_inc(&task->abort_request);
+		list_del_init(&task->pending_link);
+		kref_put(&task->ref, mpp_free_task);
+	}
+	mutex_unlock(&session->pending_lock);
+}
+
+void mpp_session_cleanup_detach(struct mpp_taskqueue *queue, struct kthread_work *work)
+{
+	struct mpp_session *session, *n;
+
+	if (!atomic_read(&queue->detach_count))
+		return;
+
+	mutex_lock(&queue->session_lock);
+	list_for_each_entry_safe(session, n, &queue->session_detach, session_link) {
+		s32 task_count = atomic_read(&session->task_count);
+
+		if (!task_count) {
+			list_del_init(&session->session_link);
+			atomic_dec(&queue->detach_count);
+		}
+
+		mutex_unlock(&queue->session_lock);
+
+		if (task_count) {
+			mpp_dbg_session("session %d:%d not finished %d task cnt %d\n",
+					session->device_type, session->index,
+					atomic_read(&queue->detach_count), task_count);
+
+			mpp_session_clear_pending(session);
+		} else {
+			mpp_dbg_session("queue detach %d\n",
+					atomic_read(&queue->detach_count));
+
+			mpp_session_deinit(session);
+		}
+
+		mutex_lock(&queue->session_lock);
+	}
+	mutex_unlock(&queue->session_lock);
+
+	if (atomic_read(&queue->detach_count)) {
+		mpp_dbg_session("queue detach %d again\n",
+				atomic_read(&queue->detach_count));
+
+		kthread_queue_work(&queue->worker, work);
+	}
+}
+
+static struct mpp_session *mpp_session_init(void)
+{
+	struct mpp_session *session = kzalloc(sizeof(*session), GFP_KERNEL);
+
+	if (!session)
+		return NULL;
+
+	session->pid = current->pid;
+
+	mutex_init(&session->pending_lock);
+	INIT_LIST_HEAD(&session->pending_list);
+	INIT_LIST_HEAD(&session->service_link);
+	INIT_LIST_HEAD(&session->session_link);
+
+	atomic_set(&session->task_count, 0);
+	atomic_set(&session->release_request, 0);
+
+	INIT_LIST_HEAD(&session->list_msgs);
+	INIT_LIST_HEAD(&session->list_msgs_idle);
+	spin_lock_init(&session->lock_msgs);
+
+	mpp_dbg_session("session %p init\n", session);
+	return session;
+}
+
+static void mpp_session_deinit_default(struct mpp_session *session)
+{
+	if (session->mpp) {
+		struct mpp_dev *mpp = session->mpp;
+
+		if (mpp->dev_ops->free_session)
+			mpp->dev_ops->free_session(session);
+
+		mpp_session_clear_pending(session);
+
+		if (session->dma) {
+			mpp_iommu_down_read(mpp->iommu_info);
+			mpp_dma_session_destroy(session->dma);
+			mpp_iommu_up_read(mpp->iommu_info);
+			session->dma = NULL;
+		}
+	}
+
+	if (session->srv) {
+		struct mpp_service *srv = session->srv;
+
+		mutex_lock(&srv->session_lock);
+		list_del_init(&session->service_link);
+		mutex_unlock(&srv->session_lock);
+	}
+
+	list_del_init(&session->session_link);
+}
+
+void mpp_session_deinit(struct mpp_session *session)
+{
+	mpp_dbg_session("session %d:%d task %d deinit\n", session->pid,
+			session->index, atomic_read(&session->task_count));
+
+	if (likely(session->deinit))
+		session->deinit(session);
+	else
+		pr_err("invalid NULL session deinit function\n");
+
+	clear_task_msgs(session);
+
+	kfree(session);
+}
+
+static void mpp_session_attach_workqueue(struct mpp_session *session,
+					 struct mpp_taskqueue *queue)
+{
+	mpp_dbg_session("session %d:%d attach\n", session->pid, session->index);
+	mutex_lock(&queue->session_lock);
+	list_add_tail(&session->session_link, &queue->session_attach);
+	mutex_unlock(&queue->session_lock);
+}
+
+static void mpp_session_detach_workqueue(struct mpp_session *session)
+{
+	struct mpp_taskqueue *queue;
+	struct mpp_dev *mpp;
+
+	if (!session->mpp || !session->mpp->queue)
+		return;
+
+	mpp_dbg_session("session %d:%d detach\n", session->pid, session->index);
+	mpp = session->mpp;
+	queue = mpp->queue;
+
+	mutex_lock(&queue->session_lock);
+	list_del_init(&session->session_link);
+	list_add_tail(&session->session_link, &queue->session_detach);
+	atomic_inc(&queue->detach_count);
+	mutex_unlock(&queue->session_lock);
+
+	mpp_taskqueue_trigger_work(mpp);
+}
+
+static int
+mpp_session_push_pending(struct mpp_session *session,
+			 struct mpp_task *task)
+{
+	kref_get(&task->ref);
+	mutex_lock(&session->pending_lock);
+	if (session->srv->timing_en) {
+		task->on_pending = ktime_get();
+		set_bit(TASK_TIMING_PENDING, &task->state);
+	}
+	list_add_tail(&task->pending_link, &session->pending_list);
+	mutex_unlock(&session->pending_lock);
+
+	return 0;
+}
+
+static int
+mpp_session_pop_pending(struct mpp_session *session,
+			struct mpp_task *task)
+{
+	mutex_lock(&session->pending_lock);
+	list_del_init(&task->pending_link);
+	mutex_unlock(&session->pending_lock);
+	kref_put(&task->ref, mpp_free_task);
+
+	return 0;
+}
+
+static struct mpp_task *
+mpp_session_get_pending_task(struct mpp_session *session)
+{
+	struct mpp_task *task = NULL;
+
+	mutex_lock(&session->pending_lock);
+	task = list_first_entry_or_null(&session->pending_list,
+					struct mpp_task,
+					pending_link);
+	mutex_unlock(&session->pending_lock);
+
+	return task;
+}
+
+void mpp_free_task(struct kref *ref)
+{
+	struct mpp_dev *mpp;
+	struct mpp_session *session;
+	struct mpp_task *task = container_of(ref, struct mpp_task, ref);
+
+	if (!task->session) {
+		mpp_err("task %p, task->session is null.\n", task);
+		return;
+	}
+	session = task->session;
+
+	mpp_debug_func(DEBUG_TASK_INFO, "session %d:%d task %d state 0x%lx abort %d\n",
+		       session->device_type, session->index, task->task_index,
+		       task->state, atomic_read(&task->abort_request));
+
+	mpp = mpp_get_task_used_device(task, session);
+	if (mpp->dev_ops->free_task)
+		mpp->dev_ops->free_task(session, task);
+
+	/* Decrease reference count */
+	atomic_dec(&session->task_count);
+	atomic_dec(&mpp->task_count);
+}
+
+static void mpp_task_timeout_work(struct work_struct *work_s)
+{
+	struct mpp_task *task = container_of(to_delayed_work(work_s),
+					     struct mpp_task,
+					     timeout_work);
+	struct mpp_dev *mpp;
+	struct mpp_session *session = task->session;
+
+	if (!session) {
+		mpp_err("task %p, task->session is null.\n", task);
+		return;
+	}
+
+	mpp = mpp_get_task_used_device(task, session);
+	if (!mpp) {
+		mpp_err("session %d:%d mpp is null\n", session->device_type, session->index);
+		return;
+	}
+	disable_irq(mpp->irq);
+	if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+		mpp_err("session %d:%d task %d has been handled\n",
+			session->device_type, session->index, task->task_index);
+		enable_irq(mpp->irq);
+		return;
+	}
+	mpp_err("session %d:%d task %d processing time out!\n",
+		session->device_type, session->index, task->task_index);
+
+	mpp_task_dump_timing(task, ktime_us_delta(ktime_get(), task->on_create));
+	set_bit(TASK_STATE_TIMEOUT, &task->state);
+
+	enable_irq(mpp->irq);
+	mpp_taskqueue_trigger_work(mpp);
+}
+
+static int mpp_process_task_default(struct mpp_session *session,
+				    struct mpp_task_msgs *msgs)
+{
+	struct mpp_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+	u32 timing_en;
+	ktime_t on_create;
+
+	if (unlikely(!mpp)) {
+		mpp_err("pid %d client %d found invalid process function\n",
+			session->pid, session->device_type);
+		return -EINVAL;
+	}
+
+	timing_en = session->srv->timing_en;
+	if (timing_en)
+		on_create = ktime_get();
+
+	if (mpp->dev_ops->alloc_task)
+		task = mpp->dev_ops->alloc_task(session, msgs);
+	if (!task) {
+		mpp_err("alloc_task failed.\n");
+		return -ENOMEM;
+	}
+
+	if (timing_en) {
+		task->on_create_end = ktime_get();
+		task->on_create = on_create;
+		set_bit(TASK_TIMING_CREATE_END, &task->state);
+		set_bit(TASK_TIMING_CREATE, &task->state);
+	}
+
+	/* ensure current device */
+	mpp = mpp_get_task_used_device(task, session);
+
+	kref_init(&task->ref);
+	init_waitqueue_head(&task->wait);
+	atomic_set(&task->abort_request, 0);
+	task->task_index = atomic_fetch_inc(&mpp->task_index);
+	task->task_id = atomic_fetch_inc(&mpp->queue->task_id);
+	INIT_DELAYED_WORK(&task->timeout_work, mpp_task_timeout_work);
+
+	if (mpp->auto_freq_en && mpp->hw_ops->get_freq)
+		mpp->hw_ops->get_freq(mpp, task);
+
+	msgs->queue = mpp->queue;
+	msgs->task = task;
+	msgs->mpp = mpp;
+
+	/*
+	 * Push task to session should be in front of push task to queue.
+	 * Otherwise, when mpp_task_finish finish and worker_thread call
+	 * task worker, it may be get a task who has push in queue but
+	 * not in session, cause some errors.
+	 */
+	atomic_inc(&session->task_count);
+	mpp_session_push_pending(session, task);
+	mpp_debug_func(DEBUG_TASK_INFO, "session %d:%d task %d state 0x%lx\n",
+		       session->device_type, session->index,
+		       task->task_index, task->state);
+
+	return 0;
+}
+
+static int mpp_process_task(struct mpp_session *session,
+			    struct mpp_task_msgs *msgs)
+{
+	if (likely(session->process_task))
+		return session->process_task(session, msgs);
+
+	pr_err("invalid NULL process task function\n");
+	return -EINVAL;
+}
+
+struct reset_control *
+mpp_reset_control_get(struct mpp_dev *mpp, enum MPP_RESET_TYPE type, const char *name)
+{
+	int index;
+	struct reset_control *rst = NULL;
+	char shared_name[32] = "shared_";
+	struct mpp_reset_group *group;
+
+	/* check reset whether belone to device alone */
+	index = of_property_match_string(mpp->dev->of_node, "reset-names", name);
+	if (index >= 0) {
+		rst = devm_reset_control_get(mpp->dev, name);
+		mpp_safe_unreset(rst);
+
+		return rst;
+	}
+
+	/* check reset whether is shared */
+	strncat(shared_name, name,
+		sizeof(shared_name) - strlen(shared_name) - 1);
+	index = of_property_match_string(mpp->dev->of_node,
+					 "reset-names", shared_name);
+	if (index < 0) {
+		dev_err(mpp->dev, "%s is not found!\n", shared_name);
+		return NULL;
+	}
+
+	if (!mpp->reset_group) {
+		dev_err(mpp->dev, "reset group is empty!\n");
+		return NULL;
+	}
+	group = mpp->reset_group;
+
+	down_write(&group->rw_sem);
+	rst = group->resets[type];
+	if (!rst) {
+		rst = devm_reset_control_get(mpp->dev, shared_name);
+		mpp_safe_unreset(rst);
+		group->resets[type] = rst;
+		group->queue = mpp->queue;
+	}
+	dev_info(mpp->dev, "reset_group->rw_sem_on=%d\n", group->rw_sem_on);
+	up_write(&group->rw_sem);
+
+	return rst;
+}
+
+int mpp_dev_reset(struct mpp_dev *mpp)
+{
+	dev_info(mpp->dev, "resetting...\n");
+
+	disable_irq(mpp->irq);
+	if (mpp->iommu_info && mpp->iommu_info->got_irq)
+		disable_irq(mpp->iommu_info->irq);
+	/*
+	 * before running, we have to switch grf ctrl bit to ensure
+	 * working in current hardware
+	 */
+	if (mpp->hw_ops->set_grf)
+		mpp->hw_ops->set_grf(mpp);
+	else
+		mpp_set_grf(mpp->grf_info);
+
+	if (mpp->auto_freq_en && mpp->hw_ops->reduce_freq)
+		mpp->hw_ops->reduce_freq(mpp);
+	/* FIXME lock resource lock of the other devices in combo */
+	mpp_iommu_down_write(mpp->iommu_info);
+	mpp_reset_down_write(mpp->reset_group);
+	atomic_set(&mpp->reset_request, 0);
+
+	if (mpp->hw_ops->reset)
+		mpp->hw_ops->reset(mpp);
+
+	/* Note: if the domain does not change, iommu attach will be return
+	 * as an empty operation. Therefore, force to close and then open,
+	 * will be update the domain. In this way, domain can really attach.
+	 */
+	mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+
+	mpp_reset_up_write(mpp->reset_group);
+	mpp_iommu_up_write(mpp->iommu_info);
+
+	enable_irq(mpp->irq);
+	if (mpp->iommu_info && mpp->iommu_info->got_irq)
+		enable_irq(mpp->iommu_info->irq);
+
+	dev_info(mpp->dev, "reset done\n");
+
+	return 0;
+}
+
+void mpp_task_run_begin(struct mpp_task *task, u32 timing_en, u32 timeout)
+{
+	preempt_disable();
+
+	set_bit(TASK_STATE_START, &task->state);
+
+	mpp_time_record(task);
+	schedule_delayed_work(&task->timeout_work, msecs_to_jiffies(timeout));
+
+	if (timing_en) {
+		task->on_sched_timeout = ktime_get();
+		set_bit(TASK_TIMING_TO_SCHED, &task->state);
+	}
+}
+
+void mpp_task_run_end(struct mpp_task *task, u32 timing_en)
+{
+	if (timing_en) {
+		task->on_run_end = ktime_get();
+		set_bit(TASK_TIMING_RUN_END, &task->state);
+	}
+
+#ifdef MODULE
+	preempt_enable();
+#else
+	preempt_enable_no_resched();
+#endif
+}
+
+static int mpp_task_run(struct mpp_dev *mpp,
+			struct mpp_task *task)
+{
+	int ret;
+	struct mpp_session *session = task->session;
+
+	mpp_debug_enter();
+
+	if (mpp->srv->timing_en) {
+		task->on_run = ktime_get();
+		set_bit(TASK_TIMING_RUN, &task->state);
+	}
+
+	/*
+	 * before running, we have to switch grf ctrl bit to ensure
+	 * working in current hardware
+	 */
+	if (mpp->hw_ops->set_grf) {
+		ret = mpp->hw_ops->set_grf(mpp);
+		if (ret) {
+			dev_err(mpp->dev, "set grf failed\n");
+			return ret;
+		}
+	} else {
+		mpp_set_grf(mpp->grf_info);
+	}
+	/*
+	 * Lock the reader locker of the device resource lock here,
+	 * release at the finish operation
+	 */
+	mpp_reset_down_read(mpp->reset_group);
+
+	/*
+	 * for iommu share hardware, should attach to ensure
+	 * working in current device
+	 */
+	ret = mpp_iommu_attach(mpp->iommu_info);
+	if (ret) {
+		dev_err(mpp->dev, "mpp_iommu_attach failed\n");
+		mpp_reset_up_read(mpp->reset_group);
+		return -ENODATA;
+	}
+
+	mpp_power_on(mpp);
+	mpp_debug_func(DEBUG_TASK_INFO, "%s session %d:%d task %d state 0x%lx\n",
+		       dev_name(mpp->dev), session->device_type,
+		       session->index, task->task_index, task->state);
+
+	if (mpp->auto_freq_en && mpp->hw_ops->set_freq)
+		mpp->hw_ops->set_freq(mpp, task);
+
+	mpp_iommu_dev_activate(mpp->iommu_info, mpp);
+	if (mpp->dev_ops->run)
+		mpp->dev_ops->run(mpp, task);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+void mpp_dev_load(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct mpp_load_info *load_info = &mpp->load_info;
+	ktime_t now;
+	s64 time_diff_us;
+
+	if (!mpp->srv->load_interval) {
+		if (mpp->load_en) {
+			mpp_dev_load_clear(mpp);
+			mpp->srv->timing_en = 0;
+			mpp->load_en = 0;
+		}
+		return;
+	}
+
+	if (!mpp->load_en) {
+		mpp->srv->timing_en = 1;
+		mpp->load_en = 1;
+		load_info->load_time = ktime_get();
+		return;
+	}
+
+	if (!mpp_task->on_run)
+		return;
+
+	now = ktime_get();
+	time_diff_us = ktime_us_delta(now, load_info->load_time);
+	load_info->busy_time += ktime_us_delta(now, mpp_task->on_run);
+	if (mpp_task->hw_time)
+		load_info->hw_busy_time += mpp_task->hw_time;
+	else
+		load_info->hw_busy_time += ktime_us_delta(mpp_task->on_irq,
+							  mpp_task->on_sched_timeout);
+	/* 1s update */
+	if (time_diff_us > mpp->srv->load_interval * 1000) {
+		u32 tmp = div64_s64(load_info->busy_time * 10000, time_diff_us);
+		u32 load = tmp / 100;
+		u32 load_frac = tmp % 100;
+		u32 max_load = 100;
+
+		if (mpp->queue->core_count > 1)
+			max_load *= mpp->queue->core_count;
+
+		load_info->load = load > max_load ? max_load : load;
+		load_info->load_frac = load > max_load ? 0 : load_frac;
+
+		tmp = div64_s64(load_info->hw_busy_time * 10000, time_diff_us);
+		load = tmp / 100;
+		load_frac = tmp % 100;
+		load_info->utilization = load > max_load ? max_load : load;
+		load_info->utilization_frac = load > max_load ? 0 : load_frac;
+
+		load_info->busy_time = 0;
+		load_info->hw_busy_time = 0;
+		load_info->load_time = now;
+	}
+}
+
+void mpp_dev_load_clear(struct mpp_dev *mpp)
+{
+	struct mpp_load_info *load_info = &mpp->load_info;
+
+	memset(load_info, 0, sizeof(*load_info));
+}
+
+static void try_process_running_task(struct mpp_dev *mpp)
+{
+	struct mpp_task *mpp_task, *n;
+	struct mpp_taskqueue *queue = mpp->queue;
+
+	/* try process running task */
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		mpp = mpp_get_task_used_device(mpp_task, mpp_task->session);
+		disable_irq(mpp->irq);
+		if (!test_bit(TASK_STATE_HANDLE, &mpp_task->state)) {
+			enable_irq(mpp->irq);
+			continue;
+		}
+
+		/* process timeout task */
+		if (test_bit(TASK_STATE_TIMEOUT, &mpp_task->state)) {
+			atomic_inc(&mpp->reset_request);
+			mpp_iommu_dev_deactivate(mpp->iommu_info, mpp);
+		}
+
+		if (mpp->auto_freq_en && mpp->hw_ops->reduce_freq &&
+		    list_empty(&mpp->queue->pending_list))
+			mpp->hw_ops->reduce_freq(mpp);
+		mpp_dev_load(mpp, mpp_task);
+		if (mpp->dev_ops->isr)
+			mpp->dev_ops->isr(mpp);
+		enable_irq(mpp->irq);
+	}
+}
+
+static void mpp_task_worker_default(struct kthread_work *work_s)
+{
+	struct mpp_task *task;
+	struct mpp_dev *mpp = container_of(work_s, struct mpp_dev, work);
+	struct mpp_taskqueue *queue = mpp->queue;
+
+	mpp_debug_enter();
+
+	try_process_running_task(mpp);
+again:
+	task = mpp_taskqueue_get_pending_task(queue);
+	if (!task)
+		goto done;
+
+	/* if task timeout and aborted, remove it */
+	if (atomic_read(&task->abort_request) > 0) {
+		mpp_taskqueue_pop_pending(queue, task);
+		goto again;
+	}
+
+	/* get device for current task */
+	mpp = task->session->mpp;
+
+	/*
+	 * In the link table mode, the prepare function of the device
+	 * will check whether I can insert a new task into device.
+	 * If the device supports the task status query(like the HEVC
+	 * encoder), it can report whether the device is busy.
+	 * If the device does not support multiple task or task status
+	 * query, leave this job to mpp service.
+	 */
+	if (mpp->dev_ops->prepare)
+		task = mpp->dev_ops->prepare(mpp, task);
+	else if (mpp_taskqueue_is_running(queue))
+		task = NULL;
+
+	/*
+	 * FIXME if the hardware supports task query, but we still need to lock
+	 * the running list and lock the mpp service in the current state.
+	 */
+	/* Push a pending task to running queue */
+	if (task) {
+		struct mpp_dev *task_mpp = mpp_get_task_used_device(task, task->session);
+
+		atomic_inc(&task_mpp->task_count);
+		mpp_taskqueue_pending_to_run(queue, task);
+		set_bit(TASK_STATE_RUNNING, &task->state);
+		if (mpp_task_run(task_mpp, task))
+			mpp_taskqueue_pop_running(queue, task);
+		else
+			goto again;
+	}
+
+done:
+	mpp_session_cleanup_detach(queue, work_s);
+}
+
+static int mpp_wait_result_default(struct mpp_session *session,
+				   struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *task;
+	struct mpp_dev *mpp;
+
+	task = mpp_session_get_pending_task(session);
+	if (!task) {
+		mpp_err("session %d:%d pending list is empty!\n",
+			session->pid, session->index);
+		return -EIO;
+	}
+	mpp = mpp_get_task_used_device(task, session);
+
+	ret = wait_event_interruptible(task->wait, test_bit(TASK_STATE_DONE, &task->state));
+	if (ret == -ERESTARTSYS)
+		mpp_err("wait task break by signal\n");
+
+	if (mpp->dev_ops->result)
+		ret = mpp->dev_ops->result(mpp, task, msgs);
+	mpp_debug_func(DEBUG_TASK_INFO, "wait done session %d:%d count %d task %d state %lx\n",
+		       session->device_type, session->index, atomic_read(&session->task_count),
+		       task->task_index, task->state);
+
+	mpp_session_pop_pending(session, task);
+
+	return ret;
+}
+
+static int mpp_wait_result(struct mpp_session *session,
+			   struct mpp_task_msgs *msgs)
+{
+	if (likely(session->wait_result))
+		return session->wait_result(session, msgs);
+
+	pr_err("invalid NULL wait result function\n");
+	return -EINVAL;
+}
+
+static int mpp_attach_service(struct mpp_dev *mpp, struct device *dev)
+{
+	u32 taskqueue_node = 0;
+	u32 reset_group_node = 0;
+	struct device_node *np = NULL;
+	struct platform_device *pdev = NULL;
+	struct mpp_taskqueue *queue = NULL;
+	int ret = 0;
+
+	np = of_parse_phandle(dev->of_node, "rockchip,srv", 0);
+	if (!np || !of_device_is_available(np)) {
+		dev_err(dev, "failed to get the mpp service node\n");
+		return -ENODEV;
+	}
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev) {
+		dev_err(dev, "failed to get mpp service from node\n");
+		return -ENODEV;
+	}
+
+	mpp->srv = platform_get_drvdata(pdev);
+	platform_device_put(pdev);
+	if (!mpp->srv) {
+		dev_err(dev, "failed attach service\n");
+		return -EINVAL;
+	}
+
+	ret = of_property_read_u32(dev->of_node,
+				   "rockchip,taskqueue-node", &taskqueue_node);
+	if (ret) {
+		dev_err(dev, "failed to get taskqueue-node\n");
+		return ret;
+	} else if (taskqueue_node >= mpp->srv->taskqueue_cnt) {
+		dev_err(dev, "taskqueue-node %d must less than %d\n",
+			taskqueue_node, mpp->srv->taskqueue_cnt);
+		return -ENODEV;
+	}
+	/* set taskqueue according dtsi */
+	queue = mpp->srv->task_queues[taskqueue_node];
+	if (!queue) {
+		dev_err(dev, "taskqueue attach to invalid node %d\n",
+			taskqueue_node);
+		return -ENODEV;
+	}
+	mpp_attach_workqueue(mpp, queue);
+
+	ret = of_property_read_u32(dev->of_node,
+				   "rockchip,resetgroup-node", &reset_group_node);
+	if (!ret) {
+		/* set resetgroup according dtsi */
+		if (reset_group_node >= mpp->srv->reset_group_cnt) {
+			dev_err(dev, "resetgroup-node %d must less than %d\n",
+				reset_group_node, mpp->srv->reset_group_cnt);
+			return -ENODEV;
+		} else {
+			mpp->reset_group = mpp->srv->reset_groups[reset_group_node];
+			if (!mpp->reset_group->queue)
+				mpp->reset_group->queue = queue;
+			if (mpp->reset_group->queue != mpp->queue)
+				mpp->reset_group->rw_sem_on = true;
+		}
+	}
+
+	return 0;
+}
+
+struct mpp_taskqueue *mpp_taskqueue_init(struct device *dev)
+{
+	struct mpp_taskqueue *queue = devm_kzalloc(dev, sizeof(*queue),
+						   GFP_KERNEL);
+	if (!queue)
+		return NULL;
+
+	mutex_init(&queue->session_lock);
+	mutex_init(&queue->pending_lock);
+	spin_lock_init(&queue->running_lock);
+	mutex_init(&queue->mmu_lock);
+	mutex_init(&queue->dev_lock);
+	INIT_LIST_HEAD(&queue->session_attach);
+	INIT_LIST_HEAD(&queue->session_detach);
+	INIT_LIST_HEAD(&queue->pending_list);
+	INIT_LIST_HEAD(&queue->running_list);
+	INIT_LIST_HEAD(&queue->mmu_list);
+	INIT_LIST_HEAD(&queue->dev_list);
+
+	/* default taskqueue has max 16 task capacity */
+	queue->task_capacity = MPP_MAX_TASK_CAPACITY;
+	atomic_set(&queue->reset_request, 0);
+	atomic_set(&queue->detach_count, 0);
+	atomic_set(&queue->task_id, 0);
+	queue->dev_active_flags = 0;
+
+	return queue;
+}
+
+static void mpp_attach_workqueue(struct mpp_dev *mpp,
+				 struct mpp_taskqueue *queue)
+{
+	s32 core_id;
+
+	INIT_LIST_HEAD(&mpp->queue_link);
+
+	mutex_lock(&queue->dev_lock);
+
+	if (mpp->core_id >= 0)
+		core_id = mpp->core_id;
+	else
+		core_id = queue->core_count;
+
+	if (core_id < 0 || core_id >= MPP_MAX_CORE_NUM) {
+		dev_err(mpp->dev, "invalid core id %d\n", core_id);
+		goto done;
+	}
+
+	/*
+	 * multi devices with no multicores share one queue,
+	 * the core_id is default value 0.
+	 */
+	if (queue->cores[core_id]) {
+		if (queue->cores[core_id] == mpp)
+			goto done;
+
+		core_id = queue->core_count;
+	}
+
+	queue->cores[core_id] = mpp;
+	queue->core_count++;
+
+	set_bit(core_id, &queue->core_idle);
+	list_add_tail(&mpp->queue_link, &queue->dev_list);
+	if (queue->core_id_max < (u32)core_id)
+		queue->core_id_max = (u32)core_id;
+
+	mpp->core_id = core_id;
+	mpp->queue = queue;
+
+	mpp_dbg_core("%s attach queue as core %d\n",
+			dev_name(mpp->dev), mpp->core_id);
+
+	if (queue->task_capacity > mpp->task_capacity)
+		queue->task_capacity = mpp->task_capacity;
+
+done:
+	mutex_unlock(&queue->dev_lock);
+}
+
+static void mpp_detach_workqueue(struct mpp_dev *mpp)
+{
+	struct mpp_taskqueue *queue = mpp->queue;
+
+	if (queue) {
+		mutex_lock(&queue->dev_lock);
+
+		queue->cores[mpp->core_id] = NULL;
+		queue->core_count--;
+
+		clear_bit(mpp->core_id, &queue->core_idle);
+		list_del_init(&mpp->queue_link);
+
+		mpp->queue = NULL;
+
+		mutex_unlock(&queue->dev_lock);
+	}
+}
+
+static int mpp_check_cmd_v1(__u32 cmd)
+{
+	bool found;
+
+	found = (cmd < MPP_CMD_QUERY_BUTT) ? true : false;
+	found = (cmd >= MPP_CMD_INIT_BASE && cmd < MPP_CMD_INIT_BUTT) ? true : found;
+	found = (cmd >= MPP_CMD_SEND_BASE && cmd < MPP_CMD_SEND_BUTT) ? true : found;
+	found = (cmd >= MPP_CMD_POLL_BASE && cmd < MPP_CMD_POLL_BUTT) ? true : found;
+	found = (cmd >= MPP_CMD_CONTROL_BASE && cmd < MPP_CMD_CONTROL_BUTT) ? true : found;
+
+	return found ? 0 : -EINVAL;
+}
+
+static inline int mpp_msg_is_last(struct mpp_request *req)
+{
+	int flag;
+
+	if (req->flags & MPP_FLAGS_MULTI_MSG)
+		flag = (req->flags & MPP_FLAGS_LAST_MSG) ? 1 : 0;
+	else
+		flag = 1;
+
+	return flag;
+}
+
+static __u32 mpp_get_cmd_butt(__u32 cmd)
+{
+	__u32 mask = 0;
+
+	switch (cmd) {
+	case MPP_CMD_QUERY_BASE:
+		mask = MPP_CMD_QUERY_BUTT;
+		break;
+	case MPP_CMD_INIT_BASE:
+		mask = MPP_CMD_INIT_BUTT;
+		break;
+
+	case MPP_CMD_SEND_BASE:
+		mask = MPP_CMD_SEND_BUTT;
+		break;
+	case MPP_CMD_POLL_BASE:
+		mask = MPP_CMD_POLL_BUTT;
+		break;
+	case MPP_CMD_CONTROL_BASE:
+		mask = MPP_CMD_CONTROL_BUTT;
+		break;
+	default:
+		mpp_err("unknown dev cmd 0x%x\n", cmd);
+		break;
+	}
+
+	return mask;
+}
+
+static int mpp_process_request(struct mpp_session *session,
+			       struct mpp_service *srv,
+			       struct mpp_request *req,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_dev *mpp;
+
+	mpp_debug(DEBUG_IOCTL, "cmd %x process\n", req->cmd);
+
+	switch (req->cmd) {
+	case MPP_CMD_QUERY_HW_SUPPORT: {
+		u32 hw_support = srv->hw_support;
+
+		mpp_debug(DEBUG_IOCTL, "hw_support %08x\n", hw_support);
+		if (put_user(hw_support, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_QUERY_HW_ID: {
+		struct mpp_hw_info *hw_info;
+
+		mpp = NULL;
+		if (session && session->mpp) {
+			mpp = session->mpp;
+		} else {
+			u32 client_type;
+
+			if (get_user(client_type, (u32 __user *)req->data))
+				return -EFAULT;
+
+			mpp_debug(DEBUG_IOCTL, "client %d\n", client_type);
+			client_type = array_index_nospec(client_type, MPP_DEVICE_BUTT);
+			if (test_bit(client_type, &srv->hw_support))
+				mpp = srv->sub_devices[client_type];
+		}
+
+		if (!mpp)
+			return -EINVAL;
+
+		hw_info = mpp->var->hw_info;
+		mpp_debug(DEBUG_IOCTL, "hw_id %08x\n", hw_info->hw_id);
+		if (put_user(hw_info->hw_id, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_QUERY_CMD_SUPPORT: {
+		__u32 cmd = 0;
+
+		if (get_user(cmd, (u32 __user *)req->data))
+			return -EINVAL;
+
+		if (put_user(mpp_get_cmd_butt(cmd), (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_INIT_CLIENT_TYPE: {
+		u32 client_type;
+
+		if (get_user(client_type, (u32 __user *)req->data))
+			return -EFAULT;
+
+		mpp_debug(DEBUG_IOCTL, "client %d\n", client_type);
+		if (client_type >= MPP_DEVICE_BUTT) {
+			mpp_err("client_type must less than %d\n",
+				MPP_DEVICE_BUTT);
+			return -EINVAL;
+		}
+		client_type = array_index_nospec(client_type, MPP_DEVICE_BUTT);
+		mpp = srv->sub_devices[client_type];
+		if (!mpp)
+			return -EINVAL;
+
+		session->device_type = (enum MPP_DEVICE_TYPE)client_type;
+		session->dma = mpp_dma_session_create(mpp->dev, mpp->session_max_buffers);
+		session->mpp = mpp;
+		if (mpp->dev_ops) {
+			if (mpp->dev_ops->process_task)
+				session->process_task =
+					mpp->dev_ops->process_task;
+
+			if (mpp->dev_ops->wait_result)
+				session->wait_result =
+					mpp->dev_ops->wait_result;
+
+			if (mpp->dev_ops->deinit)
+				session->deinit = mpp->dev_ops->deinit;
+		}
+		session->index = atomic_fetch_inc(&mpp->session_index);
+		if (mpp->dev_ops && mpp->dev_ops->init_session) {
+			ret = mpp->dev_ops->init_session(session);
+			if (ret)
+				return ret;
+		}
+
+		mpp_session_attach_workqueue(session, mpp->queue);
+	} break;
+	case MPP_CMD_INIT_DRIVER_DATA: {
+		u32 val;
+
+		mpp = session->mpp;
+		if (!mpp)
+			return -EINVAL;
+		if (get_user(val, (u32 __user *)req->data))
+			return -EFAULT;
+		if (mpp->grf_info->grf)
+			regmap_write(mpp->grf_info->grf, 0x5d8, val);
+	} break;
+	case MPP_CMD_INIT_TRANS_TABLE: {
+		if (session && req->size) {
+			int trans_tbl_size = sizeof(session->trans_table);
+
+			if (req->size > trans_tbl_size) {
+				mpp_err("init table size %d more than %d\n",
+					req->size, trans_tbl_size);
+				return -ENOMEM;
+			}
+
+			if (copy_from_user(session->trans_table,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user failed\n");
+				return -EINVAL;
+			}
+			session->trans_count =
+				req->size / sizeof(session->trans_table[0]);
+		}
+	} break;
+	case MPP_CMD_SET_REG_WRITE:
+	case MPP_CMD_SET_REG_READ:
+	case MPP_CMD_SET_REG_ADDR_OFFSET:
+	case MPP_CMD_SET_RCB_INFO: {
+		msgs->flags |= req->flags;
+		msgs->set_cnt++;
+	} break;
+	case MPP_CMD_POLL_HW_FINISH: {
+		msgs->flags |= req->flags;
+		msgs->poll_cnt++;
+		msgs->poll_req = NULL;
+	} break;
+	case MPP_CMD_POLL_HW_IRQ: {
+		if (msgs->poll_cnt || msgs->poll_req)
+			mpp_err("Do NOT poll hw irq when previous call not return\n");
+
+		msgs->flags |= req->flags;
+		msgs->poll_cnt++;
+
+		if (req->size && req->data) {
+			if (!msgs->poll_req)
+				msgs->poll_req = req;
+		} else {
+			msgs->poll_req = NULL;
+		}
+	} break;
+	case MPP_CMD_RESET_SESSION: {
+		int ret;
+		int val;
+
+		ret = readx_poll_timeout(atomic_read,
+					 &session->task_count,
+					 val, val == 0, 1000, 500000);
+		if (ret == -ETIMEDOUT) {
+			mpp_err("wait task running time out\n");
+		} else {
+			mpp = session->mpp;
+			if (!mpp)
+				return -EINVAL;
+
+			mpp_session_clear_pending(session);
+			mpp_iommu_down_write(mpp->iommu_info);
+			ret = mpp_dma_session_destroy(session->dma);
+			mpp_iommu_up_write(mpp->iommu_info);
+		}
+		return ret;
+	} break;
+	case MPP_CMD_TRANS_FD_TO_IOVA: {
+		u32 i;
+		u32 count;
+		u32 data[MPP_MAX_REG_TRANS_NUM];
+
+		mpp = session->mpp;
+		if (!mpp)
+			return -EINVAL;
+
+		if (req->size <= 0 ||
+		    req->size > sizeof(data))
+			return -EINVAL;
+
+		memset(data, 0, sizeof(data));
+		if (copy_from_user(data, req->data, req->size)) {
+			mpp_err("copy_from_user failed.\n");
+			return -EINVAL;
+		}
+		count = req->size / sizeof(u32);
+		for (i = 0; i < count; i++) {
+			struct mpp_dma_buffer *buffer;
+			int fd = data[i];
+
+			mpp_iommu_down_read(mpp->iommu_info);
+			buffer = mpp_dma_import_fd(mpp->iommu_info,
+						   session->dma, fd, 1);
+			mpp_iommu_up_read(mpp->iommu_info);
+			if (IS_ERR_OR_NULL(buffer)) {
+				mpp_err("can not import fd %d\n", fd);
+				return -EINVAL;
+			}
+			data[i] = (u32)buffer->iova;
+			mpp_debug(DEBUG_IOMMU, "fd %d => iova %08x\n",
+				  fd, data[i]);
+		}
+		if (copy_to_user(req->data, data, req->size)) {
+			mpp_err("copy_to_user failed.\n");
+			return -EINVAL;
+		}
+	} break;
+	case MPP_CMD_RELEASE_FD: {
+		u32 i;
+		int ret;
+		u32 count;
+		u32 data[MPP_MAX_REG_TRANS_NUM];
+
+		if (req->size <= 0 ||
+		    req->size > sizeof(data))
+			return -EINVAL;
+
+		memset(data, 0, sizeof(data));
+		if (copy_from_user(data, req->data, req->size)) {
+			mpp_err("copy_from_user failed.\n");
+			return -EINVAL;
+		}
+		count = req->size / sizeof(u32);
+		for (i = 0; i < count; i++) {
+			ret = mpp_dma_release_fd(session->dma, data[i]);
+			if (ret) {
+				mpp_err("release fd %d failed.\n", data[i]);
+				return ret;
+			}
+		}
+	} break;
+	default: {
+		mpp = session->mpp;
+		if (!mpp) {
+			mpp_err("pid %d not find client %d\n",
+				session->pid, session->device_type);
+			return -EINVAL;
+		}
+		if (mpp->dev_ops->ioctl)
+			return mpp->dev_ops->ioctl(session, req);
+
+		mpp_debug(DEBUG_IOCTL, "unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+static void task_msgs_add(struct mpp_task_msgs *msgs, struct list_head *head)
+{
+	struct mpp_session *session = msgs->session;
+	int ret = 0;
+
+	/* process each task */
+	if (msgs->set_cnt) {
+		/* NOTE: update msg_flags for fd over 1024 */
+		session->msg_flags = msgs->flags;
+		ret = mpp_process_task(session, msgs);
+	}
+
+	if (!ret) {
+		INIT_LIST_HEAD(&msgs->list);
+		list_add_tail(&msgs->list, head);
+	} else {
+		put_task_msgs(msgs);
+	}
+}
+
+static int mpp_collect_msgs(struct list_head *head, struct mpp_session *session,
+			    unsigned int cmd, void __user *msg)
+{
+	struct mpp_msg_v1 msg_v1;
+	struct mpp_request *req;
+	struct mpp_task_msgs *msgs = NULL;
+	int last = 1;
+	int ret;
+
+	if (cmd != MPP_IOC_CFG_V1) {
+		mpp_err("unknown ioctl cmd %x\n", cmd);
+		return -EINVAL;
+	}
+
+next:
+	/* first, parse to fixed struct */
+	if (copy_from_user(&msg_v1, msg, sizeof(msg_v1)))
+		return -EFAULT;
+
+	msg += sizeof(msg_v1);
+
+	mpp_debug(DEBUG_IOCTL, "cmd %x collect flags %08x, size %d, offset %x\n",
+		  msg_v1.cmd, msg_v1.flags, msg_v1.size, msg_v1.offset);
+
+	if (mpp_check_cmd_v1(msg_v1.cmd)) {
+		mpp_err("mpp cmd %x is not supported.\n", msg_v1.cmd);
+		return -EFAULT;
+	}
+
+	if (msg_v1.flags & MPP_FLAGS_MULTI_MSG)
+		last = (msg_v1.flags & MPP_FLAGS_LAST_MSG) ? 1 : 0;
+	else
+		last = 1;
+
+	/* check cmd for change msgs session */
+	if (msg_v1.cmd == MPP_CMD_SET_SESSION_FD) {
+		struct mpp_bat_msg bat_msg;
+		struct mpp_bat_msg __user *usr_cmd;
+		struct fd f;
+
+		/* try session switch here */
+		usr_cmd = (struct mpp_bat_msg __user *)(unsigned long)msg_v1.data_ptr;
+
+		if (copy_from_user(&bat_msg, usr_cmd, sizeof(bat_msg)))
+			return -EFAULT;
+
+		/* skip finished message */
+		if (bat_msg.flag & MPP_BAT_MSG_DONE)
+			goto session_switch_done;
+
+		f = fdget(bat_msg.fd);
+		if (!f.file) {
+			int ret = -EBADF;
+
+			mpp_err("fd %d get session failed\n", bat_msg.fd);
+
+			if (copy_to_user(&usr_cmd->ret, &ret, sizeof(usr_cmd->ret)))
+				mpp_err("copy_to_user failed.\n");
+			goto session_switch_done;
+		}
+
+		/* NOTE: add previous ready task to queue and drop empty task */
+		if (msgs) {
+			if (msgs->req_cnt)
+				task_msgs_add(msgs, head);
+			else
+				put_task_msgs(msgs);
+
+			msgs = NULL;
+		}
+
+		/* switch session */
+		session = f.file->private_data;
+		msgs = get_task_msgs(session);
+
+		if (f.file->private_data == session)
+			msgs->ext_fd = bat_msg.fd;
+
+		msgs->f = f;
+
+		mpp_debug(DEBUG_IOCTL, "fd %d, session %d msg_cnt %d\n",
+				bat_msg.fd, session->index, session->msgs_cnt);
+
+session_switch_done:
+		/* session id should NOT be the last message */
+		if (last)
+			return 0;
+
+		goto next;
+	}
+
+	if (!msgs)
+		msgs = get_task_msgs(session);
+
+	if (!msgs) {
+		pr_err("session %d:%d failed to get task msgs",
+		       session->pid, session->index);
+		return -EINVAL;
+	}
+
+	if (msgs->req_cnt >= MPP_MAX_MSG_NUM) {
+		mpp_err("session %d message count %d more than %d.\n",
+			session->index, msgs->req_cnt, MPP_MAX_MSG_NUM);
+		return -EINVAL;
+	}
+
+	req = &msgs->reqs[msgs->req_cnt++];
+	req->cmd = msg_v1.cmd;
+	req->flags = msg_v1.flags;
+	req->size = msg_v1.size;
+	req->offset = msg_v1.offset;
+	req->data = (void __user *)(unsigned long)msg_v1.data_ptr;
+
+	ret = mpp_process_request(session, session->srv, req, msgs);
+	if (ret) {
+		mpp_err("session %d process cmd %x ret %d\n",
+			session->index, req->cmd, ret);
+		return ret;
+	}
+
+	if (!last)
+		goto next;
+
+	task_msgs_add(msgs, head);
+	msgs = NULL;
+
+	return 0;
+}
+
+static void mpp_msgs_trigger(struct list_head *msgs_list)
+{
+	struct mpp_task_msgs *msgs, *n;
+	struct mpp_dev *mpp_prev = NULL;
+	struct mpp_taskqueue *queue_prev = NULL;
+
+	/* push task to queue */
+	list_for_each_entry_safe(msgs, n, msgs_list, list) {
+		struct mpp_dev *mpp;
+		struct mpp_task *task;
+		struct mpp_taskqueue *queue;
+
+		if (!msgs->set_cnt || !msgs->queue)
+			continue;
+
+		mpp = msgs->mpp;
+		task = msgs->task;
+		queue = msgs->queue;
+
+		if (queue_prev != queue) {
+			if (queue_prev && mpp_prev) {
+				mutex_unlock(&queue_prev->pending_lock);
+				mpp_taskqueue_trigger_work(mpp_prev);
+			}
+
+			if (queue)
+				mutex_lock(&queue->pending_lock);
+
+			mpp_prev = mpp;
+			queue_prev = queue;
+		}
+
+		if (test_bit(TASK_STATE_ABORT, &task->state))
+			pr_info("try to trigger abort task %d\n", task->task_id);
+
+		set_bit(TASK_STATE_PENDING, &task->state);
+		list_add_tail(&task->queue_link, &queue->pending_list);
+	}
+
+	if (mpp_prev && queue_prev) {
+		mutex_unlock(&queue_prev->pending_lock);
+		mpp_taskqueue_trigger_work(mpp_prev);
+	}
+}
+
+static void mpp_msgs_wait(struct list_head *msgs_list)
+{
+	struct mpp_task_msgs *msgs, *n;
+
+	/* poll and release each task */
+	list_for_each_entry_safe(msgs, n, msgs_list, list) {
+		struct mpp_session *session = msgs->session;
+
+		if (msgs->poll_cnt) {
+			int ret = mpp_wait_result(session, msgs);
+
+			if (ret) {
+				mpp_err("session %d wait result ret %d\n",
+					session->index, ret);
+			}
+		}
+
+		put_task_msgs(msgs);
+
+	}
+}
+
+static long mpp_dev_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct mpp_service *srv;
+	struct mpp_session *session = (struct mpp_session *)filp->private_data;
+	struct list_head msgs_list;
+	int ret = 0;
+
+	mpp_debug_enter();
+
+	if (!session || !session->srv) {
+		mpp_err("session %p\n", session);
+		return -EINVAL;
+	}
+
+	srv = session->srv;
+
+	if (atomic_read(&session->release_request) > 0) {
+		mpp_debug(DEBUG_IOCTL, "release session had request\n");
+		return -EBUSY;
+	}
+	if (atomic_read(&srv->shutdown_request) > 0) {
+		mpp_debug(DEBUG_IOCTL, "shutdown had request\n");
+		return -EBUSY;
+	}
+
+	INIT_LIST_HEAD(&msgs_list);
+
+	ret = mpp_collect_msgs(&msgs_list, session, cmd, (void __user *)arg);
+	if (ret)
+		mpp_err("collect msgs failed %d\n", ret);
+
+	mpp_msgs_trigger(&msgs_list);
+
+	mpp_msgs_wait(&msgs_list);
+
+	mpp_debug_leave();
+
+	return ret;
+}
+
+static int mpp_dev_open(struct inode *inode, struct file *filp)
+{
+	struct mpp_session *session = NULL;
+	struct mpp_service *srv = container_of(inode->i_cdev,
+					       struct mpp_service,
+					       mpp_cdev);
+	mpp_debug_enter();
+
+	session = mpp_session_init();
+	if (!session)
+		return -ENOMEM;
+
+	session->srv = srv;
+
+	if (session->srv) {
+		mutex_lock(&srv->session_lock);
+		list_add_tail(&session->service_link, &srv->session_list);
+		mutex_unlock(&srv->session_lock);
+	}
+	session->process_task = mpp_process_task_default;
+	session->wait_result = mpp_wait_result_default;
+	session->deinit = mpp_session_deinit_default;
+	filp->private_data = (void *)session;
+
+	mpp_debug_leave();
+
+	return nonseekable_open(inode, filp);
+}
+
+static int mpp_dev_release(struct inode *inode, struct file *filp)
+{
+	struct mpp_session *session = filp->private_data;
+
+	mpp_debug_enter();
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	/* wait for task all done */
+	atomic_inc(&session->release_request);
+
+	if (session->mpp || atomic_read(&session->task_count))
+		mpp_session_detach_workqueue(session);
+	else
+		mpp_session_deinit(session);
+
+	filp->private_data = NULL;
+
+	mpp_debug_leave();
+	return 0;
+}
+
+const struct file_operations rockchip_mpp_fops = {
+	.open		= mpp_dev_open,
+	.release	= mpp_dev_release,
+	.unlocked_ioctl = mpp_dev_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl   = mpp_dev_ioctl,
+#endif
+};
+
+struct mpp_mem_region *
+mpp_task_attach_fd(struct mpp_task *task, int fd)
+{
+	struct mpp_mem_region *mem_region = NULL, *loop = NULL, *n;
+	struct mpp_dma_buffer *buffer = NULL;
+	struct mpp_dev *mpp = task->session->mpp;
+	struct mpp_dma_session *dma = task->session->dma;
+	u32 mem_num = ARRAY_SIZE(task->mem_regions);
+	bool found = false;
+
+	if (fd <= 0 || !dma || !mpp)
+		return ERR_PTR(-EINVAL);
+
+	if (task->mem_count > mem_num) {
+		mpp_err("mem_count %d must less than %d\n", task->mem_count, mem_num);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* find fd whether had import */
+	list_for_each_entry_safe_reverse(loop, n, &task->mem_region_list, reg_link) {
+		if (loop->fd == fd) {
+			found = true;
+			break;
+		}
+	}
+
+	mem_region = &task->mem_regions[task->mem_count];
+	if (found) {
+		memcpy(mem_region, loop, sizeof(*loop));
+		mem_region->is_dup = true;
+	} else {
+		mpp_iommu_down_read(mpp->iommu_info);
+		buffer = mpp_dma_import_fd(mpp->iommu_info, dma, fd, 0);
+		mpp_iommu_up_read(mpp->iommu_info);
+		if (IS_ERR(buffer)) {
+			mpp_err("can't import dma-buf %d\n", fd);
+			return ERR_CAST(buffer);
+		}
+
+		mem_region->hdl = buffer;
+		mem_region->iova = buffer->iova;
+		mem_region->len = buffer->size;
+		mem_region->fd = fd;
+		mem_region->is_dup = false;
+	}
+	task->mem_count++;
+	INIT_LIST_HEAD(&mem_region->reg_link);
+	list_add_tail(&mem_region->reg_link, &task->mem_region_list);
+
+	return mem_region;
+}
+
+int mpp_translate_reg_address(struct mpp_session *session,
+			      struct mpp_task *task, int fmt,
+			      u32 *reg, struct reg_offset_info *off_inf)
+{
+	int i;
+	int cnt;
+	const u16 *tbl;
+
+	mpp_debug_enter();
+
+	if (session->trans_count > 0) {
+		cnt = session->trans_count;
+		tbl = session->trans_table;
+	} else {
+		struct mpp_dev *mpp = mpp_get_task_used_device(task, session);
+		struct mpp_trans_info *trans_info = mpp->var->trans_info;
+
+		cnt = trans_info[fmt].count;
+		tbl = trans_info[fmt].table;
+	}
+
+	for (i = 0; i < cnt; i++) {
+		int usr_fd;
+		u32 offset;
+		struct mpp_mem_region *mem_region = NULL;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			usr_fd = reg[tbl[i]];
+			offset = 0;
+		} else {
+			usr_fd = reg[tbl[i]] & 0x3ff;
+			offset = reg[tbl[i]] >> 10;
+		}
+
+		if (usr_fd == 0)
+			continue;
+
+		mem_region = mpp_task_attach_fd(task, usr_fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("reg[%3d]: 0x%08x fd %d failed\n",
+				tbl[i], reg[tbl[i]], usr_fd);
+			return PTR_ERR(mem_region);
+		}
+		mpp_debug(DEBUG_IOMMU,
+			  "reg[%3d]: %d => %pad, offset %10d, size %lx\n",
+			  tbl[i], usr_fd, &mem_region->iova,
+			  offset, mem_region->len);
+		mem_region->reg_idx = tbl[i];
+		reg[tbl[i]] = mem_region->iova + offset;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int mpp_check_req(struct mpp_request *req, int base,
+		  int max_size, u32 off_s, u32 off_e)
+{
+	int req_off;
+
+	if (req->offset < base) {
+		mpp_err("error: base %x, offset %x\n",
+			base, req->offset);
+		return -EINVAL;
+	}
+	req_off = req->offset - base;
+	if ((req_off + req->size) < off_s) {
+		mpp_err("error: req_off %x, req_size %x, off_s %x\n",
+			req_off, req->size, off_s);
+		return -EINVAL;
+	}
+	if (max_size < off_e) {
+		mpp_err("error: off_e %x, max_size %x\n",
+			off_e, max_size);
+		return -EINVAL;
+	}
+	if (req_off > max_size) {
+		mpp_err("error: req_off %x, max_size %x\n",
+			req_off, max_size);
+		return -EINVAL;
+	}
+	if ((req_off + req->size) > max_size) {
+		mpp_err("error: req_off %x, req_size %x, max_size %x\n",
+			req_off, req->size, max_size);
+		req->size = req_off + req->size - max_size;
+	}
+
+	return 0;
+}
+
+int mpp_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(off_inf->elem);
+	int cnt = req->size / sizeof(off_inf->elem[0]);
+
+	if ((cnt + off_inf->cnt) > max_size) {
+		mpp_err("count %d, total %d, max_size %d\n",
+			cnt, off_inf->cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(&off_inf->elem[off_inf->cnt],
+			   req->data, req->size)) {
+		mpp_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	off_inf->cnt += cnt;
+
+	return 0;
+}
+
+int mpp_query_reg_offset_info(struct reg_offset_info *off_inf,
+			      u32 index)
+{
+	mpp_debug_enter();
+	if (off_inf) {
+		int i;
+
+		for (i = 0; i < off_inf->cnt; i++) {
+			if (off_inf->elem[i].index == index)
+				return off_inf->elem[i].offset;
+		}
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int mpp_translate_reg_offset_info(struct mpp_task *task,
+				  struct reg_offset_info *off_inf,
+				  u32 *reg)
+{
+	mpp_debug_enter();
+
+	if (off_inf) {
+		int i;
+
+		for (i = 0; i < off_inf->cnt; i++) {
+			mpp_debug(DEBUG_IOMMU, "reg[%d] + offset %d\n",
+				  off_inf->elem[i].index,
+				  off_inf->elem[i].offset);
+			reg[off_inf->elem[i].index] += off_inf->elem[i].offset;
+		}
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int mpp_task_init(struct mpp_session *session, struct mpp_task *task)
+{
+	INIT_LIST_HEAD(&task->pending_link);
+	INIT_LIST_HEAD(&task->queue_link);
+	INIT_LIST_HEAD(&task->mem_region_list);
+	task->state = 0;
+	task->mem_count = 0;
+	task->session = session;
+
+	return 0;
+}
+
+int mpp_task_finish(struct mpp_session *session,
+		    struct mpp_task *task)
+{
+	struct mpp_dev *mpp = mpp_get_task_used_device(task, session);
+
+	if (mpp->dev_ops->finish)
+		mpp->dev_ops->finish(mpp, task);
+
+	mpp_reset_up_read(mpp->reset_group);
+	if (atomic_read(&mpp->reset_request) > 0)
+		mpp_dev_reset(mpp);
+	mpp_power_off(mpp);
+
+	set_bit(TASK_STATE_FINISH, &task->state);
+	set_bit(TASK_STATE_DONE, &task->state);
+
+	if (session->srv->timing_en) {
+		s64 time_diff;
+
+		task->on_finish = ktime_get();
+		set_bit(TASK_TIMING_FINISH, &task->state);
+
+		time_diff = ktime_us_delta(task->on_finish, task->on_create);
+
+		if (mpp->timing_check && time_diff > (s64)mpp->timing_check)
+			mpp_task_dump_timing(task, time_diff);
+	}
+
+	/* Wake up the GET thread */
+	wake_up(&task->wait);
+	mpp_taskqueue_pop_running(mpp->queue, task);
+
+	return 0;
+}
+
+int mpp_task_finalize(struct mpp_session *session,
+		      struct mpp_task *task)
+{
+	struct mpp_mem_region *mem_region = NULL, *n;
+	struct mpp_dev *mpp = mpp_get_task_used_device(task, session);
+
+	/* release memory region attach to this registers table. */
+	list_for_each_entry_safe(mem_region, n,
+				 &task->mem_region_list,
+				 reg_link) {
+		if (!mem_region->is_dup) {
+			mpp_iommu_down_read(mpp->iommu_info);
+			mpp_dma_release(session->dma, mem_region->hdl);
+			mpp_iommu_up_read(mpp->iommu_info);
+		}
+		list_del_init(&mem_region->reg_link);
+	}
+
+	return 0;
+}
+
+int mpp_task_dump_mem_region(struct mpp_dev *mpp,
+			     struct mpp_task *task)
+{
+	struct mpp_mem_region *mem = NULL, *n;
+
+	if (!task)
+		return -EIO;
+
+	mpp_err("--- dump task %d mem region ---\n", task->task_index);
+	if (!list_empty(&task->mem_region_list)) {
+		list_for_each_entry_safe(mem, n,
+					 &task->mem_region_list,
+					 reg_link) {
+			mpp_err("reg[%3d]: %pad, size %lx\n",
+				mem->reg_idx, &mem->iova, mem->len);
+		}
+	} else {
+		dev_err(mpp->dev, "no memory region mapped\n");
+	}
+
+	return 0;
+}
+
+int mpp_task_dump_reg(struct mpp_dev *mpp,
+		      struct mpp_task *task)
+{
+	if (!task)
+		return -EIO;
+
+	if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+		mpp_err("--- dump task register ---\n");
+		if (task->reg) {
+			u32 i;
+			u32 s = task->hw_info->reg_start;
+			u32 e = task->hw_info->reg_end;
+
+			for (i = s; i <= e; i++) {
+				u32 reg = i * sizeof(u32);
+
+				mpp_err("reg[%03d]: %04x: 0x%08x\n",
+					i, reg, task->reg[i]);
+			}
+		}
+	}
+
+	return 0;
+}
+
+int mpp_task_dump_hw_reg(struct mpp_dev *mpp)
+{
+	u32 i;
+	u32 s = mpp->var->hw_info->reg_start;
+	u32 e = mpp->var->hw_info->reg_end;
+
+	mpp_err("--- dump hardware register ---\n");
+	for (i = s; i <= e; i++) {
+		u32 reg = i * sizeof(u32);
+
+		mpp_err("reg[%03d]: %04x: 0x%08x\n",
+				i, reg, readl_relaxed(mpp->reg_base + reg));
+	}
+
+	return 0;
+}
+
+void mpp_reg_show(struct mpp_dev *mpp, u32 offset)
+{
+	if (!mpp)
+		return;
+
+	dev_err(mpp->dev, "reg[%03d]: %04x: 0x%08x\n",
+		offset >> 2, offset, mpp_read_relaxed(mpp, offset));
+}
+
+void mpp_reg_show_range(struct mpp_dev *mpp, u32 start, u32 end)
+{
+	u32 offset;
+
+	if (!mpp)
+		return;
+
+	for (offset = start; offset < end; offset += sizeof(u32))
+		mpp_reg_show(mpp, offset);
+}
+
+/* The device will do more probing work after this */
+int mpp_dev_probe(struct mpp_dev *mpp,
+		  struct platform_device *pdev)
+{
+	int ret;
+	struct resource *res = NULL;
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+	struct mpp_hw_info *hw_info = mpp->var->hw_info;
+
+	/* Get disable auto frequent flag from dtsi */
+	mpp->auto_freq_en = !device_property_read_bool(dev, "rockchip,disable-auto-freq");
+	/* read flag for pum idle request */
+	mpp->skip_idle = device_property_read_bool(dev, "rockchip,skip-pmu-idle-request");
+
+	/* read link table capacity */
+	ret = of_property_read_u32(np, "rockchip,task-capacity",
+				   &mpp->task_capacity);
+	if (ret)
+		mpp->task_capacity = 1;
+
+	mpp->dev = dev;
+	mpp->hw_ops = mpp->var->hw_ops;
+	mpp->dev_ops = mpp->var->dev_ops;
+
+	/* Get and attach to service */
+	ret = mpp_attach_service(mpp, dev);
+	if (ret) {
+		dev_err(dev, "failed to attach service\n");
+		return -ENODEV;
+	}
+
+	/* power domain autosuspend delay 2s */
+	pm_runtime_set_autosuspend_delay(dev, 2000);
+	pm_runtime_use_autosuspend(dev);
+
+	kthread_init_work(&mpp->work, mpp_task_worker_default);
+
+	atomic_set(&mpp->reset_request, 0);
+	atomic_set(&mpp->session_index, 0);
+	atomic_set(&mpp->task_count, 0);
+	atomic_set(&mpp->task_index, 0);
+
+	device_init_wakeup(dev, true);
+	pm_runtime_enable(dev);
+	mpp->irq = platform_get_irq(pdev, 0);
+	if (mpp->irq < 0) {
+		dev_err(dev, "No interrupt resource found\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "no memory resource defined\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+	/*
+	 * Tips: here can not use function devm_ioremap_resource. The resion is
+	 * that hevc and vdpu map the same register address region in rk3368.
+	 * However, devm_ioremap_resource will call function
+	 * devm_request_mem_region to check region. Thus, use function
+	 * devm_ioremap can avoid it.
+	 */
+	mpp->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!mpp->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		ret = -ENOMEM;
+		goto failed;
+	}
+	mpp->io_base = res->start;
+
+	/*
+	 * TODO: here or at the device itself, some device does not
+	 * have the iommu, maybe in the device is better.
+	 */
+	mpp->iommu_info = mpp_iommu_probe(dev);
+	if (IS_ERR(mpp->iommu_info)) {
+		dev_err(dev, "failed to attach iommu\n");
+		mpp->iommu_info = NULL;
+	}
+	if (mpp->hw_ops->init) {
+		ret = mpp->hw_ops->init(mpp);
+		if (ret)
+			goto failed;
+	}
+
+	/* read hardware id */
+	if (hw_info->reg_id >= 0) {
+		pm_runtime_get_sync(dev);
+		if (mpp->hw_ops->clk_on)
+			mpp->hw_ops->clk_on(mpp);
+
+		hw_info->hw_id = mpp_read(mpp, hw_info->reg_id * sizeof(u32));
+		if (mpp->hw_ops->clk_off)
+			mpp->hw_ops->clk_off(mpp);
+		pm_runtime_put_sync(dev);
+	}
+
+	return ret;
+failed:
+	mpp_detach_workqueue(mpp);
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+
+	return ret;
+}
+
+int mpp_dev_remove(struct mpp_dev *mpp)
+{
+	if (mpp->hw_ops->exit)
+		mpp->hw_ops->exit(mpp);
+
+	mpp_iommu_remove(mpp->iommu_info);
+	mpp_detach_workqueue(mpp);
+	device_init_wakeup(mpp->dev, false);
+	pm_runtime_disable(mpp->dev);
+
+	return 0;
+}
+
+void mpp_dev_shutdown(struct platform_device *pdev)
+{
+	int ret;
+	int val;
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "shutdown device\n");
+
+	atomic_inc(&mpp->srv->shutdown_request);
+	ret = readx_poll_timeout(atomic_read,
+				 &mpp->task_count,
+				 val, val == 0, 20000, 200000);
+	if (ret == -ETIMEDOUT)
+		dev_err(dev, "wait total %d running time out\n",
+			atomic_read(&mpp->task_count));
+	else
+		dev_info(dev, "shutdown success\n");
+}
+
+int mpp_dev_register_srv(struct mpp_dev *mpp, struct mpp_service *srv)
+{
+	enum MPP_DEVICE_TYPE device_type = mpp->var->device_type;
+
+	srv->sub_devices[device_type] = mpp;
+	set_bit(device_type, &srv->hw_support);
+
+	return 0;
+}
+
+irqreturn_t mpp_dev_irq(int irq, void *param)
+{
+	struct mpp_dev *mpp = param;
+	struct mpp_task *task = mpp->cur_task;
+	irqreturn_t irq_ret = IRQ_NONE;
+	u32 timing_en = mpp->srv->timing_en;
+
+	if (task && timing_en) {
+		task->on_irq = ktime_get();
+		set_bit(TASK_TIMING_IRQ, &task->state);
+	}
+
+	if (mpp->dev_ops->irq)
+		irq_ret = mpp->dev_ops->irq(mpp);
+
+	if (!task) {
+		mpp_debug(DEBUG_IRQ_CHECK, "error, task is null\n");
+		irq_ret = IRQ_HANDLED;
+		goto done;
+	}
+
+	if (irq_ret == IRQ_WAKE_THREAD) {
+		/* if wait or delayed work timeout, abort request will turn on,
+		 * isr should not to response, and handle it in delayed work
+		 */
+		if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+			dev_err(mpp->dev, "error, task %d has been handled, irq_status %#x\n",
+				task->task_index, mpp->irq_status);
+			irq_ret = IRQ_HANDLED;
+			goto done;
+		}
+		if (timing_en) {
+			task->on_cancel_timeout = ktime_get();
+			set_bit(TASK_TIMING_TO_CANCEL, &task->state);
+		}
+		cancel_delayed_work(&task->timeout_work);
+		/* normal condition, set state and wake up isr thread */
+		set_bit(TASK_STATE_IRQ, &task->state);
+		task->irq_status = mpp->irq_status;
+		mpp_iommu_dev_deactivate(mpp->iommu_info, mpp);
+		irq_ret = IRQ_HANDLED;
+		mpp_taskqueue_trigger_work(mpp);
+	}
+
+done:
+	return irq_ret;
+}
+
+u32 mpp_get_grf(struct mpp_grf_info *grf_info)
+{
+	u32 val = 0;
+
+	if (grf_info && grf_info->grf && grf_info->val)
+		regmap_read(grf_info->grf, grf_info->offset, &val);
+
+	return (val & MPP_GRF_VAL_MASK);
+}
+
+bool mpp_grf_is_changed(struct mpp_grf_info *grf_info)
+{
+	bool changed = false;
+
+	if (grf_info && grf_info->grf && grf_info->val) {
+		u32 grf_status = mpp_get_grf(grf_info);
+		u32 grf_val = grf_info->val & MPP_GRF_VAL_MASK;
+
+		changed = (grf_status == grf_val) ? false : true;
+	}
+
+	return changed;
+}
+
+int mpp_set_grf(struct mpp_grf_info *grf_info)
+{
+	if (grf_info && grf_info->grf && grf_info->val)
+		regmap_write(grf_info->grf, grf_info->offset, grf_info->val);
+
+	return 0;
+}
+
+int mpp_time_record(struct mpp_task *task)
+{
+	if (mpp_debug_unlikely(DEBUG_TIMING) && task) {
+		task->start = ktime_get();
+		task->part = task->start;
+	}
+
+	return 0;
+}
+
+int mpp_time_part_diff(struct mpp_task *task)
+{
+	if (mpp_debug_unlikely(DEBUG_TIMING)) {
+		ktime_t end;
+		struct mpp_dev *mpp = mpp_get_task_used_device(task, task->session);
+
+		end = ktime_get();
+		mpp_debug(DEBUG_PART_TIMING, "%s:%d session %d:%d part time: %lld us\n",
+			dev_name(mpp->dev), task->core_id, task->session->pid,
+			task->session->index, ktime_us_delta(end, task->part));
+		task->part = end;
+	}
+
+	return 0;
+}
+
+int mpp_time_diff(struct mpp_task *task)
+{
+	if (mpp_debug_unlikely(DEBUG_TIMING)) {
+		ktime_t end;
+		struct mpp_dev *mpp = mpp_get_task_used_device(task, task->session);
+
+		end = ktime_get();
+		mpp_debug(DEBUG_TIMING, "%s:%d session %d:%d time: %lld us\n",
+			dev_name(mpp->dev), task->core_id, task->session->pid,
+			task->session->index, ktime_us_delta(end, task->start));
+	}
+
+	return 0;
+}
+
+int mpp_time_diff_with_hw_time(struct mpp_task *task, u32 clk_hz)
+{
+	if (mpp_debug_unlikely(DEBUG_TIMING)) {
+		ktime_t end;
+		struct mpp_dev *mpp = mpp_get_task_used_device(task, task->session);
+
+		end = ktime_get();
+
+		if (clk_hz)
+			mpp_debug(DEBUG_TIMING, "%s:%d session %d:%d time: %lld us hw %d us\n",
+				dev_name(mpp->dev), task->core_id, task->session->pid,
+				task->session->index, ktime_us_delta(end, task->start),
+				task->hw_cycles / (clk_hz / 1000000));
+		else
+			mpp_debug(DEBUG_TIMING, "%s:%d session %d:%d time: %lld us\n",
+				dev_name(mpp->dev), task->core_id, task->session->pid,
+				task->session->index, ktime_us_delta(end, task->start));
+	}
+
+	return 0;
+}
+
+#define LOG_TIMING(state, id, stage, time, base) \
+	do { \
+		if (test_bit(id, &state)) \
+			pr_info("timing: %-14s : %lld us\n", stage, ktime_us_delta(time, base)); \
+		else \
+			pr_info("timing: %-14s : invalid\n", stage); \
+	} while (0)
+
+void mpp_task_dump_timing(struct mpp_task *task, s64 time_diff)
+{
+	ktime_t s = task->on_create;
+	unsigned long state = task->state;
+
+	pr_info("task %d dump timing at %lld us:", task->task_id, time_diff);
+
+	pr_info("timing: %-14s : %lld us\n", "create", ktime_to_us(s));
+	LOG_TIMING(state, TASK_TIMING_CREATE_END, "create end",     task->on_create_end, s);
+	LOG_TIMING(state, TASK_TIMING_PENDING,    "pending",        task->on_pending, s);
+	LOG_TIMING(state, TASK_TIMING_RUN,        "run",            task->on_run, s);
+	LOG_TIMING(state, TASK_TIMING_TO_SCHED,   "timeout start",  task->on_sched_timeout, s);
+	LOG_TIMING(state, TASK_TIMING_RUN_END,    "run end",        task->on_run_end, s);
+	LOG_TIMING(state, TASK_TIMING_IRQ,        "irq",            task->on_irq, s);
+	LOG_TIMING(state, TASK_TIMING_TO_CANCEL,  "timeout cancel", task->on_cancel_timeout, s);
+	LOG_TIMING(state, TASK_TIMING_FINISH,     "finish",         task->on_finish, s);
+}
+
+int mpp_write_req(struct mpp_dev *mpp, u32 *regs,
+		  u32 start_idx, u32 end_idx, u32 en_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++) {
+		if (i == en_idx)
+			continue;
+		mpp_write_relaxed(mpp, i * sizeof(u32), regs[i]);
+	}
+
+	return 0;
+}
+
+int mpp_read_req(struct mpp_dev *mpp, u32 *regs,
+		 u32 start_idx, u32 end_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++)
+		regs[i] = mpp_read_relaxed(mpp, i * sizeof(u32));
+
+	return 0;
+}
+
+int mpp_get_clk_info(struct mpp_dev *mpp,
+		     struct mpp_clk_info *clk_info,
+		     const char *name)
+{
+	int index = of_property_match_string(mpp->dev->of_node,
+					     "clock-names", name);
+
+	if (index < 0)
+		return -EINVAL;
+
+	clk_info->clk = devm_clk_get(mpp->dev, name);
+	of_property_read_u32_index(mpp->dev->of_node,
+				   "rockchip,normal-rates",
+				   index,
+				   &clk_info->normal_rate_hz);
+	of_property_read_u32_index(mpp->dev->of_node,
+				   "rockchip,advanced-rates",
+				   index,
+				   &clk_info->advanced_rate_hz);
+
+	return 0;
+}
+
+int mpp_set_clk_info_rate_hz(struct mpp_clk_info *clk_info,
+			     enum MPP_CLOCK_MODE mode,
+			     unsigned long val)
+{
+	if (!clk_info->clk || !val)
+		return 0;
+
+	switch (mode) {
+	case CLK_MODE_DEBUG:
+		clk_info->debug_rate_hz = val;
+	break;
+	case CLK_MODE_REDUCE:
+		clk_info->reduce_rate_hz = val;
+	break;
+	case CLK_MODE_NORMAL:
+		clk_info->normal_rate_hz = val;
+	break;
+	case CLK_MODE_ADVANCED:
+		clk_info->advanced_rate_hz = val;
+	break;
+	case CLK_MODE_DEFAULT:
+		clk_info->default_rate_hz = val;
+	break;
+	default:
+		mpp_err("error mode %d\n", mode);
+	break;
+	}
+
+	return 0;
+}
+
+#define MPP_REDUCE_RATE_HZ (50 * MHZ)
+
+unsigned long mpp_get_clk_info_rate_hz(struct mpp_clk_info *clk_info,
+				       enum MPP_CLOCK_MODE mode)
+{
+	unsigned long clk_rate_hz = 0;
+
+	if (!clk_info->clk)
+		return 0;
+
+	if (clk_info->debug_rate_hz)
+		return clk_info->debug_rate_hz;
+
+	switch (mode) {
+	case CLK_MODE_REDUCE: {
+		if (clk_info->reduce_rate_hz)
+			clk_rate_hz = clk_info->reduce_rate_hz;
+		else
+			clk_rate_hz = MPP_REDUCE_RATE_HZ;
+	} break;
+	case CLK_MODE_NORMAL: {
+		if (clk_info->normal_rate_hz)
+			clk_rate_hz = clk_info->normal_rate_hz;
+		else
+			clk_rate_hz = clk_info->default_rate_hz;
+	} break;
+	case CLK_MODE_ADVANCED: {
+		if (clk_info->advanced_rate_hz)
+			clk_rate_hz = clk_info->advanced_rate_hz;
+		else if (clk_info->normal_rate_hz)
+			clk_rate_hz = clk_info->normal_rate_hz;
+		else
+			clk_rate_hz = clk_info->default_rate_hz;
+	} break;
+	case CLK_MODE_DEFAULT:
+	default: {
+		clk_rate_hz = clk_info->default_rate_hz;
+	} break;
+	}
+
+	return clk_rate_hz;
+}
+
+int mpp_clk_set_rate(struct mpp_clk_info *clk_info,
+		     enum MPP_CLOCK_MODE mode)
+{
+	unsigned long clk_rate_hz;
+
+	if (!clk_info->clk)
+		return -EINVAL;
+
+	clk_rate_hz = mpp_get_clk_info_rate_hz(clk_info, mode);
+	if (clk_rate_hz) {
+		clk_info->used_rate_hz = clk_rate_hz;
+		clk_set_rate(clk_info->clk, clk_rate_hz);
+		clk_info->real_rate_hz = clk_get_rate(clk_info->clk);
+	}
+
+	return 0;
+}
+
+static int __maybe_unused mpp_common_runtime_suspend(struct device *dev)
+{
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	mpp_dev_load_clear(mpp);
+
+	return 0;
+}
+
+static int __maybe_unused mpp_common_runtime_resume(struct device *dev)
+{
+	return 0;
+}
+
+const struct dev_pm_ops mpp_common_pm_ops = {
+	SET_RUNTIME_PM_OPS(mpp_common_runtime_suspend, mpp_common_runtime_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)
+};
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int fops_show_u32(struct seq_file *file, void *v)
+{
+	u32 *val = file->private;
+
+	seq_printf(file, "%d\n", *val);
+
+	return 0;
+}
+
+static int fops_open_u32(struct inode *inode, struct file *file)
+{
+	return single_open(file, fops_show_u32, pde_data(inode));
+}
+
+static ssize_t fops_write_u32(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+{
+	int rc;
+	struct seq_file *priv = file->private_data;
+
+	rc = kstrtou32_from_user(buf, count, 0, priv->private);
+	if (rc)
+		return rc;
+
+	return count;
+}
+
+static const struct proc_ops procfs_fops_u32 = {
+	.proc_open = fops_open_u32,
+	.proc_read = seq_read,
+	.proc_release = single_release,
+	.proc_write = fops_write_u32,
+};
+
+struct proc_dir_entry *
+mpp_procfs_create_u32(const char *name, umode_t mode,
+		      struct proc_dir_entry *parent, void *data)
+{
+	return proc_create_data(name, mode, parent, &procfs_fops_u32, data);
+}
+
+void mpp_procfs_create_common(struct proc_dir_entry *parent, struct mpp_dev *mpp)
+{
+	mpp_procfs_create_u32("disable_work", 0644, parent, &mpp->disable);
+	mpp_procfs_create_u32("timing_check", 0644, parent, &mpp->timing_check);
+}
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_common.h b/drivers/video/rockchip/mpp/mpp_common.h
new file mode 100644
index 0000000000000..d60d69970f56c
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_common.h
@@ -0,0 +1,873 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#ifndef __ROCKCHIP_MPP_COMMON_H__
+#define __ROCKCHIP_MPP_COMMON_H__
+
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/dma-buf.h>
+#include <linux/kfifo.h>
+#include <linux/types.h>
+#include <linux/time.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/reset.h>
+#include <linux/irqreturn.h>
+#include <linux/poll.h>
+#include <linux/platform_device.h>
+#include <soc/rockchip/pm_domains.h>
+#include <uapi/linux/rk-mpp.h>
+
+#define MHZ				(1000 * 1000)
+#define MPP_WORK_TIMEOUT_DELAY		(500)
+
+#define MPP_MAX_MSG_NUM			(16)
+#define MPP_MAX_REG_TRANS_NUM		(80)
+#define MPP_MAX_TASK_CAPACITY		(16)
+
+/* grf mask for get value */
+#define MPP_GRF_VAL_MASK		(0xFFFF)
+
+/* max 4 cores supported */
+#define MPP_MAX_CORE_NUM		(4)
+
+/**
+ * Device type: classified by hardware feature
+ */
+enum MPP_DEVICE_TYPE {
+	MPP_DEVICE_VDPU1	= 0, /* 0x00000001 */
+	MPP_DEVICE_VDPU2	= 1, /* 0x00000002 */
+	MPP_DEVICE_VDPU1_PP	= 2, /* 0x00000004 */
+	MPP_DEVICE_VDPU2_PP	= 3, /* 0x00000008 */
+	MPP_DEVICE_AV1DEC	= 4, /* 0x00000010 */
+
+	MPP_DEVICE_HEVC_DEC	= 8, /* 0x00000100 */
+	MPP_DEVICE_RKVDEC	= 9, /* 0x00000200 */
+	MPP_DEVICE_AVSPLUS_DEC	= 12, /* 0x00001000 */
+	MPP_DEVICE_RKJPEGD	= 13, /* 0x00002000 */
+
+	MPP_DEVICE_RKVENC	= 16, /* 0x00010000 */
+	MPP_DEVICE_VEPU1	= 17, /* 0x00020000 */
+	MPP_DEVICE_VEPU2	= 18, /* 0x00040000 */
+	MPP_DEVICE_VEPU2_JPEG	= 19, /* 0x00080000 */
+	MPP_DEVICE_RKJPEGE	= 20, /* 0x00100000 */
+	MPP_DEVICE_VEPU22	= 24, /* 0x01000000 */
+
+	MPP_DEVICE_IEP2		= 28, /* 0x10000000 */
+	MPP_DEVICE_VDPP		= 29, /* 0x20000000 */
+	MPP_DEVICE_BUTT,
+};
+
+/**
+ * Driver type: classified by driver
+ */
+enum MPP_DRIVER_TYPE {
+	MPP_DRIVER_NULL = 0,
+	MPP_DRIVER_VDPU1,
+	MPP_DRIVER_VEPU1,
+	MPP_DRIVER_VDPU2,
+	MPP_DRIVER_VEPU2,
+	MPP_DRIVER_VEPU22,
+	MPP_DRIVER_RKVDEC,
+	MPP_DRIVER_RKVENC,
+	MPP_DRIVER_IEP,
+	MPP_DRIVER_IEP2,
+	MPP_DRIVER_JPGDEC,
+	MPP_DRIVER_RKVDEC2,
+	MPP_DRIVER_RKVENC2,
+	MPP_DRIVER_AV1DEC,
+	MPP_DRIVER_VDPP,
+	MPP_DRIVER_JPGENC,
+	MPP_DRIVER_BUTT,
+};
+
+enum MPP_CLOCK_MODE {
+	CLK_MODE_BASE		= 0,
+	CLK_MODE_DEFAULT	= CLK_MODE_BASE,
+	CLK_MODE_DEBUG,
+	CLK_MODE_REDUCE,
+	CLK_MODE_NORMAL,
+	CLK_MODE_ADVANCED,
+	CLK_MODE_BUTT,
+};
+
+enum MPP_RESET_TYPE {
+	RST_TYPE_BASE		= 0,
+	RST_TYPE_A		= RST_TYPE_BASE,
+	RST_TYPE_H,
+	RST_TYPE_NIU_A,
+	RST_TYPE_NIU_H,
+	RST_TYPE_CORE,
+	RST_TYPE_CABAC,
+	RST_TYPE_HEVC_CABAC,
+	RST_TYPE_BUTT,
+};
+
+enum ENC_INFO_TYPE {
+	ENC_INFO_BASE		= 0,
+	ENC_INFO_WIDTH,
+	ENC_INFO_HEIGHT,
+	ENC_INFO_FORMAT,
+	ENC_INFO_FPS_IN,
+	ENC_INFO_FPS_OUT,
+	ENC_INFO_RC_MODE,
+	ENC_INFO_BITRATE,
+	ENC_INFO_GOP_SIZE,
+	ENC_INFO_FPS_CALC,
+	ENC_INFO_PROFILE,
+
+	ENC_INFO_BUTT,
+};
+
+enum DEC_INFO_TYPE {
+	DEC_INFO_BASE		= 0,
+	DEC_INFO_WIDTH,
+	DEC_INFO_HEIGHT,
+	DEC_INFO_FORMAT,
+	DEC_INFO_BITDEPTH,
+	DEC_INFO_FPS,
+
+	DEC_INFO_BUTT,
+};
+
+enum CODEC_INFO_FLAGS {
+	CODEC_INFO_FLAG_NULL	= 0,
+	CODEC_INFO_FLAG_NUMBER,
+	CODEC_INFO_FLAG_STRING,
+
+	CODEC_INFO_FLAG_BUTT,
+};
+
+struct mpp_task;
+struct mpp_session;
+struct mpp_dma_session;
+struct mpp_taskqueue;
+struct iommu_domain;
+
+/* struct use to collect task set and poll message */
+struct mpp_task_msgs {
+	/* for ioctl msgs bat process */
+	struct list_head list;
+	struct list_head list_session;
+
+	struct mpp_session *session;
+	struct mpp_taskqueue *queue;
+	struct mpp_task *task;
+	struct mpp_dev *mpp;
+
+	/* for fd reference */
+	int ext_fd;
+	struct fd f;
+
+	u32 flags;
+	u32 req_cnt;
+	u32 set_cnt;
+	u32 poll_cnt;
+
+	struct mpp_request reqs[MPP_MAX_MSG_NUM];
+	struct mpp_request *poll_req;
+};
+
+struct mpp_grf_info {
+	u32 offset;
+	u32 val;
+	struct regmap *grf;
+};
+
+/**
+ * struct for hardware info
+ */
+struct mpp_hw_info {
+	/* register number */
+	u32 reg_num;
+	/* hardware id */
+	int reg_id;
+	u32 hw_id;
+	/* start index of register */
+	u32 reg_start;
+	/* end index of register */
+	u32 reg_end;
+	/* register of enable hardware */
+	int reg_en;
+	/* register of codec format */
+	int reg_fmt;
+	u32 reg_ret_status;
+	void *link_info;
+	u32 magic_base;
+};
+
+struct mpp_trans_info {
+	const int count;
+	const u16 * const table;
+};
+
+struct reg_offset_elem {
+	u32 index;
+	u32 offset;
+};
+
+struct reg_offset_info {
+	u32 cnt;
+	struct reg_offset_elem elem[MPP_MAX_REG_TRANS_NUM];
+};
+
+struct codec_info_elem {
+	__u32 type;
+	__u32 flag;
+	__u64 data;
+};
+
+struct mpp_clk_info {
+	struct clk *clk;
+
+	/* debug rate, from debug */
+	u32 debug_rate_hz;
+	/* normal rate, from dtsi */
+	u32 normal_rate_hz;
+	/* high performance rate, from dtsi */
+	u32 advanced_rate_hz;
+
+	u32 default_rate_hz;
+	u32 reduce_rate_hz;
+	/* record last used rate */
+	u32 used_rate_hz;
+	u32 real_rate_hz;
+};
+
+struct mpp_dev_var {
+	enum MPP_DEVICE_TYPE device_type;
+
+	/* info for each hardware */
+	struct mpp_hw_info *hw_info;
+	struct mpp_trans_info *trans_info;
+	struct mpp_hw_ops *hw_ops;
+	struct mpp_dev_ops *dev_ops;
+};
+
+struct mpp_mem_region {
+	struct list_head reg_link;
+	/* address for iommu */
+	dma_addr_t iova;
+	unsigned long len;
+	u32 reg_idx;
+	void *hdl;
+	int fd;
+	/* whether is dup import entity */
+	bool is_dup;
+};
+
+struct mpp_load_info {
+	s64 busy_time;
+	s64 hw_busy_time;
+	ktime_t load_time;
+	u32 load;
+	u32 load_frac;
+	u32 utilization;
+	u32 utilization_frac;
+};
+
+struct mpp_dev {
+	struct device *dev;
+	const struct mpp_dev_var *var;
+	struct mpp_hw_ops *hw_ops;
+	struct mpp_dev_ops *dev_ops;
+
+	/* per-device work for attached taskqueue */
+	struct kthread_work work;
+	/* the flag for get/get/reduce freq */
+	bool auto_freq_en;
+	/* the flag for pmu idle request before device reset */
+	bool skip_idle;
+
+	/*
+	 * The task capacity is the task queue length that hardware can accept.
+	 * Default 1 means normal hardware can only accept one task at once.
+	 */
+	u32 task_capacity;
+	/*
+	 * The message capacity is the max message parallel process capacity.
+	 * Default 1 means normal hardware can only accept one message at one
+	 * shot ioctl.
+	 * Multi-core hardware can accept more message at one shot ioctl.
+	 */
+	u32 msgs_cap;
+
+	int irq;
+	bool is_irq_startup;
+	u32 irq_status;
+
+	void __iomem *reg_base;
+	struct mpp_grf_info *grf_info;
+	struct mpp_iommu_info *iommu_info;
+	int (*fault_handler)(struct iommu_domain *iommu, struct device *iommu_dev,
+			     unsigned long iova, int status, void *arg);
+	resource_size_t io_base;
+
+	atomic_t reset_request;
+	atomic_t session_index;
+	atomic_t task_count;
+	atomic_t task_index;
+	/* current task in running */
+	struct mpp_task *cur_task;
+	/* set session max buffers */
+	u32 session_max_buffers;
+	struct mpp_taskqueue *queue;
+	struct mpp_reset_group *reset_group;
+	/* point to MPP Service */
+	struct mpp_service *srv;
+
+	/* multi-core data */
+	struct list_head queue_link;
+	s32 core_id;
+
+	/* common per-device procfs */
+	u32 disable;
+	u32 timing_check;
+	u32 load_en;
+	struct mpp_load_info load_info;
+};
+
+struct mpp_session {
+	enum MPP_DEVICE_TYPE device_type;
+	u32 index;
+	/* the session related device private data */
+	struct mpp_service *srv;
+	struct mpp_dev *mpp;
+	struct mpp_dma_session *dma;
+
+	/* lock for session task pending list */
+	struct mutex pending_lock;
+	/* task pending list in session */
+	struct list_head pending_list;
+
+	pid_t pid;
+	atomic_t task_count;
+	atomic_t release_request;
+	/* trans info set by user */
+	int trans_count;
+	u16 trans_table[MPP_MAX_REG_TRANS_NUM];
+	u32 msg_flags;
+	/* link to mpp_service session_list */
+	struct list_head service_link;
+	/* link to mpp_workqueue session_attach / session_detach */
+	struct list_head session_link;
+	/* private data */
+	void *priv;
+
+	/*
+	 * session handler from mpp_dev_ops
+	 * process_task - handle messages of sending task
+	 * wait_result  - handle messages of polling task
+	 * deinit	- handle session deinit
+	 */
+	int (*process_task)(struct mpp_session *session,
+			    struct mpp_task_msgs *msgs);
+	int (*wait_result)(struct mpp_session *session,
+			   struct mpp_task_msgs *msgs);
+	void (*deinit)(struct mpp_session *session);
+
+	/* max message count */
+	int msgs_cnt;
+	struct list_head list_msgs;
+	struct list_head list_msgs_idle;
+	spinlock_t lock_msgs;
+};
+
+/* task state in work thread */
+enum mpp_task_state {
+	TASK_STATE_PENDING	= 0,
+	TASK_STATE_RUNNING	= 1,
+	TASK_STATE_START	= 2,
+	TASK_STATE_HANDLE	= 3,
+	TASK_STATE_IRQ		= 4,
+	TASK_STATE_FINISH	= 5,
+	TASK_STATE_TIMEOUT	= 6,
+	TASK_STATE_DONE		= 7,
+
+	TASK_STATE_PREPARE	= 8,
+	TASK_STATE_ABORT	= 9,
+	TASK_STATE_ABORT_READY	= 10,
+	TASK_STATE_PROC_DONE	= 11,
+
+	/* timing debug state */
+	TASK_TIMING_CREATE	= 16,
+	TASK_TIMING_CREATE_END	= 17,
+	TASK_TIMING_PENDING	= 18,
+	TASK_TIMING_RUN		= 19,
+	TASK_TIMING_TO_SCHED	= 20,
+	TASK_TIMING_RUN_END	= 21,
+	TASK_TIMING_IRQ		= 22,
+	TASK_TIMING_TO_CANCEL	= 23,
+	TASK_TIMING_FINISH	= 24,
+};
+
+/* The context for the a task */
+struct mpp_task {
+	/* context belong to */
+	struct mpp_session *session;
+
+	/* link to pending list in session */
+	struct list_head pending_link;
+	/* link to done list in session */
+	struct list_head done_link;
+	/* link to list in taskqueue */
+	struct list_head queue_link;
+	/* The DMA buffer used in this task */
+	struct list_head mem_region_list;
+	u32 mem_count;
+	struct mpp_mem_region mem_regions[MPP_MAX_REG_TRANS_NUM];
+
+	/* state in the taskqueue */
+	unsigned long state;
+	atomic_t abort_request;
+	/* delayed work for hardware timeout */
+	struct delayed_work timeout_work;
+	struct kref ref;
+
+	/* record context running start time */
+	ktime_t start;
+	ktime_t part;
+
+	/* debug timing */
+	ktime_t on_create;
+	ktime_t on_create_end;
+	ktime_t on_pending;
+	ktime_t on_run;
+	ktime_t on_sched_timeout;
+	ktime_t on_run_end;
+	ktime_t on_irq;
+	ktime_t on_cancel_timeout;
+	ktime_t on_finish;
+
+	/* hardware info for current task */
+	struct mpp_hw_info *hw_info;
+	u32 task_index;
+	u32 task_id;
+	u32 *reg;
+	u32 irq_status;
+	/* event for session wait thread */
+	wait_queue_head_t wait;
+
+	/* for multi-core */
+	struct mpp_dev *mpp;
+	s32 core_id;
+	/* hw cycles */
+	u32 hw_cycles;
+	u32 hw_time;
+};
+
+struct mpp_taskqueue {
+	/* kworker for attached taskqueue */
+	struct kthread_worker worker;
+	/* task for work queue */
+	struct task_struct *kworker_task;
+
+	/* lock for session attach and session_detach */
+	struct mutex session_lock;
+	/* link to session session_link for attached sessions */
+	struct list_head session_attach;
+	/* link to session session_link for detached sessions */
+	struct list_head session_detach;
+	atomic_t detach_count;
+
+	atomic_t task_id;
+	/* lock for pending list */
+	struct mutex pending_lock;
+	struct list_head pending_list;
+	/* lock for running list */
+	spinlock_t running_lock;
+	struct list_head running_list;
+
+	/* point to MPP Service */
+	struct mpp_service *srv;
+	/* lock for mmu list */
+	struct mutex mmu_lock;
+	struct list_head mmu_list;
+	/* lock for dev list */
+	struct mutex dev_lock;
+	struct list_head dev_list;
+	/*
+	 * task_capacity in taskqueue is the minimum task capacity of the
+	 * device task capacity which is attached to the taskqueue
+	 */
+	u32 task_capacity;
+
+	/* multi-core task distribution */
+	atomic_t reset_request;
+	struct mpp_dev *cores[MPP_MAX_CORE_NUM];
+	unsigned long core_idle;
+	u32 core_id_max;
+	u32 core_count;
+	unsigned long dev_active_flags;
+};
+
+struct mpp_reset_group {
+	/* the flag for whether use rw_sem */
+	u32 rw_sem_on;
+	struct rw_semaphore rw_sem;
+	struct reset_control *resets[RST_TYPE_BUTT];
+	/* for set rw_sem */
+	struct mpp_taskqueue *queue;
+};
+
+struct mpp_service {
+	struct class *cls;
+	struct device *dev;
+	dev_t dev_id;
+	struct cdev mpp_cdev;
+	struct device *child_dev;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	unsigned long hw_support;
+	atomic_t shutdown_request;
+	/* follows for device probe */
+	struct mpp_grf_info grf_infos[MPP_DRIVER_BUTT];
+	struct platform_driver *sub_drivers[MPP_DRIVER_BUTT];
+	/* follows for attach service */
+	struct mpp_dev *sub_devices[MPP_DEVICE_BUTT];
+	u32 taskqueue_cnt;
+	struct mpp_taskqueue *task_queues[MPP_DEVICE_BUTT];
+	u32 reset_group_cnt;
+	struct mpp_reset_group *reset_groups[MPP_DEVICE_BUTT];
+
+	/* lock for session list */
+	struct mutex session_lock;
+	struct list_head session_list;
+	u32 session_count;
+
+	/* global timing record flag */
+	u32 timing_en;
+	u32 load_interval;
+};
+
+/*
+ * struct mpp_hw_ops - context specific operations for device
+ * @init	Do something when hardware probe.
+ * @exit	Do something when hardware remove.
+ * @clk_on	Enable clocks.
+ * @clk_off	Disable clocks.
+ * @get_freq	Get special freq for setting.
+ * @set_freq	Set freq to hardware.
+ * @reduce_freq	Reduce freq when hardware is not running.
+ * @reset	When error, reset hardware.
+ * @hack_run	Hack run for some soc
+ */
+struct mpp_hw_ops {
+	int (*init)(struct mpp_dev *mpp);
+	int (*exit)(struct mpp_dev *mpp);
+	int (*clk_on)(struct mpp_dev *mpp);
+	int (*clk_off)(struct mpp_dev *mpp);
+	int (*get_freq)(struct mpp_dev *mpp,
+			struct mpp_task *mpp_task);
+	int (*set_freq)(struct mpp_dev *mpp,
+			struct mpp_task *mpp_task);
+	int (*reduce_freq)(struct mpp_dev *mpp);
+	int (*reset)(struct mpp_dev *mpp);
+	int (*set_grf)(struct mpp_dev *mpp);
+	int (*hack_run)(struct mpp_dev *mpp);
+};
+
+/*
+ * struct mpp_dev_ops - context specific operations for task
+ * @alloc_task	Alloc and set task.
+ * @prepare	Check HW status for determining run next task or not.
+ * @run		Start a single {en,de}coding run. Set registers to hardware.
+ * @irq		Deal with hardware interrupt top-half.
+ * @isr		Deal with hardware interrupt bottom-half.
+ * @finish	Read back processing results and additional data from hardware.
+ * @result	Read status to userspace.
+ * @free_task	Release the resource allocate which alloc.
+ * @ioctl	Special cammand from userspace.
+ * @init_session extra initialization on session init.
+ * @free_session extra cleanup on session deinit.
+ * @dump_session information dump for session.
+ * @dump_dev    information dump for hardware device.
+ */
+struct mpp_dev_ops {
+	int (*process_task)(struct mpp_session *session,
+			    struct mpp_task_msgs *msgs);
+	int (*wait_result)(struct mpp_session *session,
+			   struct mpp_task_msgs *msgs);
+	void (*deinit)(struct mpp_session *session);
+	void (*task_worker)(struct kthread_work *work_s);
+
+	void *(*alloc_task)(struct mpp_session *session,
+			    struct mpp_task_msgs *msgs);
+	void *(*prepare)(struct mpp_dev *mpp, struct mpp_task *task);
+	int (*run)(struct mpp_dev *mpp, struct mpp_task *task);
+	int (*irq)(struct mpp_dev *mpp);
+	int (*isr)(struct mpp_dev *mpp);
+	int (*finish)(struct mpp_dev *mpp, struct mpp_task *task);
+	int (*result)(struct mpp_dev *mpp, struct mpp_task *task,
+		      struct mpp_task_msgs *msgs);
+	int (*free_task)(struct mpp_session *session,
+			 struct mpp_task *task);
+	int (*ioctl)(struct mpp_session *session, struct mpp_request *req);
+	int (*init_session)(struct mpp_session *session);
+	int (*free_session)(struct mpp_session *session);
+	int (*dump_session)(struct mpp_session *session, struct seq_file *seq);
+	int (*dump_dev)(struct mpp_dev *mpp);
+	int (*link_irq)(struct mpp_dev *mpp);
+};
+
+struct mpp_taskqueue *mpp_taskqueue_init(struct device *dev);
+
+struct mpp_mem_region *
+mpp_task_attach_fd(struct mpp_task *task, int fd);
+int mpp_translate_reg_address(struct mpp_session *session,
+			      struct mpp_task *task, int fmt,
+			      u32 *reg, struct reg_offset_info *off_inf);
+
+int mpp_check_req(struct mpp_request *req, int base,
+		  int max_size, u32 off_s, u32 off_e);
+int mpp_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				struct mpp_request *req);
+int mpp_query_reg_offset_info(struct reg_offset_info *off_inf,
+			      u32 index);
+int mpp_translate_reg_offset_info(struct mpp_task *task,
+				  struct reg_offset_info *off_inf,
+				  u32 *reg);
+int mpp_task_init(struct mpp_session *session,
+		  struct mpp_task *task);
+int mpp_task_finish(struct mpp_session *session,
+		    struct mpp_task *task);
+void mpp_task_run_begin(struct mpp_task *task, u32 timing_en, u32 timeout);
+void mpp_task_run_end(struct mpp_task *task, u32 timing_en);
+int mpp_task_finalize(struct mpp_session *session,
+		      struct mpp_task *task);
+int mpp_task_dump_mem_region(struct mpp_dev *mpp,
+			     struct mpp_task *task);
+int mpp_task_dump_reg(struct mpp_dev *mpp,
+		      struct mpp_task *task);
+int mpp_task_dump_hw_reg(struct mpp_dev *mpp);
+void mpp_task_dump_timing(struct mpp_task *task, s64 time_diff);
+
+void mpp_reg_show(struct mpp_dev *mpp, u32 offset);
+void mpp_reg_show_range(struct mpp_dev *mpp, u32 start, u32 end);
+void mpp_free_task(struct kref *ref);
+
+void mpp_session_deinit(struct mpp_session *session);
+void mpp_session_cleanup_detach(struct mpp_taskqueue *queue,
+				struct kthread_work *work);
+
+int mpp_taskqueue_pending_to_run(struct mpp_taskqueue *queue, struct mpp_task *task);
+
+int mpp_dev_probe(struct mpp_dev *mpp,
+		  struct platform_device *pdev);
+int mpp_dev_remove(struct mpp_dev *mpp);
+void mpp_dev_shutdown(struct platform_device *pdev);
+int mpp_dev_register_srv(struct mpp_dev *mpp, struct mpp_service *srv);
+
+int mpp_power_on(struct mpp_dev *mpp);
+int mpp_power_off(struct mpp_dev *mpp);
+int mpp_dev_reset(struct mpp_dev *mpp);
+
+irqreturn_t mpp_dev_irq(int irq, void *param);
+
+struct reset_control *mpp_reset_control_get(struct mpp_dev *mpp,
+					    enum MPP_RESET_TYPE type,
+					    const char *name);
+
+u32 mpp_get_grf(struct mpp_grf_info *grf_info);
+bool mpp_grf_is_changed(struct mpp_grf_info *grf_info);
+int mpp_set_grf(struct mpp_grf_info *grf_info);
+
+int mpp_time_record(struct mpp_task *task);
+int mpp_time_diff(struct mpp_task *task);
+int mpp_time_diff_with_hw_time(struct mpp_task *task, u32 clk_hz);
+int mpp_time_part_diff(struct mpp_task *task);
+
+int mpp_write_req(struct mpp_dev *mpp, u32 *regs,
+		  u32 start_idx, u32 end_idx, u32 en_idx);
+int mpp_read_req(struct mpp_dev *mpp, u32 *regs,
+		 u32 start_idx, u32 end_idx);
+
+int mpp_get_clk_info(struct mpp_dev *mpp,
+		     struct mpp_clk_info *clk_info,
+		     const char *name);
+int mpp_set_clk_info_rate_hz(struct mpp_clk_info *clk_info,
+			     enum MPP_CLOCK_MODE mode,
+			     unsigned long val);
+unsigned long mpp_get_clk_info_rate_hz(struct mpp_clk_info *clk_info,
+				       enum MPP_CLOCK_MODE mode);
+int mpp_clk_set_rate(struct mpp_clk_info *clk_info,
+		     enum MPP_CLOCK_MODE mode);
+void mpp_dev_load(struct mpp_dev *mpp, struct mpp_task *mpp_task);
+void mpp_dev_load_clear(struct mpp_dev *mpp);
+
+static inline int mpp_write(struct mpp_dev *mpp, u32 reg, u32 val)
+{
+	int idx = reg / sizeof(u32);
+
+	mpp_debug(DEBUG_SET_REG,
+		  "write reg[%03d]: %04x: 0x%08x\n", idx, reg, val);
+	writel(val, mpp->reg_base + reg);
+
+	return 0;
+}
+
+static inline int mpp_write_relaxed(struct mpp_dev *mpp, u32 reg, u32 val)
+{
+	int idx = reg / sizeof(u32);
+
+	mpp_debug(DEBUG_SET_REG,
+		  "write reg[%03d]: %04x: 0x%08x\n", idx, reg, val);
+	writel_relaxed(val, mpp->reg_base + reg);
+
+	return 0;
+}
+
+static inline u32 mpp_read(struct mpp_dev *mpp, u32 reg)
+{
+	u32 val = 0;
+	int idx = reg / sizeof(u32);
+
+	val = readl(mpp->reg_base + reg);
+	mpp_debug(DEBUG_GET_REG,
+		  "read reg[%03d]: %04x: 0x%08x\n", idx, reg, val);
+
+	return val;
+}
+
+static inline u32 mpp_read_relaxed(struct mpp_dev *mpp, u32 reg)
+{
+	u32 val = 0;
+	int idx = reg / sizeof(u32);
+
+	val = readl_relaxed(mpp->reg_base + reg);
+	mpp_debug(DEBUG_GET_REG,
+		  "read reg[%03d] %04x: 0x%08x\n", idx, reg, val);
+
+	return val;
+}
+
+static inline int mpp_safe_reset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_assert(rst);
+
+	return 0;
+}
+
+static inline int mpp_safe_unreset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_deassert(rst);
+
+	return 0;
+}
+
+static inline int mpp_clk_safe_enable(struct clk *clk)
+{
+	if (clk)
+		clk_prepare_enable(clk);
+
+	return 0;
+}
+
+static inline int mpp_clk_safe_disable(struct clk *clk)
+{
+	if (clk)
+		clk_disable_unprepare(clk);
+
+	return 0;
+}
+
+static inline int mpp_reset_down_read(struct mpp_reset_group *group)
+{
+	if (group && group->rw_sem_on)
+		down_read(&group->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_reset_up_read(struct mpp_reset_group *group)
+{
+	if (group && group->rw_sem_on)
+		up_read(&group->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_reset_down_write(struct mpp_reset_group *group)
+{
+	if (group && group->rw_sem_on)
+		down_write(&group->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_reset_up_write(struct mpp_reset_group *group)
+{
+	if (group && group->rw_sem_on)
+		up_write(&group->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_pmu_idle_request(struct mpp_dev *mpp, bool idle)
+{
+	if (mpp->skip_idle)
+		return 0;
+
+	return rockchip_pmu_idle_request(mpp->dev, idle);
+}
+
+static inline struct mpp_dev *
+mpp_get_task_used_device(const struct mpp_task *task,
+			 const struct mpp_session *session)
+{
+	return task->mpp ? task->mpp : session->mpp;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+struct proc_dir_entry *
+mpp_procfs_create_u32(const char *name, umode_t mode,
+		      struct proc_dir_entry *parent, void *data);
+void mpp_procfs_create_common(struct proc_dir_entry *parent, struct mpp_dev *mpp);
+#else
+static inline struct proc_dir_entry *
+mpp_procfs_create_u32(const char *name, umode_t mode,
+		      struct proc_dir_entry *parent, void *data)
+{
+	return 0;
+}
+static inline void mpp_procfs_create_common(struct proc_dir_entry *parent, struct mpp_dev *mpp)
+{
+}
+#endif
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+extern const char *mpp_device_name[MPP_DEVICE_BUTT];
+extern const char *enc_info_item_name[ENC_INFO_BUTT];
+#endif
+
+extern const struct file_operations rockchip_mpp_fops;
+
+extern struct platform_driver rockchip_rkvdec_driver;
+extern struct platform_driver rockchip_rkvenc_driver;
+extern struct platform_driver rockchip_vdpu1_driver;
+extern struct platform_driver rockchip_vepu1_driver;
+extern struct platform_driver rockchip_vdpu2_driver;
+extern struct platform_driver rockchip_vepu2_driver;
+extern struct platform_driver rockchip_vepu22_driver;
+extern struct platform_driver rockchip_iep2_driver;
+extern struct platform_driver rockchip_jpgdec_driver;
+extern struct platform_driver rockchip_rkvdec2_driver;
+extern struct platform_driver rockchip_rkvenc2_driver;
+extern struct platform_driver rockchip_av1dec_driver;
+extern struct platform_driver rockchip_jpgenc_driver;
+extern struct platform_driver rockchip_vdpp_driver;
+
+extern const struct dev_pm_ops mpp_common_pm_ops;
+
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_debug.h b/drivers/video/rockchip/mpp/mpp_debug.h
new file mode 100644
index 0000000000000..4108c69da894a
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_debug.h
@@ -0,0 +1,138 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#ifndef __ROCKCHIP_MPP_DEBUG_H__
+#define __ROCKCHIP_MPP_DEBUG_H__
+
+#include <linux/types.h>
+
+/*
+ * debug flag usage:
+ * +------+-------------------+
+ * | 8bit |      24bit        |
+ * +------+-------------------+
+ *  0~23 bit is for different information type
+ * 24~31 bit is for information print format
+ */
+
+#define DEBUG_POWER				0x00000001
+#define DEBUG_CLOCK				0x00000002
+#define DEBUG_IRQ_STATUS			0x00000004
+#define DEBUG_IOMMU				0x00000008
+#define DEBUG_IOCTL				0x00000010
+#define DEBUG_FUNCTION				0x00000020
+#define DEBUG_REGISTER				0x00000040
+#define DEBUG_EXTRA_INFO			0x00000080
+#define DEBUG_TIMING				0x00000100
+#define DEBUG_TASK_INFO				0x00000200
+#define DEBUG_DUMP_ERR_REG			0x00000400
+#define DEBUG_LINK_TABLE			0x00000800
+
+#define DEBUG_SET_REG				0x00001000
+#define DEBUG_GET_REG				0x00002000
+#define DEBUG_PPS_FILL				0x00004000
+#define DEBUG_IRQ_CHECK				0x00008000
+#define DEBUG_CACHE_32B				0x00010000
+
+#define DEBUG_RESET				0x00020000
+#define DEBUG_SET_REG_L2			0x00040000
+#define DEBUG_GET_REG_L2			0x00080000
+#define DEBUG_GET_PERF_VAL			0x00100000
+#define DEBUG_SRAM_INFO				0x00200000
+
+#define DEBUG_SESSION				0x00400000
+#define DEBUG_DEVICE				0x00800000
+
+#define DEBUG_CCU				0x01000000
+#define DEBUG_CORE				0x02000000
+
+#define PRINT_FUNCTION				0x80000000
+#define PRINT_LINE				0x40000000
+
+/* reuse old debug bit flag */
+#define DEBUG_PART_TIMING			0x00000080
+#define DEBUG_SLICE				0x00000002
+
+extern unsigned int mpp_dev_debug;
+
+#define mpp_debug_unlikely(type)				\
+		(unlikely(mpp_dev_debug & (type)))
+
+#define mpp_debug_func(type, fmt, args...)			\
+	do {							\
+		if (unlikely(mpp_dev_debug & (type))) {		\
+			pr_info("%s:%d: " fmt,			\
+				 __func__, __LINE__, ##args);	\
+		}						\
+	} while (0)
+#define mpp_debug(type, fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & (type))) {		\
+			pr_info(fmt, ##args);			\
+		}						\
+	} while (0)
+
+#define mpp_debug_enter()					\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_FUNCTION)) {	\
+			pr_info("%s:%d: enter\n",		\
+				 __func__, __LINE__);		\
+		}						\
+	} while (0)
+
+#define mpp_debug_leave()					\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_FUNCTION)) {	\
+			pr_info("%s:%d: leave\n",		\
+				 __func__, __LINE__);		\
+		}						\
+	} while (0)
+
+#define mpp_err(fmt, args...)					\
+		pr_err("%s:%d: " fmt, __func__, __LINE__, ##args)
+
+#define mpp_dbg_link(fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_LINK_TABLE)) {		\
+			pr_info("%s:%d: " fmt,			\
+				 __func__, __LINE__, ##args);	\
+		}						\
+	} while (0)
+
+#define mpp_dbg_session(fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_SESSION)) {	\
+			pr_info(fmt, ##args);			\
+		}						\
+	} while (0)
+
+#define mpp_dbg_ccu(fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_CCU)) {	\
+			pr_info("%s:%d: " fmt,			\
+				 __func__, __LINE__, ##args);	\
+		}						\
+	} while (0)
+
+#define mpp_dbg_core(fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_CORE)) {	\
+			pr_info(fmt, ##args);			\
+		}						\
+	} while (0)
+
+#define mpp_dbg_slice(fmt, args...)				\
+	do {							\
+		if (unlikely(mpp_dev_debug & DEBUG_SLICE)) {	\
+			pr_info(fmt, ##args);			\
+		}						\
+	} while (0)
+
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_iep2.c b/drivers/video/rockchip/mpp/mpp_iep2.c
new file mode 100644
index 0000000000000..8a90b3bfa7ff4
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_iep2.c
@@ -0,0 +1,1166 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2020 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/dma-buf.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/pm_runtime.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "rockchip_iep2_regs.h"
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define IEP2_DRIVER_NAME		"mpp-iep2"
+
+#define	IEP2_SESSION_MAX_BUFFERS		20
+
+#define TILE_WIDTH		16
+#define TILE_HEIGHT		4
+#define MVL			28
+#define MVR			27
+
+#define IOMMU_GET_BUS_ID(x)	(((x) >> 6) & 0x1f)
+#define AUX_PAGE_SIZE		SZ_4K
+
+enum rockchip_iep2_fmt {
+	ROCKCHIP_IEP2_FMT_YUV422 = 2,
+	ROCKCHIP_IEP2_FMT_YUV420
+};
+
+enum rockchip_iep2_yuv_swap {
+	ROCKCHIP_IEP2_YUV_SWAP_SP_UV,
+	ROCKCHIP_IEP2_YUV_SWAP_SP_VU,
+	ROCKCHIP_IEP2_YUV_SWAP_P0,
+	ROCKCHIP_IEP2_YUV_SWAP_P
+};
+
+enum rockchip_iep2_dil_ff_order {
+	ROCKCHIP_IEP2_DIL_FF_ORDER_TB,
+	ROCKCHIP_IEP2_DIL_FF_ORDER_BT
+};
+
+enum rockchip_iep2_dil_mode {
+	ROCKCHIP_IEP2_DIL_MODE_DISABLE,
+	ROCKCHIP_IEP2_DIL_MODE_I5O2,
+	ROCKCHIP_IEP2_DIL_MODE_I5O1T,
+	ROCKCHIP_IEP2_DIL_MODE_I5O1B,
+	ROCKCHIP_IEP2_DIL_MODE_I2O2,
+	ROCKCHIP_IEP2_DIL_MODE_I1O1T,
+	ROCKCHIP_IEP2_DIL_MODE_I1O1B,
+	ROCKCHIP_IEP2_DIL_MODE_PD,
+	ROCKCHIP_IEP2_DIL_MODE_BYPASS,
+	ROCKCHIP_IEP2_DIL_MODE_DECT
+};
+
+enum ROCKCHIP_IEP2_PD_COMP_FLAG {
+	ROCKCHIP_IEP2_PD_COMP_FLAG_CC,
+	ROCKCHIP_IEP2_PD_COMP_FLAG_CN,
+	ROCKCHIP_IEP2_PD_COMP_FLAG_NC,
+	ROCKCHIP_IEP2_PD_COMP_FLAG_NON
+};
+
+/* default iep2 mtn table */
+static u32 iep2_mtn_tab[] = {
+	0x00000000, 0x00000000, 0x00000000, 0x00000000,
+	0x01010000, 0x06050302, 0x0f0d0a08, 0x1c191512,
+	0x2b282420, 0x3634312e, 0x3d3c3a38, 0x40403f3e,
+	0x40404040, 0x40404040, 0x40404040, 0x40404040
+};
+
+#define to_iep_task(task)		\
+		container_of(task, struct iep_task, mpp_task)
+#define to_iep2_dev(dev)		\
+		container_of(dev, struct iep2_dev, mpp)
+
+struct iep2_addr {
+	u32 y;
+	u32 cbcr;
+	u32 cr;
+};
+
+struct iep2_params {
+	u32 src_fmt;
+	u32 src_yuv_swap;
+	u32 dst_fmt;
+	u32 dst_yuv_swap;
+	u32 tile_cols;
+	u32 tile_rows;
+	u32 src_y_stride;
+	u32 src_uv_stride;
+	u32 dst_y_stride;
+
+	/* current, previous, next. */
+	struct iep2_addr src[3];
+	struct iep2_addr dst[2];
+	u32 mv_addr;
+	u32 md_addr;
+
+	u32 dil_mode;
+	u32 dil_out_mode;
+	u32 dil_field_order;
+
+	u32 md_theta;
+	u32 md_r;
+	u32 md_lambda;
+
+	u32 dect_resi_thr;
+	u32 osd_area_num;
+	u32 osd_gradh_thr;
+	u32 osd_gradv_thr;
+
+	u32 osd_pos_limit_en;
+	u32 osd_pos_limit_num;
+
+	u32 osd_limit_area[2];
+
+	u32 osd_line_num;
+	u32 osd_pec_thr;
+
+	u32 osd_x_sta[8];
+	u32 osd_x_end[8];
+	u32 osd_y_sta[8];
+	u32 osd_y_end[8];
+
+	u32 me_pena;
+	u32 mv_bonus;
+	u32 mv_similar_thr;
+	u32 mv_similar_num_thr0;
+	s32 me_thr_offset;
+
+	u32 mv_left_limit;
+	u32 mv_right_limit;
+
+	s8 mv_tru_list[8];
+	u32 mv_tru_vld[8];
+
+	u32 eedi_thr0;
+
+	u32 ble_backtoma_num;
+
+	u32 comb_cnt_thr;
+	u32 comb_feature_thr;
+	u32 comb_t_thr;
+	u32 comb_osd_vld[8];
+
+	u32 mtn_en;
+	u32 mtn_tab[16];
+
+	u32 pd_mode;
+
+	u32 roi_en;
+	u32 roi_layer_num;
+	u32 roi_mode[8];
+	u32 xsta[8];
+	u32 xend[8];
+	u32 ysta[8];
+	u32 yend[8];
+};
+
+struct iep2_output {
+	u32 mv_hist[MVL + MVR + 1];
+	u32 dect_pd_tcnt;
+	u32 dect_pd_bcnt;
+	u32 dect_ff_cur_tcnt;
+	u32 dect_ff_cur_bcnt;
+	u32 dect_ff_nxt_tcnt;
+	u32 dect_ff_nxt_bcnt;
+	u32 dect_ff_ble_tcnt;
+	u32 dect_ff_ble_bcnt;
+	u32 dect_ff_nz;
+	u32 dect_ff_comb_f;
+	u32 dect_osd_cnt;
+	u32 out_comb_cnt;
+	u32 out_osd_comb_cnt;
+	u32 ff_gradt_tcnt;
+	u32 ff_gradt_bcnt;
+	u32 x_sta[8];
+	u32 x_end[8];
+	u32 y_sta[8];
+	u32 y_end[8];
+};
+
+struct iep_task {
+	struct mpp_task mpp_task;
+	struct mpp_hw_info *hw_info;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	struct iep2_params params;
+	struct iep2_output output;
+
+	struct reg_offset_info off_inf;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	/* for I1O1T page fault hack */
+	u32 src_iova_end;
+};
+
+struct iep2_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info sclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_s;
+
+	struct mpp_dma_buffer roi;
+	/* for iommu pagefault handle */
+	struct work_struct iommu_work;
+	struct workqueue_struct *iommu_wq;
+	struct page *aux_page;
+	unsigned long aux_iova;
+	unsigned long fault_iova;
+};
+
+static int iep2_addr_rnum[] = {
+	24, 27, 28, /* src cur */
+	25, 29, 30, /* src nxt */
+	26, 31, 32, /* src prv */
+	44, 46, -1, /* dst top */
+	45, 47, -1, /* dst bot */
+	34, /* mv */
+	33, /* md */
+};
+
+static int iep2_process_reg_fd(struct mpp_session *session,
+			       struct iep_task *task,
+			       struct mpp_task_msgs *msgs)
+{
+	int i;
+	/* see the detail at above table iep2_addr_rnum */
+	int addr_num =
+		ARRAY_SIZE(task->params.src) * 3 +
+		ARRAY_SIZE(task->params.dst) * 3 + 2;
+
+	u32 *paddr = &task->params.src[0].y;
+
+	for (i = 0; i < addr_num; ++i) {
+		int usr_fd;
+		u32 offset;
+		struct mpp_mem_region *mem_region = NULL;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			usr_fd = paddr[i];
+			offset = 0;
+		} else {
+			usr_fd = paddr[i] & 0x3ff;
+			offset = paddr[i] >> 10;
+		}
+
+		if (usr_fd == 0 || iep2_addr_rnum[i] == -1)
+			continue;
+
+		mem_region = mpp_task_attach_fd(&task->mpp_task, usr_fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("reg[%03d]: %08x failed\n",
+				iep2_addr_rnum[i], paddr[i]);
+			return PTR_ERR(mem_region);
+		}
+
+		mem_region->reg_idx = iep2_addr_rnum[i];
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET)
+			offset = mpp_query_reg_offset_info(&task->off_inf, mem_region->reg_idx);
+
+		mpp_debug(DEBUG_IOMMU, "reg[%3d]: %3d => %pad + offset %u\n",
+			  iep2_addr_rnum[i], usr_fd, &mem_region->iova, offset);
+		paddr[i] = mem_region->iova + offset;
+
+		/* workaround for invalid access to src image */
+		if (task->params.dil_mode == ROCKCHIP_IEP2_DIL_MODE_I1O1T &&
+			iep2_addr_rnum[i] == 24) {
+			task->src_iova_end = mem_region->iova + mem_region->len;
+		}
+
+	}
+
+	return 0;
+}
+
+static int iep2_extract_task_msg(struct iep_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			if (copy_from_user(&task->params,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user params failed\n");
+				return -EIO;
+			}
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *iep2_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct iep_task *task = NULL;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task_init(session, &task->mpp_task);
+	/* extract reqs for current task */
+	ret = iep2_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = iep2_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return &task->mpp_task;
+
+fail:
+	mpp_task_finalize(session, &task->mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static void iep2_config(struct mpp_dev *mpp, struct iep_task *task)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+	struct iep2_params *cfg = &task->params;
+	u32 reg;
+	u32 width, height;
+
+	width = cfg->tile_cols * TILE_WIDTH;
+	height = cfg->tile_rows * TILE_HEIGHT;
+
+	reg = IEP2_REG_SRC_FMT(cfg->src_fmt)
+		| IEP2_REG_SRC_YUV_SWAP(cfg->src_yuv_swap)
+		| IEP2_REG_DST_FMT(cfg->dst_fmt)
+		| IEP2_REG_DST_YUV_SWAP(cfg->dst_yuv_swap)
+		| IEP2_REG_DEBUG_DATA_EN;
+	mpp_write_relaxed(mpp, IEP2_REG_IEP_CONFIG0, reg);
+
+	mpp_write_relaxed(mpp, IEP2_REG_WORK_MODE, IEP2_REG_IEP2_MODE);
+
+	reg = IEP2_REG_SRC_PIC_WIDTH(width - 1)
+		| IEP2_REG_SRC_PIC_HEIGHT(height - 1);
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_IMG_SIZE, reg);
+
+	reg = IEP2_REG_SRC_VIR_Y_STRIDE(cfg->src_y_stride)
+		| IEP2_REG_SRC_VIR_UV_STRIDE(cfg->src_uv_stride);
+	mpp_write_relaxed(mpp, IEP2_REG_VIR_SRC_IMG_WIDTH, reg);
+
+	reg = IEP2_REG_DST_VIR_STRIDE(cfg->dst_y_stride);
+	mpp_write_relaxed(mpp, IEP2_REG_VIR_DST_IMG_WIDTH, reg);
+
+	reg = IEP2_REG_DIL_MV_HIST_EN
+		| IEP2_REG_DIL_COMB_EN
+		| IEP2_REG_DIL_BLE_EN
+		| IEP2_REG_DIL_EEDI_EN
+		| IEP2_REG_DIL_MEMC_EN
+		| IEP2_REG_DIL_OSD_EN
+		| IEP2_REG_DIL_PD_EN
+		| IEP2_REG_DIL_FF_EN
+		| IEP2_REG_DIL_FIELD_ORDER(cfg->dil_field_order)
+		| IEP2_REG_DIL_OUT_MODE(cfg->dil_out_mode)
+		| IEP2_REG_DIL_MODE(cfg->dil_mode);
+	if (cfg->roi_en)
+		reg |= IEP2_REG_DIL_ROI_EN;
+	if (cfg->md_lambda < 8)
+		reg |= IEP2_REG_DIL_MD_PRE_EN;
+	mpp_write_relaxed(mpp, IEP2_REG_DIL_CONFIG0, reg);
+
+	if (cfg->dil_mode != ROCKCHIP_IEP2_DIL_MODE_PD) {
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURY,
+				  cfg->src[0].y);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURUV,
+				  cfg->src[0].cbcr);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURV,
+				  cfg->src[0].cr);
+
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTY,
+				  cfg->src[1].y);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTUV,
+				  cfg->src[1].cbcr);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTV,
+				  cfg->src[1].cr);
+	} else {
+		struct iep2_addr *top, *bot;
+
+		switch (cfg->pd_mode) {
+		default:
+		case ROCKCHIP_IEP2_PD_COMP_FLAG_CC:
+			top = &cfg->src[0];
+			bot = &cfg->src[0];
+			break;
+		case ROCKCHIP_IEP2_PD_COMP_FLAG_CN:
+			top = &cfg->src[0];
+			bot = &cfg->src[1];
+			break;
+		case ROCKCHIP_IEP2_PD_COMP_FLAG_NC:
+			top = &cfg->src[1];
+			bot = &cfg->src[0];
+			break;
+		}
+
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURY, top->y);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURUV, top->cbcr);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_CURV, top->cr);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTY, bot->y);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTUV, bot->cbcr);
+		mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_NXTV, bot->cr);
+	}
+
+	reg = IEP2_REG_TIMEOUT_CFG_EN | 0x3ffffff;
+	mpp_write_relaxed(mpp, IEP2_REG_TIMEOUT_CFG, reg);
+
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_PREY, cfg->src[2].y);
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_PREUV, cfg->src[2].cbcr);
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_PREV, cfg->src[2].cr);
+
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_MD, cfg->md_addr);
+	mpp_write_relaxed(mpp, IEP2_REG_SRC_ADDR_MV, cfg->mv_addr);
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_MD, cfg->md_addr);
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_MV, cfg->mv_addr);
+	mpp_write_relaxed(mpp, IEP2_REG_ROI_ADDR, (u32)iep->roi.iova);
+
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_TOPY, cfg->dst[0].y);
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_TOPC, cfg->dst[0].cbcr);
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_BOTY, cfg->dst[1].y);
+	mpp_write_relaxed(mpp, IEP2_REG_DST_ADDR_BOTC, cfg->dst[1].cbcr);
+
+	reg = IEP2_REG_MD_THETA(cfg->md_theta)
+		| IEP2_REG_MD_R(cfg->md_r)
+		| IEP2_REG_MD_LAMBDA(cfg->md_lambda);
+	mpp_write_relaxed(mpp, IEP2_REG_MD_CONFIG0, reg);
+
+	reg = IEP2_REG_DECT_RESI_THR(cfg->dect_resi_thr)
+		| IEP2_REG_OSD_AREA_NUM(cfg->osd_area_num)
+		| IEP2_REG_OSD_GRADH_THR(cfg->osd_gradh_thr)
+		| IEP2_REG_OSD_GRADV_THR(cfg->osd_gradv_thr);
+	mpp_write_relaxed(mpp, IEP2_REG_DECT_CONFIG0, reg);
+
+	reg = IEP2_REG_OSD_POS_LIMIT_NUM(cfg->osd_pos_limit_num);
+	if (cfg->osd_pos_limit_en)
+		reg |= IEP2_REG_OSD_POS_LIMIT_EN;
+	mpp_write_relaxed(mpp, IEP2_REG_OSD_LIMIT_CONFIG, reg);
+
+	mpp_write_relaxed(mpp, IEP2_REG_OSD_LIMIT_AREA(0),
+			  cfg->osd_limit_area[0]);
+	mpp_write_relaxed(mpp, IEP2_REG_OSD_LIMIT_AREA(1),
+			  cfg->osd_limit_area[1]);
+
+	reg = IEP2_REG_OSD_PEC_THR(cfg->osd_pec_thr)
+		| IEP2_REG_OSD_LINE_NUM(cfg->osd_line_num);
+	mpp_write_relaxed(mpp, IEP2_REG_OSD_CONFIG0, reg);
+
+	reg = IEP2_REG_ME_PENA(cfg->me_pena)
+		| IEP2_REG_MV_BONUS(cfg->mv_bonus)
+		| IEP2_REG_MV_SIMILAR_THR(cfg->mv_similar_thr)
+		| IEP2_REG_MV_SIMILAR_NUM_THR0(cfg->mv_similar_num_thr0)
+		| IEP2_REG_ME_THR_OFFSET(cfg->me_thr_offset);
+	mpp_write_relaxed(mpp, IEP2_REG_ME_CONFIG0, reg);
+
+	reg = IEP2_REG_MV_LEFT_LIMIT((~cfg->mv_left_limit) + 1)
+		| IEP2_REG_MV_RIGHT_LIMIT(cfg->mv_right_limit);
+	mpp_write_relaxed(mpp, IEP2_REG_ME_LIMIT_CONFIG, reg);
+
+	mpp_write_relaxed(mpp, IEP2_REG_EEDI_CONFIG0,
+			  IEP2_REG_EEDI_THR0(cfg->eedi_thr0));
+	mpp_write_relaxed(mpp, IEP2_REG_BLE_CONFIG0,
+			  IEP2_REG_BLE_BACKTOMA_NUM(cfg->ble_backtoma_num));
+}
+
+static void iep2_osd_cfg(struct mpp_dev *mpp, struct iep_task *task)
+{
+	struct iep2_params *hw_cfg = &task->params;
+	int i;
+	u32 reg;
+
+	for (i = 0; i < hw_cfg->osd_area_num; ++i) {
+		reg = IEP2_REG_OSD_X_STA(hw_cfg->osd_x_sta[i])
+			| IEP2_REG_OSD_X_END(hw_cfg->osd_x_end[i])
+			| IEP2_REG_OSD_Y_STA(hw_cfg->osd_y_sta[i])
+			| IEP2_REG_OSD_Y_END(hw_cfg->osd_y_end[i]);
+		mpp_write_relaxed(mpp, IEP2_REG_OSD_AREA_CONF(i), reg);
+	}
+
+	for (; i < ARRAY_SIZE(hw_cfg->osd_x_sta); ++i)
+		mpp_write_relaxed(mpp, IEP2_REG_OSD_AREA_CONF(i), 0);
+}
+
+static void iep2_mtn_tab_cfg(struct mpp_dev *mpp, struct iep_task *task)
+{
+	struct iep2_params *hw_cfg = &task->params;
+	int i;
+	u32 *mtn_tab = hw_cfg->mtn_en ? hw_cfg->mtn_tab : iep2_mtn_tab;
+
+	for (i = 0; i < ARRAY_SIZE(hw_cfg->mtn_tab); ++i)
+		mpp_write_relaxed(mpp, IEP2_REG_DIL_MTN_TAB(i), mtn_tab[i]);
+}
+
+static u32 iep2_tru_list_vld_tab[] = {
+	IEP2_REG_MV_TRU_LIST0_4_VLD, IEP2_REG_MV_TRU_LIST1_5_VLD,
+	IEP2_REG_MV_TRU_LIST2_6_VLD, IEP2_REG_MV_TRU_LIST3_7_VLD,
+	IEP2_REG_MV_TRU_LIST0_4_VLD, IEP2_REG_MV_TRU_LIST1_5_VLD,
+	IEP2_REG_MV_TRU_LIST2_6_VLD, IEP2_REG_MV_TRU_LIST3_7_VLD
+};
+
+static void iep2_tru_list_cfg(struct mpp_dev *mpp, struct iep_task *task)
+{
+	struct iep2_params *cfg = &task->params;
+	int i;
+	u32 reg;
+
+	for (i = 0; i < ARRAY_SIZE(cfg->mv_tru_list); i += 4) {
+		reg = 0;
+
+		if (cfg->mv_tru_vld[i])
+			reg |= IEP2_REG_MV_TRU_LIST0_4(cfg->mv_tru_list[i])
+				| iep2_tru_list_vld_tab[i];
+
+		if (cfg->mv_tru_vld[i + 1])
+			reg |= IEP2_REG_MV_TRU_LIST1_5(cfg->mv_tru_list[i + 1])
+				| iep2_tru_list_vld_tab[i + 1];
+
+		if (cfg->mv_tru_vld[i + 2])
+			reg |= IEP2_REG_MV_TRU_LIST2_6(cfg->mv_tru_list[i + 2])
+				| iep2_tru_list_vld_tab[i + 2];
+
+		if (cfg->mv_tru_vld[i + 3])
+			reg |= IEP2_REG_MV_TRU_LIST3_7(cfg->mv_tru_list[i + 3])
+				| iep2_tru_list_vld_tab[i + 3];
+
+		mpp_write_relaxed(mpp, IEP2_REG_MV_TRU_LIST(i / 4), reg);
+	}
+}
+
+static void iep2_comb_cfg(struct mpp_dev *mpp, struct iep_task *task)
+{
+	struct iep2_params *hw_cfg = &task->params;
+	int i;
+	u32 reg = 0;
+
+	for (i = 0; i < ARRAY_SIZE(hw_cfg->comb_osd_vld); ++i) {
+		if (hw_cfg->comb_osd_vld[i])
+			reg |= IEP2_REG_COMB_OSD_VLD(i);
+	}
+
+	reg |= IEP2_REG_COMB_T_THR(hw_cfg->comb_t_thr)
+		| IEP2_REG_COMB_FEATRUE_THR(hw_cfg->comb_feature_thr)
+		| IEP2_REG_COMB_CNT_THR(hw_cfg->comb_cnt_thr);
+	mpp_write_relaxed(mpp, IEP2_REG_COMB_CONFIG0, reg);
+}
+
+static int iep2_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	struct iep_task *task = NULL;
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	task = to_iep_task(mpp_task);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	iep2_config(mpp, task);
+	iep2_osd_cfg(mpp, task);
+	iep2_mtn_tab_cfg(mpp, task);
+	iep2_tru_list_cfg(mpp, task);
+	iep2_comb_cfg(mpp, task);
+
+	/* set interrupt enable bits */
+	mpp_write_relaxed(mpp, IEP2_REG_INT_EN,
+			  IEP2_REG_FRM_DONE_EN
+			  | IEP2_REG_OSD_MAX_EN
+			  | IEP2_REG_BUS_ERROR_EN
+			  | IEP2_REG_TIMEOUT_EN);
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Last, flush the registers */
+	wmb();
+	/* start iep2 */
+	mpp_write(mpp, IEP2_REG_FRM_START, 1);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int iep2_irq(struct mpp_dev *mpp)
+{
+	u32 work_mode = mpp_read(mpp, IEP2_REG_WORK_MODE);
+
+	mpp_debug_enter();
+
+	if (work_mode && IEP2_GET_IEP2_MODE(work_mode) != IEP2_REG_IEP2_MODE)
+		return IRQ_NONE;
+	mpp->irq_status = mpp_read(mpp, IEP2_REG_INT_STS);
+	mpp_write(mpp, IEP2_REG_INT_CLR, 0xffffffff);
+
+	if (!IEP2_REG_RO_VALID_INT_STS(mpp->irq_status))
+		return IRQ_NONE;
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int iep2_isr(struct mpp_dev *mpp)
+{
+	struct mpp_task *mpp_task = NULL;
+	struct iep_task *task = NULL;
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	mpp_debug_enter();
+
+	mpp_task = mpp->cur_task;
+	task = to_iep_task(mpp_task);
+	if (!task) {
+		dev_err(iep->mpp.dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	if (IEP2_REG_RO_BUS_ERROR_STS(task->irq_status) ||
+	    IEP2_REG_RO_TIMEOUT_STS(task->irq_status))
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static void iep2_osd_done(struct mpp_dev *mpp, struct iep_task *task)
+{
+	int i;
+	u32 reg;
+
+	for (i = 0; i < task->output.dect_osd_cnt; ++i) {
+		reg = mpp_read(mpp, IEP2_REG_RO_OSD_AREA_X(i));
+		task->output.x_sta[i] = IEP2_REG_RO_X_STA(reg) / 16;
+		task->output.x_end[i] = IEP2_REG_RO_X_END(reg) / 16;
+
+		reg = mpp_read(mpp, IEP2_REG_RO_OSD_AREA_Y(i));
+		task->output.y_sta[i] = IEP2_REG_RO_Y_STA(reg) / 4;
+		task->output.y_end[i] = IEP2_REG_RO_Y_END(reg) / 4;
+	}
+
+	for (; i < ARRAY_SIZE(task->output.x_sta); ++i) {
+		task->output.x_sta[i] = 0;
+		task->output.x_end[i] = 0;
+		task->output.y_sta[i] = 0;
+		task->output.y_end[i] = 0;
+	}
+}
+
+static int iep2_finish(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task)
+{
+	struct iep_task *task = to_iep_task(mpp_task);
+	struct iep2_output *output = &task->output;
+	u32 i;
+	u32 reg;
+
+	mpp_debug_enter();
+
+	output->dect_pd_tcnt = mpp_read(mpp, IEP2_REG_RO_PD_TCNT);
+	output->dect_pd_bcnt = mpp_read(mpp, IEP2_REG_RO_PD_BCNT);
+	output->dect_ff_cur_tcnt = mpp_read(mpp, IEP2_REG_RO_FF_CUR_TCNT);
+	output->dect_ff_cur_bcnt = mpp_read(mpp, IEP2_REG_RO_FF_CUR_BCNT);
+	output->dect_ff_nxt_tcnt = mpp_read(mpp, IEP2_REG_RO_FF_NXT_TCNT);
+	output->dect_ff_nxt_bcnt = mpp_read(mpp, IEP2_REG_RO_FF_NXT_BCNT);
+	output->dect_ff_ble_tcnt = mpp_read(mpp, IEP2_REG_RO_FF_BLE_TCNT);
+	output->dect_ff_ble_bcnt = mpp_read(mpp, IEP2_REG_RO_FF_BLE_BCNT);
+	output->dect_ff_nz = mpp_read(mpp, IEP2_REG_RO_FF_COMB_NZ);
+	output->dect_ff_comb_f = mpp_read(mpp, IEP2_REG_RO_FF_COMB_F);
+	output->dect_osd_cnt = mpp_read(mpp, IEP2_REG_RO_OSD_NUM);
+
+	reg = mpp_read(mpp, IEP2_REG_RO_COMB_CNT);
+	output->out_comb_cnt = IEP2_REG_RO_OUT_COMB_CNT(reg);
+	output->out_osd_comb_cnt = IEP2_REG_RO_OUT_OSD_COMB_CNT(reg);
+	output->ff_gradt_tcnt = mpp_read(mpp, IEP2_REG_RO_FF_GRADT_TCNT);
+	output->ff_gradt_bcnt = mpp_read(mpp, IEP2_REG_RO_FF_GRADT_BCNT);
+
+	iep2_osd_done(mpp, task);
+
+	for (i = 0; i < ARRAY_SIZE(output->mv_hist); i += 2) {
+		reg = mpp_read(mpp, IEP2_REG_RO_MV_HIST_BIN(i / 2));
+		output->mv_hist[i] = IEP2_REG_RO_MV_HIST_EVEN(reg);
+		output->mv_hist[i + 1] = IEP2_REG_RO_MV_HIST_ODD(reg);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int iep2_result(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task,
+		       struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct iep_task *task = to_iep_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data, (u8 *)&task->output, req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int iep2_free_task(struct mpp_session *session,
+			  struct mpp_task *mpp_task)
+{
+	struct iep_task *task = to_iep_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int iep2_procfs_remove(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	if (iep->procfs) {
+		proc_remove(iep->procfs);
+		iep->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int iep2_procfs_init(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	iep->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(iep->procfs)) {
+		mpp_err("failed on mkdir\n");
+		iep->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(iep->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      iep->procfs, &iep->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      iep->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int iep2_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int iep2_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+#define IEP2_TILE_W_MAX		120
+#define IEP2_TILE_H_MAX		272
+
+static void iep2_iommu_handle_work(struct work_struct *work_s)
+{
+	int ret = 0;
+	struct iep2_dev *iep = container_of(work_s, struct iep2_dev, iommu_work);
+	struct mpp_dev *mpp = &iep->mpp;
+	unsigned long page_iova = 0;
+
+	mpp_debug_enter();
+
+	/* avoid another page fault occur after page fault */
+	mpp_iommu_down_write(mpp->iommu_info);
+
+	if (iep->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, iep->aux_iova, AUX_PAGE_SIZE);
+		iep->aux_iova = -1;
+	}
+
+	page_iova = round_down(iep->fault_iova, AUX_PAGE_SIZE);
+	ret = iommu_map(mpp->iommu_info->domain, page_iova,
+			page_to_phys(iep->aux_page), AUX_PAGE_SIZE,
+			IOMMU_READ);
+	if (ret)
+		mpp_err("iommu_map iova %lx error.\n", page_iova);
+	else
+		iep->aux_iova = page_iova;
+
+	rockchip_iommu_unmask_irq(mpp->dev);
+	mpp_iommu_up_write(mpp->iommu_info);
+
+	mpp_debug_leave();
+}
+
+static int iep2_iommu_fault_handle(struct iommu_domain *iommu,
+				     struct device *iommu_dev,
+				     unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct iep_task *task = to_iep_task(mpp_task);
+
+	mpp_debug_enter();
+	mpp_debug(DEBUG_IOMMU, "IOMMU_GET_BUS_ID(status)=%d\n", IOMMU_GET_BUS_ID(status));
+	rockchip_iommu_mask_irq(mpp->dev);
+	if (IOMMU_GET_BUS_ID(status) &&
+	    task->params.dil_mode == ROCKCHIP_IEP2_DIL_MODE_I1O1T &&
+	    task->src_iova_end == iova) {
+		iep->fault_iova = iova;
+		queue_work(iep->iommu_wq, &iep->iommu_work);
+	} else {
+		mpp_task_dump_mem_region(mpp, mpp_task);
+		atomic_inc(&mpp->reset_request);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int iep2_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_IEP2];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &iep->aclk_info, "aclk");
+	if (ret)
+		mpp_err("failed on clk_get aclk\n");
+	ret = mpp_get_clk_info(mpp, &iep->hclk_info, "hclk");
+	if (ret)
+		mpp_err("failed on clk_get hclk\n");
+	ret = mpp_get_clk_info(mpp, &iep->sclk_info, "sclk");
+	if (ret)
+		mpp_err("failed on clk_get sclk\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&iep->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	iep->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "rst_a");
+	if (!iep->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	iep->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "rst_h");
+	if (!iep->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	iep->rst_s = mpp_reset_control_get(mpp, RST_TYPE_CORE, "rst_s");
+	if (!iep->rst_s)
+		mpp_err("No sclk reset resource define\n");
+
+	iep->roi.size = IEP2_TILE_W_MAX * IEP2_TILE_H_MAX;
+	iep->roi.vaddr = dma_alloc_coherent(mpp->dev, iep->roi.size,
+					    &iep->roi.iova,
+					    GFP_KERNEL);
+	if (!iep->roi.vaddr) {
+		dev_err(mpp->dev, "allocate roi buffer failed\n");
+		return -ENOMEM;
+	}
+
+	/* for mmu pagefault */
+	iep->aux_page = alloc_page(GFP_KERNEL | GFP_DMA32);
+	if (!iep->aux_page) {
+		dev_err(mpp->dev, "allocate a page for auxiliary usage\n");
+		return -ENOMEM;
+	}
+	iep->aux_iova = -1;
+
+	iep->iommu_wq = create_singlethread_workqueue("iommu_wq");
+	if (!iep->iommu_wq) {
+		mpp_err("failed to create workqueue\n");
+		return -ENOMEM;
+	}
+	INIT_WORK(&iep->iommu_work, iep2_iommu_handle_work);
+
+	mpp->fault_handler = iep2_iommu_fault_handle;
+
+	return 0;
+}
+
+static int iep2_exit(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	if (iep->iommu_wq) {
+		destroy_workqueue(iep->iommu_wq);
+		iep->iommu_wq = NULL;
+	}
+
+	if (iep->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, iep->aux_iova, AUX_PAGE_SIZE);
+		iep->aux_iova = -1;
+	}
+
+	if (iep->aux_page) {
+		__free_page(iep->aux_page);
+		iep->aux_page = NULL;
+	}
+
+
+	return 0;
+}
+
+static int iep2_clk_on(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	mpp_clk_safe_enable(iep->aclk_info.clk);
+	mpp_clk_safe_enable(iep->hclk_info.clk);
+	mpp_clk_safe_enable(iep->sclk_info.clk);
+
+	return 0;
+}
+
+static int iep2_clk_off(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	mpp_clk_safe_disable(iep->aclk_info.clk);
+	mpp_clk_safe_disable(iep->hclk_info.clk);
+	mpp_clk_safe_disable(iep->sclk_info.clk);
+
+	return 0;
+}
+
+static int iep2_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+	struct iep_task *task = to_iep_task(mpp_task);
+
+	mpp_clk_set_rate(&iep->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int iep2_reset(struct mpp_dev *mpp)
+{
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	int ret = 0;
+	u32 rst_status = 0;
+
+	/* soft rest first */
+	mpp_write(mpp, IEP2_REG_IEP_CONFIG0, IEP2_REG_ACLK_SRESET_P);
+	ret = readl_relaxed_poll_timeout(mpp->reg_base + IEP2_REG_STATUS,
+					 rst_status,
+					 rst_status & IEP2_REG_ARST_FINISH_DONE,
+					 0, 5);
+	if (ret) {
+		mpp_err("soft reset timeout, use cru reset\n");
+		if (iep->rst_a && iep->rst_h && iep->rst_s) {
+			/* Don't skip this or iommu won't work after reset */
+			mpp_pmu_idle_request(mpp, true);
+			mpp_safe_reset(iep->rst_a);
+			mpp_safe_reset(iep->rst_h);
+			mpp_safe_reset(iep->rst_s);
+			udelay(5);
+			mpp_safe_unreset(iep->rst_a);
+			mpp_safe_unreset(iep->rst_h);
+			mpp_safe_unreset(iep->rst_s);
+			mpp_pmu_idle_request(mpp, false);
+		}
+	}
+
+	return 0;
+}
+
+static struct mpp_hw_ops iep_v2_hw_ops = {
+	.init = iep2_init,
+	.exit = iep2_exit,
+	.clk_on = iep2_clk_on,
+	.clk_off = iep2_clk_off,
+	.set_freq = iep2_set_freq,
+	.reset = iep2_reset,
+};
+
+static struct mpp_dev_ops iep_v2_dev_ops = {
+	.alloc_task = iep2_alloc_task,
+	.run = iep2_run,
+	.irq = iep2_irq,
+	.isr = iep2_isr,
+	.finish = iep2_finish,
+	.result = iep2_result,
+	.free_task = iep2_free_task,
+};
+
+static struct mpp_hw_info iep2_hw_info = {
+	.reg_id = -1,
+};
+
+static const struct mpp_dev_var iep2_v2_data = {
+	.device_type = MPP_DEVICE_IEP2,
+	.hw_ops = &iep_v2_hw_ops,
+	.dev_ops = &iep_v2_dev_ops,
+	.hw_info = &iep2_hw_info,
+};
+
+static const struct of_device_id mpp_iep2_match[] = {
+	{
+		.compatible = "rockchip,iep-v2",
+		.data = &iep2_v2_data,
+	},
+#ifdef CONFIG_CPU_RV1126
+	{
+		.compatible = "rockchip,rv1126-iep",
+		.data = &iep2_v2_data,
+	},
+#endif
+	{},
+};
+
+static int iep2_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct iep2_dev *iep = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	dev_info(dev, "probe device\n");
+	iep = devm_kzalloc(dev, sizeof(struct iep2_dev), GFP_KERNEL);
+	if (!iep)
+		return -ENOMEM;
+
+	mpp = &iep->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_iep2_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = IEP2_SESSION_MAX_BUFFERS;
+	iep2_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int iep2_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct iep2_dev *iep = to_iep2_dev(mpp);
+
+	dma_free_coherent(dev, iep->roi.size, iep->roi.vaddr, iep->roi.iova);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	iep2_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_iep2_driver = {
+	.probe = iep2_probe,
+	.remove = iep2_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = IEP2_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_iep2_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_iep2_driver);
+
diff --git a/drivers/video/rockchip/mpp/mpp_iommu.c b/drivers/video/rockchip/mpp/mpp_iommu.c
new file mode 100644
index 0000000000000..9a3589314e0e2
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_iommu.c
@@ -0,0 +1,689 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <linux/delay.h>
+#include <linux/dma-buf-cache.h>
+#include <linux/dma-mapping.h>
+#include <linux/iommu.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/kref.h>
+#include <linux/slab.h>
+#include <linux/pm_runtime.h>
+
+#ifdef CONFIG_ARM_DMA_USE_IOMMU
+#include <asm/dma-iommu.h>
+#endif
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "mpp_debug.h"
+#include "mpp_iommu.h"
+#include "mpp_common.h"
+
+struct mpp_dma_buffer *
+mpp_dma_find_buffer_fd(struct mpp_dma_session *dma, int fd)
+{
+	struct dma_buf *dmabuf;
+	struct mpp_dma_buffer *out = NULL;
+	struct mpp_dma_buffer *buffer = NULL, *n;
+	int find = 0;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return NULL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n,
+				 &dma->used_list, link) {
+		/*
+		 * fd may dup several and point the same dambuf.
+		 * thus, here should be distinguish with the dmabuf.
+		 */
+		if (buffer->dmabuf == dmabuf) {
+			out = buffer;
+			find = 1;
+			list_move_tail(&buffer->link, &buffer->dma->used_list);
+			break;
+		}
+	}
+	if (!find) {
+		list_for_each_entry_safe(buffer, n,
+					&dma->static_list, link) {
+			/*
+			 * fd may dup several and point the same dambuf.
+			 * thus, here should be distinguish with the dmabuf.
+			 */
+			if (buffer->dmabuf == dmabuf) {
+				out = buffer;
+				list_move_tail(&buffer->link, &buffer->dma->static_list);
+				break;
+			}
+		}
+	}
+
+	mutex_unlock(&dma->list_mutex);
+	dma_buf_put(dmabuf);
+
+	return out;
+}
+
+/* Release the buffer from the current list */
+static void mpp_dma_release_buffer(struct kref *ref)
+{
+	struct mpp_dma_buffer *buffer =
+		container_of(ref, struct mpp_dma_buffer, ref);
+
+	buffer->dma->buffer_count--;
+	list_move_tail(&buffer->link, &buffer->dma->unused_list);
+
+	dma_buf_unmap_attachment(buffer->attach, buffer->sgt, buffer->dir);
+	dma_buf_detach(buffer->dmabuf, buffer->attach);
+	dma_buf_put(buffer->dmabuf);
+	buffer->dma = NULL;
+	buffer->dmabuf = NULL;
+	buffer->attach = NULL;
+	buffer->sgt = NULL;
+	buffer->copy_sgt = NULL;
+	buffer->iova = 0;
+	buffer->size = 0;
+	buffer->vaddr = NULL;
+	buffer->last_used = 0;
+}
+
+/* Remove the oldest buffer when count more than the setting */
+static int
+mpp_dma_remove_extra_buffer(struct mpp_dma_session *dma)
+{
+	struct mpp_dma_buffer *n;
+	struct mpp_dma_buffer *removable = NULL, *buffer = NULL;
+
+	if (dma->buffer_count > dma->max_buffers) {
+		mutex_lock(&dma->list_mutex);
+		list_for_each_entry_safe(buffer, n,
+					 &dma->used_list,
+					 link) {
+			if (kref_read(&buffer->ref) == 1) {
+				removable = buffer;
+				break;
+			}
+		}
+		if (removable)
+			kref_put(&removable->ref, mpp_dma_release_buffer);
+		mutex_unlock(&dma->list_mutex);
+	}
+
+	return 0;
+}
+
+int mpp_dma_release(struct mpp_dma_session *dma,
+		    struct mpp_dma_buffer *buffer)
+{
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, mpp_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+int mpp_dma_release_fd(struct mpp_dma_session *dma, int fd)
+{
+	struct device *dev = dma->dev;
+	struct mpp_dma_buffer *buffer = NULL;
+
+	buffer = mpp_dma_find_buffer_fd(dma, fd);
+	if (IS_ERR_OR_NULL(buffer)) {
+		dev_err(dev, "can not find %d buffer in list\n", fd);
+
+		return -EINVAL;
+	}
+
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, mpp_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+struct mpp_dma_buffer *
+mpp_dma_alloc(struct device *dev, size_t size)
+{
+	size_t align_size;
+	dma_addr_t iova;
+	struct  mpp_dma_buffer *buffer;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (!buffer)
+		return NULL;
+
+	align_size = PAGE_ALIGN(size);
+	buffer->vaddr = dma_alloc_coherent(dev, align_size, &iova, GFP_KERNEL);
+	if (!buffer->vaddr)
+		goto fail_dma_alloc;
+
+	buffer->size = align_size;
+	buffer->iova = iova;
+	buffer->dev = dev;
+
+	return buffer;
+fail_dma_alloc:
+	kfree(buffer);
+	return NULL;
+}
+
+int mpp_dma_free(struct mpp_dma_buffer *buffer)
+{
+	dma_free_coherent(buffer->dev, buffer->size,
+			buffer->vaddr, buffer->iova);
+	buffer->vaddr = NULL;
+	buffer->iova = 0;
+	buffer->size = 0;
+	buffer->dev = NULL;
+	kfree(buffer);
+
+	return 0;
+}
+
+struct mpp_dma_buffer *mpp_dma_import_fd(struct mpp_iommu_info *iommu_info,
+					 struct mpp_dma_session *dma,
+					 int fd, int static_use)
+{
+	int ret = 0;
+	struct sg_table *sgt;
+	struct dma_buf *dmabuf;
+	struct mpp_dma_buffer *buffer;
+	struct dma_buf_attachment *attach;
+
+	if (!dma) {
+		mpp_err("dma session is null\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	/* remove the oldest before add buffer */
+	if (!IS_ENABLED(CONFIG_DMABUF_CACHE))
+		mpp_dma_remove_extra_buffer(dma);
+
+	/* Check whether in dma session */
+	buffer = mpp_dma_find_buffer_fd(dma, fd);
+	if (!IS_ERR_OR_NULL(buffer)) {
+		if (kref_get_unless_zero(&buffer->ref))
+			return buffer;
+		dev_dbg(dma->dev, "missing the fd %d\n", fd);
+	}
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf)) {
+		ret = PTR_ERR(dmabuf);
+		mpp_err("dma_buf_get fd %d failed(%d)\n", fd, ret);
+		return ERR_PTR(ret);
+	}
+	/* A new DMA buffer */
+	mutex_lock(&dma->list_mutex);
+	buffer = list_first_entry_or_null(&dma->unused_list,
+					   struct mpp_dma_buffer,
+					   link);
+	if (!buffer) {
+		ret = -ENOMEM;
+		mutex_unlock(&dma->list_mutex);
+		goto fail;
+	}
+	list_del_init(&buffer->link);
+	mutex_unlock(&dma->list_mutex);
+
+	buffer->dmabuf = dmabuf;
+	buffer->dir = DMA_BIDIRECTIONAL;
+
+	attach = dma_buf_attach(buffer->dmabuf, dma->dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		mpp_err("dma_buf_attach fd %d failed(%d)\n", fd, ret);
+		goto fail_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, buffer->dir);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		mpp_err("dma_buf_map_attachment fd %d failed(%d)\n", fd, ret);
+		goto fail_map;
+	}
+	buffer->iova = sg_dma_address(sgt->sgl);
+	buffer->size = sg_dma_len(sgt->sgl);
+	buffer->attach = attach;
+	buffer->sgt = sgt;
+	buffer->dma = dma;
+
+	kref_init(&buffer->ref);
+
+	if (!static_use && !IS_ENABLED(CONFIG_DMABUF_CACHE))
+		/* Increase the reference for used outside the buffer pool */
+		kref_get(&buffer->ref);
+
+	mutex_lock(&dma->list_mutex);
+	dma->buffer_count++;
+	if (static_use)
+		list_add_tail(&buffer->link, &dma->static_list);
+	else
+		list_add_tail(&buffer->link, &dma->used_list);
+	mutex_unlock(&dma->list_mutex);
+
+	return buffer;
+
+fail_map:
+	dma_buf_detach(buffer->dmabuf, attach);
+fail_attach:
+	mutex_lock(&dma->list_mutex);
+	list_add_tail(&buffer->link, &dma->unused_list);
+	mutex_unlock(&dma->list_mutex);
+fail:
+	dma_buf_put(dmabuf);
+	return ERR_PTR(ret);
+}
+
+int mpp_dma_unmap_kernel(struct mpp_dma_session *dma,
+			 struct mpp_dma_buffer *buffer)
+{
+	struct iosys_map map = IOSYS_MAP_INIT_VADDR(buffer->vaddr);
+	struct dma_buf *dmabuf = buffer->dmabuf;
+
+	if (IS_ERR_OR_NULL(map.vaddr) ||
+	    IS_ERR_OR_NULL(dmabuf))
+		return -EINVAL;
+
+	dma_buf_vunmap(dmabuf, &map);
+	buffer->vaddr = NULL;
+
+	dma_buf_end_cpu_access(dmabuf, DMA_FROM_DEVICE);
+
+	return 0;
+}
+
+int mpp_dma_map_kernel(struct mpp_dma_session *dma,
+		       struct mpp_dma_buffer *buffer)
+{
+	int ret;
+	struct iosys_map map;
+	struct dma_buf *dmabuf = buffer->dmabuf;
+
+	if (IS_ERR_OR_NULL(dmabuf))
+		return -EINVAL;
+
+	ret = dma_buf_begin_cpu_access(dmabuf, DMA_FROM_DEVICE);
+	if (ret) {
+		dev_dbg(dma->dev, "can't access the dma buffer\n");
+		goto failed_access;
+	}
+
+	ret = dma_buf_vmap(dmabuf, &map);
+	if (ret) {
+		dev_dbg(dma->dev, "can't vmap the dma buffer\n");
+		goto failed_vmap;
+	}
+
+	buffer->vaddr = map.vaddr;
+
+	return 0;
+
+failed_vmap:
+	dma_buf_end_cpu_access(dmabuf, DMA_FROM_DEVICE);
+failed_access:
+
+	return ret;
+}
+
+int mpp_dma_session_destroy(struct mpp_dma_session *dma)
+{
+	struct mpp_dma_buffer *n, *buffer = NULL;
+
+	if (!dma)
+		return -EINVAL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n,
+				 &dma->used_list,
+				 link) {
+		kref_put(&buffer->ref, mpp_dma_release_buffer);
+	}
+	list_for_each_entry_safe(buffer, n,
+				 &dma->static_list,
+				 link) {
+		kref_put(&buffer->ref, mpp_dma_release_buffer);
+	}
+	mutex_unlock(&dma->list_mutex);
+
+	kfree(dma);
+
+	return 0;
+}
+
+struct mpp_dma_session *
+mpp_dma_session_create(struct device *dev, u32 max_buffers)
+{
+	int i;
+	struct mpp_dma_session *dma = NULL;
+	struct mpp_dma_buffer *buffer = NULL;
+
+	dma = kzalloc(sizeof(*dma), GFP_KERNEL);
+	if (!dma)
+		return NULL;
+
+	mutex_init(&dma->list_mutex);
+	INIT_LIST_HEAD(&dma->unused_list);
+	INIT_LIST_HEAD(&dma->used_list);
+	INIT_LIST_HEAD(&dma->static_list);
+
+	if (max_buffers > MPP_SESSION_MAX_BUFFERS) {
+		mpp_debug(DEBUG_IOCTL, "session_max_buffer %d must less than %d\n",
+			  max_buffers, MPP_SESSION_MAX_BUFFERS);
+		dma->max_buffers = MPP_SESSION_MAX_BUFFERS;
+	} else {
+		dma->max_buffers = max_buffers;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(dma->dma_bufs); i++) {
+		buffer = &dma->dma_bufs[i];
+		buffer->dma = dma;
+		INIT_LIST_HEAD(&buffer->link);
+		list_add_tail(&buffer->link, &dma->unused_list);
+	}
+	dma->dev = dev;
+
+	return dma;
+}
+
+/*
+ * begin cpu access => for_cpu = true
+ * end cpu access => for_cpu = false
+ */
+void mpp_dma_buf_sync(struct mpp_dma_buffer *buffer, u32 offset, u32 length,
+		      enum dma_data_direction dir, bool for_cpu)
+{
+	struct device *dev = buffer->dma->dev;
+	struct sg_table *sgt = buffer->sgt;
+	struct scatterlist *sg = sgt->sgl;
+	dma_addr_t sg_dma_addr = sg_dma_address(sg);
+	unsigned int len = 0;
+	int i;
+
+	for_each_sgtable_sg(sgt, sg, i) {
+		unsigned int sg_offset, sg_left, size = 0;
+
+		len += sg->length;
+		if (len <= offset) {
+			sg_dma_addr += sg->length;
+			continue;
+		}
+
+		sg_left = len - offset;
+		sg_offset = sg->length - sg_left;
+
+		size = (length < sg_left) ? length : sg_left;
+
+		if (for_cpu)
+			dma_sync_single_range_for_cpu(dev, sg_dma_addr,
+						      sg_offset, size, dir);
+		else
+			dma_sync_single_range_for_device(dev, sg_dma_addr,
+							 sg_offset, size, dir);
+
+		offset += size;
+		length -= size;
+		sg_dma_addr += sg->length;
+
+		if (length == 0)
+			break;
+	}
+}
+
+int mpp_iommu_detach(struct mpp_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_detach_group(info->domain, info->group);
+	return 0;
+}
+
+int mpp_iommu_attach(struct mpp_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain == iommu_get_domain_for_dev(info->dev))
+		return 0;
+
+	return iommu_attach_group(info->domain, info->group);
+}
+
+static int mpp_iommu_handle(struct iommu_domain *iommu,
+			    struct device *iommu_dev,
+			    unsigned long iova,
+			    int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+
+	/*
+	 * Mask iommu irq, in order for iommu not repeatedly trigger pagefault.
+	 * Until the pagefault task finish by hw timeout.
+	 */
+	if (mpp)
+		rockchip_iommu_mask_irq(mpp->dev);
+
+	dev_err(iommu_dev, "fault addr 0x%08lx status %x arg %p\n",
+		iova, status, arg);
+
+	if (!mpp) {
+		dev_err(iommu_dev, "pagefault without device to handle\n");
+		return 0;
+	}
+
+	if (mpp->cur_task)
+		mpp_task_dump_mem_region(mpp, mpp->cur_task);
+
+	if (mpp->dev_ops && mpp->dev_ops->dump_dev)
+		mpp->dev_ops->dump_dev(mpp);
+	else
+		mpp_task_dump_hw_reg(mpp);
+
+	return 0;
+}
+
+struct mpp_iommu_info *
+mpp_iommu_probe(struct device *dev)
+{
+	int ret = 0;
+	struct device_node *np = NULL;
+	struct platform_device *pdev = NULL;
+	struct mpp_iommu_info *info = NULL;
+	struct iommu_domain *domain = NULL;
+	struct iommu_group *group = NULL;
+#ifdef CONFIG_ARM_DMA_USE_IOMMU
+	struct dma_iommu_mapping *mapping;
+#endif
+	np = of_parse_phandle(dev->of_node, "iommus", 0);
+	if (!np || !of_device_is_available(np)) {
+		mpp_err("failed to get device node\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev) {
+		mpp_err("failed to get platform device\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	group = iommu_group_get(dev);
+	if (!group) {
+		ret = -EINVAL;
+		goto err_put_pdev;
+	}
+
+	/*
+	 * On arm32-arch, group->default_domain should be NULL,
+	 * domain store in mapping created by arm32-arch.
+	 * we re-attach domain here
+	 */
+#ifdef CONFIG_ARM_DMA_USE_IOMMU
+	if (!iommu_group_default_domain(group)) {
+		mapping = to_dma_iommu_mapping(dev);
+		WARN_ON(!mapping);
+		domain = mapping->domain;
+	}
+#endif
+	if (!domain) {
+		domain = iommu_get_domain_for_dev(dev);
+		if (!domain) {
+			ret = -EINVAL;
+			goto err_put_group;
+		}
+	}
+
+	info = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		ret = -ENOMEM;
+		goto err_put_group;
+	}
+
+	init_rwsem(&info->rw_sem_self);
+	info->rw_sem = &info->rw_sem_self;
+	spin_lock_init(&info->dev_lock);
+	info->dev = dev;
+	info->pdev = pdev;
+	info->group = group;
+	info->domain = domain;
+	info->dev_active = NULL;
+	info->irq = platform_get_irq(pdev, 0);
+	info->got_irq = (info->irq < 0) ? false : true;
+
+	return info;
+
+err_put_group:
+	if (group)
+		iommu_group_put(group);
+err_put_pdev:
+	if (pdev)
+		platform_device_put(pdev);
+
+	return ERR_PTR(ret);
+}
+
+int mpp_iommu_remove(struct mpp_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_group_put(info->group);
+	platform_device_put(info->pdev);
+
+	return 0;
+}
+
+int mpp_iommu_refresh(struct mpp_iommu_info *info, struct device *dev)
+{
+	int ret;
+
+	if (!info)
+		return 0;
+
+	/* disable iommu */
+	ret = rockchip_iommu_disable(dev);
+	if (ret)
+		return ret;
+	/* re-enable iommu */
+	return rockchip_iommu_enable(dev);
+}
+
+int mpp_iommu_flush_tlb(struct mpp_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain && info->domain->ops)
+		iommu_flush_iotlb_all(info->domain);
+
+	return 0;
+}
+
+int mpp_iommu_dev_activate(struct mpp_iommu_info *info, struct mpp_dev *dev)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+
+	if (info->dev_active || !dev) {
+		dev_err(info->dev, "can not activate %s -> %s\n",
+			info->dev_active ? dev_name(info->dev_active->dev) : NULL,
+			dev ? dev_name(dev->dev) : NULL);
+		ret = -EINVAL;
+	} else {
+		info->dev_active = dev;
+		/* switch domain pagefault handler and arg depending on device */
+		iommu_set_fault_handler(info->domain, dev->fault_handler ?
+					dev->fault_handler : mpp_iommu_handle, dev);
+
+		dev_dbg(info->dev, "activate -> %p %s\n", dev, dev_name(dev->dev));
+	}
+
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return ret;
+}
+
+int mpp_iommu_dev_deactivate(struct mpp_iommu_info *info, struct mpp_dev *dev)
+{
+	unsigned long flags;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+
+	if (info->dev_active != dev)
+		dev_err(info->dev, "can not deactivate %s when %s activated\n",
+			dev_name(dev->dev),
+			info->dev_active ? dev_name(info->dev_active->dev) : NULL);
+
+	dev_dbg(info->dev, "deactivate %p\n", info->dev_active);
+	info->dev_active = NULL;
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return 0;
+}
+
+int mpp_iommu_reserve_iova(struct mpp_iommu_info *info, dma_addr_t iova, size_t size)
+{
+
+	struct iommu_domain *domain;
+	struct mpp_iommu_dma_cookie *cookie;
+	struct iova_domain *iovad;
+	unsigned long pfn_lo, pfn_hi;
+
+	if (!info)
+		return 0;
+
+	domain = info->domain;
+	if (!domain || !domain->iova_cookie)
+		return -EINVAL;
+
+	cookie = (struct mpp_iommu_dma_cookie *)domain->iova_cookie;
+	iovad = &cookie->iovad;
+
+	/* iova will be freed automatically by put_iova_domain() */
+	pfn_lo = iova_pfn(iovad, iova);
+	pfn_hi = iova_pfn(iovad, iova + size - 1);
+	if (!reserve_iova(iovad, pfn_lo, pfn_hi))
+		return -EINVAL;
+
+	return 0;
+
+}
diff --git a/drivers/video/rockchip/mpp/mpp_iommu.h b/drivers/video/rockchip/mpp/mpp_iommu.h
new file mode 100644
index 0000000000000..265f276de2813
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_iommu.h
@@ -0,0 +1,190 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#ifndef __ROCKCHIP_MPP_IOMMU_H__
+#define __ROCKCHIP_MPP_IOMMU_H__
+
+#include <linux/iommu.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/iova.h>
+
+enum iommu_dma_cookie_type {
+	IOMMU_DMA_IOVA_COOKIE,
+	IOMMU_DMA_MSI_COOKIE,
+};
+
+/* Keep in mind: member order must keep align with struct iommu_dma_cookie */
+struct mpp_iommu_dma_cookie {
+	enum iommu_dma_cookie_type type;
+	/* Full allocator for IOMMU_DMA_IOVA_COOKIE */
+	struct iova_domain iovad;
+};
+
+struct mpp_dma_buffer {
+	/* link to dma session buffer list */
+	struct list_head link;
+
+	/* dma session belong */
+	struct mpp_dma_session *dma;
+	/* DMABUF information */
+	struct dma_buf *dmabuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	struct sg_table *copy_sgt;
+	enum dma_data_direction dir;
+
+	dma_addr_t iova;
+	unsigned long size;
+	void *vaddr;
+
+	struct kref ref;
+	ktime_t last_used;
+	/* alloc by device */
+	struct device *dev;
+};
+
+#define MPP_SESSION_MAX_BUFFERS		60
+
+struct mpp_dma_session {
+	/* the buffer used in session */
+	struct list_head unused_list;
+	struct list_head used_list;
+	/*
+	 * For those buffer import by ioctl MPP_CMD_TRANS_FD_TO_IOVA,
+	 * move to static_list instead of used_list and don't increase extra kref,
+	 * so that it will release when user space call ioctl MPP_CMD_RELEASE_FD.
+	 */
+	struct list_head static_list;
+	struct mpp_dma_buffer dma_bufs[MPP_SESSION_MAX_BUFFERS];
+	/* the mutex for the above buffer list */
+	struct mutex list_mutex;
+	/* the max buffer num for the buffer list */
+	u32 max_buffers;
+	/* the count for the buffer list */
+	int buffer_count;
+
+	struct device *dev;
+};
+
+struct mpp_rk_iommu {
+	struct list_head link;
+	u32 grf_val;
+	int mmu_num;
+	u32 base_addr[2];
+	void __iomem *bases[2];
+	u32 dte_addr;
+	u32 is_paged;
+};
+
+struct mpp_dev;
+
+struct mpp_iommu_info {
+	struct rw_semaphore *rw_sem;
+	struct rw_semaphore rw_sem_self;
+
+	struct device *dev;
+	struct platform_device *pdev;
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+	struct mpp_rk_iommu *iommu;
+	iommu_fault_handler_t hdl;
+
+	spinlock_t dev_lock;
+	struct mpp_dev *dev_active;
+
+	int irq;
+	int got_irq;
+};
+
+struct mpp_dma_session *
+mpp_dma_session_create(struct device *dev, u32 max_buffers);
+int mpp_dma_session_destroy(struct mpp_dma_session *dma);
+
+struct mpp_dma_buffer *
+mpp_dma_alloc(struct device *dev, size_t size);
+int mpp_dma_free(struct mpp_dma_buffer *buffer);
+
+struct mpp_dma_buffer *
+mpp_dma_import_fd(struct mpp_iommu_info *iommu_info,
+		  struct mpp_dma_session *dma, int fd, int static_use);
+int mpp_dma_release(struct mpp_dma_session *dma,
+		    struct mpp_dma_buffer *buffer);
+int mpp_dma_release_fd(struct mpp_dma_session *dma, int fd);
+
+int mpp_dma_unmap_kernel(struct mpp_dma_session *dma,
+			 struct mpp_dma_buffer *buffer);
+int mpp_dma_map_kernel(struct mpp_dma_session *dma,
+		       struct mpp_dma_buffer *buffer);
+struct mpp_dma_buffer *mpp_dma_find_buffer_fd(struct mpp_dma_session *dma, int fd);
+void mpp_dma_buf_sync(struct mpp_dma_buffer *buffer, u32 offset, u32 length,
+		      enum dma_data_direction dir, bool for_cpu);
+
+struct mpp_iommu_info *
+mpp_iommu_probe(struct device *dev);
+int mpp_iommu_remove(struct mpp_iommu_info *info);
+
+int mpp_iommu_attach(struct mpp_iommu_info *info);
+int mpp_iommu_detach(struct mpp_iommu_info *info);
+
+int mpp_iommu_refresh(struct mpp_iommu_info *info, struct device *dev);
+int mpp_iommu_flush_tlb(struct mpp_iommu_info *info);
+int mpp_av1_iommu_disable(struct device *dev);
+int mpp_av1_iommu_enable(struct device *dev);
+
+int mpp_iommu_dev_activate(struct mpp_iommu_info *info, struct mpp_dev *dev);
+int mpp_iommu_dev_deactivate(struct mpp_iommu_info *info, struct mpp_dev *dev);
+int mpp_iommu_reserve_iova(struct mpp_iommu_info *info, dma_addr_t iova, size_t size);
+
+static inline int mpp_iommu_down_read(struct mpp_iommu_info *info)
+{
+	if (info)
+		down_read(info->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_iommu_up_read(struct mpp_iommu_info *info)
+{
+	if (info)
+		up_read(info->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_iommu_down_write(struct mpp_iommu_info *info)
+{
+	if (info)
+		down_write(info->rw_sem);
+
+	return 0;
+}
+
+static inline int mpp_iommu_up_write(struct mpp_iommu_info *info)
+{
+	if (info)
+		up_write(info->rw_sem);
+
+	return 0;
+}
+
+static inline void mpp_iommu_enable_irq(struct mpp_iommu_info *info)
+{
+	if (info && info->got_irq)
+		enable_irq(info->irq);
+}
+
+static inline void mpp_iommu_disable_irq(struct mpp_iommu_info *info)
+{
+	if (info && info->got_irq)
+		disable_irq(info->irq);
+}
+
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_jpgdec.c b/drivers/video/rockchip/mpp/mpp_jpgdec.c
new file mode 100644
index 0000000000000..732126e83c75b
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_jpgdec.c
@@ -0,0 +1,660 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2020 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define JPGDEC_DRIVER_NAME		"mpp_jpgdec"
+
+#define JPGDEC_HWID_VPU720		0xdb1f0006
+
+#define	JPGDEC_SESSION_MAX_BUFFERS	40
+/* The maximum registers number of all the version */
+#define JPGDEC_REG_NUM			42
+#define JPGDEC_REG_HW_ID_INDEX		0
+#define JPGDEC_REG_START_INDEX		0
+#define JPGDEC_REG_END_INDEX		41
+
+#define JPGDEC_GET_PROD_NUM(x)		(((x) >> 16) & 0xffff)
+#define JPGDEC_GET_SUPPORT_BIT(x)	(((x) >> 8) & 0x1)
+
+#define JPGDEC_REG_INT_EN_BASE		0x004
+#define JPGDEC_REG_INT_EN_INDEX		(1)
+
+#define JPGDEC_CARE_STREAM_ERROR_EN	BIT(16)
+#define JPGDEC_EMPTY_FORCE_END		BIT(15)
+#define JPGDEC_SOFT_RSET_READY		BIT(14)
+#define JPGDEC_BUF_EMPTY_STA		BIT(13)
+#define JPGDEC_TIMEOUT_STA		BIT(12)
+#define JPGDEC_ERROR_STA		BIT(11)
+#define JPGDEC_BUS_STA			BIT(10)
+#define JPGDEC_REDAY_STA		BIT(9)
+#define JPGDEC_IRQ			BIT(8)
+#define JPGDEC_WAIT_RESET_EN		BIT(7)
+#define JPGDEC_IRQ_RAW			BIT(6)
+#define JPGDEC_SOFT_REST_EN		BIT(5)
+#define JPGDEC_BUF_EMPTY_RELOAD_EN	BIT(4)
+#define JPGDEC_BUF_EMPTY_EN		BIT(3)
+#define JPGDEC_TIMEOUT_EN		BIT(2)
+#define JPGDEC_IRQ_DIS			BIT(1)
+#define JPGDEC_START_EN			BIT(0)
+
+#define JPGDEC_REG_SYS_BASE		0x008
+#define JPGDEC_FORCE_SOFTRESET_VALID	BIT(17)
+
+#define JPGDEC_REG_PIC_INFO_BASE	0x00c
+#define JPGDEC_REG_PIC_INFO_INDEX	(3)
+#define JPGDEC_GET_WIDTH(x)		(((x) & 0xffff) + 1)
+#define JPGDEC_GET_HEIGHT(x)		((((x) >> 16) & 0xffff) + 1)
+
+#define JPGDEC_REG_STREAM_RLC_BASE		0x030
+#define JPGDEC_REG_STREAM_RLC_BASE_INDEX	(12)
+
+#define JPGDEC_REG_PERF_WORKING_CNT	0x9c
+
+#define to_jpgdec_task(task)	\
+		container_of(task, struct jpgdec_task, mpp_task)
+#define to_jpgdec_dev(dev)	\
+		container_of(dev, struct jpgdec_dev, mpp)
+
+struct jpgdec_task {
+	struct mpp_task mpp_task;
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[JPGDEC_REG_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 strm_addr;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct jpgdec_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+};
+
+static struct mpp_hw_info jpgdec_v1_hw_info = {
+	.reg_num = JPGDEC_REG_NUM,
+	.reg_id = JPGDEC_REG_HW_ID_INDEX,
+	.reg_start = JPGDEC_REG_START_INDEX,
+	.reg_end = JPGDEC_REG_END_INDEX,
+	.reg_en = JPGDEC_REG_INT_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_jpgdec[] = {
+	9, 10, 11, 12, 13,
+};
+
+#define JPEGDEC_FMT_DEFAULT		0
+static struct mpp_trans_info jpgdec_v1_trans[] = {
+	[JPEGDEC_FMT_DEFAULT] = {
+		.count = ARRAY_SIZE(trans_tbl_jpgdec),
+		.table = trans_tbl_jpgdec,
+	},
+};
+
+static int jpgdec_process_reg_fd(struct mpp_session *session,
+				 struct jpgdec_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					JPEGDEC_FMT_DEFAULT, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+	return 0;
+}
+
+static int jpgdec_extract_task_msg(struct jpgdec_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *jpgdec_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct jpgdec_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = jpgdec_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = jpgdec_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->strm_addr = task->reg[JPGDEC_REG_STREAM_RLC_BASE_INDEX];
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int jpgdec_soft_reset(struct mpp_dev *mpp)
+{
+	mpp_write(mpp, JPGDEC_REG_SYS_BASE, JPGDEC_FORCE_SOFTRESET_VALID);
+	mpp_write(mpp, JPGDEC_REG_INT_EN_BASE, JPGDEC_SOFT_REST_EN);
+
+	return 0;
+}
+
+static int jpgdec_run(struct mpp_dev *mpp,
+		      struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct jpgdec_task *task = to_jpgdec_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* set registers for hardware */
+	reg_en = mpp_task->hw_info->reg_en;
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		int s = req->offset / sizeof(u32);
+		int e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, JPGDEC_REG_INT_EN_BASE,
+		  task->reg[reg_en] | JPGDEC_START_EN);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int jpgdec_finish(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	u32 dec_get;
+	s32 dec_length;
+	struct mpp_request *req;
+	struct jpgdec_task *task = to_jpgdec_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+	/* revert hack for irq status */
+	task->reg[JPGDEC_REG_INT_EN_INDEX] = task->irq_status;
+	/* revert hack for decoded length */
+	dec_get = mpp_read_relaxed(mpp, JPGDEC_REG_STREAM_RLC_BASE);
+	dec_length = dec_get - task->strm_addr;
+	task->reg[JPGDEC_REG_STREAM_RLC_BASE_INDEX] = dec_length << 10;
+	/*
+	 * If the softrest_rdy bit is low,
+	 * it means that the soft-reset of the previous frame
+	 * has not been completed.We have to manually trigger to do soft-reset.
+	 */
+	if (!(task->irq_status & JPGDEC_SOFT_RSET_READY) &&
+	    (mpp->var->hw_info->hw_id < JPGDEC_HWID_VPU720) &&
+	    !atomic_read(&mpp->reset_request))
+		jpgdec_soft_reset(mpp);
+
+	mpp_debug(DEBUG_REGISTER,
+		  "dec_get %08x dec_length %d\n", dec_get, dec_length);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int jpgdec_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct jpgdec_task *task = to_jpgdec_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int jpgdec_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct jpgdec_task *task = to_jpgdec_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int jpgdec_procfs_remove(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int jpgdec_procfs_init(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	dec->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int jpgdec_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int jpgdec_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int jpgdec_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int jpgdec_clk_on(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int jpgdec_clk_off(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	mpp_clk_safe_disable(dec->aclk_info.clk);
+	mpp_clk_safe_disable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int jpgdec_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+	struct jpgdec_task *task = to_jpgdec_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int jpgdec_reduce_freq(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int jpgdec_irq(struct mpp_dev *mpp)
+{
+	u32 clr_mask = 0;
+
+	mpp->irq_status = mpp_read(mpp, JPGDEC_REG_INT_EN_BASE);
+
+	if (mpp->var->hw_info->hw_id == JPGDEC_HWID_VPU720) {
+		clr_mask = (~(0x00fe7f40 & mpp->irq_status)) & (0xff0180bf & mpp->irq_status);
+		mpp_debug(DEBUG_IRQ_STATUS, "irq status 0x%08x, mask 0x%08x\n",
+			  mpp->irq_status, clr_mask);
+		mpp_write(mpp, JPGDEC_REG_INT_EN_BASE, clr_mask);
+	}
+
+	if (!(mpp->irq_status & JPGDEC_IRQ_RAW))
+		return IRQ_NONE;
+	mpp_write(mpp, JPGDEC_REG_INT_EN_BASE, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int jpgdec_isr(struct mpp_dev *mpp)
+{
+	int error_mask;
+	struct jpgdec_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_task->hw_cycles = mpp_read(mpp, JPGDEC_REG_PERF_WORKING_CNT);
+	mpp_time_diff_with_hw_time(mpp_task, dec->aclk_info.real_rate_hz);
+	mpp->cur_task = NULL;
+	task = to_jpgdec_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	error_mask = JPGDEC_BUS_STA | JPGDEC_ERROR_STA |
+		     JPGDEC_TIMEOUT_STA | JPGDEC_BUF_EMPTY_STA;
+
+	if (error_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int jpgdec_reset(struct mpp_dev *mpp)
+{
+	struct jpgdec_dev *dec = to_jpgdec_dev(mpp);
+
+	if (dec->rst_a && dec->rst_h) {
+		mpp_debug(DEBUG_RESET, "reset in\n");
+
+		/* Don't skip this or iommu won't work after reset */
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+
+		mpp_debug(DEBUG_RESET, "reset out\n");
+	}
+	mpp_write(mpp, JPGDEC_REG_INT_EN_BASE, 0);
+
+	return 0;
+}
+
+static struct mpp_hw_ops jpgdec_v1_hw_ops = {
+	.init = jpgdec_init,
+	.clk_on = jpgdec_clk_on,
+	.clk_off = jpgdec_clk_off,
+	.set_freq = jpgdec_set_freq,
+	.reduce_freq = jpgdec_reduce_freq,
+	.reset = jpgdec_reset,
+};
+
+static struct mpp_dev_ops jpgdec_v1_dev_ops = {
+	.alloc_task = jpgdec_alloc_task,
+	.run = jpgdec_run,
+	.irq = jpgdec_irq,
+	.isr = jpgdec_isr,
+	.finish = jpgdec_finish,
+	.result = jpgdec_result,
+	.free_task = jpgdec_free_task,
+};
+
+static const struct mpp_dev_var jpgdec_v1_data = {
+	.device_type = MPP_DEVICE_RKJPEGD,
+	.hw_info = &jpgdec_v1_hw_info,
+	.trans_info = jpgdec_v1_trans,
+	.hw_ops = &jpgdec_v1_hw_ops,
+	.dev_ops = &jpgdec_v1_dev_ops,
+};
+
+static const struct of_device_id mpp_jpgdec_dt_match[] = {
+	{
+		.compatible = "rockchip,rkv-jpeg-decoder-v1",
+		.data = &jpgdec_v1_data,
+	},
+	{},
+};
+
+static int jpgdec_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct jpgdec_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	dev_info(dev, "probe device\n");
+	dec = devm_kzalloc(dev, sizeof(struct jpgdec_dev), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_jpgdec_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = JPGDEC_SESSION_MAX_BUFFERS;
+
+	jpgdec_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int jpgdec_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	jpgdec_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_jpgdec_driver = {
+	.probe = jpgdec_probe,
+	.remove = jpgdec_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = JPGDEC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_jpgdec_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_jpgdec_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_jpgenc.c b/drivers/video/rockchip/mpp/mpp_jpgenc.c
new file mode 100644
index 0000000000000..922f962a4a433
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_jpgenc.c
@@ -0,0 +1,651 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2024 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Johnson.Ding, johnson.ding@rock-chips.com
+ *
+ */
+
+#include <linux/clk.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <linux/dev_printk.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define JPGENC_DRIVER_NAME		"mpp_jpgenc"
+
+#define JPGENC_SESSION_MAX_BUFFERS	40
+#define JPGENC_REG_NUM			79
+
+#define JPGENC_REG_HW_ID_INDEX		0
+#define JPGENC_REG_START_INDEX		0
+#define JPGENC_REG_END_INDEX		78
+#define JPGENC_REG_START_EN_BASE	0x4
+#define JPGENC_REG_START_EN_INDEX	1
+#define JPGENC_REG_INT_EN_BASE		0x10
+#define JPGENC_REG_INT_EN_INDEX		4
+#define JPGENC_REG_INT_MASK_BASE	0x14
+#define JPGENC_REG_INT_MASK_INDEX	5
+#define JPGENC_REG_INT_CLR_BASE		0x18
+#define JPGENC_REG_INT_CLR_INDEX	6
+#define JPGENC_REG_INT_STATUS_BASE	0x1c
+#define JPGENC_REG_INT_STATUS_INDEX	7
+#define JPGENC_REG_PERF_WORKING_CNT	0xe4
+
+#define JPGENC_REG_ENC_RSL_INDEX	(29)
+#define JPGENC_GET_WIDTH(x)		((((x) & 0x1fff) + 1) << 3)
+#define JPGENC_GET_HEIGHT(x)		(((((x) >> 16) & 0x1fff) + 1) << 3)
+
+#define JPGENC_START_EN			BIT(8)
+#define JPGENC_INT_ST_ENC_DONE		BIT(0)
+
+#define to_jpgenc_task(task)	\
+		container_of(task, struct jpgenc_task, mpp_task)
+#define to_jpgenc_dev(dev)	\
+		container_of(dev, struct jpgenc_dev, mpp)
+
+enum VEPU_CMD {
+	VEPU_CMD_NONE = 0,
+	VEPU_CMD_ONE_FRAME,
+	VEPU_CMD_MULTI_FRAME_START,
+	VEPU_CMD_MULTI_FRAME_UPDATE,
+	VEPU_CMD_LINK_TABLE_FORCE_PAUSE,
+	VEPU_CMD_LINK_TABLE_RESUME,
+	VEPU_CMD_SAFE_CLR,
+	VEPU_CMD_FORCE_CLR,
+};
+
+enum JPGENC_MODE {
+	JPGENC_MODE_ONEFRAME		= 0,
+	JPGENC_MODE_LINK_ADD		= 1,
+	JPGENC_MODE_LINK_ONEFRAME	= 2,
+	JPGENC_MODE_BUTT,
+};
+
+struct jpgenc_reg_msg {
+	u32 base_s;
+	u32 base_e;
+	/* class base for link */
+	u32 link_s;
+	/* class end for link */
+	u32 link_e;
+	/* class bytes for link */
+	u32 link_len;
+};
+
+struct jpgenc_task {
+	struct mpp_task mpp_task;
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[JPGENC_REG_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	/* image info */
+	u32 width;
+	u32 height;
+	u32 pixels;
+	struct mpp_dma_buffer *bs_buf;
+	u32 offset_bs;
+};
+
+struct jpgenc_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info core_clk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+
+	// /* for link mode */
+	enum JPGENC_MODE link_mode;
+	// atomic_t link_task_cnt;
+	// u32 link_run;
+	// u32 jpeg_wr_addr;
+};
+
+static struct mpp_hw_info jpgenc_v1_hw_info = {
+	.reg_num = JPGENC_REG_NUM,
+	.reg_id = JPGENC_REG_HW_ID_INDEX,
+	.reg_start = JPGENC_REG_START_INDEX,
+	.reg_end = JPGENC_REG_END_INDEX,
+	.reg_en = JPGENC_REG_START_EN_INDEX,
+};
+
+static const u16 trans_tbl_jpgenc[] = {
+	9,	// link table base
+	16,	// address of JPEG Q table
+	17,	// top address of JPEG bitstream
+	18,	// bottom address of JPEG bitstream
+	19,	// read address of JPEG bitstream
+	20,	// start address of JPEG bitstream
+	21,	// base address of ECS length buffer
+	22,	// base address of the 1st storage area for video source buffer
+	23,	// base address of the 2nd storage area for video source buffer
+	24	// base address of the 3rd storage area for video source buffer
+};
+
+#define JPGENC_FMT_DEFAULT 0
+static struct mpp_trans_info jpgenc_v1_trans[] = {
+	[JPGENC_FMT_DEFAULT] = {
+		.count = ARRAY_SIZE(trans_tbl_jpgenc),
+		.table = trans_tbl_jpgenc,
+	},
+};
+
+static int jpgenc_process_reg_fd(struct mpp_session *session, struct jpgenc_task *task)
+{
+	int ret = 0;
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					JPGENC_FMT_DEFAULT, task->reg, &task->off_inf);
+
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task, &task->off_inf, task->reg);
+	return 0;
+}
+
+static int jpgenc_extract_task_msg(struct jpgenc_task *task, struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg), off_s, off_e);
+
+			if (ret)
+				continue;
+
+			if (copy_from_user((u8 *)task->reg + req->offset, req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+
+			memcpy(&task->w_reqs[task->w_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg), off_s, off_e);
+
+			if (ret)
+				continue;
+
+			memcpy(&task->r_reqs[task->r_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *jpgenc_alloc_task(struct mpp_session *session, struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct jpgenc_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* process fd in register */
+	ret = jpgenc_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = jpgenc_process_reg_fd(session, task);
+		if (ret)
+			goto fail;
+
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+	task->width = JPGENC_GET_WIDTH(task->reg[JPGENC_REG_ENC_RSL_INDEX]);
+	task->height = JPGENC_GET_HEIGHT(task->reg[JPGENC_REG_ENC_RSL_INDEX]);
+	task->pixels = task->width * task->height;
+	mpp_debug(DEBUG_TASK_INFO, "width=%d, height=%d\n", task->width, task->height);
+
+	mpp_debug_leave();
+	return mpp_task;
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int jpgenc_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct jpgenc_task *task = to_jpgenc_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* set registers for hardware */
+	reg_en = mpp_task->hw_info->reg_en;
+	task->reg[JPGENC_REG_INT_EN_INDEX] = 0xff;
+	task->reg[JPGENC_REG_INT_MASK_INDEX] = 0xff;
+
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		u32 s = req->offset / sizeof(u32);
+		u32 e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the register before starting device */
+	wmb();
+	mpp_write(mpp, JPGENC_REG_START_EN_BASE, task->reg[reg_en] | JPGENC_START_EN);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int jpgenc_finish(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	struct mpp_request *req;
+	struct jpgenc_task *task = to_jpgenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+
+	task->reg[JPGENC_REG_INT_STATUS_INDEX] = task->irq_status;
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int jpgenc_result(struct mpp_dev *mpp, struct mpp_task *mpp_task, struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct jpgenc_task *task = to_jpgenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		if (copy_to_user(req->data,
+				(u8 *)task->reg + req->offset,
+				req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			mpp_debug_leave();
+			return -EIO;
+		}
+	}
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int jpgenc_free_task(struct mpp_session *session, struct mpp_task *mpp_task)
+{
+	struct jpgenc_task *task = to_jpgenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	mpp_debug_leave();
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int jpgenc_procfs_remove(struct mpp_dev *mpp)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	if (enc->procfs) {
+		proc_remove(enc->procfs);
+		enc->procfs = NULL;
+	}
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int jpgenc_show_session_info(struct seq_file *seq, void *offset)
+{
+	return 0;
+}
+
+static int jpgenc_procfs_init(struct mpp_dev *mpp)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	enc->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+
+	if (IS_ERR_OR_NULL(enc->procfs)) {
+		mpp_err("failed on open procfs\n");
+		enc->procfs = NULL;
+		return -EIO;
+	}
+
+	mpp_procfs_create_common(enc->procfs, mpp);
+	mpp_procfs_create_u32("aclk", 0644, enc->procfs, &enc->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("hclk", 0644, enc->procfs, &enc->hclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644, enc->procfs, &mpp->session_max_buffers);
+	mpp_procfs_create_u32("link_mode", 0644, enc->procfs, &enc->link_mode);
+	proc_create_single_data("sessions-info", 0444, enc->procfs, jpgenc_show_session_info, mpp);
+
+	mpp_debug_leave();
+	return 0;
+}
+#else
+static inline int jpgenc_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int jpgenc_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int jpgenc_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	ret = mpp_get_clk_info(mpp, &enc->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+
+	ret = mpp_get_clk_info(mpp, &enc->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+
+	mpp_set_clk_info_rate_hz(&enc->aclk_info, CLK_MODE_DEFAULT, 700 * MHZ);
+
+	enc->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!enc->rst_a)
+		mpp_err("No aclk reset resource define\n");
+
+	enc->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!enc->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int jpgenc_clk_on(struct mpp_dev *mpp)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_clk_safe_enable(enc->aclk_info.clk);
+	mpp_clk_safe_enable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int jpgenc_clk_off(struct mpp_dev *mpp)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_clk_safe_disable(enc->aclk_info.clk);
+	mpp_clk_safe_disable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int jpgenc_set_freq(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+	struct jpgenc_task *task = to_jpgenc_task(mpp_task);
+
+	mpp_clk_set_rate(&enc->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int jpgenc_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, JPGENC_REG_INT_STATUS_BASE);
+	mpp_write(mpp, JPGENC_REG_INT_CLR_BASE, mpp->irq_status);
+	if (!(mpp->irq_status & JPGENC_INT_ST_ENC_DONE))
+		return IRQ_NONE;
+
+	mpp_write(mpp, JPGENC_REG_START_EN_BASE, 0);
+	return IRQ_WAKE_THREAD;
+}
+
+static int jpgenc_isr(struct mpp_dev *mpp)
+{
+	int err_mask = 0x340;
+	struct jpgenc_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+
+	mpp_task->hw_cycles = mpp_read(mpp, JPGENC_REG_PERF_WORKING_CNT);
+	mpp_time_diff_with_hw_time(mpp_task, enc->aclk_info.real_rate_hz);
+	mpp->cur_task = NULL;
+	task = to_jpgenc_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int jpgenc_reset(struct mpp_dev *mpp)
+{
+	struct jpgenc_dev *enc = to_jpgenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	if (enc->rst_a && enc->rst_h) {
+		mpp_debug(DEBUG_RESET, "reset n\n");
+
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(enc->rst_a);
+		mpp_safe_reset(enc->rst_h);
+		udelay(5);
+		mpp_safe_unreset(enc->rst_a);
+		mpp_safe_unreset(enc->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+
+		mpp_debug(DEBUG_RESET, "reset out\n");
+	}
+	mpp_write(mpp, JPGENC_REG_INT_EN_BASE, 0);
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static struct mpp_hw_ops jpgenc_v1_hw_ops = {
+	.init = jpgenc_init,
+	.clk_on = jpgenc_clk_on,
+	.clk_off = jpgenc_clk_off,
+	.set_freq = jpgenc_set_freq,
+	.reset = jpgenc_reset,
+};
+
+static struct mpp_dev_ops jpgenc_v1_dev_ops = {
+	.alloc_task = jpgenc_alloc_task,
+	.prepare = NULL,
+	.run = jpgenc_run,
+	.irq = jpgenc_irq,
+	.isr = jpgenc_isr,
+	.finish = jpgenc_finish,
+	.result = jpgenc_result,
+	.free_task = jpgenc_free_task,
+	.ioctl = NULL,
+	.init_session = NULL,
+	.free_session = NULL,
+	.dump_session = NULL,
+};
+
+static const struct mpp_dev_var jpgenc_v1_data = {
+	.device_type = MPP_DEVICE_RKJPEGE,
+	.hw_info = &jpgenc_v1_hw_info,
+	.trans_info = jpgenc_v1_trans,
+	.hw_ops = &jpgenc_v1_hw_ops,
+	.dev_ops = &jpgenc_v1_dev_ops,
+};
+
+static const struct of_device_id mpp_jpgenc_dt_match[] = {
+	{
+		.compatible = "rockchip,rkv-jpeg-encoder-v1",
+		.data = &jpgenc_v1_data,
+	},
+	{},
+};
+
+static int jpgenc_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct jpgenc_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	dev_info(dev, "probe device\n");
+	enc = devm_kzalloc(dev, sizeof(struct jpgenc_dev), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_jpgenc_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *) match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq, mpp_dev_irq, NULL,
+					IRQF_SHARED, dev_name(dev), mpp);
+
+	if (ret) {
+		dev_err(dev, "register interrupt runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = JPGENC_SESSION_MAX_BUFFERS;
+	jpgenc_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probe finish");
+	return 0;
+}
+
+static int jpgenc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	jpgenc_procfs_remove(mpp);
+	return 0;
+}
+
+struct platform_driver rockchip_jpgenc_driver = {
+	.probe = jpgenc_probe,
+	.remove = jpgenc_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = JPGENC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_jpgenc_dt_match),
+	},
+};
+EXPORT_SYMBOL(rockchip_jpgenc_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_rkvdec.c b/drivers/video/rockchip/mpp/mpp_rkvdec.c
new file mode 100644
index 0000000000000..89214c2c7e5b3
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvdec.c
@@ -0,0 +1,1928 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/devfreq.h>
+#include <linux/gfp.h>
+#include <linux/interrupt.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/kernel.h>
+#include <linux/thermal.h>
+#include <linux/notifier.h>
+#include <linux/proc_fs.h>
+#include <linux/rockchip/rockchip_sip.h>
+#include <linux/regulator/consumer.h>
+
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_sip.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "hack/mpp_hack_px30.h"
+
+#define RKVDEC_DRIVER_NAME		"mpp_rkvdec"
+
+#define IOMMU_GET_BUS_ID(x)		(((x) >> 6) & 0x1f)
+#define IOMMU_PAGE_SIZE			SZ_4K
+
+#define	RKVDEC_SESSION_MAX_BUFFERS	40
+/* The maximum registers number of all the version */
+#define HEVC_DEC_REG_NUM		68
+#define HEVC_DEC_REG_HW_ID_INDEX	0
+#define HEVC_DEC_REG_START_INDEX	0
+#define HEVC_DEC_REG_END_INDEX		67
+
+#define RKVDEC_V1_REG_NUM		78
+#define RKVDEC_V1_REG_HW_ID_INDEX	0
+#define RKVDEC_V1_REG_START_INDEX	0
+#define RKVDEC_V1_REG_END_INDEX		77
+
+#define RKVDEC_V2_REG_NUM		109
+#define RKVDEC_V2_REG_HW_ID_INDEX	0
+#define RKVDEC_V2_REG_START_INDEX	0
+#define RKVDEC_V2_REG_END_INDEX		108
+
+#define RKVDEC_REG_INT_EN		0x004
+#define RKVDEC_REG_INT_EN_INDEX		(1)
+#define RKVDEC_WR_DDR_ALIGN_EN		BIT(23)
+#define RKVDEC_FORCE_SOFT_RESET_VALID	BIT(21)
+#define RKVDEC_SOFTWARE_RESET_EN	BIT(20)
+#define RKVDEC_INT_COLMV_REF_ERROR	BIT(17)
+#define RKVDEC_INT_BUF_EMPTY		BIT(16)
+#define RKVDEC_INT_TIMEOUT		BIT(15)
+#define RKVDEC_INT_STRM_ERROR		BIT(14)
+#define RKVDEC_INT_BUS_ERROR		BIT(13)
+#define RKVDEC_DEC_INT_RAW		BIT(9)
+#define RKVDEC_DEC_INT			BIT(8)
+#define RKVDEC_DEC_TIMEOUT_EN		BIT(5)
+#define RKVDEC_DEC_IRQ_DIS		BIT(4)
+#define RKVDEC_CLOCK_GATE_EN		BIT(1)
+#define RKVDEC_DEC_START		BIT(0)
+
+#define RKVDEC_REG_SYS_CTRL		0x008
+#define RKVDEC_REG_SYS_CTRL_INDEX	(2)
+#define RKVDEC_RGE_WIDTH_INDEX		(3)
+#define RKVDEC_GET_FORMAT(x)		(((x) >> 20) & 0x3)
+#define REVDEC_GET_PROD_NUM(x)		(((x) >> 16) & 0xffff)
+#define RKVDEC_GET_WIDTH(x)		(((x) & 0x3ff) << 4)
+#define RKVDEC_FMT_H265D		(0)
+#define RKVDEC_FMT_H264D		(1)
+#define RKVDEC_FMT_VP9D			(2)
+
+#define RKVDEC_REG_RLC_BASE		0x010
+#define RKVDEC_REG_RLC_BASE_INDEX	(4)
+
+#define RKVDEC_RGE_YSTRDE_INDEX		(8)
+#define RKVDEC_GET_YSTRDE(x)		(((x) & 0x1fffff) << 4)
+
+#define RKVDEC_REG_PPS_BASE		0x0a0
+#define RKVDEC_REG_PPS_BASE_INDEX	(42)
+
+#define RKVDEC_REG_VP9_REFCOLMV_BASE		0x0d0
+#define RKVDEC_REG_VP9_REFCOLMV_BASE_INDEX	(52)
+
+#define RKVDEC_REG_CACHE0_SIZE_BASE	0x41c
+#define RKVDEC_REG_CACHE1_SIZE_BASE	0x45c
+#define RKVDEC_REG_CLR_CACHE0_BASE	0x410
+#define RKVDEC_REG_CLR_CACHE1_BASE	0x450
+
+#define RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS	BIT(0)
+#define RKVDEC_CACHE_PERMIT_READ_ALLOCATE	BIT(1)
+#define RKVDEC_CACHE_LINE_SIZE_64_BYTES		BIT(4)
+
+#define RKVDEC_POWER_CTL_INDEX		(99)
+#define RKVDEC_POWER_CTL_BASE		0x018c
+
+#define FALLBACK_STATIC_TEMPERATURE	55000
+
+#define to_rkvdec_task(task)		\
+		container_of(task, struct rkvdec_task, mpp_task)
+#define to_rkvdec_dev(dev)		\
+		container_of(dev, struct rkvdec_dev, mpp)
+
+enum RKVDEC_MODE {
+	RKVDEC_MODE_NONE,
+	RKVDEC_MODE_ONEFRAME,
+	RKVDEC_MODE_BUTT
+};
+
+enum SET_CLK_EVENT {
+	EVENT_POWER_ON = 0,
+	EVENT_POWER_OFF,
+	EVENT_ADJUST,
+	EVENT_THERMAL,
+	EVENT_BUTT,
+};
+
+struct rkvdec_task {
+	struct mpp_task mpp_task;
+
+	enum RKVDEC_MODE link_mode;
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[RKVDEC_V2_REG_NUM];
+	struct reg_offset_info off_inf;
+
+	u32 strm_addr;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	/* ystride info */
+	u32 pixels;
+};
+
+struct rkvdec_dev {
+	struct mpp_dev mpp;
+	/* sip smc reset lock */
+	struct mutex sip_reset_lock;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info core_clk_info;
+	struct mpp_clk_info cabac_clk_info;
+	struct mpp_clk_info hevc_cabac_clk_info;
+	u32 default_max_load;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_niu_a;
+	struct reset_control *rst_niu_h;
+	struct reset_control *rst_core;
+	struct reset_control *rst_cabac;
+	struct reset_control *rst_hevc_cabac;
+
+	unsigned long aux_iova;
+	struct page *aux_page;
+#ifdef CONFIG_PM_DEVFREQ
+	struct regulator *vdd;
+	struct devfreq *devfreq;
+	struct devfreq *parent_devfreq;
+	struct notifier_block devfreq_nb;
+	struct rockchip_opp_info opp_info;
+	/* set clk lock */
+	struct mutex set_clk_lock;
+	unsigned int thermal_div;
+	unsigned long volt;
+	unsigned long devf_aclk_rate_hz;
+	unsigned long devf_core_rate_hz;
+	unsigned long devf_cabac_rate_hz;
+#endif
+	/* record last infos */
+	u32 last_fmt;
+	bool had_reset;
+	bool grf_changed;
+};
+
+/*
+ * hardware information
+ */
+static struct mpp_hw_info rk_hevcdec_hw_info = {
+	.reg_num = HEVC_DEC_REG_NUM,
+	.reg_id = HEVC_DEC_REG_HW_ID_INDEX,
+	.reg_start = HEVC_DEC_REG_START_INDEX,
+	.reg_end = HEVC_DEC_REG_END_INDEX,
+	.reg_en = RKVDEC_REG_INT_EN_INDEX,
+};
+
+static struct mpp_hw_info rkvdec_v1_hw_info = {
+	.reg_num = RKVDEC_V1_REG_NUM,
+	.reg_id = RKVDEC_V1_REG_HW_ID_INDEX,
+	.reg_start = RKVDEC_V1_REG_START_INDEX,
+	.reg_end = RKVDEC_V1_REG_END_INDEX,
+	.reg_en = RKVDEC_REG_INT_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_h264d[] = {
+	4, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
+	23, 24, 41, 42, 43, 48, 75
+};
+
+static const u16 trans_tbl_h265d[] = {
+	4, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
+	23, 24, 42, 43
+};
+
+static const u16 trans_tbl_vp9d[] = {
+	4, 6, 7, 11, 12, 13, 14, 15, 16
+};
+
+static struct mpp_trans_info rk_hevcdec_trans[] = {
+	[RKVDEC_FMT_H265D] = {
+		.count = ARRAY_SIZE(trans_tbl_h265d),
+		.table = trans_tbl_h265d,
+	},
+};
+
+static struct mpp_trans_info rkvdec_v1_trans[] = {
+	[RKVDEC_FMT_H265D] = {
+		.count = ARRAY_SIZE(trans_tbl_h265d),
+		.table = trans_tbl_h265d,
+	},
+	[RKVDEC_FMT_H264D] = {
+		.count = ARRAY_SIZE(trans_tbl_h264d),
+		.table = trans_tbl_h264d,
+	},
+	[RKVDEC_FMT_VP9D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp9d),
+		.table = trans_tbl_vp9d,
+	},
+};
+
+#ifdef CONFIG_PM_DEVFREQ
+static int rkvdec_devf_set_clk(struct rkvdec_dev *dec,
+			       unsigned long aclk_rate_hz,
+			       unsigned long core_rate_hz,
+			       unsigned long cabac_rate_hz,
+			       unsigned int event)
+{
+	struct clk *aclk = dec->aclk_info.clk;
+	struct clk *clk_core = dec->core_clk_info.clk;
+	struct clk *clk_cabac = dec->cabac_clk_info.clk;
+
+	mutex_lock(&dec->set_clk_lock);
+
+	switch (event) {
+	case EVENT_POWER_ON:
+		clk_set_rate(aclk, dec->devf_aclk_rate_hz);
+		clk_set_rate(clk_core, dec->devf_core_rate_hz);
+		clk_set_rate(clk_cabac, dec->devf_cabac_rate_hz);
+		dec->thermal_div = 0;
+		break;
+	case EVENT_POWER_OFF:
+		clk_set_rate(aclk, aclk_rate_hz);
+		clk_set_rate(clk_core, core_rate_hz);
+		clk_set_rate(clk_cabac, cabac_rate_hz);
+		dec->thermal_div = 0;
+		break;
+	case EVENT_ADJUST:
+		if (!dec->thermal_div) {
+			clk_set_rate(aclk, aclk_rate_hz);
+			clk_set_rate(clk_core, core_rate_hz);
+			clk_set_rate(clk_cabac, cabac_rate_hz);
+		} else {
+			clk_set_rate(aclk,
+				     aclk_rate_hz / dec->thermal_div);
+			clk_set_rate(clk_core,
+				     core_rate_hz / dec->thermal_div);
+			clk_set_rate(clk_cabac,
+				     cabac_rate_hz / dec->thermal_div);
+		}
+		dec->devf_aclk_rate_hz = aclk_rate_hz;
+		dec->devf_core_rate_hz = core_rate_hz;
+		dec->devf_cabac_rate_hz = cabac_rate_hz;
+		break;
+	case EVENT_THERMAL:
+		dec->thermal_div = dec->devf_aclk_rate_hz / aclk_rate_hz;
+		if (dec->thermal_div > 4)
+			dec->thermal_div = 4;
+		if (dec->thermal_div) {
+			clk_set_rate(aclk,
+				     dec->devf_aclk_rate_hz / dec->thermal_div);
+			clk_set_rate(clk_core,
+				     dec->devf_core_rate_hz / dec->thermal_div);
+			clk_set_rate(clk_cabac,
+				     dec->devf_cabac_rate_hz / dec->thermal_div);
+		}
+		break;
+	}
+
+	mutex_unlock(&dec->set_clk_lock);
+
+	return 0;
+}
+
+static int devfreq_target(struct device *dev,
+			  unsigned long *freq, u32 flags)
+{
+	int ret = 0;
+	unsigned int clk_event;
+	struct dev_pm_opp *opp;
+	unsigned long target_volt, target_freq;
+	unsigned long aclk_rate_hz, core_rate_hz, cabac_rate_hz;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct devfreq *devfreq = dec->devfreq;
+	struct devfreq_dev_status *stat = &devfreq->last_status;
+	unsigned long old_clk_rate = stat->current_frequency;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp)) {
+		dev_err(dev, "Failed to find opp for %lu Hz\n", *freq);
+		return PTR_ERR(opp);
+	}
+	target_freq = dev_pm_opp_get_freq(opp);
+	target_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	if (target_freq < *freq) {
+		clk_event = EVENT_THERMAL;
+		aclk_rate_hz = target_freq;
+		core_rate_hz = target_freq;
+		cabac_rate_hz = target_freq;
+	} else {
+		clk_event = stat->busy_time ? EVENT_POWER_ON : EVENT_POWER_OFF;
+		aclk_rate_hz = dec->devf_aclk_rate_hz;
+		core_rate_hz = dec->devf_core_rate_hz;
+		cabac_rate_hz = dec->devf_cabac_rate_hz;
+	}
+
+	if (old_clk_rate == target_freq) {
+		if (dec->volt == target_volt)
+			return ret;
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "Cannot set voltage %lu uV\n",
+				target_volt);
+			return ret;
+		}
+		dec->volt = target_volt;
+		return 0;
+	}
+
+	if (old_clk_rate < target_freq) {
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set voltage %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+
+	dev_dbg(dev, "%lu-->%lu\n", old_clk_rate, target_freq);
+	rkvdec_devf_set_clk(dec, aclk_rate_hz, core_rate_hz, cabac_rate_hz, clk_event);
+	stat->current_frequency = target_freq;
+
+	if (old_clk_rate > target_freq) {
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set vol %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+	dec->volt = target_volt;
+
+	return ret;
+}
+
+static int devfreq_get_cur_freq(struct device *dev,
+				unsigned long *freq)
+{
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	*freq = clk_get_rate(dec->aclk_info.clk);
+
+	return 0;
+}
+
+static int devfreq_get_dev_status(struct device *dev,
+				  struct devfreq_dev_status *stat)
+{
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct devfreq *devfreq = dec->devfreq;
+
+	memcpy(stat, &devfreq->last_status, sizeof(*stat));
+
+	return 0;
+}
+
+static struct devfreq_dev_profile devfreq_profile = {
+	.target	= devfreq_target,
+	.get_cur_freq = devfreq_get_cur_freq,
+	.get_dev_status	= devfreq_get_dev_status,
+	.is_cooling_device = true,
+};
+
+static int devfreq_notifier_call(struct notifier_block *nb,
+				 unsigned long event,
+				 void *data)
+{
+	struct rkvdec_dev *dec = container_of(nb,
+					      struct rkvdec_dev,
+					      devfreq_nb);
+
+	if (!dec)
+		return NOTIFY_OK;
+
+	if (event == DEVFREQ_PRECHANGE)
+		mutex_lock(&dec->sip_reset_lock);
+	else if (event == DEVFREQ_POSTCHANGE)
+		mutex_unlock(&dec->sip_reset_lock);
+
+	return NOTIFY_OK;
+}
+#endif
+
+/*
+ * NOTE: rkvdec/rkhevc put scaling list address in pps buffer hardware will read
+ * it by pps id in video stream data.
+ *
+ * So we need to translate the address in iommu case. The address data is also
+ * 10bit fd + 22bit offset mode.
+ * Because userspace decoder do not give the pps id in the register file sets
+ * kernel driver need to translate each scaling list address in pps buffer which
+ * means 256 pps for H.264, 64 pps for H.265.
+ *
+ * In order to optimize the performance kernel driver ask userspace decoder to
+ * set all scaling list address in pps buffer to the same one which will be used
+ * on current decoding task. Then kernel driver can only translate the first
+ * address then copy it all pps buffer.
+ */
+static int fill_scaling_list_pps(struct rkvdec_task *task,
+				 int fd, int offset, int count,
+				 int pps_info_size, int sub_addr_offset)
+{
+	struct dma_buf *dmabuf = NULL;
+	struct iosys_map map;
+	u8 *pps = NULL;
+	u32 scaling_fd = 0;
+	int ret = 0;
+	u32 base = sub_addr_offset;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR_OR_NULL(dmabuf)) {
+		mpp_err("invliad pps buffer\n");
+		return -ENOENT;
+	}
+
+	ret = dma_buf_begin_cpu_access(dmabuf, DMA_FROM_DEVICE);
+	if (ret) {
+		mpp_err("can't access the pps buffer\n");
+		goto access_failed;
+	}
+
+	ret = dma_buf_vmap(dmabuf, &map);
+	if (ret) {
+		mpp_err("can't access the pps buffer\n");
+		goto vmap_failed;
+	}
+	pps = map.vaddr + offset;
+	/* NOTE: scaling buffer in pps, have no offset */
+	memcpy(&scaling_fd, pps + base, sizeof(scaling_fd));
+	scaling_fd = le32_to_cpu(scaling_fd);
+	if (scaling_fd > 0) {
+		struct mpp_mem_region *mem_region = NULL;
+		u32 tmp = 0;
+		int i = 0;
+
+		mem_region = mpp_task_attach_fd(&task->mpp_task,
+						scaling_fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("scaling list fd %d attach failed\n", scaling_fd);
+			ret = PTR_ERR(mem_region);
+			goto task_fd_failed;
+		}
+
+		tmp = mem_region->iova & 0xffffffff;
+		tmp = cpu_to_le32(tmp);
+		mpp_debug(DEBUG_PPS_FILL,
+			  "pps at %p, scaling fd: %3d => %pad + offset %10d\n",
+			  pps, scaling_fd, &mem_region->iova, offset);
+
+		/* Fill the scaling list address in each pps entries */
+		for (i = 0; i < count; i++, base += pps_info_size)
+			memcpy(pps + base, &tmp, sizeof(tmp));
+	}
+
+task_fd_failed:
+	dma_buf_vunmap(dmabuf, &map);
+vmap_failed:
+	dma_buf_end_cpu_access(dmabuf, DMA_FROM_DEVICE);
+access_failed:
+	dma_buf_put(dmabuf);
+
+	return ret;
+}
+
+static int rkvdec_process_scl_fd(struct mpp_session *session,
+				 struct rkvdec_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+	int pps_fd;
+	u32 pps_offset;
+	int idx = RKVDEC_REG_PPS_BASE_INDEX;
+	u32 fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+
+	if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+		pps_fd = task->reg[idx];
+		pps_offset = 0;
+	} else {
+		pps_fd = task->reg[idx] & 0x3ff;
+		pps_offset = task->reg[idx] >> 10;
+	}
+
+	pps_offset += mpp_query_reg_offset_info(&task->off_inf, idx);
+	if (pps_fd > 0) {
+		int pps_info_offset;
+		int pps_info_count;
+		int pps_info_size;
+		int scaling_list_addr_offset;
+
+		switch (fmt) {
+		case RKVDEC_FMT_H264D:
+			pps_info_offset = pps_offset;
+			pps_info_count = 256;
+			pps_info_size = 32;
+			scaling_list_addr_offset = 23;
+			break;
+		case RKVDEC_FMT_H265D:
+			pps_info_offset = pps_offset;
+			pps_info_count = 64;
+			pps_info_size = 80;
+			scaling_list_addr_offset = 74;
+			break;
+		default:
+			pps_info_offset = 0;
+			pps_info_count = 0;
+			pps_info_size = 0;
+			scaling_list_addr_offset = 0;
+			break;
+		}
+
+		mpp_debug(DEBUG_PPS_FILL,
+			  "scaling list filling parameter:\n");
+		mpp_debug(DEBUG_PPS_FILL,
+			  "pps_info_offset %d\n", pps_info_offset);
+		mpp_debug(DEBUG_PPS_FILL,
+			  "pps_info_count  %d\n", pps_info_count);
+		mpp_debug(DEBUG_PPS_FILL,
+			  "pps_info_size   %d\n", pps_info_size);
+		mpp_debug(DEBUG_PPS_FILL,
+			  "scaling_list_addr_offset %d\n",
+			  scaling_list_addr_offset);
+
+		if (pps_info_count) {
+			ret = fill_scaling_list_pps(task, pps_fd,
+						    pps_info_offset,
+						    pps_info_count,
+						    pps_info_size,
+						    scaling_list_addr_offset);
+			if (ret) {
+				mpp_err("fill pps failed\n");
+				goto fail;
+			}
+		}
+	}
+
+fail:
+	return ret;
+}
+
+static int rkvdec_process_reg_fd(struct mpp_session *session,
+				 struct rkvdec_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+	u32 fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+
+	/*
+	 * special offset scale case
+	 *
+	 * This translation is for fd + offset translation.
+	 * One register has 32bits. We need to transfer both buffer file
+	 * handle and the start address offset so we packet file handle
+	 * and offset together using below format.
+	 *
+	 *  0~9  bit for buffer file handle range 0 ~ 1023
+	 * 10~31 bit for offset range 0 ~ 4M
+	 *
+	 * But on 4K case the offset can be larger the 4M
+	 * So on VP9 4K decoder colmv base we scale the offset by 16
+	 */
+	if (fmt == RKVDEC_FMT_VP9D) {
+		int fd;
+		u32 offset;
+		dma_addr_t iova = 0;
+		struct mpp_mem_region *mem_region = NULL;
+		int idx = RKVDEC_REG_VP9_REFCOLMV_BASE_INDEX;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			fd = task->reg[idx];
+			offset = 0;
+		} else {
+			fd = task->reg[idx] & 0x3ff;
+			offset = task->reg[idx] >> 10 << 4;
+		}
+		mem_region = mpp_task_attach_fd(&task->mpp_task, fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("reg[%03d]: %08x fd %d attach failed\n",
+				idx, task->reg[idx], fd);
+			return -EFAULT;
+		}
+
+		iova = mem_region->iova;
+		task->reg[idx] = iova + offset;
+	}
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					fmt, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+	return 0;
+}
+
+static int rkvdec_extract_task_msg(struct rkvdec_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *rkvdec_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct rkvdec_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = rkvdec_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in pps for 264 and 265 */
+	if (!(msgs->flags & MPP_FLAGS_SCL_FD_NO_TRANS)) {
+		ret = rkvdec_process_scl_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = rkvdec_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->strm_addr = task->reg[RKVDEC_REG_RLC_BASE_INDEX];
+	task->link_mode = RKVDEC_MODE_ONEFRAME;
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	/* get resolution info */
+	task->pixels = RKVDEC_GET_YSTRDE(task->reg[RKVDEC_RGE_YSTRDE_INDEX]);
+	mpp_debug(DEBUG_TASK_INFO, "ystride=%d\n", task->pixels);
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static void *rkvdec_prepare_with_reset(struct mpp_dev *mpp,
+				       struct mpp_task *mpp_task)
+{
+	unsigned long flags;
+	struct mpp_task *out_task = NULL;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	spin_lock_irqsave(&mpp->queue->running_lock, flags);
+	out_task = list_empty(&mpp->queue->running_list) ? mpp_task : NULL;
+	spin_unlock_irqrestore(&mpp->queue->running_lock, flags);
+
+	if (out_task && !dec->had_reset) {
+		struct rkvdec_task *task = to_rkvdec_task(out_task);
+		u32 fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+
+		/* in 3399 3228 and 3229 chips, when 264 switch vp9,
+		 * hardware will timeout, and can't recover problem.
+		 * so reset it when 264 switch vp9, before hardware run.
+		 */
+		if (dec->last_fmt == RKVDEC_FMT_H264D && fmt == RKVDEC_FMT_VP9D) {
+			mpp_power_on(mpp);
+			mpp_dev_reset(mpp);
+			mpp_power_off(mpp);
+		}
+	}
+
+	return out_task;
+}
+
+static int rkvdec_run(struct mpp_dev *mpp,
+		      struct mpp_task *mpp_task)
+{
+	int i;
+	u32 reg_en;
+	struct rkvdec_task *task = NULL;
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	task = to_rkvdec_task(mpp_task);
+	reg_en = mpp_task->hw_info->reg_en;
+	switch (task->link_mode) {
+	case RKVDEC_MODE_ONEFRAME: {
+		u32 reg;
+
+		/* set cache size */
+		reg = RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS
+			| RKVDEC_CACHE_PERMIT_READ_ALLOCATE;
+		if (!mpp_debug_unlikely(DEBUG_CACHE_32B))
+			reg |= RKVDEC_CACHE_LINE_SIZE_64_BYTES;
+
+		mpp_write_relaxed(mpp, RKVDEC_REG_CACHE0_SIZE_BASE, reg);
+		mpp_write_relaxed(mpp, RKVDEC_REG_CACHE1_SIZE_BASE, reg);
+		/* clear cache */
+		mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE0_BASE, 1);
+		mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE1_BASE, 1);
+		/* set registers for hardware */
+		for (i = 0; i < task->w_req_cnt; i++) {
+			int s, e;
+			struct mpp_request *req = &task->w_reqs[i];
+
+			s = req->offset / sizeof(u32);
+			e = s + req->size / sizeof(u32);
+			mpp_write_req(mpp, task->reg, s, e, reg_en);
+		}
+		/* init current task */
+		mpp->cur_task = mpp_task;
+		mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+		/* Flush the register before the start the device */
+		wmb();
+		mpp_write(mpp, RKVDEC_REG_INT_EN,
+			  task->reg[reg_en] | RKVDEC_DEC_START);
+
+		mpp_task_run_end(mpp_task, timing_en);
+	} break;
+	default:
+		break;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec_3328_run(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	u32 fmt = 0;
+	u32 cfg = 0;
+	struct rkvdec_task *task = NULL;
+
+	mpp_debug_enter();
+
+	task = to_rkvdec_task(mpp_task);
+
+	/*
+	 * HW defeat workaround: VP9 and H.265 power save optimization cause decoding
+	 * corruption, disable optimization here.
+	 */
+	fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+	if (fmt == RKVDEC_FMT_VP9D || fmt == RKVDEC_FMT_H265D) {
+		cfg = task->reg[RKVDEC_POWER_CTL_INDEX] | 0xFFFF;
+		task->reg[RKVDEC_POWER_CTL_INDEX] = cfg & (~(1 << 12));
+		mpp_write_relaxed(mpp, RKVDEC_POWER_CTL_BASE,
+				  task->reg[RKVDEC_POWER_CTL_INDEX]);
+	}
+
+	rkvdec_run(mpp, mpp_task);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec_1126_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	if (task->link_mode == RKVDEC_MODE_ONEFRAME)
+		mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	return rkvdec_run(mpp, mpp_task);
+}
+
+static int rkvdec_px30_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	return rkvdec_run(mpp, mpp_task);
+}
+
+static int rkvdec_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, RKVDEC_REG_INT_EN);
+	if (!(mpp->irq_status & RKVDEC_DEC_INT_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int rkvdec_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct rkvdec_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+
+	mpp_debug_enter();
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		goto done;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_rkvdec_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	switch (task->link_mode) {
+	case RKVDEC_MODE_ONEFRAME: {
+		mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+
+		err_mask = RKVDEC_INT_BUF_EMPTY
+			| RKVDEC_INT_BUS_ERROR
+			| RKVDEC_INT_COLMV_REF_ERROR
+			| RKVDEC_INT_STRM_ERROR
+			| RKVDEC_INT_TIMEOUT;
+
+		if (err_mask & task->irq_status)
+			atomic_inc(&mpp->reset_request);
+
+		mpp_task_finish(mpp_task->session, mpp_task);
+	} break;
+	default:
+		break;
+	}
+done:
+	mpp_debug_leave();
+	return IRQ_HANDLED;
+}
+
+static int rkvdec_3328_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct rkvdec_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mpp_debug_enter();
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		goto done;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_rkvdec_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+
+	err_mask = RKVDEC_INT_BUF_EMPTY
+		| RKVDEC_INT_BUS_ERROR
+		| RKVDEC_INT_COLMV_REF_ERROR
+		| RKVDEC_INT_STRM_ERROR
+		| RKVDEC_INT_TIMEOUT;
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	/* unmap reserve buffer */
+	if (dec->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, dec->aux_iova, IOMMU_PAGE_SIZE);
+		dec->aux_iova = -1;
+	}
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+done:
+	mpp_debug_leave();
+	return IRQ_HANDLED;
+}
+
+static int rkvdec_finish(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 dec_get;
+	s32 dec_length;
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	mpp_debug_enter();
+
+	switch (task->link_mode) {
+	case RKVDEC_MODE_ONEFRAME: {
+		u32 s, e;
+		struct mpp_request *req;
+
+		/* read register after running */
+		for (i = 0; i < task->r_req_cnt; i++) {
+			req = &task->r_reqs[i];
+			s = req->offset / sizeof(u32);
+			e = s + req->size / sizeof(u32);
+			mpp_read_req(mpp, task->reg, s, e);
+		}
+		/* revert hack for irq status */
+		task->reg[RKVDEC_REG_INT_EN_INDEX] = task->irq_status;
+		/* revert hack for decoded length */
+		dec_get = mpp_read_relaxed(mpp, RKVDEC_REG_RLC_BASE);
+		dec_length = dec_get - task->strm_addr;
+		task->reg[RKVDEC_REG_RLC_BASE_INDEX] = dec_length << 10;
+		mpp_debug(DEBUG_REGISTER,
+			  "dec_get %08x dec_length %d\n", dec_get, dec_length);
+	} break;
+	default:
+		break;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec_finish_with_record_info(struct mpp_dev *mpp,
+					  struct mpp_task *mpp_task)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	rkvdec_finish(mpp, mpp_task);
+	dec->last_fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+	dec->had_reset = (atomic_read(&mpp->reset_request) > 0) ? true : false;
+
+	return 0;
+}
+
+static int rkvdec_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int rkvdec_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int rkvdec_procfs_remove(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvdec_procfs_init(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	dec->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_core", 0644,
+			      dec->procfs, &dec->core_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_cabac", 0644,
+			      dec->procfs, &dec->cabac_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_hevc_cabac", 0644,
+			      dec->procfs, &dec->hevc_cabac_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int rkvdec_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvdec_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int rkvdec_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mutex_init(&dec->sip_reset_lock);
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_RKVDEC];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->core_clk_info, "clk_core");
+	if (ret)
+		mpp_err("failed on clk_get clk_core\n");
+	ret = mpp_get_clk_info(mpp, &dec->cabac_clk_info, "clk_cabac");
+	if (ret)
+		mpp_err("failed on clk_get clk_cabac\n");
+	ret = mpp_get_clk_info(mpp, &dec->hevc_cabac_clk_info, "clk_hevc_cabac");
+	if (ret)
+		mpp_err("failed on clk_get clk_hevc_cabac\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->core_clk_info, CLK_MODE_DEFAULT, 200 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->cabac_clk_info, CLK_MODE_DEFAULT, 200 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->hevc_cabac_clk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load", &dec->default_max_load);
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	dec->rst_niu_a = mpp_reset_control_get(mpp, RST_TYPE_NIU_A, "niu_a");
+	if (!dec->rst_niu_a)
+		mpp_err("No niu aclk reset resource define\n");
+	dec->rst_niu_h = mpp_reset_control_get(mpp, RST_TYPE_NIU_H, "niu_h");
+	if (!dec->rst_niu_h)
+		mpp_err("No niu hclk reset resource define\n");
+	dec->rst_core = mpp_reset_control_get(mpp, RST_TYPE_CORE, "video_core");
+	if (!dec->rst_core)
+		mpp_err("No core reset resource define\n");
+	dec->rst_cabac = mpp_reset_control_get(mpp, RST_TYPE_CABAC, "video_cabac");
+	if (!dec->rst_cabac)
+		mpp_err("No cabac reset resource define\n");
+	dec->rst_hevc_cabac = mpp_reset_control_get(mpp, RST_TYPE_HEVC_CABAC, "video_hevc_cabac");
+	if (!dec->rst_hevc_cabac)
+		mpp_err("No hevc cabac reset resource define\n");
+
+	return 0;
+}
+
+static int rkvdec_px30_init(struct mpp_dev *mpp)
+{
+	rkvdec_init(mpp);
+	return px30_workaround_combo_init(mpp);
+}
+
+static int rkvdec_3036_init(struct mpp_dev *mpp)
+{
+	rkvdec_init(mpp);
+	set_bit(mpp->var->device_type, &mpp->queue->dev_active_flags);
+	return 0;
+}
+
+static int rkvdec_3328_iommu_hdl(struct iommu_domain *iommu,
+				 struct device *iommu_dev,
+				 unsigned long iova,
+				 int status, void *arg)
+{
+	int ret = 0;
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	/*
+	 * defeat workaround, invalidate address generated when rk322x
+	 * hevc decoder tile mode pre-fetch colmv data.
+	 */
+	if (IOMMU_GET_BUS_ID(status) == 2) {
+		unsigned long page_iova = 0;
+		/* avoid another page fault occur after page fault */
+		if (dec->aux_iova != -1) {
+			iommu_unmap(mpp->iommu_info->domain, dec->aux_iova, IOMMU_PAGE_SIZE);
+			dec->aux_iova = -1;
+		}
+
+		page_iova = round_down(iova, IOMMU_PAGE_SIZE);
+		ret = iommu_map(mpp->iommu_info->domain, page_iova,
+				page_to_phys(dec->aux_page), IOMMU_PAGE_SIZE,
+				IOMMU_READ | IOMMU_WRITE);
+		if (!ret)
+			dec->aux_iova = page_iova;
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_PM_DEVFREQ
+static int rkvdec_devfreq_remove(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	devfreq_unregister_opp_notifier(mpp->dev, dec->devfreq);
+	rockchip_uninit_opp_table(mpp->dev, &dec->opp_info);
+
+	return 0;
+}
+
+static int rkvdec_devfreq_init(struct mpp_dev *mpp)
+{
+	int ret = 0;
+	struct devfreq_dev_status *stat;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mutex_init(&dec->set_clk_lock);
+	dec->parent_devfreq = devfreq_get_devfreq_by_phandle(mpp->dev, "rkvdec_devfreq", 0);
+	if (IS_ERR_OR_NULL(dec->parent_devfreq)) {
+		if (PTR_ERR(dec->parent_devfreq) == -EPROBE_DEFER) {
+			dev_warn(mpp->dev, "parent devfreq is not ready, retry\n");
+
+			return -EPROBE_DEFER;
+		}
+	} else {
+		dec->devfreq_nb.notifier_call = devfreq_notifier_call;
+		devm_devfreq_register_notifier(mpp->dev,
+					       dec->parent_devfreq,
+					       &dec->devfreq_nb,
+					       DEVFREQ_TRANSITION_NOTIFIER);
+	}
+
+	dec->vdd = devm_regulator_get_optional(mpp->dev, "vcodec");
+	if (IS_ERR_OR_NULL(dec->vdd)) {
+		if (PTR_ERR(dec->vdd) == -EPROBE_DEFER) {
+			dev_warn(mpp->dev, "vcodec regulator not ready, retry\n");
+
+			return -EPROBE_DEFER;
+		}
+		dev_warn(mpp->dev, "no regulator for vcodec\n");
+
+		return 0;
+	}
+
+	ret = rockchip_init_opp_table(mpp->dev, &dec->opp_info, NULL, "vcodec");
+	if (ret) {
+		dev_err(mpp->dev, "Failed to init_opp_table\n");
+		return ret;
+	}
+	dec->devfreq = devm_devfreq_add_device(mpp->dev, &devfreq_profile,
+					       "userspace", NULL);
+	if (IS_ERR(dec->devfreq)) {
+		ret = PTR_ERR(dec->devfreq);
+		return ret;
+	}
+
+	stat = &dec->devfreq->last_status;
+	stat->current_frequency = clk_get_rate(dec->aclk_info.clk);
+
+	ret = devfreq_register_opp_notifier(mpp->dev, dec->devfreq);
+	if (ret < 0) {
+		dev_err(mpp->dev, "failed to register opp notifier\n");
+		return ret;
+	}
+
+	return 0;
+}
+#else
+static inline int rkvdec_devfreq_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvdec_devfreq_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int rkvdec_3328_init(struct mpp_dev *mpp)
+{
+	int ret = 0;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	rkvdec_init(mpp);
+
+	/* warkaround for mmu pagefault */
+	dec->aux_page = alloc_page(GFP_KERNEL);
+	if (!dec->aux_page) {
+		dev_err(mpp->dev, "allocate a page for auxiliary usage\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+	dec->aux_iova = -1;
+	mpp->fault_handler = rkvdec_3328_iommu_hdl;
+
+	ret = rkvdec_devfreq_init(mpp);
+done:
+	return ret;
+}
+
+static int rkvdec_3328_exit(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	if (dec->aux_page)
+		__free_page(dec->aux_page);
+
+	if (dec->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, dec->aux_iova, IOMMU_PAGE_SIZE);
+		dec->aux_iova = -1;
+	}
+	rkvdec_devfreq_remove(mpp);
+
+	return 0;
+}
+
+static int rkvdec_clk_on(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+	mpp_clk_safe_enable(dec->core_clk_info.clk);
+	mpp_clk_safe_enable(dec->cabac_clk_info.clk);
+	mpp_clk_safe_enable(dec->hevc_cabac_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvdec_clk_off(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	clk_disable_unprepare(dec->aclk_info.clk);
+	clk_disable_unprepare(dec->hclk_info.clk);
+	clk_disable_unprepare(dec->core_clk_info.clk);
+	clk_disable_unprepare(dec->cabac_clk_info.clk);
+	clk_disable_unprepare(dec->hevc_cabac_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvdec_get_freq(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	u32 task_cnt;
+	u32 workload;
+	struct mpp_task *loop = NULL, *n;
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct rkvdec_task *task = to_rkvdec_task(mpp_task);
+
+	/* if not set max load, consider not have advanced mode */
+	if (!dec->default_max_load || !task->pixels)
+		return 0;
+
+	task_cnt = 1;
+	workload = task->pixels;
+	/* calc workload in pending list */
+	mutex_lock(&mpp->queue->pending_lock);
+	list_for_each_entry_safe(loop, n,
+				 &mpp->queue->pending_list,
+				 queue_link) {
+		struct rkvdec_task *loop_task = to_rkvdec_task(loop);
+
+		task_cnt++;
+		workload += loop_task->pixels;
+	}
+	mutex_unlock(&mpp->queue->pending_lock);
+
+	if (workload > dec->default_max_load)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	mpp_debug(DEBUG_TASK_INFO, "pending task %d, workload %d, clk_mode=%d\n",
+		  task_cnt, workload, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvdec_3328_get_freq(struct mpp_dev *mpp,
+				struct mpp_task *mpp_task)
+{
+	u32 fmt;
+	u32 ddr_align_en;
+	struct rkvdec_task *task =  to_rkvdec_task(mpp_task);
+
+	fmt = RKVDEC_GET_FORMAT(task->reg[RKVDEC_REG_SYS_CTRL_INDEX]);
+	ddr_align_en = task->reg[RKVDEC_REG_INT_EN_INDEX] & RKVDEC_WR_DDR_ALIGN_EN;
+	if (fmt == RKVDEC_FMT_H264D && ddr_align_en)
+		task->clk_mode = CLK_MODE_ADVANCED;
+	else
+		rkvdec_get_freq(mpp, mpp_task);
+
+	return 0;
+}
+
+static int rkvdec_3368_set_grf(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	dec->grf_changed = mpp_grf_is_changed(mpp->grf_info);
+	mpp_set_grf(mpp->grf_info);
+
+	return 0;
+}
+
+static int rkvdec_3036_set_grf(struct mpp_dev *mpp)
+{
+	int grf_changed;
+	struct mpp_dev *loop = NULL, *n;
+	struct mpp_taskqueue *queue = mpp->queue;
+	bool pd_is_on;
+
+	grf_changed = mpp_grf_is_changed(mpp->grf_info);
+	if (grf_changed) {
+
+		/*
+		 * in this case, devices share the queue also share the same pd&clk,
+		 * so use mpp->dev's pd to control all the process is okay
+		 */
+		pd_is_on = rockchip_pmu_pd_is_on(mpp->dev);
+		if (!pd_is_on)
+			rockchip_pmu_pd_on(mpp->dev);
+		mpp->hw_ops->clk_on(mpp);
+
+		list_for_each_entry_safe(loop, n, &queue->dev_list, queue_link) {
+			if (test_bit(loop->var->device_type, &queue->dev_active_flags)) {
+				mpp_set_grf(loop->grf_info);
+				if (loop->hw_ops->clk_on)
+					loop->hw_ops->clk_on(loop);
+				if (loop->hw_ops->reset)
+					loop->hw_ops->reset(loop);
+				rockchip_iommu_disable(loop->dev);
+				if (loop->hw_ops->clk_off)
+					loop->hw_ops->clk_off(loop);
+				clear_bit(loop->var->device_type, &queue->dev_active_flags);
+			}
+		}
+
+		mpp_set_grf(mpp->grf_info);
+		rockchip_iommu_enable(mpp->dev);
+		set_bit(mpp->var->device_type, &queue->dev_active_flags);
+
+		mpp->hw_ops->clk_off(mpp);
+		if (!pd_is_on)
+			rockchip_pmu_pd_off(mpp->dev);
+	}
+
+
+	return 0;
+}
+
+static int rkvdec_set_freq(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct rkvdec_task *task =  to_rkvdec_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->core_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->cabac_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->hevc_cabac_clk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvdec_3368_set_freq(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct rkvdec_task *task =  to_rkvdec_task(mpp_task);
+
+	/* if grf changed, need reset iommu for rk3368 */
+	if (dec->grf_changed) {
+		mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+		dec->grf_changed = false;
+	}
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->core_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->cabac_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->hevc_cabac_clk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvdec_3328_set_freq(struct mpp_dev *mpp,
+				struct mpp_task *mpp_task)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+	struct rkvdec_task *task =  to_rkvdec_task(mpp_task);
+
+#ifdef CONFIG_PM_DEVFREQ
+	if (dec->devfreq) {
+		struct devfreq_dev_status *stat;
+		unsigned long aclk_rate_hz, core_rate_hz, cabac_rate_hz;
+
+		stat = &dec->devfreq->last_status;
+		stat->busy_time = 1;
+		stat->total_time = 1;
+		aclk_rate_hz = mpp_get_clk_info_rate_hz(&dec->aclk_info,
+							task->clk_mode);
+		core_rate_hz = mpp_get_clk_info_rate_hz(&dec->core_clk_info,
+							task->clk_mode);
+		cabac_rate_hz = mpp_get_clk_info_rate_hz(&dec->cabac_clk_info,
+							 task->clk_mode);
+		rkvdec_devf_set_clk(dec, aclk_rate_hz,
+				    core_rate_hz, cabac_rate_hz,
+				    EVENT_ADJUST);
+	}
+#else
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->core_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->cabac_clk_info, task->clk_mode);
+#endif
+
+	return 0;
+}
+
+static int rkvdec_reduce_freq(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&dec->core_clk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&dec->cabac_clk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&dec->hevc_cabac_clk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int rkvdec_3328_reduce_freq(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+#ifdef CONFIG_PM_DEVFREQ
+	if (dec->devfreq) {
+		struct devfreq_dev_status *stat;
+		unsigned long aclk_rate_hz, core_rate_hz, cabac_rate_hz;
+
+		stat = &dec->devfreq->last_status;
+		stat->busy_time = 0;
+		stat->total_time = 1;
+		aclk_rate_hz = mpp_get_clk_info_rate_hz(&dec->aclk_info,
+							CLK_MODE_REDUCE);
+		core_rate_hz = mpp_get_clk_info_rate_hz(&dec->core_clk_info,
+							CLK_MODE_REDUCE);
+		cabac_rate_hz = mpp_get_clk_info_rate_hz(&dec->cabac_clk_info,
+							 CLK_MODE_REDUCE);
+		rkvdec_devf_set_clk(dec, aclk_rate_hz,
+				    core_rate_hz, cabac_rate_hz,
+				    EVENT_ADJUST);
+	}
+#else
+	mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&dec->core_clk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&dec->cabac_clk_info, CLK_MODE_REDUCE);
+#endif
+
+	return 0;
+}
+
+static int rkvdec_reset(struct mpp_dev *mpp)
+{
+	struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+	mpp_debug_enter();
+	if (dec->rst_a && dec->rst_h) {
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(dec->rst_niu_a);
+		mpp_safe_reset(dec->rst_niu_h);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		mpp_safe_reset(dec->rst_core);
+		mpp_safe_reset(dec->rst_cabac);
+		mpp_safe_reset(dec->rst_hevc_cabac);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_niu_h);
+		mpp_safe_unreset(dec->rst_niu_a);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		mpp_safe_unreset(dec->rst_core);
+		mpp_safe_unreset(dec->rst_cabac);
+		mpp_safe_unreset(dec->rst_hevc_cabac);
+		mpp_pmu_idle_request(mpp, false);
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec_sip_reset(struct mpp_dev *mpp)
+{
+	if (IS_REACHABLE(CONFIG_ROCKCHIP_SIP)) {
+		/* The reset flow in arm trustzone firmware */
+		struct rkvdec_dev *dec = to_rkvdec_dev(mpp);
+
+		mutex_lock(&dec->sip_reset_lock);
+		sip_smc_vpu_reset(0, 0, 0);
+		mutex_unlock(&dec->sip_reset_lock);
+
+		return 0;
+	} else {
+		return rkvdec_reset(mpp);
+	}
+}
+
+static struct mpp_hw_ops rkvdec_v1_hw_ops = {
+	.init = rkvdec_init,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_get_freq,
+	.set_freq = rkvdec_set_freq,
+	.reduce_freq = rkvdec_reduce_freq,
+	.reset = rkvdec_reset,
+};
+
+static struct mpp_hw_ops rkvdec_px30_hw_ops = {
+	.init = rkvdec_px30_init,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_get_freq,
+	.set_freq = rkvdec_set_freq,
+	.reduce_freq = rkvdec_reduce_freq,
+	.reset = rkvdec_reset,
+	.set_grf = px30_workaround_combo_switch_grf,
+};
+
+static struct mpp_hw_ops rkvdec_3036_hw_ops = {
+	.init = rkvdec_3036_init,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_get_freq,
+	.set_freq = rkvdec_set_freq,
+	.reduce_freq = rkvdec_reduce_freq,
+	.reset = rkvdec_reset,
+	.set_grf = rkvdec_3036_set_grf,
+};
+
+static struct mpp_hw_ops rkvdec_3399_hw_ops = {
+	.init = rkvdec_init,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_get_freq,
+	.set_freq = rkvdec_set_freq,
+	.reduce_freq = rkvdec_reduce_freq,
+	.reset = rkvdec_reset,
+};
+
+static struct mpp_hw_ops rkvdec_3368_hw_ops = {
+	.init = rkvdec_init,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_get_freq,
+	.set_freq = rkvdec_3368_set_freq,
+	.reduce_freq = rkvdec_reduce_freq,
+	.reset = rkvdec_reset,
+	.set_grf = rkvdec_3368_set_grf,
+};
+
+static struct mpp_dev_ops rkvdec_v1_dev_ops = {
+	.alloc_task = rkvdec_alloc_task,
+	.run = rkvdec_run,
+	.irq = rkvdec_irq,
+	.isr = rkvdec_isr,
+	.finish = rkvdec_finish,
+	.result = rkvdec_result,
+	.free_task = rkvdec_free_task,
+};
+
+static struct mpp_dev_ops rkvdec_px30_dev_ops = {
+	.alloc_task = rkvdec_alloc_task,
+	.run = rkvdec_px30_run,
+	.irq = rkvdec_irq,
+	.isr = rkvdec_isr,
+	.finish = rkvdec_finish,
+	.result = rkvdec_result,
+	.free_task = rkvdec_free_task,
+};
+
+static struct mpp_hw_ops rkvdec_3328_hw_ops = {
+	.init = rkvdec_3328_init,
+	.exit = rkvdec_3328_exit,
+	.clk_on = rkvdec_clk_on,
+	.clk_off = rkvdec_clk_off,
+	.get_freq = rkvdec_3328_get_freq,
+	.set_freq = rkvdec_3328_set_freq,
+	.reduce_freq = rkvdec_3328_reduce_freq,
+	.reset = rkvdec_sip_reset,
+};
+
+static struct mpp_dev_ops rkvdec_3328_dev_ops = {
+	.alloc_task = rkvdec_alloc_task,
+	.run = rkvdec_3328_run,
+	.irq = rkvdec_irq,
+	.isr = rkvdec_3328_isr,
+	.finish = rkvdec_finish,
+	.result = rkvdec_result,
+	.free_task = rkvdec_free_task,
+};
+
+static struct mpp_dev_ops rkvdec_3399_dev_ops = {
+	.alloc_task = rkvdec_alloc_task,
+	.prepare = rkvdec_prepare_with_reset,
+	.run = rkvdec_run,
+	.irq = rkvdec_irq,
+	.isr = rkvdec_isr,
+	.finish = rkvdec_finish_with_record_info,
+	.result = rkvdec_result,
+	.free_task = rkvdec_free_task,
+};
+
+static struct mpp_dev_ops rkvdec_1126_dev_ops = {
+	.alloc_task = rkvdec_alloc_task,
+	.run = rkvdec_1126_run,
+	.irq = rkvdec_irq,
+	.isr = rkvdec_isr,
+	.finish = rkvdec_finish,
+	.result = rkvdec_result,
+	.free_task = rkvdec_free_task,
+};
+static const struct mpp_dev_var rk_hevcdec_data = {
+	.device_type = MPP_DEVICE_HEVC_DEC,
+	.hw_info = &rk_hevcdec_hw_info,
+	.trans_info = rk_hevcdec_trans,
+	.hw_ops = &rkvdec_v1_hw_ops,
+	.dev_ops = &rkvdec_v1_dev_ops,
+};
+
+static const struct mpp_dev_var rk_hevcdec_3036_data = {
+	.device_type = MPP_DEVICE_HEVC_DEC,
+	.hw_info = &rk_hevcdec_hw_info,
+	.trans_info = rk_hevcdec_trans,
+	.hw_ops = &rkvdec_3036_hw_ops,
+	.dev_ops = &rkvdec_v1_dev_ops,
+};
+
+static const struct mpp_dev_var rk_hevcdec_3368_data = {
+	.device_type = MPP_DEVICE_HEVC_DEC,
+	.hw_info = &rk_hevcdec_hw_info,
+	.trans_info = rk_hevcdec_trans,
+	.hw_ops = &rkvdec_3368_hw_ops,
+	.dev_ops = &rkvdec_v1_dev_ops,
+};
+
+static const struct mpp_dev_var rk_hevcdec_px30_data = {
+	.device_type = MPP_DEVICE_HEVC_DEC,
+	.hw_info = &rk_hevcdec_hw_info,
+	.trans_info = rk_hevcdec_trans,
+	.hw_ops = &rkvdec_px30_hw_ops,
+	.dev_ops = &rkvdec_px30_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_v1_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v1_hw_info,
+	.trans_info = rkvdec_v1_trans,
+	.hw_ops = &rkvdec_v1_hw_ops,
+	.dev_ops = &rkvdec_v1_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_3399_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v1_hw_info,
+	.trans_info = rkvdec_v1_trans,
+	.hw_ops = &rkvdec_3399_hw_ops,
+	.dev_ops = &rkvdec_3399_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_3328_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v1_hw_info,
+	.trans_info = rkvdec_v1_trans,
+	.hw_ops = &rkvdec_3328_hw_ops,
+	.dev_ops = &rkvdec_3328_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_1126_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v1_hw_info,
+	.trans_info = rkvdec_v1_trans,
+	.hw_ops = &rkvdec_v1_hw_ops,
+	.dev_ops = &rkvdec_1126_dev_ops,
+};
+
+static const struct of_device_id mpp_rkvdec_dt_match[] = {
+	{
+		.compatible = "rockchip,hevc-decoder",
+		.data = &rk_hevcdec_data,
+	},
+#ifdef CONFIG_CPU_PX30
+	{
+		.compatible = "rockchip,hevc-decoder-px30",
+		.data = &rk_hevcdec_px30_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3036
+	{
+		.compatible = "rockchip,hevc-decoder-rk3036",
+		.data = &rk_hevcdec_3036_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3368
+	{
+		.compatible = "rockchip,hevc-decoder-rk3368",
+		.data = &rk_hevcdec_3368_data,
+	},
+#endif
+	{
+		.compatible = "rockchip,rkv-decoder-v1",
+		.data = &rkvdec_v1_data,
+	},
+#ifdef CONFIG_CPU_RK3399
+	{
+		.compatible = "rockchip,rkv-decoder-rk3399",
+		.data = &rkvdec_3399_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3328
+	{
+		.compatible = "rockchip,rkv-decoder-rk3328",
+		.data = &rkvdec_3328_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RV1126
+	{
+		.compatible = "rockchip,rkv-decoder-rv1126",
+		.data = &rkvdec_1126_data,
+	},
+#endif
+	{},
+};
+
+static int rkvdec_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct rkvdec_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	dev_info(dev, "probing start\n");
+	dec = devm_kzalloc(dev, sizeof(*dec), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_rkvdec_dt_match,
+				      pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return ret;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = RKVDEC_SESSION_MAX_BUFFERS;
+	rkvdec_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int rkvdec_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = platform_get_drvdata(pdev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	rkvdec_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_rkvdec_driver = {
+	.probe = rkvdec_probe,
+	.remove = rkvdec_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = RKVDEC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_rkvdec_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_rkvdec_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_rkvdec2.c b/drivers/video/rockchip/mpp/mpp_rkvdec2.c
new file mode 100644
index 0000000000000..2168851006161
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvdec2.c
@@ -0,0 +1,2132 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2020 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <linux/pm_runtime.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#include "mpp_rkvdec2_link.h"
+
+#include "hack/mpp_rkvdec2_hack_rk3568.c"
+#include "hack/mpp_hack_rk3576.h"
+
+#include <soc/rockchip/rockchip_dmc.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_system_monitor.h>
+#include <soc/rockchip/rockchip_iommu.h>
+
+#ifdef CONFIG_PM_DEVFREQ
+#include "../drivers/devfreq/governor.h"
+#endif
+
+/*
+ * hardware information
+ */
+static struct mpp_hw_info rkvdec_v2_hw_info = {
+	.reg_num = 279,
+	.reg_id = 0,
+	.reg_start = 0,
+	.reg_end = 278,
+	.reg_en = 10,
+	.reg_fmt = 9,
+	.reg_ret_status = 224,
+	.link_info = &rkvdec_link_v2_hw_info,
+};
+
+static struct mpp_hw_info rkvdec_rk356x_hw_info = {
+	.reg_num = 279,
+	.reg_id = 0,
+	.reg_start = 0,
+	.reg_end = 278,
+	.reg_en = 10,
+	.reg_fmt = 9,
+	.reg_ret_status = 224,
+	.link_info = &rkvdec_link_rk356x_hw_info,
+};
+
+static struct mpp_hw_info rkvdec_vdpu382_hw_info = {
+	.reg_num = 279,
+	.reg_id = 0,
+	.reg_start = 0,
+	.reg_end = 278,
+	.reg_en = 10,
+	.reg_fmt = 9,
+	.reg_ret_status = 224,
+	.link_info = &rkvdec_link_vdpu382_hw_info,
+};
+
+static struct mpp_hw_info rkvdec_vdpu383_hw_info = {
+	.reg_num = 360,
+	.reg_id = 0,
+	.reg_start = 0,
+	.reg_end = 359,
+	.reg_en = 16,
+	.reg_fmt = 8,
+	.reg_ret_status = 15,
+	.magic_base = 0x100,
+	.link_info = &rkvdec_link_vdpu383_hw_info,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_h264d[] = {
+	128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,
+	161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,
+	177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,
+	192, 193, 194, 195, 196, 197, 198, 199
+};
+
+static const u16 trans_tbl_h265d[] = {
+	128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,
+	161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,
+	177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,
+	192, 193, 194, 195, 196, 197, 198, 199
+};
+
+static const u16 trans_tbl_vp9d[] = {
+	128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,
+	160, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 180, 181, 182, 183,
+	184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199
+};
+
+static const u16 trans_tbl_avs2d[] = {
+	128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,
+	161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,
+	177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,
+	192, 193, 194, 195, 196, 197, 198, 199
+};
+
+static struct mpp_trans_info rkvdec_v2_trans[] = {
+	[RKVDEC_FMT_H265D] = {
+		.count = ARRAY_SIZE(trans_tbl_h265d),
+		.table = trans_tbl_h265d,
+	},
+	[RKVDEC_FMT_H264D] = {
+		.count = ARRAY_SIZE(trans_tbl_h264d),
+		.table = trans_tbl_h264d,
+	},
+	[RKVDEC_FMT_VP9D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp9d),
+		.table = trans_tbl_vp9d,
+	},
+	[RKVDEC_FMT_AVS2] = {
+		.count = ARRAY_SIZE(trans_tbl_avs2d),
+		.table = trans_tbl_avs2d,
+	}
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_vdpu383_tbl_h265d[] = {
+	128, 129, 130, 131, 132, 133, 134, 140, 142, 144, 146, 148, 150, 152, 154,
+	156, 158, 160, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,
+	180, 181, 182, 183, 184, 185, 192, 194, 195, 196, 197, 198, 199, 200, 201,
+	202, 203, 204, 205, 206, 207, 208, 209, 210, 216, 217, 218, 219, 220, 221,
+	222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232
+};
+
+static const u16 trans_vdpu383_tbl_h264d[] = {
+	128, 129, 130, 131, 132, 133, 134, 140, 142, 144, 146, 148, 150, 152, 154,
+	156, 158, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,
+	181, 182, 183, 184, 185, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202,
+	203, 204, 205, 206, 207, 208, 209, 210, 216, 217, 218, 219, 220, 221, 222,
+	223, 224, 225, 226, 227, 228, 229, 230, 231, 232
+};
+
+static const u16 trans_vdpu383_tbl_vp9d[] = {
+	128, 129, 130, 131, 132, 133, 134, 140, 142, 144, 146, 148, 150, 152, 154,
+	156, 158, 160, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,
+	180, 181, 182, 183, 184, 185, 192, 194, 195, 196, 197, 198, 199, 200, 201,
+	202, 203, 204, 205, 206, 207, 208, 209, 210, 216, 217, 218, 219, 220, 221,
+	222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232
+};
+
+static const u16 trans_vdpu383_tbl_avs2d[] = {
+	128, 129, 130, 131, 132, 133, 134, 140, 142, 144, 146, 148, 150, 152, 154,
+	156, 158, 160, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,
+	180, 181, 182, 183, 184, 185, 192, 194, 195, 196, 197, 198, 199, 200, 201,
+	202, 203, 204, 205, 206, 207, 208, 209, 210, 216, 217, 218, 219, 220, 221,
+	222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232
+};
+
+static const u16 trans_vdpu383_tbl_av1d[] = {
+	128, 131, 133, 134, 140, 142, 144, 146, 148, 150, 152, 154,
+	156, 158, 160, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,
+	180, 181, 182, 183, 184, 185, 192, 194, 195, 196, 197, 198, 199, 200, 201,
+	202, 203, 204, 205, 206, 207, 208, 209, 210, 216, 217, 218, 219, 220, 221,
+	222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232
+};
+
+static struct mpp_trans_info rkvdec_vdpu383_trans[] = {
+	[RKVDEC_FMT_H265D] = {
+		.count = ARRAY_SIZE(trans_vdpu383_tbl_h265d),
+		.table = trans_vdpu383_tbl_h265d,
+	},
+	[RKVDEC_FMT_H264D] = {
+		.count = ARRAY_SIZE(trans_vdpu383_tbl_h264d),
+		.table = trans_vdpu383_tbl_h264d,
+	},
+	[RKVDEC_FMT_VP9D] = {
+		.count = ARRAY_SIZE(trans_vdpu383_tbl_vp9d),
+		.table = trans_vdpu383_tbl_vp9d,
+	},
+	[RKVDEC_FMT_AVS2] = {
+		.count = ARRAY_SIZE(trans_vdpu383_tbl_avs2d),
+		.table = trans_vdpu383_tbl_avs2d,
+	},
+	[RKVDEC_FMT_AV1D] = {
+		.count = ARRAY_SIZE(trans_vdpu383_tbl_av1d),
+		.table = trans_vdpu383_tbl_av1d,
+	}
+};
+
+static int mpp_extract_rcb_info(struct rkvdec2_rcb_info *rcb_inf,
+				struct mpp_request *req)
+{
+	u32 max_size = ARRAY_SIZE(rcb_inf->elem);
+	u32 cnt = req->size / sizeof(rcb_inf->elem[0]);
+
+	if (req->size > sizeof(rcb_inf->elem)) {
+		mpp_err("count %d,max_size %d\n", cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(rcb_inf->elem, req->data, req->size)) {
+		mpp_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	rcb_inf->cnt = cnt;
+
+	return 0;
+}
+
+static int rkvdec2_extract_task_msg(struct mpp_session *session,
+				    struct rkvdec2_task *task,
+				    struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg), off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			u32 req_base;
+			u32 max_size;
+
+			if (req->offset >= RKVDEC_PERF_SEL_OFFSET) {
+				req_base = RKVDEC_PERF_SEL_OFFSET;
+				max_size = sizeof(task->reg_sel);
+			} else {
+				req_base = 0;
+				max_size = sizeof(task->reg);
+			}
+
+			ret = mpp_check_req(req, req_base, max_size, 0, max_size);
+			if (ret)
+				continue;
+
+			memcpy(&task->r_reqs[task->r_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		case MPP_CMD_SET_RCB_INFO: {
+			struct rkvdec2_session_priv *priv = session->priv;
+
+			if (priv)
+				mpp_extract_rcb_info(&priv->rcb_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+int mpp_set_rcbbuf(struct mpp_dev *mpp, struct mpp_session *session,
+		   struct mpp_task *task)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec2_session_priv *priv = session->priv;
+
+	mpp_debug_enter();
+
+	if (priv && dec->rcb_iova) {
+		u32 i;
+		u32 reg_idx, rcb_size, rcb_offset;
+		struct rkvdec2_rcb_info *rcb_inf = &priv->rcb_inf;
+		u32 width = priv->codec_info[DEC_INFO_WIDTH].val;
+
+		if (width < dec->rcb_min_width)
+			goto done;
+
+		rcb_offset = 0;
+		for (i = 0; i < rcb_inf->cnt; i++) {
+			reg_idx = rcb_inf->elem[i].index;
+			rcb_size = rcb_inf->elem[i].size;
+			if ((rcb_offset + rcb_size) > dec->rcb_size) {
+				mpp_debug(DEBUG_SRAM_INFO,
+					  "rcb: reg %d use original buffer\n", reg_idx);
+				continue;
+			}
+			mpp_debug(DEBUG_SRAM_INFO, "rcb: reg %d offset %d, size %d\n",
+				  reg_idx, rcb_offset, rcb_size);
+			task->reg[reg_idx] = dec->rcb_iova + rcb_offset;
+			rcb_offset += rcb_size;
+		}
+	}
+done:
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int rkvdec2_task_init(struct mpp_dev *mpp, struct mpp_session *session,
+		      struct rkvdec2_task *task, struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = &task->mpp_task;
+
+	mpp_debug_enter();
+
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = rkvdec2_extract_task_msg(session, task, msgs);
+	if (ret)
+		return ret;
+
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		int reg_fmt = mpp_task->hw_info->reg_fmt;
+		u32 fmt = RKVDEC_GET_FORMAT(task->reg[reg_fmt]);
+
+		ret = mpp_translate_reg_address(session, mpp_task,
+						fmt, task->reg, &task->off_inf);
+		if (ret)
+			goto fail;
+
+		mpp_translate_reg_offset_info(mpp_task, &task->off_inf, task->reg);
+	}
+
+	task->strm_addr = task->reg[RKVDEC_REG_RLC_BASE_INDEX];
+	task->clk_mode = CLK_MODE_NORMAL;
+	task->slot_idx = -1;
+	init_waitqueue_head(&mpp_task->wait);
+	/* get resolution info */
+	if (session->priv) {
+		struct rkvdec2_session_priv *priv = session->priv;
+		u32 width = priv->codec_info[DEC_INFO_WIDTH].val;
+		u32 bitdepth = priv->codec_info[DEC_INFO_BITDEPTH].val;
+
+		task->width =  (bitdepth > 8) ? ((width * bitdepth + 7) >> 3) : width;
+		task->height = priv->codec_info[DEC_INFO_HEIGHT].val;
+		task->pixels = task->width * task->height;
+		mpp_debug(DEBUG_TASK_INFO, "width=%d, bitdepth=%d, height=%d\n",
+			  width, bitdepth, task->height);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	return ret;
+}
+
+void *rkvdec2_alloc_task(struct mpp_session *session,
+			 struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct rkvdec2_task *task;
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	ret = rkvdec2_task_init(session->mpp, session, task, msgs);
+	if (ret) {
+		kfree(task);
+		return NULL;
+	}
+	mpp_set_rcbbuf(session->mpp, session, &task->mpp_task);
+
+	return &task->mpp_task;
+}
+
+static void *rkvdec2_rk3568_alloc_task(struct mpp_session *session,
+				       struct mpp_task_msgs *msgs)
+{
+	u32 fmt;
+	int reg_fmt;
+	struct mpp_task *mpp_task = NULL;
+	struct rkvdec2_task *task = NULL;
+
+	mpp_task = rkvdec2_alloc_task(session, msgs);
+	if (!mpp_task)
+		return NULL;
+
+	task = to_rkvdec2_task(mpp_task);
+	reg_fmt = mpp_task->hw_info->reg_fmt;
+	fmt = RKVDEC_GET_FORMAT(task->reg[reg_fmt]);
+	/* workaround for rk356x, fix the hw bug of cabac/cavlc switch only in h264d */
+	task->need_hack = (fmt == RKVDEC_FMT_H264D);
+
+	return mpp_task;
+}
+
+static int rkvdec2_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+	u32 reg_en = mpp_task->hw_info->reg_en;
+	/* set cache size */
+	u32 reg = RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS |
+		  RKVDEC_CACHE_PERMIT_READ_ALLOCATE;
+	u32 i;
+
+	mpp_debug_enter();
+
+	if (!mpp_debug_unlikely(DEBUG_CACHE_32B))
+		reg |= RKVDEC_CACHE_LINE_SIZE_64_BYTES;
+
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE0_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE1_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE2_SIZE_BASE, reg);
+	/* clear cache */
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE0_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE1_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE2_BASE, 1);
+
+	/* set registers for hardware */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		u32 s, e;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, RKVDEC_REG_START_EN_BASE, task->reg[reg_en] | RKVDEC_START_EN);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec2_rk3568_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	int ret = 0;
+
+	mpp_debug_enter();
+
+	/*
+	 * run fix before task processing
+	 * workaround for rk356x, fix the hw bug of cabac/cavlc switch only in h264d
+	 */
+	if (task->need_hack)
+		rkvdec2_3568_hack_fix(mpp);
+
+	ret = rkvdec2_run(mpp, mpp_task);
+
+	mpp_debug_leave();
+
+	return ret;
+}
+
+static int rkvdec_vdpu383_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	u32 timing_en = mpp->srv->timing_en;
+	struct rkvdec_link_dev *link = dec->link_dec;
+
+	/* set cache size */
+	u32 reg = RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS |
+		  RKVDEC_CACHE_PERMIT_READ_ALLOCATE;
+	u32 i;
+
+	mpp_debug_enter();
+
+	if (!mpp_debug_unlikely(DEBUG_CACHE_32B))
+		reg |= RKVDEC_CACHE_LINE_SIZE_64_BYTES;
+
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE0_SIZE_BASE, reg);
+	/* clear cache */
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE0_BASE, 1);
+	/* init max outstanding read */
+	mpp_write_relaxed(mpp, RKVDEC_REG_MAX_READS, 0x1c);
+
+	/* set registers for hardware */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		u32 s, e;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_write_req(mpp, task->reg, s, e, -1);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* set ip time out threshold, ip watch-dog */
+	writel_relaxed(0x7fffff, link->reg_base + link->info->ip_time_base);
+
+	/* set ip func to def val */
+	writel_relaxed(link->info->ip_en_val, link->reg_base + link->info->ip_en_base);
+
+	/* Flush the register before the start the device */
+	wmb();
+	/* enable hardware */
+	writel(RKVDEC_START_EN, link->reg_base + link->info->en_base);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec2_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, RKVDEC_REG_INT_EN);
+	if (!(mpp->irq_status & RKVDEC_IRQ_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int rkvdec_vdpu383_irq(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link = dec->link_dec;
+	u32 irq_val;
+	u32 irq_bits = link->info->irq_mask >> 16;
+	u32 status_bits = link->info->status_mask >> 16;
+
+	/* read and clear irq */
+	irq_val = readl_relaxed(link->reg_base + link->info->irq_base);
+	if (!(irq_val & irq_bits))
+		return IRQ_NONE;
+	writel(link->info->irq_mask, link->reg_base + link->info->irq_base);
+
+	/* read and clear status */
+	mpp->irq_status = readl_relaxed(link->reg_base + link->info->status_base);
+	writel(link->info->status_mask, link->reg_base + link->info->status_base);
+
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x : %08x\n", irq_val, mpp->irq_status);
+
+	/* wake isr to handle current task */
+	if (mpp->irq_status & status_bits)
+		return IRQ_WAKE_THREAD;
+
+	return IRQ_NONE;
+}
+
+static int rkvdec2_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct rkvdec2_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_info *link_info = mpp->var->hw_info->link_info;
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_task->hw_cycles = mpp_read(mpp, RKVDEC_PERF_WORKING_CNT);
+	mpp_time_diff_with_hw_time(mpp_task, dec->cycle_clk->real_rate_hz);
+	mpp->cur_task = NULL;
+	task = to_rkvdec2_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+	err_mask = link_info->err_mask;
+	if (err_mask & task->irq_status) {
+		atomic_inc(&mpp->reset_request);
+		if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+			mpp_debug(DEBUG_DUMP_ERR_REG, "irq_status: %08x\n", task->irq_status);
+			mpp_task_dump_hw_reg(mpp);
+		}
+	}
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+	return IRQ_HANDLED;
+}
+
+static int rkvdec_vdpu383_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct rkvdec2_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_task->hw_cycles = mpp_read(mpp, RKVDEC_PERF_WORKING_CNT);
+	mpp_task->hw_time = mpp_task->hw_cycles / (dec->cycle_clk->real_rate_hz / 1000000);
+	mpp_time_diff_with_hw_time(mpp_task, dec->cycle_clk->real_rate_hz);
+	mpp->cur_task = NULL;
+	task = to_rkvdec2_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+	/* TODO */
+
+	/* handle error */
+	err_mask = dec->link_dec->info->err_mask;
+	if (err_mask & task->irq_status) {
+		atomic_inc(&mpp->reset_request);
+		if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+			mpp_debug(DEBUG_DUMP_ERR_REG, "irq_status: %08x\n", task->irq_status);
+			mpp_task_dump_hw_reg(mpp);
+		}
+	}
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+	return IRQ_HANDLED;
+}
+
+static int rkvdec2_read_perf_sel(struct mpp_dev *mpp, u32 *regs, u32 s, u32 e)
+{
+	u32 i;
+	u32 sel0, sel1, sel2, val;
+
+	for (i = s; i < e; i += 3) {
+		/* set sel */
+		sel0 = i;
+		sel1 = ((i + 1) < e) ? (i + 1) : 0;
+		sel2 = ((i + 2) < e) ? (i + 2) : 0;
+		val = RKVDEC_SET_PERF_SEL(sel0, sel1, sel2);
+		writel_relaxed(val, mpp->reg_base + RKVDEC_PERF_SEL_BASE);
+		/* read data */
+		regs[sel0] = readl_relaxed(mpp->reg_base + RKVDEC_SEL_VAL0_BASE);
+		mpp_debug(DEBUG_GET_PERF_VAL, "sel[%d]:%u\n", sel0, regs[sel0]);
+		if (sel1) {
+			regs[sel1] = readl_relaxed(mpp->reg_base + RKVDEC_SEL_VAL1_BASE);
+			mpp_debug(DEBUG_GET_PERF_VAL, "sel[%d]:%u\n", sel1, regs[sel1]);
+		}
+		if (sel2) {
+			regs[sel2] = readl_relaxed(mpp->reg_base + RKVDEC_SEL_VAL2_BASE);
+			mpp_debug(DEBUG_GET_PERF_VAL, "sel[%d]:%u\n", sel2, regs[sel2]);
+		}
+	}
+
+	return 0;
+}
+
+static int rkvdec2_finish(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 dec_get;
+	s32 dec_length;
+	u32 reg_ret_status;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct mpp_request *req;
+	u32 s, e;
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		/* read perf register */
+		if (req->offset >= RKVDEC_PERF_SEL_OFFSET) {
+			u32 off = req->offset - RKVDEC_PERF_SEL_OFFSET;
+
+			s = off / sizeof(u32);
+			e = s + req->size / sizeof(u32);
+			rkvdec2_read_perf_sel(mpp, task->reg_sel, s, e);
+		} else {
+			s = req->offset / sizeof(u32);
+			e = s + req->size / sizeof(u32);
+			mpp_read_req(mpp, task->reg, s, e);
+		}
+	}
+	/* revert hack for irq status */
+	reg_ret_status = mpp->var->hw_info->reg_ret_status;
+	task->reg[reg_ret_status] = task->irq_status;
+
+	/* revert hack for decoded length */
+	dec_get = mpp_read_relaxed(mpp, RKVDEC_REG_RLC_BASE);
+	dec_length = dec_get - task->strm_addr;
+	task->reg[RKVDEC_REG_RLC_BASE_INDEX] = dec_length << 10;
+	mpp_debug(DEBUG_REGISTER, "dec_get %08x dec_length %d\n", dec_get, dec_length);
+
+	if (mpp->srv->timing_en) {
+		s64 time_diff;
+
+		mpp_task->on_finish = ktime_get();
+		set_bit(TASK_TIMING_FINISH, &mpp_task->state);
+
+		time_diff = ktime_us_delta(mpp_task->on_finish, mpp_task->on_create);
+
+		if (mpp->timing_check && time_diff > (s64)mpp->timing_check)
+			mpp_task_dump_timing(mpp_task, time_diff);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int rkvdec2_result(struct mpp_dev *mpp, struct mpp_task *mpp_task,
+		   struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (req->offset >= RKVDEC_PERF_SEL_OFFSET) {
+			u32 off = req->offset - RKVDEC_PERF_SEL_OFFSET;
+
+			if (copy_to_user(req->data,
+					 (u8 *)task->reg_sel + off,
+					 req->size)) {
+				mpp_err("copy_to_user perf_sel fail\n");
+				return -EIO;
+			}
+		} else {
+			if (copy_to_user(req->data,
+					 (u8 *)task->reg + req->offset,
+					 req->size)) {
+				mpp_err("copy_to_user reg fail\n");
+				return -EIO;
+			}
+		}
+	}
+
+	return 0;
+}
+
+int rkvdec2_free_task(struct mpp_session *session, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+static int rkvdec2_control(struct mpp_session *session, struct mpp_request *req)
+{
+	switch (req->cmd) {
+	case MPP_CMD_SEND_CODEC_INFO: {
+		u32 i;
+		u32 cnt;
+		struct codec_info_elem elem;
+		struct rkvdec2_session_priv *priv;
+
+		if (!session || !session->priv) {
+			mpp_err("session info null\n");
+			return -EINVAL;
+		}
+		priv = session->priv;
+
+		cnt = req->size / sizeof(elem);
+		cnt = (cnt > DEC_INFO_BUTT) ? DEC_INFO_BUTT : cnt;
+		mpp_debug(DEBUG_IOCTL, "codec info count %d\n", cnt);
+		for (i = 0; i < cnt; i++) {
+			if (copy_from_user(&elem, req->data + i * sizeof(elem), sizeof(elem))) {
+				mpp_err("copy_from_user failed\n");
+				continue;
+			}
+			if (elem.type > DEC_INFO_BASE && elem.type < DEC_INFO_BUTT &&
+			    elem.flag > CODEC_INFO_FLAG_NULL && elem.flag < CODEC_INFO_FLAG_BUTT) {
+				elem.type = array_index_nospec(elem.type, DEC_INFO_BUTT);
+				priv->codec_info[elem.type].flag = elem.flag;
+				priv->codec_info[elem.type].val = elem.data;
+			} else {
+				mpp_err("codec info invalid, type %d, flag %d\n",
+					elem.type, elem.flag);
+			}
+		}
+	} break;
+	default: {
+		mpp_err("unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+int rkvdec2_free_session(struct mpp_session *session)
+{
+	if (session && session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvdec2_init_session(struct mpp_session *session)
+{
+	struct rkvdec2_session_priv *priv;
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+	session->priv = priv;
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int rkvdec2_procfs_remove(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvdec2_show_pref_sel_offset(struct seq_file *file, void *v)
+{
+	seq_printf(file, "0x%08x\n", RKVDEC_PERF_SEL_OFFSET);
+
+	return 0;
+}
+
+static int rkvdec2_procfs_init(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	char name[32];
+
+	if (!mpp->dev || !mpp->dev->of_node || !mpp->dev->of_node->name ||
+	    !mpp->srv || !mpp->srv->procfs)
+		return -EINVAL;
+
+	snprintf(name, sizeof(name) - 1, "%s%d",
+		 mpp->dev->of_node->name, mpp->core_id);
+	dec->procfs = proc_mkdir(name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_core", 0644,
+			      dec->procfs, &dec->core_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_cabac", 0644,
+			      dec->procfs, &dec->cabac_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_hevc_cabac", 0644,
+			      dec->procfs, &dec->hevc_cabac_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+	proc_create_single("perf_sel_offset", 0444,
+			   dec->procfs, rkvdec2_show_pref_sel_offset);
+	mpp_procfs_create_u32("task_count", 0644,
+			      dec->procfs, &mpp->task_index);
+
+	return 0;
+}
+#else
+static inline int rkvdec2_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvdec2_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_PM_DEVFREQ
+static int rkvdec2_devfreq_target(struct device *dev,
+				  unsigned long *freq, u32 flags)
+{
+	struct dev_pm_opp *opp;
+	unsigned long target_volt, target_freq;
+	int ret = 0;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct devfreq *devfreq = dec->devfreq;
+	struct devfreq_dev_status *stat = &devfreq->last_status;
+	unsigned long old_clk_rate = stat->current_frequency;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp)) {
+		dev_err(dev, "Failed to find opp for %lu Hz\n", *freq);
+		return PTR_ERR(opp);
+	}
+	target_freq = dev_pm_opp_get_freq(opp);
+	target_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	if (old_clk_rate == target_freq) {
+		dec->core_last_rate_hz = target_freq;
+		if (dec->volt == target_volt)
+			return ret;
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "Cannot set voltage %lu uV\n",
+				target_volt);
+			return ret;
+		}
+		dec->volt = target_volt;
+		return 0;
+	}
+
+	if (old_clk_rate < target_freq) {
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set voltage %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+
+	dev_dbg(dev, "%lu-->%lu\n", old_clk_rate, target_freq);
+	clk_set_rate(dec->core_clk_info.clk, target_freq);
+	stat->current_frequency = target_freq;
+	dec->core_last_rate_hz = target_freq;
+
+	if (old_clk_rate > target_freq) {
+		ret = regulator_set_voltage(dec->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set vol %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+	dec->volt = target_volt;
+
+	return ret;
+}
+
+static int rkvdec2_devfreq_get_dev_status(struct device *dev,
+					  struct devfreq_dev_status *stat)
+{
+	return 0;
+}
+
+static int rkvdec2_devfreq_get_cur_freq(struct device *dev,
+					unsigned long *freq)
+{
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	*freq = dec->core_last_rate_hz;
+
+	return 0;
+}
+
+static struct devfreq_dev_profile rkvdec2_devfreq_profile = {
+	.target	= rkvdec2_devfreq_target,
+	.get_dev_status	= rkvdec2_devfreq_get_dev_status,
+	.get_cur_freq = rkvdec2_devfreq_get_cur_freq,
+	.is_cooling_device = true,
+};
+
+static int devfreq_vdec2_ondemand_func(struct devfreq *df, unsigned long *freq)
+{
+	struct rkvdec2_dev *dec = df->data;
+
+	if (dec)
+		*freq = dec->core_rate_hz;
+	else
+		*freq = df->previous_freq;
+
+	return 0;
+}
+
+static int devfreq_vdec2_ondemand_handler(struct devfreq *devfreq,
+					  unsigned int event, void *data)
+{
+	return 0;
+}
+
+static struct devfreq_governor devfreq_vdec2_ondemand = {
+	.name = "vdec2_ondemand",
+	.get_target_freq = devfreq_vdec2_ondemand_func,
+	.event_handler = devfreq_vdec2_ondemand_handler,
+};
+
+static struct monitor_dev_profile vdec2_mdevp = {
+	.type = MONITOR_TYPE_DEV,
+	.low_temp_adjust = rockchip_monitor_dev_low_temp_adjust,
+	.high_temp_adjust = rockchip_monitor_dev_high_temp_adjust,
+};
+
+static int rkvdec2_devfreq_init(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct clk *clk_core = dec->core_clk_info.clk;
+	struct rockchip_opp_info *opp_info = &dec->opp_info;
+	int ret = 0;
+
+	if (!clk_core)
+		return 0;
+
+	dec->vdd = devm_regulator_get_optional(mpp->dev, "vdec");
+	if (IS_ERR_OR_NULL(dec->vdd)) {
+		if (PTR_ERR(dec->vdd) == -EPROBE_DEFER) {
+			dev_warn(mpp->dev, "vdec regulator not ready, retry\n");
+
+			return -EPROBE_DEFER;
+		}
+		dev_info(mpp->dev, "no regulator, devfreq is disabled\n");
+
+		return 0;
+	}
+
+	ret = rockchip_init_opp_table(mpp->dev, opp_info, NULL, "vdec");
+	if (ret) {
+		dev_err(mpp->dev, "failed to init_opp_table\n");
+		return ret;
+	}
+
+	ret = devfreq_add_governor(&devfreq_vdec2_ondemand);
+	if (ret) {
+		dev_err(mpp->dev, "failed to add vdec2_ondemand governor\n");
+		goto governor_err;
+	}
+
+	rkvdec2_devfreq_profile.initial_freq = clk_get_rate(clk_core);
+
+	dec->devfreq = devm_devfreq_add_device(mpp->dev,
+					       &rkvdec2_devfreq_profile,
+					       "vdec2_ondemand", (void *)dec);
+	if (IS_ERR(dec->devfreq)) {
+		ret = PTR_ERR(dec->devfreq);
+		dec->devfreq = NULL;
+		goto devfreq_err;
+	}
+	dec->devfreq->last_status.total_time = 1;
+	dec->devfreq->last_status.busy_time = 1;
+
+	devfreq_register_opp_notifier(mpp->dev, dec->devfreq);
+
+	vdec2_mdevp.data = dec->devfreq;
+	vdec2_mdevp.opp_info = opp_info;
+	dec->mdev_info = rockchip_system_monitor_register(mpp->dev, &vdec2_mdevp);
+	if (IS_ERR(dec->mdev_info)) {
+		dev_dbg(mpp->dev, "without system monitor\n");
+		dec->mdev_info = NULL;
+	}
+
+	return 0;
+
+devfreq_err:
+	devfreq_remove_governor(&devfreq_vdec2_ondemand);
+governor_err:
+	dev_pm_opp_of_remove_table(mpp->dev);
+
+	return ret;
+}
+
+static int rkvdec2_devfreq_remove(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	if (dec->mdev_info)
+		rockchip_system_monitor_unregister(dec->mdev_info);
+	if (dec->devfreq)
+		devfreq_unregister_opp_notifier(mpp->dev, dec->devfreq);
+	devfreq_remove_governor(&devfreq_vdec2_ondemand);
+	rockchip_uninit_opp_table(mpp->dev, &dec->opp_info);
+
+	return 0;
+}
+
+void mpp_devfreq_set_core_rate(struct mpp_dev *mpp, enum MPP_CLOCK_MODE mode)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	if (dec->devfreq) {
+		unsigned long core_rate_hz;
+
+		mutex_lock(&dec->devfreq->lock);
+		core_rate_hz = mpp_get_clk_info_rate_hz(&dec->core_clk_info, mode);
+		if (dec->core_rate_hz != core_rate_hz) {
+			dec->core_rate_hz = core_rate_hz;
+			update_devfreq(dec->devfreq);
+		}
+		mutex_unlock(&dec->devfreq->lock);
+	}
+
+	mpp_clk_set_rate(&dec->core_clk_info, mode);
+}
+#else
+static inline int rkvdec2_devfreq_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvdec2_devfreq_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+void mpp_devfreq_set_core_rate(struct mpp_dev *mpp, enum MPP_CLOCK_MODE mode)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mpp_clk_set_rate(&dec->core_clk_info, mode);
+}
+#endif
+
+static int rkvdec2_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mutex_init(&dec->sip_reset_lock);
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_RKVDEC];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->core_clk_info, "clk_core");
+	if (ret)
+		mpp_err("failed on clk_get clk_core\n");
+	ret = mpp_get_clk_info(mpp, &dec->cabac_clk_info, "clk_cabac");
+	if (ret)
+		mpp_err("failed on clk_get clk_cabac\n");
+	ret = mpp_get_clk_info(mpp, &dec->hevc_cabac_clk_info, "clk_hevc_cabac");
+	if (ret)
+		mpp_err("failed on clk_get clk_hevc_cabac\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->core_clk_info, CLK_MODE_DEFAULT, 200 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->cabac_clk_info, CLK_MODE_DEFAULT, 200 * MHZ);
+	mpp_set_clk_info_rate_hz(&dec->hevc_cabac_clk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	dec->cycle_clk = &dec->aclk_info;
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load", &dec->default_max_load);
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	dec->rst_niu_a = mpp_reset_control_get(mpp, RST_TYPE_NIU_A, "niu_a");
+	if (!dec->rst_niu_a)
+		mpp_err("No niu aclk reset resource define\n");
+	dec->rst_niu_h = mpp_reset_control_get(mpp, RST_TYPE_NIU_H, "niu_h");
+	if (!dec->rst_niu_h)
+		mpp_err("No niu hclk reset resource define\n");
+	dec->rst_core = mpp_reset_control_get(mpp, RST_TYPE_CORE, "video_core");
+	if (!dec->rst_core)
+		mpp_err("No core reset resource define\n");
+	dec->rst_cabac = mpp_reset_control_get(mpp, RST_TYPE_CABAC, "video_cabac");
+	if (!dec->rst_cabac)
+		mpp_err("No cabac reset resource define\n");
+	dec->rst_hevc_cabac = mpp_reset_control_get(mpp, RST_TYPE_HEVC_CABAC, "video_hevc_cabac");
+	if (!dec->rst_hevc_cabac)
+		mpp_err("No hevc cabac reset resource define\n");
+
+	ret = rkvdec2_devfreq_init(mpp);
+	if (ret)
+		mpp_err("failed to add vdec devfreq\n");
+
+	return ret;
+}
+
+static int rkvdec2_rk3568_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	dec->fix = mpp_dma_alloc(mpp->dev, FIX_RK3568_BUF_SIZE);
+	ret = dec->fix ? 0 : -ENOMEM;
+	if (!ret)
+		rkvdec2_3568_hack_data_setup(dec->fix);
+	else
+		dev_err(mpp->dev, "failed to create buffer for hack\n");
+
+	ret = rkvdec2_init(mpp);
+
+	return ret;
+}
+
+static int rkvdec2_rk3576_init(struct mpp_dev *mpp)
+{
+	int ret;
+
+	rk3576_workaround_init(mpp);
+
+	ret = rkvdec2_init(mpp);
+
+	return ret;
+}
+
+static int rkvdec2_rk3568_exit(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	rkvdec2_devfreq_remove(mpp);
+
+	if (dec->fix)
+		mpp_dma_free(dec->fix);
+
+	return 0;
+}
+
+static int rkvdec2_rk3576_exit(struct mpp_dev *mpp)
+{
+	rkvdec2_devfreq_remove(mpp);
+
+	rk3576_workaround_exit(mpp);
+
+	return 0;
+}
+
+static int rkvdec2_clk_on(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+	mpp_clk_safe_enable(dec->core_clk_info.clk);
+	mpp_clk_safe_enable(dec->cabac_clk_info.clk);
+	mpp_clk_safe_enable(dec->hevc_cabac_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvdec2_clk_off(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	clk_disable_unprepare(dec->aclk_info.clk);
+	clk_disable_unprepare(dec->hclk_info.clk);
+	clk_disable_unprepare(dec->core_clk_info.clk);
+	clk_disable_unprepare(dec->cabac_clk_info.clk);
+	clk_disable_unprepare(dec->hevc_cabac_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvdec2_get_freq(struct mpp_dev *mpp,
+			    struct mpp_task *mpp_task)
+{
+	u32 task_cnt;
+	u32 workload;
+	struct mpp_task *loop = NULL, *n;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+	/* if not set max load, consider not have advanced mode */
+	if (!dec->default_max_load || !task->pixels)
+		return 0;
+
+	task_cnt = 1;
+	workload = task->pixels;
+	/* calc workload in pending list */
+	mutex_lock(&mpp->queue->pending_lock);
+	list_for_each_entry_safe(loop, n,
+				 &mpp->queue->pending_list,
+				 queue_link) {
+		struct rkvdec2_task *loop_task = to_rkvdec2_task(loop);
+
+		task_cnt++;
+		workload += loop_task->pixels;
+	}
+	mutex_unlock(&mpp->queue->pending_lock);
+
+	if (workload > dec->default_max_load)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	mpp_debug(DEBUG_TASK_INFO, "pending task %d, workload %d, clk_mode=%d\n",
+		  task_cnt, workload, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvdec2_set_freq(struct mpp_dev *mpp,
+			    struct mpp_task *mpp_task)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec2_task *task =  to_rkvdec2_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->cabac_clk_info, task->clk_mode);
+	mpp_clk_set_rate(&dec->hevc_cabac_clk_info, task->clk_mode);
+	mpp_devfreq_set_core_rate(mpp, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvdec2_vdpu382_reset(struct mpp_dev *mpp)
+{
+	int ret = 0;
+
+	/*
+	 * only for rk3528
+	 * use mmu reset as soft reset
+	 * rkvdec will reset together when rkvdec_mmu force reset
+	 */
+	ret = rockchip_iommu_force_reset(mpp->dev);
+	mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+	if (ret) {
+		mpp_err("soft mmu reset fail, ret %d\n", ret);
+		return rkvdec2_reset(mpp);
+	}
+
+	return ret;
+
+}
+
+static int rkvdec2_rk3562_reset(struct mpp_dev *mpp)
+{
+	int ret = 0;
+
+	/*
+	 * only for rk3562
+	 * use mmu reset and cru reset
+	 * rkvdec will reset together when rkvdec_mmu force reset
+	 */
+	ret = rockchip_iommu_force_reset(mpp->dev);
+	mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+	if (ret)
+		mpp_err("soft mmu reset fail, ret %d\n", ret);
+
+	return rkvdec2_reset(mpp);
+
+}
+
+static int rkvdec2_sip_reset(struct mpp_dev *mpp)
+{
+	mpp_debug_enter();
+
+	if (IS_REACHABLE(CONFIG_ROCKCHIP_SIP)) {
+		/* sip reset */
+		rockchip_dmcfreq_lock();
+		sip_smc_vpu_reset(0, 0, 0);
+		rockchip_dmcfreq_unlock();
+	} else {
+		rkvdec2_reset(mpp);
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int rkvdec2_reset(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mpp_debug_enter();
+
+	/* cru reset */
+	if (dec->rst_a && dec->rst_h) {
+		mpp_debug(DEBUG_RESET, "cru reset in\n");
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(dec->rst_niu_a);
+		mpp_safe_reset(dec->rst_niu_h);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		mpp_safe_reset(dec->rst_core);
+		mpp_safe_reset(dec->rst_cabac);
+		mpp_safe_reset(dec->rst_hevc_cabac);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_niu_h);
+		mpp_safe_unreset(dec->rst_niu_a);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		mpp_safe_unreset(dec->rst_core);
+		mpp_safe_unreset(dec->rst_cabac);
+		mpp_safe_unreset(dec->rst_hevc_cabac);
+		mpp_pmu_idle_request(mpp, false);
+		mpp_debug(DEBUG_RESET, "cru reset out\n");
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec_vdpu383_reset(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link = dec->link_dec;
+	int ret = 0;
+	u32 irq_status = 0;
+
+	mpp_debug_enter();
+
+	/* disable irq */
+	writel(link->info->ip_en_val & BIT(15), link->reg_base + link->info->ip_en_base);
+	/* use ip reset to reset core and mmu */
+	writel(link->info->ip_reset_en, link->reg_base + link->info->ip_reset_base);
+	ret = readl_relaxed_poll_timeout(link->reg_base + link->info->status_base,
+					 irq_status,
+					 irq_status & 0x800,
+					 0, 200);
+	if (ret)
+		dev_err(mpp->dev, "reset timeout\n");
+	/* clear reset ready status bit */
+	writel(link->info->ip_reset_mask, link->reg_base + link->info->status_base);
+
+	/* clear irq and status */
+	writel_relaxed(0xffff0000, link->reg_base + link->info->irq_base);
+	writel_relaxed(0xffff0000, link->reg_base + link->info->status_base);
+
+	/* enable irq */
+	writel(link->info->ip_en_val, link->reg_base + link->info->ip_en_base);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static struct mpp_hw_ops rkvdec_v2_hw_ops = {
+	.init = rkvdec2_init,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec2_reset,
+};
+
+static struct mpp_hw_ops rkvdec_rk3568_hw_ops = {
+	.init = rkvdec2_rk3568_init,
+	.exit = rkvdec2_rk3568_exit,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec2_sip_reset,
+};
+
+static struct mpp_hw_ops rkvdec_rk3588_hw_ops = {
+	.init = rkvdec2_init,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec2_sip_reset,
+};
+
+static struct mpp_hw_ops rkvdec_vdpu382_hw_ops = {
+	.init = rkvdec2_init,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec2_vdpu382_reset,
+};
+
+static struct mpp_hw_ops rkvdec_rk3562_hw_ops = {
+	.init = rkvdec2_init,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec2_rk3562_reset,
+};
+
+static struct mpp_hw_ops rkvdec_rk3576_hw_ops = {
+	.init = rkvdec2_rk3576_init,
+	.exit = rkvdec2_rk3576_exit,
+	.clk_on = rkvdec2_clk_on,
+	.clk_off = rkvdec2_clk_off,
+	.get_freq = rkvdec2_get_freq,
+	.set_freq = rkvdec2_set_freq,
+	.reset = rkvdec_vdpu383_reset,
+	.hack_run = rk3576_workaround_run,
+};
+
+static struct mpp_dev_ops rkvdec_v2_dev_ops = {
+	.alloc_task = rkvdec2_alloc_task,
+	.run = rkvdec2_run,
+	.irq = rkvdec2_irq,
+	.isr = rkvdec2_isr,
+	.finish = rkvdec2_finish,
+	.result = rkvdec2_result,
+	.free_task = rkvdec2_free_task,
+	.ioctl = rkvdec2_control,
+	.init_session = rkvdec2_init_session,
+	.free_session = rkvdec2_free_session,
+};
+
+static struct mpp_dev_ops rkvdec_rk3568_dev_ops = {
+	.alloc_task = rkvdec2_rk3568_alloc_task,
+	.run = rkvdec2_rk3568_run,
+	.irq = rkvdec2_irq,
+	.isr = rkvdec2_isr,
+	.finish = rkvdec2_finish,
+	.result = rkvdec2_result,
+	.free_task = rkvdec2_free_task,
+	.ioctl = rkvdec2_control,
+	.init_session = rkvdec2_init_session,
+	.free_session = rkvdec2_free_session,
+	.dump_dev = rkvdec_link_dump,
+};
+
+static struct mpp_dev_ops rkvdec_vdpu383_dev_ops = {
+	.alloc_task = rkvdec2_alloc_task,
+	.run = rkvdec_vdpu383_run,
+	.irq = rkvdec_vdpu383_irq,
+	.isr = rkvdec_vdpu383_isr,
+	.finish = rkvdec2_finish,
+	.result = rkvdec2_result,
+	.free_task = rkvdec2_free_task,
+	.ioctl = rkvdec2_control,
+	.init_session = rkvdec2_init_session,
+	.free_session = rkvdec2_free_session,
+	.link_irq = rkvdec_vdpu383_link_irq,
+};
+
+static const struct mpp_dev_var rkvdec_v2_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v2_hw_info,
+	.trans_info = rkvdec_v2_trans,
+	.hw_ops = &rkvdec_v2_hw_ops,
+	.dev_ops = &rkvdec_v2_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_rk3568_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_rk356x_hw_info,
+	.trans_info = rkvdec_v2_trans,
+	.hw_ops = &rkvdec_rk3568_hw_ops,
+	.dev_ops = &rkvdec_rk3568_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_rk3528_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_vdpu382_hw_info,
+	.trans_info = rkvdec_v2_trans,
+	.hw_ops = &rkvdec_vdpu382_hw_ops,
+	.dev_ops = &rkvdec_v2_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_rk3562_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_vdpu382_hw_info,
+	.trans_info = rkvdec_v2_trans,
+	.hw_ops = &rkvdec_rk3562_hw_ops,
+	.dev_ops = &rkvdec_v2_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_rk3588_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_v2_hw_info,
+	.trans_info = rkvdec_v2_trans,
+	.hw_ops = &rkvdec_rk3588_hw_ops,
+	.dev_ops = &rkvdec_v2_dev_ops,
+};
+
+static const struct mpp_dev_var rkvdec_rk3576_data = {
+	.device_type = MPP_DEVICE_RKVDEC,
+	.hw_info = &rkvdec_vdpu383_hw_info,
+	.trans_info = rkvdec_vdpu383_trans,
+	.hw_ops = &rkvdec_rk3576_hw_ops,
+	.dev_ops = &rkvdec_vdpu383_dev_ops,
+};
+
+static const struct of_device_id mpp_rkvdec2_dt_match[] = {
+	{
+		.compatible = "rockchip,rkv-decoder-v2",
+		.data = &rkvdec_v2_data,
+	},
+#ifdef CONFIG_CPU_RK3568
+	{
+		.compatible = "rockchip,rkv-decoder-rk3568",
+		.data = &rkvdec_rk3568_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3588
+	{
+		.compatible = "rockchip,rkv-decoder-v2-ccu",
+		.data = &rkvdec_rk3588_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3528
+	{
+		.compatible = "rockchip,rkv-decoder-rk3528",
+		.data = &rkvdec_rk3528_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3562
+	{
+		.compatible = "rockchip,rkv-decoder-rk3562",
+		.data = &rkvdec_rk3562_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3576
+	{
+		.compatible = "rockchip,rkv-decoder-rk3576",
+		.data = &rkvdec_rk3576_data,
+	},
+#endif
+	{},
+};
+
+static int rkvdec2_ccu_remove(struct device *dev)
+{
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+
+	return 0;
+}
+
+static int rkvdec2_ccu_probe(struct platform_device *pdev)
+{
+	struct rkvdec2_ccu *ccu;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+	u32 ccu_mode;
+
+	ccu = devm_kzalloc(dev, sizeof(*ccu), GFP_KERNEL);
+	if (!ccu)
+		return -ENOMEM;
+
+	ccu->dev = dev;
+	/* use task-level soft ccu default */
+	ccu->ccu_mode = RKVDEC2_CCU_TASK_SOFT;
+	atomic_set(&ccu->power_enabled, 0);
+	INIT_LIST_HEAD(&ccu->unused_list);
+	INIT_LIST_HEAD(&ccu->used_list);
+	platform_set_drvdata(pdev, ccu);
+
+	if (!of_property_read_u32(dev->of_node, "rockchip,ccu-mode", &ccu_mode)) {
+		if (ccu_mode <= RKVDEC2_CCU_MODE_NULL || ccu_mode >= RKVDEC2_CCU_MODE_BUTT)
+			ccu_mode = RKVDEC2_CCU_TASK_SOFT;
+		ccu->ccu_mode = (enum RKVDEC2_CCU_MODE)ccu_mode;
+	}
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "ccu");
+	if (!res) {
+		dev_err(dev, "no memory resource defined\n");
+		return -ENODEV;
+	}
+
+	ccu->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!ccu->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		return -ENODEV;
+	}
+
+	ccu->aclk_info.clk = devm_clk_get(dev, "aclk_ccu");
+	if (!ccu->aclk_info.clk)
+		mpp_err("failed on clk_get ccu aclk\n");
+
+	ccu->rst_a = devm_reset_control_get(dev, "video_ccu");
+	if (ccu->rst_a)
+		mpp_safe_unreset(ccu->rst_a);
+	else
+		mpp_err("failed on clk_get ccu reset\n");
+
+	/* power domain autosuspend delay 2s */
+	pm_runtime_set_autosuspend_delay(dev, 2000);
+	pm_runtime_use_autosuspend(dev);
+	device_init_wakeup(dev, true);
+	pm_runtime_enable(dev);
+
+	dev_info(dev, "ccu-mode: %d\n", ccu->ccu_mode);
+	return 0;
+}
+
+static int rkvdec2_alloc_rcbbuf(struct platform_device *pdev, struct rkvdec2_dev *dec)
+{
+	int ret;
+	u32 vals[2];
+	dma_addr_t iova;
+	u32 rcb_size, sram_size;
+	struct device_node *sram_np;
+	struct resource sram_res;
+	resource_size_t sram_start, sram_end;
+	struct iommu_domain *domain;
+	struct device *dev = &pdev->dev;
+
+	/* get rcb iova start and size */
+	ret = device_property_read_u32_array(dev, "rockchip,rcb-iova", vals, 2);
+	if (ret) {
+		dev_err(dev, "could not find property rcb-iova\n");
+		return ret;
+	}
+	iova = PAGE_ALIGN(vals[0]);
+	rcb_size = PAGE_ALIGN(vals[1]);
+	if (!rcb_size) {
+		dev_err(dev, "rcb_size invalid.\n");
+		return -EINVAL;
+	}
+	/* alloc reserve iova for rcb */
+	ret = mpp_iommu_reserve_iova(dec->mpp.iommu_info, iova, rcb_size);
+	if (ret) {
+		dev_err(dev, "alloc rcb iova error.\n");
+		return ret;
+	}
+	/* get sram device node */
+	sram_np = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	if (!sram_np) {
+		dev_err(dev, "could not find phandle sram\n");
+		return -ENODEV;
+	}
+	/* get sram start and size */
+	ret = of_address_to_resource(sram_np, 0, &sram_res);
+	of_node_put(sram_np);
+	if (ret) {
+		dev_err(dev, "find sram res error\n");
+		return ret;
+	}
+	/* check sram start and size is PAGE_SIZE align */
+	sram_start = round_up(sram_res.start, PAGE_SIZE);
+	sram_end = round_down(sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (sram_end <= sram_start) {
+		dev_err(dev, "no available sram, phy_start %pa, phy_end %pa\n",
+			&sram_start, &sram_end);
+		return -ENOMEM;
+	}
+	sram_size = sram_end - sram_start;
+	sram_size = rcb_size < sram_size ? rcb_size : sram_size;
+	/* iova map to sram */
+	domain = dec->mpp.iommu_info->domain;
+	ret = iommu_map(domain, iova, sram_start, sram_size, IOMMU_READ | IOMMU_WRITE);
+	if (ret) {
+		dev_err(dev, "sram iommu_map error.\n");
+		return ret;
+	}
+	/* alloc dma for the remaining buffer, sram + dma */
+	if (sram_size < rcb_size) {
+		struct page *page;
+		size_t page_size = PAGE_ALIGN(rcb_size - sram_size);
+
+		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, get_order(page_size));
+		if (!page) {
+			dev_err(dev, "unable to allocate pages\n");
+			ret = -ENOMEM;
+			goto err_sram_map;
+		}
+		/* iova map to dma */
+		ret = iommu_map(domain, iova + sram_size, page_to_phys(page),
+				page_size, IOMMU_READ | IOMMU_WRITE);
+		if (ret) {
+			dev_err(dev, "page iommu_map error.\n");
+			__free_pages(page, get_order(page_size));
+			goto err_sram_map;
+		}
+		dec->rcb_page = page;
+	}
+	dec->sram_size = sram_size;
+	dec->rcb_size = rcb_size;
+	dec->rcb_iova = iova;
+	dev_info(dev, "sram_start %pa\n", &sram_start);
+	dev_info(dev, "rcb_iova %pad\n", &dec->rcb_iova);
+	dev_info(dev, "sram_size %u\n", dec->sram_size);
+	dev_info(dev, "rcb_size %u\n", dec->rcb_size);
+
+	ret = of_property_read_u32(dev->of_node, "rockchip,rcb-min-width", &dec->rcb_min_width);
+	if (!ret && dec->rcb_min_width)
+		dev_info(dev, "min_width %u\n", dec->rcb_min_width);
+
+	/* if have, read rcb_info */
+	dec->rcb_info_count = device_property_count_u32(dev, "rockchip,rcb-info");
+	if (dec->rcb_info_count > 0 &&
+	    dec->rcb_info_count <= (sizeof(dec->rcb_infos) / sizeof(u32))) {
+		u32 i;
+
+		ret = device_property_read_u32_array(dev, "rockchip,rcb-info",
+						     dec->rcb_infos, dec->rcb_info_count);
+		if (!ret) {
+			dev_info(dev, "rcb_info_count %u\n", dec->rcb_info_count);
+			for (i = 0; i < dec->rcb_info_count; i += 2)
+				dev_info(dev, "[%u, %u]\n",
+					 dec->rcb_infos[i], dec->rcb_infos[i+1]);
+		}
+	}
+
+	return 0;
+
+err_sram_map:
+	iommu_unmap(domain, iova, sram_size);
+
+	return ret;
+}
+
+static int rkvdec2_core_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rkvdec2_dev *dec;
+	struct mpp_dev *mpp;
+	struct device *dev = &pdev->dev;
+	irq_handler_t irq_proc = NULL;
+
+	dec = devm_kzalloc(dev, sizeof(*dec), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+	mpp->is_irq_startup = false;
+	if (dev->of_node) {
+		struct device_node *np = pdev->dev.of_node;
+		const struct of_device_id *match;
+
+		match = of_match_node(mpp_rkvdec2_dt_match, dev->of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+		mpp->core_id = of_alias_get_id(np, "rkvdec");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return ret;
+	}
+	dec->mmu_base = ioremap(dec->mpp.io_base + 0x600, 0x80);
+	if (!dec->mmu_base)
+		dev_err(dev, "mmu base map failed!\n");
+
+	/* attach core to ccu */
+	ret = rkvdec2_attach_ccu(dev, dec);
+	if (ret) {
+		dev_err(dev, "attach ccu failed\n");
+		return ret;
+	}
+
+	/* alloc rcb buffer */
+	rkvdec2_alloc_rcbbuf(pdev, dec);
+
+	/* set device for link */
+	ret = rkvdec2_ccu_link_init(pdev, dec);
+	if (ret)
+		return ret;
+
+	mpp->dev_ops->alloc_task = rkvdec2_ccu_alloc_task;
+	if (dec->ccu->ccu_mode == RKVDEC2_CCU_TASK_SOFT) {
+		mpp->dev_ops->task_worker = rkvdec2_soft_ccu_worker;
+		irq_proc = rkvdec2_soft_ccu_irq;
+		mpp->fault_handler = rkvdec2_soft_ccu_iommu_fault_handle;
+	} else if (dec->ccu->ccu_mode == RKVDEC2_CCU_TASK_HARD) {
+		if (mpp->core_id == 0 && mpp->task_capacity > 1) {
+			dec->link_dec->task_capacity = mpp->task_capacity;
+			ret = rkvdec2_ccu_alloc_table(dec, dec->link_dec);
+			if (ret)
+				return ret;
+		}
+		mpp->dev_ops->task_worker = rkvdec2_hard_ccu_worker;
+		irq_proc = rkvdec2_hard_ccu_irq;
+		mpp->fault_handler = rkvdec2_hard_ccu_iommu_fault_handle;
+	}
+	kthread_init_work(&mpp->work, mpp->dev_ops->task_worker);
+
+	/* get irq request */
+	ret = devm_request_threaded_irq(dev, mpp->irq, irq_proc, NULL,
+					IRQF_SHARED, dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+	/*make sure mpp->irq is startup then can be en/disable*/
+	mpp->is_irq_startup = true;
+
+	mpp->session_max_buffers = RKVDEC_SESSION_MAX_BUFFERS;
+	rkvdec2_procfs_init(mpp);
+
+	/* if is main-core, register to mpp service */
+	if (mpp->core_id == 0)
+		mpp_dev_register_srv(mpp, mpp->srv);
+
+	return ret;
+}
+
+static int rkvdec2_probe_default(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct rkvdec2_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	irq_handler_t irq_proc = NULL;
+	int ret = 0;
+
+	dec = devm_kzalloc(dev, sizeof(*dec), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_rkvdec2_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return ret;
+	}
+
+	rkvdec2_alloc_rcbbuf(pdev, dec);
+	rkvdec2_link_init(pdev, dec);
+
+	irq_proc = mpp_dev_irq;
+	if (dec->link_dec && (mpp->task_capacity > 1)) {
+		irq_proc = rkvdec2_link_irq_proc;
+		mpp->dev_ops->process_task = rkvdec2_link_process_task;
+		mpp->dev_ops->wait_result = rkvdec2_link_wait_result;
+		mpp->dev_ops->task_worker = rkvdec2_link_worker;
+		mpp->dev_ops->deinit = rkvdec2_link_session_deinit;
+		kthread_init_work(&mpp->work, rkvdec2_link_worker);
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq, irq_proc, NULL,
+					IRQF_SHARED, dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = RKVDEC_SESSION_MAX_BUFFERS;
+	rkvdec2_procfs_init(mpp);
+	if (dec->link_dec && (mpp->task_capacity > 1))
+		rkvdec2_link_procfs_init(mpp);
+
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+
+	dev_info(dev, "probing finish\n");
+
+	/* work workaround */
+	if (dec->fix && mpp->hw_ops->hack_run)
+		mpp->hw_ops->hack_run(mpp);
+
+	return ret;
+}
+
+static int rkvdec2_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "%s, probing start\n", np->name);
+
+	if (strstr(np->name, "ccu"))
+		ret = rkvdec2_ccu_probe(pdev);
+	else if (strstr(np->name, "core"))
+		ret = rkvdec2_core_probe(pdev);
+	else
+		ret = rkvdec2_probe_default(pdev);
+
+	dev_info(dev, "probing finish\n");
+
+	return ret;
+}
+
+static int rkvdec2_free_rcbbuf(struct platform_device *pdev, struct rkvdec2_dev *dec)
+{
+	struct iommu_domain *domain;
+
+	if (dec->rcb_page) {
+		size_t page_size = PAGE_ALIGN(dec->rcb_size - dec->sram_size);
+		int order = min(get_order(page_size), MAX_ORDER);
+
+		__free_pages(dec->rcb_page, order);
+	}
+	if (dec->rcb_iova) {
+		domain = dec->mpp.iommu_info->domain;
+		iommu_unmap(domain, dec->rcb_iova, dec->rcb_size);
+	}
+
+	return 0;
+}
+
+static int rkvdec2_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	if (strstr(dev_name(dev), "ccu")) {
+		dev_info(dev, "remove ccu device\n");
+		rkvdec2_ccu_remove(dev);
+	} else {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+		struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+		dev_info(dev, "remove device\n");
+		if (dec->mmu_base) {
+			iounmap(dec->mmu_base);
+			dec->mmu_base = NULL;
+		}
+		rkvdec2_free_rcbbuf(pdev, dec);
+		mpp_dev_remove(mpp);
+		rkvdec2_procfs_remove(mpp);
+		rkvdec2_link_remove(mpp, dec->link_dec);
+	}
+
+	return 0;
+}
+
+static void rkvdec2_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	if (!strstr(dev_name(dev), "ccu"))
+		mpp_dev_shutdown(pdev);
+}
+
+static int __maybe_unused rkvdec2_runtime_suspend(struct device *dev)
+{
+	if (strstr(dev_name(dev), "ccu")) {
+		struct rkvdec2_ccu *ccu = dev_get_drvdata(dev);
+
+		mpp_clk_safe_disable(ccu->aclk_info.clk);
+	} else {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+		if (mpp->is_irq_startup) {
+			/* disable core irq */
+			disable_irq(mpp->irq);
+			if (mpp->iommu_info && mpp->iommu_info->got_irq)
+				/* disable mmu irq */
+				disable_irq(mpp->iommu_info->irq);
+		}
+
+		/*
+		 * to ensure hardware is fully idle,
+		 * reset and wait for reset ready before suspend.
+		 */
+		if (mpp->hw_ops->reset)
+			mpp->hw_ops->reset(mpp);
+		mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+
+		if (mpp->hw_ops->clk_off)
+			mpp->hw_ops->clk_off(mpp);
+
+		mpp_dev_load_clear(mpp);
+	}
+
+	return 0;
+}
+
+static int __maybe_unused rkvdec2_runtime_resume(struct device *dev)
+{
+	if (strstr(dev_name(dev), "ccu")) {
+		struct rkvdec2_ccu *ccu = dev_get_drvdata(dev);
+
+		mpp_clk_safe_enable(ccu->aclk_info.clk);
+	} else {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+		struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+		if (mpp->hw_ops->clk_on)
+			mpp->hw_ops->clk_on(mpp);
+		if (mpp->is_irq_startup) {
+			/* enable core irq */
+			enable_irq(mpp->irq);
+			/* enable mmu irq */
+			if (mpp->iommu_info && mpp->iommu_info->got_irq)
+				enable_irq(mpp->iommu_info->irq);
+		}
+		/* work workaround */
+		if (dec->fix && mpp->hw_ops->hack_run)
+			mpp->hw_ops->hack_run(mpp);
+	}
+
+	return 0;
+}
+
+static const struct dev_pm_ops rkvdec2_pm_ops = {
+	SET_RUNTIME_PM_OPS(rkvdec2_runtime_suspend, rkvdec2_runtime_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)
+};
+
+struct platform_driver rockchip_rkvdec2_driver = {
+	.probe = rkvdec2_probe,
+	.remove = rkvdec2_remove,
+	.shutdown = rkvdec2_shutdown,
+	.driver = {
+		.name = RKVDEC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_rkvdec2_dt_match),
+		.pm = &rkvdec2_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_rkvdec2_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_rkvdec2.h b/drivers/video/rockchip/mpp/mpp_rkvdec2.h
new file mode 100644
index 0000000000000..1d14d184933ff
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvdec2.h
@@ -0,0 +1,233 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Herman Chen <herman.chen@rock-chips.com>
+ *
+ */
+#ifndef __ROCKCHIP_MPP_RKVDEC2_H__
+#define __ROCKCHIP_MPP_RKVDEC2_H__
+
+#include <linux/iopoll.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/kernel.h>
+#include <linux/thermal.h>
+#include <linux/notifier.h>
+#include <linux/proc_fs.h>
+#include <linux/nospec.h>
+#include <linux/rockchip/rockchip_sip.h>
+#include <linux/regulator/consumer.h>
+
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_sip.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define RKVDEC_DRIVER_NAME		"mpp_rkvdec2"
+
+#define RKVDEC_REG_IMPORTANT_BASE	0x2c
+#define RKVDEC_REG_IMPORTANT_INDEX	11
+#define RKVDEC_SOFTREST_EN		BIT(20)
+
+#define	RKVDEC_SESSION_MAX_BUFFERS	40
+/* The maximum registers number of all the version */
+#define RKVDEC_REG_NUM			360
+
+#define REVDEC_GET_PROD_NUM(x)		(((x) >> 16) & 0xffff)
+#define RKVDEC_GET_FORMAT(x)		((x) & 0x3ff)
+
+#define RKVDEC_REG_START_EN_BASE       0x28
+
+#define RKVDEC_START_EN			BIT(0)
+
+#define RKVDEC_REG_YSTRIDE_INDEX	20
+#define RKVDEC_REG_CORE_CTRL_INDEX	28
+#define RKVDEC_REG_FILM_IDX_MASK	(0x3ff0000)
+
+#define RKVDEC_REG_RLC_BASE		0x200
+#define RKVDEC_REG_RLC_BASE_INDEX	(128)
+
+#define RKVDEC_REG_INT_EN		0x380
+
+#define RKVDEC_SOFT_RESET_READY		BIT(9)
+#define RKVDEC_CABAC_END_STA		BIT(8)
+#define RKVDEC_COLMV_REF_ERR_STA	BIT(7)
+#define RKVDEC_BUF_EMPTY_STA		BIT(6)
+#define RKVDEC_TIMEOUT_STA		BIT(5)
+#define RKVDEC_ERROR_STA		BIT(4)
+#define RKVDEC_BUS_STA			BIT(3)
+#define RKVDEC_READY_STA		BIT(2)
+#define RKVDEC_IRQ_RAW			BIT(1)
+#define RKVDEC_IRQ			BIT(0)
+#define RKVDEC_PERF_WORKING_CNT		0x41c
+
+/* perf sel reference register */
+#define RKVDEC_PERF_SEL_OFFSET		0x20000
+#define RKVDEC_PERF_SEL_NUM		64
+#define RKVDEC_PERF_SEL_BASE		0x424
+#define RKVDEC_SEL_VAL0_BASE		0x428
+#define RKVDEC_SEL_VAL1_BASE		0x42c
+#define RKVDEC_SEL_VAL2_BASE		0x430
+#define RKVDEC_SET_PERF_SEL(a, b, c)	((a) | ((b) << 8) | ((c) << 16))
+
+/* cache reference register */
+#define RKVDEC_REG_CACHE0_SIZE_BASE	0x51c
+#define RKVDEC_REG_CACHE1_SIZE_BASE	0x55c
+#define RKVDEC_REG_CACHE2_SIZE_BASE	0x59c
+#define RKVDEC_REG_CLR_CACHE0_BASE	0x510
+#define RKVDEC_REG_MAX_READS		0x518
+#define RKVDEC_REG_CLR_CACHE1_BASE	0x550
+#define RKVDEC_REG_CLR_CACHE2_BASE	0x590
+
+#define RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS	BIT(0)
+#define RKVDEC_CACHE_PERMIT_READ_ALLOCATE	BIT(1)
+#define RKVDEC_CACHE_LINE_SIZE_64_BYTES		BIT(4)
+
+#define to_rkvdec2_task(task)		\
+		container_of(task, struct rkvdec2_task, mpp_task)
+#define to_rkvdec2_dev(dev)		\
+		container_of(dev, struct rkvdec2_dev, mpp)
+
+enum RKVDEC_FMT {
+	RKVDEC_FMT_H265D	= 0,
+	RKVDEC_FMT_H264D	= 1,
+	RKVDEC_FMT_VP9D		= 2,
+	RKVDEC_FMT_AVS2		= 3,
+	RKVDEC_FMT_AV1D		= 4,
+};
+
+#define RKVDEC_MAX_RCB_NUM		(16)
+
+struct rcb_info_elem {
+	u32 index;
+	u32 size;
+};
+
+struct rkvdec2_rcb_info {
+	u32 cnt;
+	struct rcb_info_elem elem[RKVDEC_MAX_RCB_NUM];
+};
+
+struct rkvdec2_task {
+	struct mpp_task mpp_task;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[RKVDEC_REG_NUM];
+	struct reg_offset_info off_inf;
+
+	/* perf sel data back */
+	u32 reg_sel[RKVDEC_PERF_SEL_NUM];
+
+	u32 strm_addr;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	/* image info */
+	u32 width;
+	u32 height;
+	u32 pixels;
+
+	/* task index for link table rnunning list */
+	int slot_idx;
+	u32 need_hack;
+
+	/* link table DMA buffer */
+	struct mpp_dma_buffer *table;
+};
+
+struct rkvdec2_session_priv {
+	/* codec info from user */
+	struct {
+		/* show mode */
+		u32 flag;
+		/* item data */
+		u64 val;
+	} codec_info[DEC_INFO_BUTT];
+	/* rcb_info for sram */
+	struct rkvdec2_rcb_info rcb_inf;
+};
+
+struct rkvdec2_dev {
+	struct mpp_dev mpp;
+	/* sip smc reset lock */
+	struct mutex sip_reset_lock;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info core_clk_info;
+	struct mpp_clk_info cabac_clk_info;
+	struct mpp_clk_info hevc_cabac_clk_info;
+	struct mpp_clk_info *cycle_clk;
+
+	u32 default_max_load;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_niu_a;
+	struct reset_control *rst_niu_h;
+	struct reset_control *rst_core;
+	struct reset_control *rst_cabac;
+	struct reset_control *rst_hevc_cabac;
+
+#ifdef CONFIG_PM_DEVFREQ
+	struct regulator *vdd;
+	struct devfreq *devfreq;
+	unsigned long volt;
+	unsigned long core_rate_hz;
+	unsigned long core_last_rate_hz;
+	struct monitor_dev_info *mdev_info;
+	struct rockchip_opp_info opp_info;
+#endif
+
+	/* internal rcb-memory */
+	u32 sram_size;
+	u32 rcb_size;
+	dma_addr_t rcb_iova;
+	struct page *rcb_page;
+	u32 rcb_min_width;
+	u32 rcb_info_count;
+	u32 rcb_infos[RKVDEC_MAX_RCB_NUM * 2];
+
+	/* for link mode */
+	struct rkvdec_link_dev *link_dec;
+	struct mpp_dma_buffer *fix;
+
+	/* for ccu link mode */
+	struct rkvdec2_ccu *ccu;
+	u32 core_mask;
+	u32 task_index;
+	/* mmu info */
+	void __iomem *mmu_base;
+	u32 mmu_fault;
+};
+
+int mpp_set_rcbbuf(struct mpp_dev *mpp, struct mpp_session *session,
+		   struct mpp_task *task);
+int rkvdec2_task_init(struct mpp_dev *mpp, struct mpp_session *session,
+		      struct rkvdec2_task *task, struct mpp_task_msgs *msgs);
+void *rkvdec2_alloc_task(struct mpp_session *session,
+			 struct mpp_task_msgs *msgs);
+int rkvdec2_free_task(struct mpp_session *session, struct mpp_task *mpp_task);
+
+int rkvdec2_free_session(struct mpp_session *session);
+
+int rkvdec2_result(struct mpp_dev *mpp, struct mpp_task *mpp_task,
+		   struct mpp_task_msgs *msgs);
+int rkvdec2_reset(struct mpp_dev *mpp);
+
+void mpp_devfreq_set_core_rate(struct mpp_dev *mpp, enum MPP_CLOCK_MODE mode);
+
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_rkvdec2_link.c b/drivers/video/rockchip/mpp/mpp_rkvdec2_link.c
new file mode 100644
index 0000000000000..e13bf2d55c040
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvdec2_link.c
@@ -0,0 +1,2677 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Herman Chen <herman.chen@rock-chips.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_dmc.h>
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "mpp_rkvdec2_link.h"
+
+#include "hack/mpp_rkvdec2_link_hack_rk3568.c"
+
+#define RKVDEC2_LINK_HACK_TASK_FLAG	(0xff)
+
+/* vdpu381 link hw info for rk3588 */
+struct rkvdec_link_info rkvdec_link_v2_hw_info = {
+	.tb_reg_num = 218,
+	.tb_reg_next = 0,
+	.tb_reg_r = 1,
+	.tb_reg_second_en = 8,
+
+	.part_w_num = 6,
+	.part_r_num = 2,
+	.part_w[0] = {
+		.tb_reg_off = 4,
+		.reg_start = 8,
+		.reg_num = 28,
+	},
+	.part_w[1] = {
+		.tb_reg_off = 32,
+		.reg_start = 64,
+		.reg_num = 52,
+	},
+	.part_w[2] = {
+		.tb_reg_off = 84,
+		.reg_start = 128,
+		.reg_num = 16,
+	},
+	.part_w[3] = {
+		.tb_reg_off = 100,
+		.reg_start = 160,
+		.reg_num = 48,
+	},
+	.part_w[4] = {
+		.tb_reg_off = 148,
+		.reg_start = 224,
+		.reg_num = 16,
+	},
+	.part_w[5] = {
+		.tb_reg_off = 164,
+		.reg_start = 256,
+		.reg_num = 16,
+	},
+	.part_r[0] = {
+		.tb_reg_off = 180,
+		.reg_start = 224,
+		.reg_num = 10,
+	},
+	.part_r[1] = {
+		.tb_reg_off = 190,
+		.reg_start = 258,
+		.reg_num = 28,
+	},
+	.tb_reg_int = 180,
+	.tb_reg_cycle = 195,
+	.hack_setup = 0,
+	.reg_status = {
+		.dec_num_mask = 0x3fffffff,
+		.err_flag_base = 0x010,
+		.err_flag_bit = BIT(31),
+	},
+	.irq_base = 0x00,
+	.next_addr_base = 0x1c,
+	.err_mask = 0xf0,
+};
+
+/* vdpu34x link hw info for rk356x */
+struct rkvdec_link_info rkvdec_link_rk356x_hw_info = {
+	.tb_reg_num = 202,
+	.tb_reg_next = 0,
+	.tb_reg_r = 1,
+	.tb_reg_second_en = 8,
+
+	.part_w_num = 6,
+	.part_r_num = 2,
+	.part_w[0] = {
+		.tb_reg_off = 4,
+		.reg_start = 8,
+		.reg_num = 20,
+	},
+	.part_w[1] = {
+		.tb_reg_off = 24,
+		.reg_start = 64,
+		.reg_num = 52,
+	},
+	.part_w[2] = {
+		.tb_reg_off = 76,
+		.reg_start = 128,
+		.reg_num = 16,
+	},
+	.part_w[3] = {
+		.tb_reg_off = 92,
+		.reg_start = 160,
+		.reg_num = 40,
+	},
+	.part_w[4] = {
+		.tb_reg_off = 132,
+		.reg_start = 224,
+		.reg_num = 16,
+	},
+	.part_w[5] = {
+		.tb_reg_off = 148,
+		.reg_start = 256,
+		.reg_num = 16,
+	},
+	.part_r[0] = {
+		.tb_reg_off = 164,
+		.reg_start = 224,
+		.reg_num = 10,
+	},
+	.part_r[1] = {
+		.tb_reg_off = 174,
+		.reg_start = 258,
+		.reg_num = 28,
+	},
+	.tb_reg_int = 164,
+	.tb_reg_cycle = 179,
+	.hack_setup = 1,
+	.reg_status = {
+		.dec_num_mask = 0x3fffffff,
+		.err_flag_base = 0x010,
+		.err_flag_bit = BIT(31),
+	},
+	.irq_base = 0x00,
+	.next_addr_base = 0x1c,
+	.err_mask = 0xf0,
+};
+
+/* vdpu382 link hw info */
+struct rkvdec_link_info rkvdec_link_vdpu382_hw_info = {
+	.tb_reg_num = 222,
+	.tb_reg_next = 0,
+	.tb_reg_r = 1,
+	.tb_reg_second_en = 8,
+
+	.part_w_num = 6,
+	.part_r_num = 2,
+	.part_w[0] = {
+		.tb_reg_off = 4,
+		.reg_start = 8,
+		.reg_num = 28,
+	},
+	.part_w[1] = {
+		.tb_reg_off = 32,
+		.reg_start = 64,
+		.reg_num = 52,
+	},
+	.part_w[2] = {
+		.tb_reg_off = 84,
+		.reg_start = 128,
+		.reg_num = 16,
+	},
+	.part_w[3] = {
+		.tb_reg_off = 100,
+		.reg_start = 160,
+		.reg_num = 48,
+	},
+	.part_w[4] = {
+		.tb_reg_off = 148,
+		.reg_start = 224,
+		.reg_num = 16,
+	},
+	.part_w[5] = {
+		.tb_reg_off = 164,
+		.reg_start = 256,
+		.reg_num = 16,
+	},
+	.part_r[0] = {
+		.tb_reg_off = 180,
+		.reg_start = 224,
+		.reg_num = 12,
+	},
+	.part_r[1] = {
+		.tb_reg_off = 192,
+		.reg_start = 258,
+		.reg_num = 30,
+	},
+	.tb_reg_int = 180,
+	.hack_setup = 0,
+	.tb_reg_cycle = 197,
+	.reg_status = {
+		.dec_num_mask = 0x000fffff,
+		.err_flag_base = 0x024,
+		.err_flag_bit = BIT(8),
+	},
+	.irq_base = 0x00,
+	.next_addr_base = 0x1c,
+	.err_mask = 0xf0,
+};
+
+/* vdpu383 link hw info */
+struct rkvdec_link_info rkvdec_link_vdpu383_hw_info = {
+	.tb_reg_num = 256,
+	.tb_reg_next = 0,
+	.tb_reg_r = 1,
+	.tb_reg_second_en = -1,
+	.tb_reg_debug = 2,
+	.tb_reg_seg0 = 3,
+	.tb_reg_seg1 = 4,
+	.tb_reg_seg2 = 5,
+
+	.part_w_num = 3,
+	.part_r_num = 2,
+	.part_w[0] = {
+		.tb_reg_off = 80,
+		.reg_start = 8,
+		.reg_num = 24,
+	},
+	.part_w[1] = {
+		.tb_reg_off = 104,
+		.reg_start = 64,
+		.reg_num = 44,
+	},
+	.part_w[2] = {
+		.tb_reg_off = 148,
+		.reg_start = 128,
+		.reg_num = 108,
+	},
+	.part_r[0] = {
+		.tb_reg_off = 16,
+		.reg_start = 15,
+		.reg_num = 1,
+	},
+	.part_r[1] = {
+		.tb_reg_off = 20,
+		.reg_start = 320,
+		.reg_num = 40,
+	},
+	.tb_reg_int = 16,
+	.tb_reg_cycle = 27,
+	.reg_status = {
+		.dec_num_mask = 0x3fffffff,
+		.err_flag_base = 0x04c,
+		.err_flag_bit = 0x3fe,
+	},
+	.next_addr_base = 0x20,
+	.ip_reset_base = 0x44,
+	.ip_reset_en = BIT(0),
+	.irq_base = 0x48,
+	.irq_mask = 0x30000,
+	.status_base = 0x4c,
+	.status_mask = 0x3ff0000,
+	.err_mask = 0x3fe,
+	.ip_reset_mask = 0x8000000,
+	.ip_time_base = 0x54,
+	.en_base = 0x40,
+	.ip_en_base = 0x58,
+	.ip_en_val = 0x01000000,
+};
+
+static void rkvdec2_link_free_task(struct kref *ref);
+static void rkvdec2_link_timeout_proc(struct work_struct *work_s);
+static int rkvdec2_link_iommu_fault_handle(struct iommu_domain *iommu,
+					   struct device *iommu_dev,
+					   unsigned long iova,
+					   int status, void *arg);
+
+static void rkvdec_link_status_update(struct rkvdec_link_dev *dev)
+{
+	void __iomem *reg_base = dev->reg_base;
+	u32 error_ff0, error_ff1;
+	u32 enable_ff0, enable_ff1;
+	u32 loop_count = 10;
+	u32 val;
+	struct rkvdec_link_info *link_info = dev->info;
+	u32 dec_num_mask = link_info->reg_status.dec_num_mask;
+	u32 err_flag_base = link_info->reg_status.err_flag_base;
+	u32 err_flag_bit = link_info->reg_status.err_flag_bit;
+
+	error_ff1 = (readl(reg_base + err_flag_base) & err_flag_bit) ? 1 : 0;
+	enable_ff1 = readl(reg_base + RKVDEC_LINK_EN_BASE);
+
+	dev->irq_status = readl(reg_base + link_info->irq_base);
+	dev->iova_curr = readl(reg_base + RKVDEC_LINK_CFG_ADDR_BASE);
+	dev->link_mode = readl(reg_base + RKVDEC_LINK_MODE_BASE);
+	dev->total = readl(reg_base + RKVDEC_LINK_TOTAL_NUM_BASE);
+	dev->iova_next = readl(reg_base + link_info->next_addr_base);
+
+	do {
+		val = readl(reg_base + RKVDEC_LINK_DEC_NUM_BASE);
+		error_ff0 = (readl(reg_base + err_flag_base) & err_flag_bit) ? 1 : 0;
+		enable_ff0 = readl(reg_base + RKVDEC_LINK_EN_BASE);
+
+		if (error_ff0 == error_ff1 && enable_ff0 == enable_ff1)
+			break;
+
+		error_ff1 = error_ff0;
+		enable_ff1 = enable_ff0;
+	} while (--loop_count);
+
+	dev->error = error_ff0;
+	dev->decoded_status = val;
+	dev->decoded = val & dec_num_mask;
+	dev->enabled = enable_ff0;
+
+	if (!loop_count)
+		dev_info(dev->dev, "reach last 10 count\n");
+}
+
+static void rkvdec_link_node_dump(const char *func, struct rkvdec_link_dev *dev)
+{
+	u32 *table_base = (u32 *)dev->table->vaddr;
+	u32 reg_count = dev->link_reg_count;
+	u32 iova = (u32)dev->table->iova;
+	u32 *reg = NULL;
+	u32 i, j;
+
+	for (i = 0; i < dev->task_capacity; i++) {
+		reg = table_base + i * reg_count;
+
+		mpp_err("slot %d link config iova %08x:\n", i,
+			iova + i * dev->link_node_size);
+
+		for (j = 0; j < reg_count; j++) {
+			mpp_err("reg%03d 0x%08x\n", j, reg[j]);
+			udelay(100);
+		}
+	}
+}
+
+static void rkvdec_core_reg_dump(const char *func, struct rkvdec_link_dev *dev)
+{
+	struct mpp_dev *mpp = dev->mpp;
+	u32 s = mpp->var->hw_info->reg_start;
+	u32 e = mpp->var->hw_info->reg_end;
+	u32 i;
+
+	mpp_err("--- dump hardware register ---\n");
+
+	for (i = s; i <= e; i++) {
+		u32 reg = i * sizeof(u32);
+
+		mpp_err("reg[%03d]: %04x: 0x%08x\n",
+			i, reg, readl_relaxed(mpp->reg_base + reg));
+		udelay(100);
+	}
+}
+
+static void rkvdec_link_reg_dump(const char *func, struct rkvdec_link_dev *dev)
+{
+	mpp_err("dump link config status from %s\n", func);
+	mpp_err("reg 0 %08x - irq status\n", dev->irq_status);
+	mpp_err("reg 1 %08x - cfg addr\n", dev->iova_curr);
+	mpp_err("reg 2 %08x - link mode\n", dev->link_mode);
+	mpp_err("reg 4 %08x - decoded num\n", dev->decoded_status);
+	mpp_err("reg 5 %08x - total num\n", dev->total);
+	mpp_err("reg 6 %08x - link mode en\n", dev->enabled);
+	mpp_err("reg 6 %08x - next ltb addr\n", dev->iova_next);
+}
+
+static void rkvdec_link_counter(const char *func, struct rkvdec_link_dev *dev)
+{
+	mpp_err("dump link counter from %s\n", func);
+
+	mpp_err("task pending %d running %d\n",
+		atomic_read(&dev->task_pending), dev->task_running);
+}
+
+int rkvdec_link_dump(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *dev = dec->link_dec;
+
+	rkvdec_link_status_update(dev);
+	rkvdec_link_reg_dump(__func__, dev);
+	rkvdec_link_counter(__func__, dev);
+	rkvdec_core_reg_dump(__func__, dev);
+	rkvdec_link_node_dump(__func__, dev);
+
+	return 0;
+}
+
+static void rkvdec2_clear_cache(struct mpp_dev *mpp)
+{
+	/* set cache size */
+	u32 reg = RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS |
+		  RKVDEC_CACHE_PERMIT_READ_ALLOCATE;
+
+	if (!mpp_debug_unlikely(DEBUG_CACHE_32B))
+		reg |= RKVDEC_CACHE_LINE_SIZE_64_BYTES;
+
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE0_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE1_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE2_SIZE_BASE, reg);
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE0_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE1_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE2_BASE, 1);
+	/* init max outstanding read */
+	mpp_write_relaxed(mpp, RKVDEC_REG_MAX_READS, 0x1c);
+}
+
+static int rkvdec2_link_enqueue(struct rkvdec_link_dev *link_dec,
+				struct mpp_task *mpp_task)
+{
+	void __iomem *reg_base = link_dec->reg_base;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct mpp_dma_buffer *table = task->table;
+	struct rkvdec_link_info *link_info = link_dec->info;
+	u32 link_en = 0;
+	u32 frame_num = 1;
+	u32 link_mode;
+	u32 timing_en = link_dec->mpp->srv->timing_en;
+
+	link_en = readl(reg_base + RKVDEC_LINK_EN_BASE);
+	/* finish last work flow */
+	if (!link_en) {
+		rkvdec2_clear_cache(link_dec->mpp);
+		/* cleanup counter in hardware */
+		writel(0, reg_base + RKVDEC_LINK_MODE_BASE);
+		/* start config before all registers are set */
+		wmb();
+		writel(RKVDEC_LINK_BIT_CFG_DONE, reg_base + RKVDEC_LINK_CFG_CTRL_BASE);
+		/* write zero count config */
+		wmb();
+		/* clear counter and enable link mode hardware */
+		writel(RKVDEC_LINK_BIT_EN, reg_base + RKVDEC_LINK_EN_BASE);
+		writel_relaxed(table->iova, reg_base + RKVDEC_LINK_CFG_ADDR_BASE);
+		link_mode = frame_num;
+	} else
+		link_mode = (frame_num | RKVDEC_LINK_BIT_ADD_MODE);
+
+	/* set link mode */
+	writel_relaxed(link_mode, reg_base + RKVDEC_LINK_MODE_BASE);
+
+	/* set ip func to def val */
+	if (link_info->ip_en_base)
+		writel_relaxed(link_info->ip_en_val, reg_base + link_info->ip_en_base);
+
+	/* start config before all registers are set */
+	wmb();
+
+	mpp_iommu_flush_tlb(link_dec->mpp->iommu_info);
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	link_dec->task_running++;
+	/* configure done */
+	writel(RKVDEC_LINK_BIT_CFG_DONE, reg_base + RKVDEC_LINK_CFG_CTRL_BASE);
+	if (!link_en) {
+		/* start hardware before all registers are set */
+		wmb();
+		/* clear counter and enable link mode hardware */
+		writel(RKVDEC_LINK_BIT_EN, reg_base + RKVDEC_LINK_EN_BASE);
+	}
+	mpp_task_run_end(mpp_task, timing_en);
+
+	return 0;
+}
+
+static int rkvdec2_link_finish(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	struct mpp_dma_buffer *table = link_dec->table;
+	struct rkvdec_link_info *info = link_dec->info;
+	struct rkvdec_link_part *part = info->part_r;
+	u32 *tb_reg = (u32 *)table->vaddr;
+	u32 off, s, n;
+	u32 i;
+	u32 reg_ret_status;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < info->part_r_num; i++) {
+		off = part[i].tb_reg_off;
+		s = part[i].reg_start;
+		n = part[i].reg_num;
+		memcpy(&task->reg[s], &tb_reg[off], n * sizeof(u32));
+	}
+	/* revert hack for irq status */
+	reg_ret_status = mpp->var->hw_info->reg_ret_status;
+	task->reg[reg_ret_status] = task->irq_status;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static void *rkvdec2_link_prepare(struct mpp_dev *mpp,
+				  struct mpp_task *mpp_task)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	struct mpp_dma_buffer *table = NULL;
+	struct rkvdec_link_part *part;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct rkvdec_link_info *info = link_dec->info;
+	u32 i, off, s, n;
+	u32 *tb_reg;
+
+	mpp_debug_enter();
+
+	if (test_bit(TASK_STATE_PREPARE, &mpp_task->state)) {
+		dev_err(mpp->dev, "task %d has prepared\n", mpp_task->task_index);
+		return mpp_task;
+	}
+
+	table = list_first_entry_or_null(&link_dec->unused_list, struct mpp_dma_buffer, link);
+
+	if (!table)
+		return NULL;
+
+	/* fill regs value */
+	tb_reg = (u32 *)table->vaddr;
+	part = info->part_w;
+	for (i = 0; i < info->part_w_num; i++) {
+		off = part[i].tb_reg_off;
+		s = part[i].reg_start;
+		n = part[i].reg_num;
+		memcpy(&tb_reg[off], &task->reg[s], n * sizeof(u32));
+	}
+
+	/* setup error mode flag */
+	if (info->tb_reg_second_en > 0) {
+		tb_reg[9] |= BIT(18) | BIT(9);
+		tb_reg[info->tb_reg_second_en] |= RKVDEC_WAIT_RESET_EN;
+	}
+
+	/* memset read registers */
+	part = info->part_r;
+	for (i = 0; i < info->part_r_num; i++) {
+		off = part[i].tb_reg_off;
+		n = part[i].reg_num;
+		memset(&tb_reg[off], 0, n * sizeof(u32));
+	}
+
+	/* set node registers */
+	if (info->tb_reg_debug > 0)
+		tb_reg[info->tb_reg_debug] = table->iova + info->part_r[0].tb_reg_off * sizeof(u32);
+	if (info->tb_reg_seg0 > 0)
+		tb_reg[info->tb_reg_seg0] = table->iova + info->part_w[0].tb_reg_off * sizeof(u32);
+	if (info->tb_reg_seg1 > 0)
+		tb_reg[info->tb_reg_seg1] = table->iova + info->part_w[1].tb_reg_off * sizeof(u32);
+	if (info->tb_reg_seg2 > 0)
+		tb_reg[info->tb_reg_seg2] = table->iova + info->part_w[2].tb_reg_off * sizeof(u32);
+
+	list_move_tail(&table->link, &link_dec->used_list);
+	task->table = table;
+	set_bit(TASK_STATE_PREPARE, &mpp_task->state);
+
+	mpp_dbg_link("session %d task %d prepare pending %d running %d\n",
+		     mpp_task->session->index, mpp_task->task_index,
+		     atomic_read(&link_dec->task_pending), link_dec->task_running);
+	mpp_debug_leave();
+
+	return mpp_task;
+}
+
+static int rkvdec2_link_reset(struct mpp_dev *mpp)
+{
+
+	dev_info(mpp->dev, "resetting...\n");
+
+	disable_irq(mpp->irq);
+	mpp_iommu_disable_irq(mpp->iommu_info);
+
+	/* FIXME lock resource lock of the other devices in combo */
+	mpp_iommu_down_write(mpp->iommu_info);
+	mpp_reset_down_write(mpp->reset_group);
+	atomic_set(&mpp->reset_request, 0);
+
+	rockchip_save_qos(mpp->dev);
+
+	if (mpp->hw_ops->reset)
+		mpp->hw_ops->reset(mpp);
+
+	rockchip_restore_qos(mpp->dev);
+
+	/* Note: if the domain does not change, iommu attach will be return
+	 * as an empty operation. Therefore, force to close and then open,
+	 * will be update the domain. In this way, domain can really attach.
+	 */
+	mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+
+	mpp_reset_up_write(mpp->reset_group);
+	mpp_iommu_up_write(mpp->iommu_info);
+
+	enable_irq(mpp->irq);
+	mpp_iommu_enable_irq(mpp->iommu_info);
+	dev_info(mpp->dev, "reset done\n");
+
+	return 0;
+}
+
+static int rkvdec2_link_irq(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link = dec->link_dec;
+	u32 irq_status = 0;
+
+	irq_status = readl(link->reg_base + link->info->irq_base);
+
+	if (irq_status & RKVDEC_LINK_BIT_IRQ_RAW) {
+		u32 enabled = readl(link->reg_base + RKVDEC_LINK_EN_BASE);
+
+		if (!enabled) {
+			u32 bus = mpp_read_relaxed(mpp, 273 * 4);
+
+			if (bus & 0x7ffff)
+				dev_info(link->dev,
+					 "invalid bus status %08x\n", bus);
+		}
+
+		link->irq_status = irq_status;
+		mpp->irq_status = mpp_read_relaxed(mpp, RKVDEC_REG_INT_EN);
+
+		writel_relaxed(0, link->reg_base + link->info->irq_base);
+	}
+
+	mpp_debug(DEBUG_IRQ_STATUS | DEBUG_LINK_TABLE, "irq_status: %08x : %08x\n",
+		  irq_status, mpp->irq_status);
+
+	return 0;
+}
+
+int rkvdec_vdpu383_link_irq(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link = dec->link_dec;
+	u32 irq_val;
+	u32 irq_bits = link->info->irq_mask >> 16;
+
+	/* read irq and status */
+	irq_val = readl_relaxed(link->reg_base + link->info->irq_base);
+	if (irq_val & irq_bits) {
+		link->irq_status = irq_val;
+		mpp->irq_status = readl_relaxed(link->reg_base + link->info->status_base);
+
+		/* clear irq and status */
+		writel_relaxed(0xffff0000, link->reg_base + link->info->irq_base);
+		writel_relaxed(0xffff0000, link->reg_base + link->info->status_base);
+	}
+	mpp_debug(DEBUG_IRQ_STATUS | DEBUG_LINK_TABLE, "irq_status: %08x : %08x\n",
+		  irq_val, mpp->irq_status);
+
+	return 0;
+}
+
+int rkvdec2_link_remove(struct mpp_dev *mpp, struct rkvdec_link_dev *link_dec)
+{
+	mpp_debug_enter();
+
+	if (link_dec && link_dec->table) {
+		mpp_dma_free(link_dec->table);
+		link_dec->table = NULL;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec2_link_alloc_table(struct mpp_dev *mpp,
+				    struct rkvdec_link_dev *link_dec)
+{
+	int ret;
+	struct mpp_dma_buffer *table;
+	struct rkvdec_link_info *info = link_dec->info;
+	/* NOTE: link table address requires 64 align */
+	u32 task_capacity = link_dec->task_capacity;
+	u32 link_node_size = ALIGN(info->tb_reg_num * sizeof(u32), 256);
+	u32 link_info_size = task_capacity * link_node_size;
+	u32 *v_curr;
+	u32 io_curr, io_next, io_start;
+	u32 offset_r = info->part_r[0].tb_reg_off * sizeof(u32);
+	u32 i;
+
+	table = mpp_dma_alloc(mpp->dev, link_info_size);
+	if (!table) {
+		ret = -ENOMEM;
+		goto err_free_node;
+	}
+
+	link_dec->link_node_size = link_node_size;
+	link_dec->link_reg_count = link_node_size >> 2;
+	io_start = table->iova;
+
+	for (i = 0; i < task_capacity; i++) {
+		v_curr  = (u32 *)(table->vaddr + i * link_node_size);
+		io_curr = io_start + i * link_node_size;
+		io_next = (i == task_capacity - 1) ?
+			  io_start : io_start + (i + 1) * link_node_size;
+
+		v_curr[info->tb_reg_next] = io_next;
+		v_curr[info->tb_reg_r] = io_curr + offset_r;
+	}
+
+	link_dec->table	     = table;
+
+	return 0;
+err_free_node:
+	rkvdec2_link_remove(mpp, link_dec);
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+int rkvdec2_link_procfs_init(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	if (!link_dec)
+		return 0;
+
+	link_dec->statistic_count = 0;
+
+	if (dec->procfs)
+		mpp_procfs_create_u32("statistic_count", 0644,
+				      dec->procfs, &link_dec->statistic_count);
+
+	return 0;
+}
+#else
+int rkvdec2_link_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+int rkvdec2_link_init(struct platform_device *pdev, struct rkvdec2_dev *dec)
+{
+	int ret = 0;
+	struct resource *res = NULL;
+	struct rkvdec_link_dev *link_dec = NULL;
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = &dec->mpp;
+	struct mpp_dma_buffer *table;
+	u32 i;
+
+	mpp_debug_enter();
+
+	link_dec = devm_kzalloc(dev, sizeof(*link_dec), GFP_KERNEL);
+	if (!link_dec) {
+		ret = -ENOMEM;
+		goto done;
+	}
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "link");
+	if (res)
+		link_dec->info = mpp->var->hw_info->link_info;
+	else {
+		dev_err(dev, "link mode resource not found\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+
+	link_dec->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!link_dec->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		ret = -ENOMEM;
+		goto done;
+	}
+
+	link_dec->task_capacity = mpp->task_capacity;
+	/* if capacity<2, then not to alloc table array */
+	if (link_dec->task_capacity < 2)
+		goto out;
+
+	ret = rkvdec2_link_alloc_table(&dec->mpp, link_dec);
+	if (ret)
+		goto done;
+
+	/* alloc table pointer array */
+	table = devm_kmalloc_array(mpp->dev, mpp->task_capacity,
+				   sizeof(*table), GFP_KERNEL | __GFP_ZERO);
+	if (!table)
+		return -ENOMEM;
+
+	/* init table array */
+	link_dec->table_array = table;
+	INIT_LIST_HEAD(&link_dec->used_list);
+	INIT_LIST_HEAD(&link_dec->unused_list);
+	for (i = 0; i < mpp->task_capacity; i++) {
+		table[i].iova = link_dec->table->iova + i * link_dec->link_node_size;
+		table[i].vaddr = link_dec->table->vaddr + i * link_dec->link_node_size;
+		table[i].size = link_dec->link_node_size;
+		INIT_LIST_HEAD(&table[i].link);
+		list_add_tail(&table[i].link, &link_dec->unused_list);
+	}
+
+	if (dec->fix)
+		rkvdec2_link_hack_data_setup(dec->fix);
+
+	mpp->fault_handler = rkvdec2_link_iommu_fault_handle;
+
+out:
+	link_dec->mpp = mpp;
+	link_dec->dev = dev;
+	atomic_set(&link_dec->task_timeout, 0);
+	atomic_set(&link_dec->task_pending, 0);
+	atomic_set(&link_dec->power_enabled, 0);
+	link_dec->irq_enabled = 1;
+
+	dec->link_dec = link_dec;
+	dev_info(dev, "link mode probe finish\n");
+
+done:
+	if (ret) {
+		if (link_dec) {
+			if (link_dec->reg_base) {
+				devm_iounmap(dev, link_dec->reg_base);
+				link_dec->reg_base = NULL;
+			}
+			devm_kfree(dev, link_dec);
+			link_dec = NULL;
+		}
+		dec->link_dec = NULL;
+	}
+	mpp_debug_leave();
+
+	return ret;
+}
+
+static void rkvdec2_link_free_task(struct kref *ref)
+{
+	struct mpp_dev *mpp;
+	struct mpp_session *session;
+	struct mpp_task *task = container_of(ref, struct mpp_task, ref);
+
+	if (!task->session) {
+		mpp_err("task %d task->session is null.\n", task->task_id);
+		return;
+	}
+	session = task->session;
+
+	mpp_debug_func(DEBUG_TASK_INFO, "task %d:%d state 0x%lx\n",
+		       session->index, task->task_id, task->state);
+	if (!session->mpp) {
+		mpp_err("session %d session->mpp is null.\n", session->index);
+		return;
+	}
+	mpp = session->mpp;
+	list_del_init(&task->queue_link);
+
+	rkvdec2_free_task(session, task);
+	/* Decrease reference count */
+	atomic_dec(&session->task_count);
+	atomic_dec(&mpp->task_count);
+}
+
+static void rkvdec2_link_trigger_work(struct mpp_dev *mpp)
+{
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+}
+
+static int rkvdec2_link_power_on(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	if (!atomic_xchg(&link_dec->power_enabled, 1)) {
+		if (mpp_iommu_attach(mpp->iommu_info)) {
+			dev_err(mpp->dev, "mpp_iommu_attach failed\n");
+			return -ENODATA;
+		}
+		pm_runtime_get_sync(mpp->dev);
+		pm_stay_awake(mpp->dev);
+
+		if (mpp->hw_ops->clk_on)
+			mpp->hw_ops->clk_on(mpp);
+
+		if (!link_dec->irq_enabled) {
+			enable_irq(mpp->irq);
+			mpp_iommu_enable_irq(mpp->iommu_info);
+			link_dec->irq_enabled = 1;
+		}
+
+		mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_ADVANCED);
+		mpp_clk_set_rate(&dec->cabac_clk_info, CLK_MODE_ADVANCED);
+		mpp_clk_set_rate(&dec->hevc_cabac_clk_info, CLK_MODE_ADVANCED);
+		mpp_devfreq_set_core_rate(mpp, CLK_MODE_ADVANCED);
+		mpp_iommu_dev_activate(mpp->iommu_info, mpp);
+	}
+	return 0;
+}
+
+static void rkvdec2_link_power_off(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	if (atomic_xchg(&link_dec->power_enabled, 0)) {
+		disable_irq(mpp->irq);
+		mpp_iommu_disable_irq(mpp->iommu_info);
+		link_dec->irq_enabled = 0;
+
+		if (mpp->hw_ops->clk_off)
+			mpp->hw_ops->clk_off(mpp);
+
+		pm_relax(mpp->dev);
+		pm_runtime_mark_last_busy(mpp->dev);
+		pm_runtime_put_autosuspend(mpp->dev);
+
+		mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_NORMAL);
+		mpp_clk_set_rate(&dec->cabac_clk_info, CLK_MODE_NORMAL);
+		mpp_clk_set_rate(&dec->hevc_cabac_clk_info, CLK_MODE_NORMAL);
+		mpp_devfreq_set_core_rate(mpp, CLK_MODE_NORMAL);
+		mpp_iommu_dev_deactivate(mpp->iommu_info, mpp);
+	}
+}
+
+static void rkvdec2_link_timeout_proc(struct work_struct *work_s)
+{
+	struct mpp_dev *mpp;
+	struct rkvdec2_dev *dec;
+	struct mpp_session *session;
+	struct mpp_task *task = container_of(to_delayed_work(work_s),
+					     struct mpp_task, timeout_work);
+
+	if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+		mpp_err("task %d state %lx has been handled\n",
+			task->task_id, task->state);
+		return;
+	}
+
+	if (!task->session) {
+		mpp_err("task %d session is null.\n", task->task_id);
+		return;
+	}
+	session = task->session;
+
+	if (!session->mpp) {
+		mpp_err("task %d:%d mpp is null.\n", session->index,
+			task->task_id);
+		return;
+	}
+	mpp = session->mpp;
+	set_bit(TASK_STATE_TIMEOUT, &task->state);
+
+	dec = to_rkvdec2_dev(mpp);
+	atomic_inc(&dec->link_dec->task_timeout);
+
+	dev_err(mpp->dev, "session %d task %d state %#lx timeout, cnt %d\n",
+		session->index, task->task_index, task->state,
+		atomic_read(&dec->link_dec->task_timeout));
+
+	rkvdec2_link_trigger_work(mpp);
+}
+
+static int rkvdec2_link_iommu_fault_handle(struct iommu_domain *iommu,
+					    struct device *iommu_dev,
+					    unsigned long iova,
+					    int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct mpp_task *mpp_task = NULL, *n;
+	struct mpp_taskqueue *queue;
+	unsigned long flags;
+	u32 dump_mem_region = 0;
+
+	/*
+	 * Mask iommu irq, in order for iommu not repeatedly trigger pagefault.
+	 * Until the pagefault task finish by hw timeout.
+	 */
+	if (mpp)
+		rockchip_iommu_mask_irq(mpp->dev);
+
+	dev_err(iommu_dev, "fault addr 0x%08lx status %x arg %p\n",
+		iova, status, arg);
+
+	if (!mpp) {
+		dev_err(iommu_dev, "pagefault without device to handle\n");
+		return 0;
+	}
+	queue = mpp->queue;
+	spin_lock_irqsave(&queue->running_lock, flags);
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		struct rkvdec_link_info *info = dec->link_dec->info;
+		struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+		u32 *tb_reg = (u32 *)task->table->vaddr;
+		u32 irq_status = tb_reg[info->tb_reg_int];
+
+		if (!irq_status) {
+			dump_mem_region = 1;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+
+	if (dump_mem_region)
+		mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_hw_reg(mpp);
+
+	dec->mmu_fault = 1;
+
+	return 0;
+}
+
+static void rkvdec2_link_resend(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct mpp_task *mpp_task, *n;
+
+	link_dec->task_running = 0;
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		dev_err(mpp->dev, "resend task %d\n", mpp_task->task_index);
+		cancel_delayed_work_sync(&mpp_task->timeout_work);
+		clear_bit(TASK_STATE_TIMEOUT, &mpp_task->state);
+		clear_bit(TASK_STATE_HANDLE, &mpp_task->state);
+		rkvdec2_link_enqueue(link_dec, mpp_task);
+	}
+}
+
+static void rkvdec2_link_try_dequeue(struct mpp_dev *mpp)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct mpp_task *mpp_task = NULL, *n;
+	struct rkvdec_link_info *info = link_dec->info;
+	u32 reset_flag = 0;
+	u32 iommu_fault = dec->mmu_fault && (mpp->irq_status & RKVDEC_TIMEOUT_STA);
+	u32 link_en = atomic_read(&link_dec->power_enabled) ?
+		      readl(link_dec->reg_base + RKVDEC_LINK_EN_BASE) : 0;
+	u32 force_dequeue = iommu_fault || !link_en;
+	u32 dequeue_cnt = 0;
+	unsigned long flags;
+
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		/*
+		 * Because there are multiple tasks enqueue at the same time,
+		 * soft timeout may be triggered at the same time, but in reality only
+		 * first task is being timeout because of the hardware stuck,
+		 * so only process the first task.
+		 */
+		u32 timeout_flag = dequeue_cnt ? 0 : test_bit(TASK_STATE_TIMEOUT, &mpp_task->state);
+		struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+		u32 *tb_reg = (u32 *)task->table->vaddr;
+		u32 abort_flag = test_bit(TASK_STATE_ABORT, &mpp_task->state);
+		u32 irq_status = tb_reg[info->tb_reg_int];
+		u32 task_done = irq_status || timeout_flag || abort_flag;
+
+		/*
+		 * there are some cases will cause hw cannot write reg to ddr:
+		 * 1. iommu pagefault
+		 * 2. link stop(link_en == 0) because of err task, it is a rk356x issue.
+		 * so need force dequeue one task.
+		 */
+		if (force_dequeue)
+			task_done = 1;
+
+		if (!task_done)
+			break;
+
+		dequeue_cnt++;
+		/* check hack task only for rk356x*/
+		if (task->need_hack == RKVDEC2_LINK_HACK_TASK_FLAG) {
+			cancel_delayed_work_sync(&mpp_task->timeout_work);
+			list_move_tail(&task->table->link, &link_dec->unused_list);
+			list_del_init(&mpp_task->queue_link);
+			link_dec->task_running--;
+			link_dec->hack_task_running--;
+			kfree(task);
+			mpp_dbg_link("hack running %d irq_status %#08x timeout %d abort %d\n",
+				     link_dec->hack_task_running, irq_status,
+				     timeout_flag, abort_flag);
+			continue;
+		}
+
+		/*
+		 * if timeout/abort/force dequeue found, reset and stop hw first.
+		 */
+		if ((timeout_flag || abort_flag || force_dequeue) && !reset_flag) {
+			dev_err(mpp->dev, "session %d task %d timeout %d abort %d force_dequeue %d\n",
+				mpp_task->session->index, mpp_task->task_index,
+				timeout_flag, abort_flag, force_dequeue);
+			rkvdec2_link_reset(mpp);
+			reset_flag = 1;
+			dec->mmu_fault = 0;
+			mpp->irq_status = 0;
+			force_dequeue = 0;
+		}
+
+		cancel_delayed_work_sync(&mpp_task->timeout_work);
+
+		task->irq_status = irq_status;
+		mpp_task->hw_cycles = tb_reg[info->tb_reg_cycle];
+		mpp_task->hw_time = mpp_task->hw_cycles /
+				    (dec->cycle_clk->real_rate_hz / 1000000);
+		mpp_time_diff_with_hw_time(mpp_task, dec->cycle_clk->real_rate_hz);
+		rkvdec2_link_finish(mpp, mpp_task);
+
+		spin_lock_irqsave(&queue->running_lock, flags);
+		list_move_tail(&task->table->link, &link_dec->unused_list);
+		list_del_init(&mpp_task->queue_link);
+		spin_unlock_irqrestore(&queue->running_lock, flags);
+		link_dec->task_running--;
+
+		set_bit(TASK_STATE_HANDLE, &mpp_task->state);
+		set_bit(TASK_STATE_PROC_DONE, &mpp_task->state);
+		set_bit(TASK_STATE_FINISH, &mpp_task->state);
+		set_bit(TASK_STATE_DONE, &mpp_task->state);
+		if (test_bit(TASK_STATE_ABORT, &mpp_task->state))
+			set_bit(TASK_STATE_ABORT_READY, &mpp_task->state);
+
+		mpp_dbg_link("session %d task %d irq_status %#08x timeout %d abort %d\n",
+			     mpp_task->session->index, mpp_task->task_index,
+			     irq_status, timeout_flag, abort_flag);
+
+		if (irq_status & info->err_mask) {
+			dev_err(mpp->dev,
+				"session %d task %d irq_status %#08x timeout %u abort %u\n",
+				mpp_task->session->index, mpp_task->task_index,
+				irq_status, timeout_flag, abort_flag);
+			if (!reset_flag)
+				atomic_inc(&mpp->reset_request);
+		}
+
+		wake_up(&mpp_task->wait);
+		mpp_dev_load(mpp, mpp_task);
+		kref_put(&mpp_task->ref, rkvdec2_link_free_task);
+	}
+
+	/* resend running task after reset */
+	if (reset_flag && !list_empty(&queue->running_list))
+		rkvdec2_link_resend(mpp);
+}
+
+static int mpp_task_queue(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+	mpp_debug_enter();
+
+	rkvdec2_link_power_on(mpp);
+
+	/* hack for rk356x */
+	if (task->need_hack) {
+		u32 *tb_reg;
+		struct mpp_dma_buffer *table;
+		struct rkvdec2_task *hack_task;
+		struct rkvdec_link_info *info = link_dec->info;
+
+		/* need reserved 2 unused task for need hack task */
+		if (link_dec->task_running > (link_dec->task_capacity - 2))
+			return -EBUSY;
+
+		table = list_first_entry_or_null(&link_dec->unused_list,
+						 struct mpp_dma_buffer,
+						 link);
+		if (!table)
+			return -EBUSY;
+
+		hack_task = kzalloc(sizeof(*hack_task), GFP_KERNEL);
+
+		if (!hack_task)
+			return -ENOMEM;
+
+		mpp_task_init(mpp_task->session, &hack_task->mpp_task);
+		INIT_DELAYED_WORK(&hack_task->mpp_task.timeout_work,
+					rkvdec2_link_timeout_proc);
+
+		tb_reg = (u32 *)table->vaddr;
+		memset(tb_reg + info->part_r[0].tb_reg_off, 0, info->part_r[0].reg_num);
+		rkvdec2_3568_hack_fix_link(tb_reg + 4);
+		list_move_tail(&table->link, &link_dec->used_list);
+		hack_task->table = table;
+		hack_task->need_hack = RKVDEC2_LINK_HACK_TASK_FLAG;
+		rkvdec2_link_enqueue(link_dec, &hack_task->mpp_task);
+		mpp_taskqueue_pending_to_run(queue, &hack_task->mpp_task);
+		link_dec->hack_task_running++;
+		mpp_dbg_link("hack task send to hw, hack running %d\n",
+			     link_dec->hack_task_running);
+	}
+
+	/* process normal */
+	if (!rkvdec2_link_prepare(mpp, mpp_task))
+		return -EBUSY;
+
+	if (mpp->srv->timing_en) {
+		mpp_task->on_run = ktime_get();
+		set_bit(TASK_TIMING_RUN, &mpp_task->state);
+	}
+	rkvdec2_link_enqueue(link_dec, mpp_task);
+
+	set_bit(TASK_STATE_RUNNING, &mpp_task->state);
+	atomic_dec(&link_dec->task_pending);
+	mpp_taskqueue_pending_to_run(queue, mpp_task);
+
+	mpp_dbg_link("session %d task %d send to hw pending %d running %d\n",
+		     mpp_task->session->index, mpp_task->task_index,
+		     atomic_read(&link_dec->task_pending), link_dec->task_running);
+	mpp_debug_leave();
+
+	return 0;
+}
+
+irqreturn_t rkvdec2_link_irq_proc(int irq, void *param)
+{
+	struct mpp_dev *mpp = param;
+	int ret;
+
+	if (mpp->dev_ops->link_irq)
+		ret = mpp->dev_ops->link_irq(mpp);
+	else
+		ret = rkvdec2_link_irq(mpp);
+	if (!ret)
+		rkvdec2_link_trigger_work(mpp);
+
+	return IRQ_HANDLED;
+}
+
+static struct mpp_task *
+mpp_session_get_pending_task(struct mpp_session *session)
+{
+	struct mpp_task *task = NULL;
+
+	mutex_lock(&session->pending_lock);
+	task = list_first_entry_or_null(&session->pending_list, struct mpp_task,
+					pending_link);
+	mutex_unlock(&session->pending_lock);
+
+	return task;
+}
+
+static int task_is_done(struct mpp_task *task)
+{
+	return test_bit(TASK_STATE_PROC_DONE, &task->state);
+}
+
+static int mpp_session_pop_pending(struct mpp_session *session,
+				   struct mpp_task *task)
+{
+	mutex_lock(&session->pending_lock);
+	list_del_init(&task->pending_link);
+	mutex_unlock(&session->pending_lock);
+	kref_put(&task->ref, rkvdec2_link_free_task);
+
+	return 0;
+}
+
+static int mpp_session_pop_done(struct mpp_session *session,
+				struct mpp_task *task)
+{
+	set_bit(TASK_STATE_DONE, &task->state);
+
+	return 0;
+}
+
+int rkvdec2_link_process_task(struct mpp_session *session,
+			      struct mpp_task_msgs *msgs)
+{
+	struct mpp_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+	struct rkvdec_link_info *link_info = mpp->var->hw_info->link_info;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	task = rkvdec2_alloc_task(session, msgs);
+	if (!task) {
+		mpp_err("alloc_task failed.\n");
+		return -ENOMEM;
+	}
+
+	if (link_info->hack_setup) {
+		u32 fmt;
+		int reg_fmt;
+		struct rkvdec2_task *dec_task = NULL;
+
+		dec_task = to_rkvdec2_task(task);
+		reg_fmt = task->hw_info->reg_fmt;
+		fmt = RKVDEC_GET_FORMAT(dec_task->reg[reg_fmt]);
+		dec_task->need_hack = (fmt == RKVDEC_FMT_H264D);
+	}
+
+	kref_init(&task->ref);
+	atomic_set(&task->abort_request, 0);
+	task->task_index = atomic_fetch_inc(&mpp->task_index);
+	task->task_id = atomic_fetch_inc(&mpp->queue->task_id);
+	INIT_DELAYED_WORK(&task->timeout_work, rkvdec2_link_timeout_proc);
+
+	atomic_inc(&session->task_count);
+
+	kref_get(&task->ref);
+	mutex_lock(&session->pending_lock);
+	list_add_tail(&task->pending_link, &session->pending_list);
+	mutex_unlock(&session->pending_lock);
+
+	kref_get(&task->ref);
+	mutex_lock(&mpp->queue->pending_lock);
+	list_add_tail(&task->queue_link, &mpp->queue->pending_list);
+	mutex_unlock(&mpp->queue->pending_lock);
+	atomic_inc(&link_dec->task_pending);
+
+	/* push current task to queue */
+	atomic_inc(&mpp->task_count);
+	set_bit(TASK_STATE_PENDING, &task->state);
+	/* trigger current queue to run task */
+	rkvdec2_link_trigger_work(mpp);
+	kref_put(&task->ref, rkvdec2_link_free_task);
+
+	return 0;
+}
+
+int rkvdec2_link_wait_result(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	struct mpp_dev *mpp = session->mpp;
+	struct mpp_task *mpp_task;
+	int ret;
+
+	mpp_task = mpp_session_get_pending_task(session);
+	if (!mpp_task) {
+		mpp_err("session %p pending list is empty!\n", session);
+		return -EIO;
+	}
+
+	ret = wait_event_interruptible(mpp_task->wait, task_is_done(mpp_task));
+	if (ret == -ERESTARTSYS)
+		mpp_err("wait task break by signal\n");
+
+	ret = rkvdec2_result(mpp, mpp_task, msgs);
+
+	mpp_session_pop_done(session, mpp_task);
+	mpp_debug_func(DEBUG_TASK_INFO, "wait done session %d:%d count %d task %d state %lx\n",
+		       session->device_type, session->index, atomic_read(&session->task_count),
+		       mpp_task->task_index, mpp_task->state);
+
+	mpp_session_pop_pending(session, mpp_task);
+	return ret;
+}
+
+void rkvdec2_link_worker(struct kthread_work *work_s)
+{
+	struct mpp_dev *mpp = container_of(work_s, struct mpp_dev, work);
+	struct mpp_task *task;
+	struct mpp_taskqueue *queue = mpp->queue;
+	u32 all_done;
+
+	mpp_debug_enter();
+
+	/* dequeue running task */
+	rkvdec2_link_try_dequeue(mpp);
+
+	/* process reset */
+	if (atomic_read(&mpp->reset_request)) {
+		rkvdec2_link_reset(mpp);
+		/* resend running task after reset */
+		if (!list_empty(&queue->running_list))
+			rkvdec2_link_resend(mpp);
+	}
+
+again:
+	/* get pending task to process */
+	mutex_lock(&queue->pending_lock);
+	task = list_first_entry_or_null(&queue->pending_list, struct mpp_task,
+					queue_link);
+	mutex_unlock(&queue->pending_lock);
+	if (!task)
+		goto done;
+
+	/* check abort task */
+	if (atomic_read(&task->abort_request)) {
+		mutex_lock(&queue->pending_lock);
+		list_del_init(&task->queue_link);
+
+		set_bit(TASK_STATE_ABORT_READY, &task->state);
+		set_bit(TASK_STATE_PROC_DONE, &task->state);
+
+		mutex_unlock(&queue->pending_lock);
+		wake_up(&task->wait);
+		kref_put(&task->ref, rkvdec2_link_free_task);
+		goto again;
+	}
+
+	/* queue task to hw */
+	if (!mpp_task_queue(mpp, task))
+		goto again;
+
+done:
+
+	/* if no task in pending and running list, power off device */
+	mutex_lock(&queue->pending_lock);
+	all_done = list_empty(&queue->pending_list) && list_empty(&queue->running_list);
+	mutex_unlock(&queue->pending_lock);
+
+	if (all_done)
+		rkvdec2_link_power_off(mpp);
+
+	mpp_session_cleanup_detach(queue, work_s);
+
+	mpp_debug_leave();
+}
+
+void rkvdec2_link_session_deinit(struct mpp_session *session)
+{
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	rkvdec2_free_session(session);
+
+	if (session->dma) {
+		mpp_dbg_session("session %d destroy dma\n", session->index);
+		mpp_iommu_down_write(mpp->iommu_info);
+		mpp_dma_session_destroy(session->dma);
+		mpp_iommu_up_write(mpp->iommu_info);
+		session->dma = NULL;
+	}
+	if (session->srv) {
+		struct mpp_service *srv = session->srv;
+
+		mutex_lock(&srv->session_lock);
+		list_del_init(&session->service_link);
+		mutex_unlock(&srv->session_lock);
+	}
+	list_del_init(&session->session_link);
+
+	mpp_dbg_session("session %d release\n", session->index);
+
+	mpp_debug_leave();
+}
+
+#define RKVDEC2_1080P_PIXELS	(1920*1080)
+#define RKVDEC2_4K_PIXELS	(4096*2304)
+#define RKVDEC2_8K_PIXELS	(7680*4320)
+#define RKVDEC2_CCU_TIMEOUT_20MS	(0xefffff)
+#define RKVDEC2_CCU_TIMEOUT_50MS	(0x2cfffff)
+#define RKVDEC2_CCU_TIMEOUT_100MS	(0x4ffffff)
+
+static u32 rkvdec2_ccu_get_timeout_threshold(struct rkvdec2_task *task)
+{
+	u32 pixels = task->pixels;
+
+	if (pixels < RKVDEC2_1080P_PIXELS)
+		return RKVDEC2_CCU_TIMEOUT_20MS;
+	else if (pixels < RKVDEC2_4K_PIXELS)
+		return RKVDEC2_CCU_TIMEOUT_50MS;
+	else
+		return RKVDEC2_CCU_TIMEOUT_100MS;
+}
+
+int rkvdec2_attach_ccu(struct device *dev, struct rkvdec2_dev *dec)
+{
+	int ret;
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvdec2_ccu *ccu;
+
+	mpp_debug_enter();
+
+	np = of_parse_phandle(dev->of_node, "rockchip,ccu", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	ccu = platform_get_drvdata(pdev);
+	if (!ccu)
+		return -ENOMEM;
+
+	ret = of_property_read_u32(dev->of_node, "rockchip,core-mask", &dec->core_mask);
+	if (ret)
+		return ret;
+	dev_info(dev, "core_mask=%08x\n", dec->core_mask);
+
+	/* if not the main-core, then attach the main core domain to current */
+	if (dec->mpp.core_id != 0) {
+		struct mpp_taskqueue *queue;
+		struct mpp_iommu_info *ccu_info, *cur_info;
+
+		queue = dec->mpp.queue;
+		/* set the ccu-domain for current device */
+		ccu_info = queue->cores[0]->iommu_info;
+		cur_info = dec->mpp.iommu_info;
+		if (cur_info)
+			cur_info->domain = ccu_info->domain;
+		mpp_iommu_attach(cur_info);
+	}
+
+	dec->ccu = ccu;
+
+	dev_info(dev, "attach ccu as core %d\n", dec->mpp.core_id);
+	mpp_debug_enter();
+
+	return 0;
+}
+
+static void rkvdec2_ccu_timeout_work(struct work_struct *work_s)
+{
+	struct mpp_dev *mpp;
+	struct mpp_task *task = container_of(to_delayed_work(work_s),
+					     struct mpp_task, timeout_work);
+
+	if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+		mpp_err("task %d state %lx has been handled\n",
+			task->task_id, task->state);
+		return;
+	}
+
+	if (!task->session) {
+		mpp_err("task %d session is null.\n", task->task_id);
+		return;
+	}
+	mpp = mpp_get_task_used_device(task, task->session);
+	mpp_err("%s, task %d state %#lx timeout\n", dev_name(mpp->dev),
+		task->task_index, task->state);
+	set_bit(TASK_STATE_TIMEOUT, &task->state);
+	atomic_inc(&mpp->reset_request);
+	atomic_inc(&mpp->queue->reset_request);
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+}
+
+int rkvdec2_ccu_link_init(struct platform_device *pdev, struct rkvdec2_dev *dec)
+{
+	struct resource *res;
+	struct rkvdec_link_dev *link_dec;
+	struct device *dev = &pdev->dev;
+
+	mpp_debug_enter();
+
+	/* link structure */
+	link_dec = devm_kzalloc(dev, sizeof(*link_dec), GFP_KERNEL);
+	if (!link_dec)
+		return -ENOMEM;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "link");
+	if (!res)
+		return -ENOMEM;
+
+	link_dec->info = dec->mpp.var->hw_info->link_info;
+	link_dec->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!link_dec->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		return -ENOMEM;
+	}
+
+	dec->link_dec = link_dec;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec2_ccu_power_on(struct mpp_taskqueue *queue,
+				struct rkvdec2_ccu *ccu)
+{
+	if (!atomic_xchg(&ccu->power_enabled, 1)) {
+		u32 i;
+		struct mpp_dev *mpp;
+
+		/* ccu pd and clk on */
+		pm_runtime_get_sync(ccu->dev);
+		pm_stay_awake(ccu->dev);
+		mpp_clk_safe_enable(ccu->aclk_info.clk);
+		/* core pd and clk on */
+		for (i = 0; i < queue->core_count; i++) {
+			struct rkvdec2_dev *dec;
+
+			mpp = queue->cores[i];
+			dec = to_rkvdec2_dev(mpp);
+			pm_runtime_get_sync(mpp->dev);
+			pm_stay_awake(mpp->dev);
+			if (mpp->hw_ops->clk_on)
+				mpp->hw_ops->clk_on(mpp);
+
+			mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_NORMAL);
+			mpp_clk_set_rate(&dec->cabac_clk_info, CLK_MODE_NORMAL);
+			mpp_clk_set_rate(&dec->hevc_cabac_clk_info, CLK_MODE_NORMAL);
+			mpp_devfreq_set_core_rate(mpp, CLK_MODE_NORMAL);
+			mpp_iommu_dev_activate(mpp->iommu_info, mpp);
+		}
+		mpp_debug(DEBUG_CCU, "power on\n");
+	}
+
+	return 0;
+}
+
+static int rkvdec2_ccu_power_off(struct mpp_taskqueue *queue,
+				 struct rkvdec2_ccu *ccu)
+{
+	if (atomic_xchg(&ccu->power_enabled, 0)) {
+		u32 i;
+		struct mpp_dev *mpp;
+
+		/* ccu pd and clk off */
+		mpp_clk_safe_disable(ccu->aclk_info.clk);
+		pm_relax(ccu->dev);
+		pm_runtime_mark_last_busy(ccu->dev);
+		pm_runtime_put_autosuspend(ccu->dev);
+		/* core pd and clk off */
+		for (i = 0; i < queue->core_count; i++) {
+			mpp = queue->cores[i];
+
+			if (mpp->hw_ops->clk_off)
+				mpp->hw_ops->clk_off(mpp);
+			pm_relax(mpp->dev);
+			pm_runtime_mark_last_busy(mpp->dev);
+			pm_runtime_put_autosuspend(mpp->dev);
+			mpp_iommu_dev_deactivate(mpp->iommu_info, mpp);
+		}
+		mpp_debug(DEBUG_CCU, "power off\n");
+	}
+
+	return 0;
+}
+
+static int rkvdec2_soft_ccu_dequeue(struct mpp_taskqueue *queue)
+{
+	struct mpp_task *mpp_task = NULL, *n;
+	unsigned long flags;
+
+	mpp_debug_enter();
+
+	list_for_each_entry_safe(mpp_task, n,
+				 &queue->running_list,
+				 queue_link) {
+		struct mpp_dev *mpp = mpp_get_task_used_device(mpp_task, mpp_task->session);
+		struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+		u32 irq_status = mpp->irq_status;
+		u32 timeout_flag = test_bit(TASK_STATE_TIMEOUT, &mpp_task->state);
+		u32 abort_flag = test_bit(TASK_STATE_ABORT, &mpp_task->state);
+		u32 timing_en = mpp->srv->timing_en;
+		u32 reg_ret_status = mpp->var->hw_info->reg_ret_status;
+
+		if (irq_status || timeout_flag || abort_flag) {
+			struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+			if (timing_en) {
+				mpp_task->on_irq = ktime_get();
+				set_bit(TASK_TIMING_IRQ, &mpp_task->state);
+
+				mpp_task->on_cancel_timeout = mpp_task->on_irq;
+				set_bit(TASK_TIMING_TO_CANCEL, &mpp_task->state);
+			}
+
+			set_bit(TASK_STATE_HANDLE, &mpp_task->state);
+			cancel_delayed_work(&mpp_task->timeout_work);
+			mpp_task->hw_cycles = mpp_read(mpp, RKVDEC_PERF_WORKING_CNT);
+			mpp_task->hw_time = mpp_task->hw_cycles /
+					    (dec->cycle_clk->real_rate_hz / 1000000);
+			mpp_time_diff_with_hw_time(mpp_task, dec->cycle_clk->real_rate_hz);
+			task->irq_status = irq_status;
+			mpp_debug(DEBUG_IRQ_CHECK, "irq_status=%08x, timeout=%u, abort=%u\n",
+				  irq_status, timeout_flag, abort_flag);
+			if (irq_status && mpp->dev_ops->finish)
+				mpp->dev_ops->finish(mpp, mpp_task);
+			else
+				task->reg[reg_ret_status] = RKVDEC_TIMEOUT_STA;
+
+			set_bit(TASK_STATE_FINISH, &mpp_task->state);
+			set_bit(TASK_STATE_DONE, &mpp_task->state);
+
+			set_bit(mpp->core_id, &queue->core_idle);
+			mpp_dbg_core("set core %d idle %lx\n", mpp->core_id, queue->core_idle);
+			/* Wake up the GET thread */
+			wake_up(&mpp_task->wait);
+			/* free task */
+			spin_lock_irqsave(&queue->running_lock, flags);
+			list_del_init(&mpp_task->queue_link);
+			spin_unlock_irqrestore(&queue->running_lock, flags);
+			mpp_dev_load(mpp, mpp_task);
+			kref_put(&mpp_task->ref, mpp_free_task);
+		} else {
+			/* NOTE: break when meet not finish */
+			break;
+		}
+	}
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int rkvdec2_soft_ccu_reset(struct mpp_taskqueue *queue,
+				  struct rkvdec2_ccu *ccu)
+{
+	int i;
+
+	for (i = queue->core_count - 1; i >= 0; i--) {
+		u32 val;
+
+		struct mpp_dev *mpp = queue->cores[i];
+		struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+		if (mpp->disable)
+			continue;
+
+		dev_info(mpp->dev, "resetting for err %#x\n", mpp->irq_status);
+		disable_hardirq(mpp->irq);
+
+		/* foce idle, disconnect core and ccu */
+		writel(dec->core_mask, ccu->reg_base + RKVDEC_CCU_CORE_IDLE_BASE);
+
+		/* soft reset */
+		mpp_write(mpp, RKVDEC_REG_IMPORTANT_BASE, RKVDEC_SOFTREST_EN);
+		udelay(5);
+		val = mpp_read(mpp, RKVDEC_REG_INT_EN);
+		if (!(val & RKVDEC_SOFT_RESET_READY))
+			mpp_err("soft reset fail, int %08x\n", val);
+		mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+
+		/* check bus idle */
+		val = mpp_read(mpp, RKVDEC_REG_DEBUG_INT_BASE);
+		if (!(val & RKVDEC_BIT_BUS_IDLE))
+			mpp_err("bus busy\n");
+
+		if (IS_REACHABLE(CONFIG_ROCKCHIP_SIP)) {
+			/* sip reset */
+			rockchip_dmcfreq_lock();
+			sip_smc_vpu_reset(i, 0, 0);
+			rockchip_dmcfreq_unlock();
+		} else {
+			rkvdec2_reset(mpp);
+		}
+		/* clear error mask */
+		writel(dec->core_mask & RKVDEC_CCU_CORE_RW_MASK,
+		       ccu->reg_base + RKVDEC_CCU_CORE_ERR_BASE);
+		/* connect core and ccu */
+		writel(dec->core_mask & RKVDEC_CCU_CORE_RW_MASK,
+		       ccu->reg_base + RKVDEC_CCU_CORE_IDLE_BASE);
+		mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+		atomic_set(&mpp->reset_request, 0);
+
+		enable_irq(mpp->irq);
+		dev_info(mpp->dev, "reset done\n");
+	}
+	atomic_set(&queue->reset_request, 0);
+
+	return 0;
+}
+
+void *rkvdec2_ccu_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct rkvdec2_task *task;
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	ret = rkvdec2_task_init(session->mpp, session, task, msgs);
+	if (ret) {
+		kfree(task);
+		return NULL;
+	}
+
+	return &task->mpp_task;
+}
+
+static struct mpp_dev *rkvdec2_ccu_dev_match_by_iommu(struct mpp_taskqueue *queue,
+						      struct device *iommu_dev)
+{
+	struct mpp_dev *mpp = NULL;
+	struct rkvdec2_dev *dec = NULL;
+	u32 mmu[2] = {0, 0x40};
+	u32 i;
+
+	for (i = 0; i < queue->core_count; i++) {
+		struct mpp_dev *core = queue->cores[i];
+
+		if (&core->iommu_info->pdev->dev == iommu_dev) {
+			mpp = core;
+			dec = to_rkvdec2_dev(mpp);
+		}
+	}
+
+	if (!dec || !dec->mmu_base)
+		goto out;
+
+	/* there are two iommus */
+	for (i = 0; i < 2; i++) {
+		u32 status = readl(dec->mmu_base + mmu[i] + 0x4);
+		u32 iova = readl(dec->mmu_base + mmu[i] + 0xc);
+		u32 is_write = (status & BIT(5)) ? 1 : 0;
+
+		if (status && iova)
+			dev_err(iommu_dev, "core %d pagfault at iova %#08x type %s status %#x\n",
+				mpp->core_id, iova, is_write ? "write" : "read", status);
+	}
+out:
+	return mpp;
+}
+
+int rkvdec2_soft_ccu_iommu_fault_handle(struct iommu_domain *iommu,
+					struct device *iommu_dev,
+					unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct mpp_task *mpp_task;
+
+	mpp_debug_enter();
+
+	mpp = rkvdec2_ccu_dev_match_by_iommu(queue, iommu_dev);
+	if (!mpp) {
+		dev_err(iommu_dev, "iommu fault, but no dev match\n");
+		return 0;
+	}
+	/*
+	 * Mask iommu irq, in order for iommu not repeatedly trigger pagefault.
+	 * Until the pagefault task finish by hw timeout.
+	 */
+	rockchip_iommu_mask_irq(mpp->dev);
+	mpp_task = mpp->cur_task;
+	if (mpp_task)
+		mpp_task_dump_mem_region(mpp, mpp_task);
+
+	atomic_inc(&mpp->queue->reset_request);
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+int rkvdec2_hard_ccu_iommu_fault_handle(struct iommu_domain *iommu,
+					struct device *iommu_dev,
+					unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct mpp_task *mpp_task = NULL, *n;
+	struct rkvdec2_dev *dec;
+	u32 err_task_iova;
+	unsigned long flags;
+
+	mpp_debug_enter();
+
+	mpp = rkvdec2_ccu_dev_match_by_iommu(queue, iommu_dev);
+	if (!mpp) {
+		dev_err(iommu_dev, "iommu fault, but no dev match\n");
+		return 0;
+	}
+
+	dec = to_rkvdec2_dev(mpp);
+	err_task_iova = readl(dec->link_dec->reg_base + 0x4);
+	dev_err(mpp->dev, "core %d err task iova %#08x\n", mpp->core_id, err_task_iova);
+	rockchip_iommu_mask_irq(mpp->dev);
+
+	spin_lock_irqsave(&queue->running_lock, flags);
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+		if ((u32)task->table->iova == err_task_iova) {
+			mpp_task_dump_mem_region(mpp, mpp_task);
+			set_bit(TASK_STATE_ABORT, &mpp_task->state);
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+	atomic_inc(&mpp->queue->reset_request);
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+irqreturn_t rkvdec2_soft_ccu_irq(int irq, void *param)
+{
+	struct mpp_dev *mpp = param;
+	u32 irq_status = mpp_read_relaxed(mpp, RKVDEC_REG_INT_EN);
+	struct rkvdec_link_info *link_info = mpp->var->hw_info->link_info;
+
+	if (irq_status & RKVDEC_IRQ_RAW) {
+		mpp_debug(DEBUG_IRQ_STATUS, "irq_status=%08x\n", irq_status);
+		if (irq_status & link_info->err_mask) {
+			atomic_inc(&mpp->reset_request);
+			atomic_inc(&mpp->queue->reset_request);
+		}
+		mpp_write(mpp, RKVDEC_REG_INT_EN, 0);
+		mpp->irq_status = irq_status;
+		kthread_queue_work(&mpp->queue->worker, &mpp->work);
+		return IRQ_HANDLED;
+	}
+	return IRQ_NONE;
+}
+
+static inline int rkvdec2_set_core_info(u32 *reg, int idx)
+{
+	u32 val = (idx << 16) & RKVDEC_REG_FILM_IDX_MASK;
+
+	reg[RKVDEC_REG_CORE_CTRL_INDEX] &= ~RKVDEC_REG_FILM_IDX_MASK;
+
+	reg[RKVDEC_REG_CORE_CTRL_INDEX] |= val;
+
+	return 0;
+}
+
+static int rkvdec2_soft_ccu_enqueue(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i, reg_en, reg;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* set reg for link */
+	reg = RKVDEC_LINK_BIT_CORE_WORK_MODE | RKVDEC_LINK_BIT_CCU_WORK_MODE;
+	writel_relaxed(reg, link_dec->reg_base + link_dec->info->irq_base);
+
+	/* set reg for ccu */
+	writel_relaxed(RKVDEC_CCU_BIT_WORK_EN, dec->ccu->reg_base + RKVDEC_CCU_WORK_BASE);
+	writel_relaxed(RKVDEC_CCU_BIT_WORK_MODE, dec->ccu->reg_base + RKVDEC_CCU_WORK_MODE_BASE);
+	writel_relaxed(dec->core_mask, dec->ccu->reg_base + RKVDEC_CCU_CORE_WORK_BASE);
+
+	/* set cache size */
+	reg = RKVDEC_CACHE_PERMIT_CACHEABLE_ACCESS |
+		  RKVDEC_CACHE_PERMIT_READ_ALLOCATE;
+	if (!mpp_debug_unlikely(DEBUG_CACHE_32B))
+		reg |= RKVDEC_CACHE_LINE_SIZE_64_BYTES;
+
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE0_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE1_SIZE_BASE, reg);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CACHE2_SIZE_BASE, reg);
+	/* clear cache */
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE0_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE1_BASE, 1);
+	mpp_write_relaxed(mpp, RKVDEC_REG_CLR_CACHE2_BASE, 1);
+
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	/* disable multicore pu/colmv offset req timeout reset */
+	task->reg[RKVDEC_REG_EN_MODE_SET] |= BIT(1);
+	task->reg[RKVDEC_REG_TIMEOUT_THRESHOLD] = rkvdec2_ccu_get_timeout_threshold(task);
+	/* set registers for hardware */
+	reg_en = mpp_task->hw_info->reg_en;
+	for (i = 0; i < task->w_req_cnt; i++) {
+		u32 s, e;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	mpp->irq_status = 0;
+	writel_relaxed(dec->core_mask, dec->ccu->reg_base + RKVDEC_CCU_CORE_STA_BASE);
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, RKVDEC_REG_START_EN_BASE, task->reg[reg_en] | RKVDEC_START_EN);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static struct mpp_dev *rkvdec2_get_idle_core(struct mpp_taskqueue *queue,
+					     struct mpp_task *mpp_task)
+{
+	u32 i = 0;
+	struct rkvdec2_dev *dec = NULL;
+
+	for (i = 0; i < queue->core_count; i++) {
+		struct mpp_dev *mpp = queue->cores[i];
+		struct rkvdec2_dev *core = to_rkvdec2_dev(mpp);
+
+		if (mpp->disable)
+			continue;
+
+		if (test_bit(i, &queue->core_idle)) {
+			if (!dec) {
+				dec = core;
+				continue;
+			}
+			/* set the less work core */
+			if (core->task_index < dec->task_index)
+				dec = core;
+		}
+	}
+	/* if get core */
+	if (dec) {
+		mpp_task->mpp = &dec->mpp;
+		mpp_task->core_id = dec->mpp.core_id;
+		clear_bit(mpp_task->core_id, &queue->core_idle);
+		dec->task_index++;
+		atomic_inc(&dec->mpp.task_count);
+		mpp_dbg_core("clear core %d idle\n", mpp_task->core_id);
+		return mpp_task->mpp;
+	}
+
+	return NULL;
+}
+
+static bool rkvdec2_core_working(struct mpp_taskqueue *queue)
+{
+	struct mpp_dev *mpp;
+	bool flag = false;
+	u32 i = 0;
+
+	for (i = 0; i < queue->core_count; i++) {
+		mpp = queue->cores[i];
+		if (mpp->disable)
+			continue;
+		if (!test_bit(i, &queue->core_idle)) {
+			flag = true;
+			break;
+		}
+	}
+
+	return flag;
+}
+
+void rkvdec2_soft_ccu_worker(struct kthread_work *work_s)
+{
+	struct mpp_task *mpp_task;
+	struct mpp_dev *mpp = container_of(work_s, struct mpp_dev, work);
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mpp_debug_enter();
+
+	/* 1. process all finished task in running list */
+	rkvdec2_soft_ccu_dequeue(queue);
+
+	/* 2. process reset request */
+	if (atomic_read(&queue->reset_request)) {
+		if (!rkvdec2_core_working(queue)) {
+			rkvdec2_ccu_power_on(queue, dec->ccu);
+			rkvdec2_soft_ccu_reset(queue, dec->ccu);
+		}
+	}
+
+	/* 3. process pending task */
+	while (1) {
+		if (atomic_read(&queue->reset_request))
+			break;
+		/* get one task form pending list */
+		mutex_lock(&queue->pending_lock);
+		mpp_task = list_first_entry_or_null(&queue->pending_list,
+						struct mpp_task, queue_link);
+		mutex_unlock(&queue->pending_lock);
+		if (!mpp_task)
+			break;
+
+		if (test_bit(TASK_STATE_ABORT, &mpp_task->state)) {
+			mutex_lock(&queue->pending_lock);
+			list_del_init(&mpp_task->queue_link);
+
+			set_bit(TASK_STATE_ABORT_READY, &mpp_task->state);
+			set_bit(TASK_STATE_PROC_DONE, &mpp_task->state);
+
+			mutex_unlock(&queue->pending_lock);
+			wake_up(&mpp_task->wait);
+			kref_put(&mpp_task->ref, rkvdec2_link_free_task);
+			continue;
+		}
+		/* find one core is idle */
+		mpp = rkvdec2_get_idle_core(queue, mpp_task);
+		if (!mpp)
+			break;
+
+		if (mpp->srv->timing_en) {
+			mpp_task->on_run = ktime_get();
+			set_bit(TASK_TIMING_RUN, &mpp_task->state);
+		}
+
+		/* set session index */
+		rkvdec2_set_core_info(mpp_task->reg, mpp_task->session->index);
+		/* set rcb buffer */
+		mpp_set_rcbbuf(mpp, mpp_task->session, mpp_task);
+
+		INIT_DELAYED_WORK(&mpp_task->timeout_work, rkvdec2_ccu_timeout_work);
+		rkvdec2_ccu_power_on(queue, dec->ccu);
+		rkvdec2_soft_ccu_enqueue(mpp, mpp_task);
+		/* pending to running */
+		mpp_taskqueue_pending_to_run(queue, mpp_task);
+		set_bit(TASK_STATE_RUNNING, &mpp_task->state);
+	}
+
+	/* 4. poweroff when running and pending list are empty */
+	if (list_empty(&queue->running_list) &&
+	    list_empty(&queue->pending_list))
+		rkvdec2_ccu_power_off(queue, dec->ccu);
+
+	/* 5. check session detach out of queue */
+	mpp_session_cleanup_detach(queue, work_s);
+
+	mpp_debug_leave();
+}
+
+int rkvdec2_ccu_alloc_table(struct rkvdec2_dev *dec,
+			    struct rkvdec_link_dev *link_dec)
+{
+	int ret;
+	u32 i;
+	struct mpp_dma_buffer *table;
+	struct mpp_dev *mpp = &dec->mpp;
+
+	mpp_debug_enter();
+
+	/* alloc table pointer array */
+	table = devm_kmalloc_array(mpp->dev, mpp->task_capacity,
+				   sizeof(*table), GFP_KERNEL | __GFP_ZERO);
+	if (!table)
+		return -ENOMEM;
+
+	/* alloc table buffer */
+	ret = rkvdec2_link_alloc_table(mpp, link_dec);
+	if (ret)
+		return ret;
+
+	/* init table array */
+	dec->ccu->table_array = table;
+	for (i = 0; i < mpp->task_capacity; i++) {
+		table[i].iova = link_dec->table->iova + i * link_dec->link_node_size;
+		table[i].vaddr = link_dec->table->vaddr + i * link_dec->link_node_size;
+		table[i].size = link_dec->link_node_size;
+		INIT_LIST_HEAD(&table[i].link);
+		list_add_tail(&table[i].link, &dec->ccu->unused_list);
+	}
+
+	return 0;
+}
+
+static void rkvdec2_dump_ccu(struct rkvdec2_ccu *ccu)
+{
+	u32 i;
+
+	for (i = 0; i < 10; i++)
+		mpp_err("ccu:reg[%d]=%08x\n", i, readl(ccu->reg_base + 4 * i));
+
+	for (i = 16; i < 22; i++)
+		mpp_err("ccu:reg[%d]=%08x\n", i, readl(ccu->reg_base + 4 * i));
+}
+
+static void rkvdec2_dump_link(struct rkvdec2_dev *dec)
+{
+	u32 i;
+
+	for (i = 0; i < 10; i++)
+		mpp_err("link:reg[%d]=%08x\n", i, readl(dec->link_dec->reg_base + 4 * i));
+}
+
+static void rkvdec2_dump_core(struct mpp_dev *mpp, struct rkvdec2_task *task)
+{
+	u32 j;
+
+	if (task) {
+		for (j = 0; j < 273; j++)
+			mpp_err("reg[%d]=%08x, %08x\n", j, mpp_read(mpp, j*4), task->reg[j]);
+	} else {
+		for (j = 0; j < 273; j++)
+			mpp_err("reg[%d]=%08x\n", j, mpp_read(mpp, j*4));
+	}
+}
+
+irqreturn_t rkvdec2_hard_ccu_irq(int irq, void *param)
+{
+	u32 irq_status;
+	struct mpp_dev *mpp = param;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	irq_status = readl(link_dec->reg_base + link_dec->info->irq_base);
+	dec->ccu->ccu_core_work_mode = readl(dec->ccu->reg_base + RKVDEC_CCU_CORE_WORK_BASE);
+	if (irq_status & RKVDEC_LINK_BIT_IRQ_RAW) {
+		link_dec->irq_status = irq_status;
+		mpp->irq_status = mpp_read(mpp, RKVDEC_REG_INT_EN);
+		mpp_debug(DEBUG_IRQ_STATUS, "core %d link_irq=%08x, core_irq=%08x\n",
+			  mpp->core_id, irq_status, mpp->irq_status);
+
+		writel(irq_status & 0xfffff0ff,
+		       link_dec->reg_base + link_dec->info->irq_base);
+
+		kthread_queue_work(&mpp->queue->worker, &mpp->work);
+		return IRQ_HANDLED;
+	}
+
+	return IRQ_NONE;
+}
+
+static int rkvdec2_hard_ccu_finish(struct rkvdec_link_info *hw, struct rkvdec2_task *task)
+{
+	u32 i, off, s, n;
+	u32 reg_ret_status;
+	struct rkvdec_link_part *part = hw->part_r;
+	u32 *tb_reg = (u32 *)task->table->vaddr;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < hw->part_r_num; i++) {
+		off = part[i].tb_reg_off;
+		s = part[i].reg_start;
+		n = part[i].reg_num;
+		memcpy(&task->reg[s], &tb_reg[off], n * sizeof(u32));
+	}
+	/* revert hack for irq status */
+	reg_ret_status = task->mpp_task.hw_info->reg_ret_status;
+	task->reg[reg_ret_status] = task->irq_status;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvdec2_hard_ccu_dequeue(struct mpp_taskqueue *queue,
+				    struct rkvdec2_ccu *ccu,
+				    struct rkvdec_link_info *hw)
+{
+	struct mpp_task *mpp_task = NULL, *n;
+	u32 dump_reg = 0;
+	u32 dequeue_none = 0;
+	unsigned long flags;
+
+	mpp_debug_enter();
+	list_for_each_entry_safe(mpp_task, n, &queue->running_list, queue_link) {
+		u32 timeout_flag = test_bit(TASK_STATE_TIMEOUT, &mpp_task->state);
+		u32 abort_flag = test_bit(TASK_STATE_ABORT, &mpp_task->state);
+		struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+		u32 *tb_reg = (u32 *)task->table->vaddr;
+		u32 irq_status = tb_reg[hw->tb_reg_int];
+		u32 ccu_decoded_num, ccu_total_dec_num;
+
+		ccu_decoded_num = readl(ccu->reg_base + RKVDEC_CCU_DEC_NUM_BASE);
+		ccu_total_dec_num = readl(ccu->reg_base + RKVDEC_CCU_TOTAL_NUM_BASE);
+		mpp_debug(DEBUG_IRQ_CHECK,
+			  "session %d task %d w:h[%d %d] err %d irq_status %#x timeout=%u abort=%u iova %08x next %08x ccu[%d %d]\n",
+			  mpp_task->session->index, mpp_task->task_index, task->width,
+			  task->height, !!(irq_status & hw->err_mask), irq_status,
+			  timeout_flag, abort_flag, (u32)task->table->iova,
+			  ((u32 *)task->table->vaddr)[hw->tb_reg_next],
+			  ccu_decoded_num, ccu_total_dec_num);
+
+		if (irq_status || timeout_flag || abort_flag) {
+			struct rkvdec2_dev *dec = to_rkvdec2_dev(queue->cores[0]);
+
+			set_bit(TASK_STATE_HANDLE, &mpp_task->state);
+			cancel_delayed_work(&mpp_task->timeout_work);
+			mpp_task->hw_cycles = tb_reg[hw->tb_reg_cycle];
+			mpp_task->hw_time = mpp_task->hw_cycles /
+					    (dec->cycle_clk->real_rate_hz / 1000000);
+			mpp_time_diff_with_hw_time(mpp_task, dec->cycle_clk->real_rate_hz);
+			task->irq_status = irq_status ? irq_status : RKVDEC_ERROR_STA;
+
+			if (irq_status)
+				rkvdec2_hard_ccu_finish(hw, task);
+
+			set_bit(TASK_STATE_FINISH, &mpp_task->state);
+			set_bit(TASK_STATE_DONE, &mpp_task->state);
+
+			if (timeout_flag && !dump_reg && mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+				u32 i;
+
+				mpp_err("###### ccu #####\n");
+				rkvdec2_dump_ccu(ccu);
+				for (i = 0; i < queue->core_count; i++) {
+					mpp_err("###### core %d #####\n", i);
+					rkvdec2_dump_link(to_rkvdec2_dev(queue->cores[i]));
+					rkvdec2_dump_core(queue->cores[i], task);
+				}
+				dump_reg = 1;
+			}
+			spin_lock_irqsave(&queue->running_lock, flags);
+			list_move_tail(&task->table->link, &ccu->unused_list);
+			/* free task */
+			list_del_init(&mpp_task->queue_link);
+			spin_unlock_irqrestore(&queue->running_lock, flags);
+			/* Wake up the GET thread */
+			wake_up(&mpp_task->wait);
+			if ((irq_status & hw->err_mask) || timeout_flag) {
+				pr_err("session %d task %d irq_status %#x timeout=%u abort=%u\n",
+					mpp_task->session->index, mpp_task->task_index,
+					irq_status, timeout_flag, abort_flag);
+				atomic_inc(&queue->reset_request);
+			}
+			mpp_dev_load(mpp_task->session->mpp, mpp_task);
+			kref_put(&mpp_task->ref, mpp_free_task);
+		} else {
+			dequeue_none++;
+			/*
+			 * there are only 2 cores,
+			 * if dequeue not finish task more than 2,
+			 * means the others task still not get run by hw, can break early.
+			 */
+			if (dequeue_none > 2)
+				break;
+		}
+	}
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static int rkvdec2_hard_ccu_reset(struct mpp_taskqueue *queue, struct rkvdec2_ccu *ccu)
+{
+	u32 i = 0;
+
+	mpp_debug_enter();
+
+	/* reset and active core */
+	for (i = 0; i < queue->core_count; i++) {
+		u32 val = 0;
+		struct mpp_dev *mpp = queue->cores[i];
+		struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+		if (mpp->disable)
+			continue;
+		dev_info(mpp->dev, "resetting...\n");
+		disable_hardirq(mpp->irq);
+		/* force idle */
+		writel(dec->core_mask, ccu->reg_base + RKVDEC_CCU_CORE_IDLE_BASE);
+		writel(0, ccu->reg_base + RKVDEC_CCU_WORK_BASE);
+
+		{
+			/* soft reset */
+			u32 val;
+
+			mpp_write(mpp, RKVDEC_REG_IMPORTANT_BASE, RKVDEC_SOFTREST_EN);
+			udelay(5);
+			val = mpp_read(mpp, RKVDEC_REG_INT_EN);
+			if (!(val & RKVDEC_SOFT_RESET_READY))
+				mpp_err("soft reset fail, int %08x\n", val);
+
+			// /* cru reset */
+			// dev_info(mpp->dev, "cru reset\n");
+			// rkvdec2_reset(mpp);
+		}
+#if IS_ENABLED(CONFIG_ROCKCHIP_SIP)
+		rockchip_dmcfreq_lock();
+		sip_smc_vpu_reset(i, 0, 0);
+		rockchip_dmcfreq_unlock();
+#else
+		rkvdec2_reset(mpp);
+#endif
+		mpp_iommu_refresh(mpp->iommu_info, mpp->dev);
+		enable_irq(mpp->irq);
+		atomic_set(&mpp->reset_request, 0);
+		val = mpp_read_relaxed(mpp, 272*4);
+		dev_info(mpp->dev, "reset done, idle %d\n", (val & 1));
+	}
+	/* reset ccu */
+	mpp_safe_reset(ccu->rst_a);
+	udelay(5);
+	mpp_safe_unreset(ccu->rst_a);
+
+	mpp_debug_leave();
+	return 0;
+}
+
+static struct mpp_task *
+rkvdec2_hard_ccu_prepare(struct mpp_task *mpp_task,
+			 struct rkvdec2_ccu *ccu, struct rkvdec_link_info *hw)
+{
+	u32 i, off, s, n;
+	u32 *tb_reg;
+	struct mpp_dma_buffer *table = NULL;
+	struct rkvdec_link_part *part;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+
+	mpp_debug_enter();
+
+	if (test_bit(TASK_STATE_PREPARE, &mpp_task->state))
+		return mpp_task;
+
+	/* ensure that cur table iova points to the next link table*/
+	{
+		struct mpp_dma_buffer *table0 = NULL, *table1 = NULL, *n;
+
+		list_for_each_entry_safe(table, n, &ccu->unused_list, link) {
+			if (!table0) {
+				table0 = table;
+				continue;
+			}
+			if (!table1)
+				table1 = table;
+			break;
+		}
+		if (!table0 || !table1)
+			return NULL;
+		((u32 *)table0->vaddr)[hw->tb_reg_next] = table1->iova;
+		table = table0;
+	}
+
+	/* set session idx */
+	rkvdec2_set_core_info(task->reg, mpp_task->session->index);
+	tb_reg = (u32 *)table->vaddr;
+	part = hw->part_w;
+
+	/* disable multicore pu/colmv offset req timeout reset */
+	task->reg[RKVDEC_REG_EN_MODE_SET] |= BIT(1);
+	task->reg[RKVDEC_REG_TIMEOUT_THRESHOLD] = rkvdec2_ccu_get_timeout_threshold(task);
+
+	for (i = 0; i < hw->part_w_num; i++) {
+		off = part[i].tb_reg_off;
+		s = part[i].reg_start;
+		n = part[i].reg_num;
+		memcpy(&tb_reg[off], &task->reg[s], n * sizeof(u32));
+	}
+
+	/* memset read registers */
+	part = hw->part_r;
+	for (i = 0; i < hw->part_r_num; i++) {
+		off = part[i].tb_reg_off;
+		n = part[i].reg_num;
+		memset(&tb_reg[off], 0, n * sizeof(u32));
+	}
+	list_move_tail(&table->link, &ccu->used_list);
+	task->table = table;
+	set_bit(TASK_STATE_PREPARE, &mpp_task->state);
+	mpp_dbg_ccu("session %d task %d iova %08x next %08x\n",
+		    mpp_task->session->index, mpp_task->task_index, (u32)task->table->iova,
+		    ((u32 *)task->table->vaddr)[hw->tb_reg_next]);
+
+	mpp_debug_leave();
+
+	return mpp_task;
+}
+
+static int rkvdec2_ccu_link_fix_rcb_regs(struct rkvdec2_dev *dec)
+{
+	int ret = 0;
+	u32 i, val;
+	u32 reg, reg_idx, rcb_size, rcb_offset;
+	struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+	if (!dec->rcb_iova && !dec->rcb_info_count)
+		goto done;
+	/* check whether fixed */
+	val = readl(link_dec->reg_base + link_dec->info->irq_base);
+	if (val & RKVDEC_CCU_BIT_FIX_RCB)
+		goto done;
+	/* set registers */
+	rcb_offset = 0;
+	for (i = 0; i < dec->rcb_info_count; i += 2) {
+		reg_idx = dec->rcb_infos[i];
+		rcb_size = dec->rcb_infos[i + 1];
+		mpp_debug(DEBUG_SRAM_INFO,
+			  "rcb: reg %u size %u offset %u sram_size %u rcb_size %u\n",
+			  reg_idx, rcb_size, rcb_offset, dec->sram_size, dec->rcb_size);
+		if ((rcb_offset + rcb_size) > dec->rcb_size) {
+			mpp_err("rcb: reg[%u] set failed.\n", reg_idx);
+			ret = -ENOMEM;
+			goto done;
+		}
+		reg = dec->rcb_iova + rcb_offset;
+		mpp_write(&dec->mpp, reg_idx * sizeof(u32), reg);
+		rcb_offset += rcb_size;
+	}
+
+	val |= RKVDEC_CCU_BIT_FIX_RCB;
+	writel(val, link_dec->reg_base + link_dec->info->irq_base);
+done:
+	return ret;
+}
+
+static int rkvdec2_hard_ccu_enqueue(struct rkvdec2_ccu *ccu,
+				    struct mpp_task *mpp_task,
+				    struct mpp_taskqueue *queue,
+				    struct mpp_dev *mpp)
+{
+	u32 ccu_en, work_mode, link_mode;
+	struct rkvdec2_task *task = to_rkvdec2_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	if (test_bit(TASK_STATE_START, &mpp_task->state))
+		goto done;
+
+	ccu_en = readl(ccu->reg_base + RKVDEC_CCU_WORK_BASE);
+	mpp_dbg_ccu("ccu_en=%d\n", ccu_en);
+	if (!ccu_en) {
+		u32 i;
+
+		/* set work mode */
+		work_mode = 0;
+		for (i = 0; i < queue->core_count; i++) {
+			u32 val;
+			struct mpp_dev *core = queue->cores[i];
+			struct rkvdec2_dev *dec = to_rkvdec2_dev(core);
+			struct rkvdec_link_dev *link_dec = dec->link_dec;
+
+			if (mpp->disable)
+				continue;
+			work_mode |= dec->core_mask;
+			rkvdec2_ccu_link_fix_rcb_regs(dec);
+			/* control by ccu */
+			val = readl(link_dec->reg_base + link_dec->info->irq_base);
+			val |= RKVDEC_LINK_BIT_CCU_WORK_MODE;
+			writel(val, link_dec->reg_base + link_dec->info->irq_base);
+		}
+		writel(work_mode, ccu->reg_base + RKVDEC_CCU_CORE_WORK_BASE);
+		ccu->ccu_core_work_mode = readl(ccu->reg_base + RKVDEC_CCU_CORE_WORK_BASE);
+		mpp_dbg_ccu("ccu_work_mode=%08x, ccu_work_status=%08x\n",
+			    readl(ccu->reg_base + RKVDEC_CCU_CORE_WORK_BASE),
+			    readl(ccu->reg_base + RKVDEC_CCU_CORE_STA_BASE));
+
+		/* set auto gating */
+		writel(RKVDEC_CCU_BIT_AUTOGATE, ccu->reg_base + RKVDEC_CCU_CTRL_BASE);
+		/* link start base */
+		writel(task->table->iova, ccu->reg_base + RKVDEC_CCU_CFG_ADDR_BASE);
+		/* enable link */
+		writel(RKVDEC_CCU_BIT_WORK_EN, ccu->reg_base + RKVDEC_CCU_WORK_BASE);
+	}
+
+	/* set link mode */
+	link_mode = ccu_en ? RKVDEC_CCU_BIT_ADD_MODE : 0;
+	writel(link_mode | RKVDEC_LINK_ADD_CFG_NUM, ccu->reg_base + RKVDEC_CCU_LINK_MODE_BASE);
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	/* wmb */
+	wmb();
+	INIT_DELAYED_WORK(&mpp_task->timeout_work, rkvdec2_ccu_timeout_work);
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+	/* configure done */
+	writel(RKVDEC_CCU_BIT_CFG_DONE, ccu->reg_base + RKVDEC_CCU_CFG_DONE_BASE);
+	mpp_task_run_end(mpp_task, timing_en);
+
+	set_bit(TASK_STATE_RUNNING, &mpp_task->state);
+	mpp_dbg_ccu("session %d task %d iova=%08x task->state=%lx link_mode=%08x\n",
+		    mpp_task->session->index, mpp_task->task_index,
+		    (u32)task->table->iova, mpp_task->state,
+		    readl(ccu->reg_base + RKVDEC_CCU_LINK_MODE_BASE));
+done:
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static void rkvdec2_hard_ccu_resend_tasks(struct mpp_dev *mpp, struct mpp_taskqueue *queue)
+{
+	struct rkvdec2_task *task_pre = NULL;
+	struct mpp_task *loop = NULL, *n;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	/* re sort running list */
+	list_for_each_entry_safe(loop, n, &queue->running_list, queue_link) {
+		struct rkvdec2_task *task = to_rkvdec2_task(loop);
+		u32 *tb_reg = (u32 *)task->table->vaddr;
+		u32 irq_status = tb_reg[dec->link_dec->info->tb_reg_int];
+
+		if (!irq_status) {
+			if (task_pre) {
+				tb_reg = (u32 *)task_pre->table->vaddr;
+				tb_reg[dec->link_dec->info->tb_reg_next] = task->table->iova;
+			}
+			task_pre = task;
+		}
+	}
+
+	if (task_pre) {
+		struct mpp_dma_buffer *tbl;
+		u32 *tb_reg;
+
+		tbl = list_first_entry_or_null(&dec->ccu->unused_list,
+				struct mpp_dma_buffer, link);
+		WARN_ON(!tbl);
+		if (tbl) {
+			tb_reg = (u32 *)task_pre->table->vaddr;
+			tb_reg[dec->link_dec->info->tb_reg_next] = tbl->iova;
+		}
+	}
+
+	/* resend */
+	list_for_each_entry_safe(loop, n, &queue->running_list, queue_link) {
+		struct rkvdec2_task *task = to_rkvdec2_task(loop);
+		u32 *tb_reg = (u32 *)task->table->vaddr;
+		u32 irq_status = tb_reg[dec->link_dec->info->tb_reg_int];
+
+		mpp_dbg_ccu("reback: session %d task %d iova %08x next %08x irq_status 0x%08x\n",
+				loop->session->index, loop->task_index, (u32)task->table->iova,
+				tb_reg[dec->link_dec->info->tb_reg_next], irq_status);
+
+		if (!irq_status) {
+			cancel_delayed_work(&loop->timeout_work);
+			clear_bit(TASK_STATE_START, &loop->state);
+			rkvdec2_hard_ccu_enqueue(dec->ccu, loop, queue, mpp);
+		}
+	}
+}
+
+void rkvdec2_hard_ccu_worker(struct kthread_work *work_s)
+{
+	struct mpp_task *mpp_task;
+	struct mpp_dev *mpp = container_of(work_s, struct mpp_dev, work);
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct rkvdec2_dev *dec = to_rkvdec2_dev(mpp);
+
+	mpp_debug_enter();
+
+	/* 1. process all finished task in running list */
+	rkvdec2_hard_ccu_dequeue(queue, dec->ccu, dec->link_dec->info);
+
+	/* 2. process reset request */
+	if (atomic_read(&queue->reset_request) &&
+	    (list_empty(&queue->running_list) || !dec->ccu->ccu_core_work_mode)) {
+		/*
+		 * cancel running list timeout work to avoid
+		 * sw timeout causeby reset long time
+		 */
+		struct mpp_task *loop = NULL, *n;
+
+		list_for_each_entry_safe(loop, n, &queue->running_list, queue_link) {
+			cancel_delayed_work(&loop->timeout_work);
+		}
+		/* reset process */
+		rkvdec2_hard_ccu_reset(queue, dec->ccu);
+		atomic_set(&queue->reset_request, 0);
+
+		/* relink running task iova in list, and resend them to hw */
+		if (!list_empty(&queue->running_list))
+			rkvdec2_hard_ccu_resend_tasks(mpp, queue);
+	}
+
+	/* 3. process pending task */
+	while (1) {
+		if (atomic_read(&queue->reset_request))
+			break;
+
+		/* get one task form pending list */
+		mutex_lock(&queue->pending_lock);
+		mpp_task = list_first_entry_or_null(&queue->pending_list,
+						struct mpp_task, queue_link);
+		mutex_unlock(&queue->pending_lock);
+
+		if (!mpp_task)
+			break;
+		if (test_bit(TASK_STATE_ABORT, &mpp_task->state)) {
+			mutex_lock(&queue->pending_lock);
+			list_del_init(&mpp_task->queue_link);
+			mutex_unlock(&queue->pending_lock);
+			kref_put(&mpp_task->ref, mpp_free_task);
+			continue;
+		}
+
+		mpp_task = rkvdec2_hard_ccu_prepare(mpp_task, dec->ccu, dec->link_dec->info);
+		if (!mpp_task)
+			break;
+		if (mpp->srv->timing_en) {
+			mpp_task->on_run = ktime_get();
+			set_bit(TASK_TIMING_RUN, &mpp_task->state);
+		}
+		rkvdec2_ccu_power_on(queue, dec->ccu);
+		rkvdec2_hard_ccu_enqueue(dec->ccu, mpp_task, queue, mpp);
+		mpp_taskqueue_pending_to_run(queue, mpp_task);
+	}
+	/* 4. poweroff when running and pending list are empty */
+	mutex_lock(&queue->pending_lock);
+	if (list_empty(&queue->running_list) &&
+	    list_empty(&queue->pending_list))
+		rkvdec2_ccu_power_off(queue, dec->ccu);
+	mutex_unlock(&queue->pending_lock);
+
+	/* 5. check session detach out of queue */
+	mpp_session_cleanup_detach(queue, work_s);
+
+	mpp_debug_leave();
+}
diff --git a/drivers/video/rockchip/mpp/mpp_rkvdec2_link.h b/drivers/video/rockchip/mpp/mpp_rkvdec2_link.h
new file mode 100644
index 0000000000000..ca6ae6acce0d2
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvdec2_link.h
@@ -0,0 +1,254 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Herman Chen <herman.chen@rock-chips.com>
+ */
+#ifndef __ROCKCHIP_MPP_RKVDEC2_LINK_H__
+#define __ROCKCHIP_MPP_RKVDEC2_LINK_H__
+
+#include "mpp_rkvdec2.h"
+
+#define RKVDEC_REG_IMPORTANT_BASE	0x2c
+#define RKVDEC_REG_IMPORTANT_INDEX	11
+#define RKVDEC_SOFTREST_EN		BIT(20)
+
+#define RKVDEC_REG_SECOND_EN_BASE	0x30
+#define RKVDEC_REG_SECOND_EN_INDEX	12
+#define RKVDEC_WAIT_RESET_EN		BIT(7)
+
+#define RKVDEC_REG_EN_MODE_SET		13
+
+#define RKVDEC_REG_DEBUG_INT_BASE	0x440
+#define RKVDEC_REG_DEBUG_INT_INDEX	272
+#define RKVDEC_BIT_BUS_IDLE		BIT(0)
+
+#define RKVDEC_REG_TIMEOUT_THRESHOLD	32
+
+/* define for link hardware */
+#define RKVDEC_LINK_ADD_CFG_NUM		1
+
+#define RKVDEC_LINK_BIT_IRQ_RAW		BIT(9)
+#define RKVDEC_LINK_BIT_CORE_WORK_MODE	BIT(16)
+#define RKVDEC_LINK_BIT_CCU_WORK_MODE	BIT(17)
+
+#define RKVDEC_LINK_CFG_ADDR_BASE	0x004
+
+#define RKVDEC_LINK_MODE_BASE		0x008
+#define RKVDEC_LINK_BIT_ADD_MODE	BIT(31)
+
+#define RKVDEC_LINK_CFG_CTRL_BASE	0x00c
+#define RKVDEC_LINK_BIT_CFG_DONE	BIT(0)
+
+#define RKVDEC_LINK_DEC_NUM_BASE	0x010
+
+#define RKVDEC_LINK_TOTAL_NUM_BASE	0x014
+
+#define RKVDEC_LINK_EN_BASE		0x018
+#define RKVDEC_LINK_BIT_EN		BIT(0)
+
+/* define for ccu link hardware */
+#define RKVDEC_CCU_CTRL_BASE		0x000
+#define RKVDEC_CCU_BIT_AUTOGATE		BIT(0)
+#define RKVDEC_CCU_BIT_FIX_RCB		BIT(20)
+
+#define RKVDEC_CCU_CFG_ADDR_BASE	0x004
+#define RKVDEC_CCU_LINK_MODE_BASE	0x008
+#define RKVDEC_CCU_BIT_ADD_MODE		BIT(31)
+
+#define RKVDEC_CCU_CFG_DONE_BASE	0x00c
+#define RKVDEC_CCU_BIT_CFG_DONE		BIT(0)
+
+#define RKVDEC_CCU_DEC_NUM_BASE		0x010
+#define RKVDEC_CCU_TOTAL_NUM_BASE	0x014
+
+#define RKVDEC_CCU_WORK_BASE		0x018
+#define RKVDEC_CCU_BIT_WORK_EN		BIT(0)
+
+#define RKVDEC_CCU_SEND_NUM_BASE	0x024
+#define RKVDEC_CCU_WORK_MODE_BASE	0x040
+#define RKVDEC_CCU_BIT_WORK_MODE	BIT(0)
+
+#define RKVDEC_CCU_CORE_WORK_BASE	0x044
+#define RKVDEC_CCU_CORE_STA_BASE	0x048
+#define RKVDEC_CCU_CORE_IDLE_BASE	0x04c
+#define RKVDEC_CCU_CORE_ERR_BASE	0x054
+
+#define RKVDEC_CCU_CORE_RW_MASK		0x30000
+
+#define RKVDEC_MAX_WRITE_PART	6
+#define RKVDEC_MAX_READ_PART	2
+
+struct rkvdec_link_part {
+	/* register offset of table buffer */
+	u32 tb_reg_off;
+	/* start idx of task register */
+	u32 reg_start;
+	/* number of task register */
+	u32 reg_num;
+};
+
+struct rkvdec_link_status {
+	u32 dec_num_mask;
+	u32 err_flag_base;
+	u32 err_flag_bit;
+};
+
+struct rkvdec_link_info {
+	dma_addr_t iova;
+	/* total register for link table buffer */
+	u32 tb_reg_num;
+	/* next link table addr in table buffer */
+	u32 tb_reg_next;
+	/* current read back addr in table buffer */
+	u32 tb_reg_r;
+	/* secondary enable in table buffer */
+	int tb_reg_debug;
+	int tb_reg_seg0;
+	int tb_reg_seg1;
+	int tb_reg_seg2;
+	int tb_reg_second_en;
+	u32 part_w_num;
+	u32 part_r_num;
+
+	struct rkvdec_link_part part_w[RKVDEC_MAX_WRITE_PART];
+	struct rkvdec_link_part part_r[RKVDEC_MAX_READ_PART];
+
+	/* interrupt read back in table buffer */
+	u32 tb_reg_int;
+	u32 tb_reg_cycle;
+	bool hack_setup;
+	struct rkvdec_link_status reg_status;
+
+	/* for next link node addr */
+	u32 next_addr_base;
+
+	/* register for vdpu383 later */
+	u32 ip_reset_base;
+	u32 ip_reset_en;
+	u32 irq_base;
+	u32 irq_mask;
+	u32 status_base;
+	u32 status_mask;
+	u32 err_mask;
+	u32 ip_reset_mask;
+	u32 ip_time_base;
+	u32 en_base;
+	u32 ip_en_base;
+	u32 ip_en_val;
+};
+
+struct rkvdec_link_dev {
+	struct device *dev;
+	struct mpp_dev *mpp;
+	void __iomem *reg_base;
+	u32 enabled;
+	u32 link_mode;
+	u32 decoded_status;
+	u32 irq_status;
+	u32 iova_curr;
+	u32 iova_next;
+	u32 decoded;
+	u32 total;
+	u32 error;
+	u32 hack_task_running;
+
+	struct rkvdec_link_info *info;
+	struct mpp_dma_buffer *table;
+	u32 link_node_size;
+	u32 link_reg_count;
+
+	/* taskqueue variables */
+	u32 task_running;
+	atomic_t task_pending;
+	/* timeout can be trigger in different thread so atomic is needed */
+	atomic_t task_timeout;
+	u32 task_timeout_prev;
+
+	/* link mode hardware status */
+	atomic_t power_enabled;
+	u32 irq_enabled;
+
+	/* debug variable */
+	u32 statistic_count;
+	u64 task_cycle_sum;
+	u32 task_cnt;
+	u64 stuff_cycle_sum;
+	u32 stuff_cnt;
+
+	/* link info */
+	u32 task_capacity;
+	struct mpp_dma_buffer *table_array;
+	struct list_head unused_list;
+	struct list_head used_list;
+};
+
+enum RKVDEC2_CCU_MODE {
+	RKVDEC2_CCU_MODE_NULL		= 0,
+	RKVDEC2_CCU_TASK_SOFT		= 1,
+	RKVDEC2_CCU_TASK_HARD		= 2,
+	RKVDEC2_CCU_MODE_BUTT,
+};
+
+struct rkvdec2_ccu {
+	struct device *dev;
+	/* register base */
+	void __iomem *reg_base;
+
+	atomic_t power_enabled;
+	struct mpp_clk_info aclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	enum RKVDEC2_CCU_MODE ccu_mode;
+	u32 ccu_core_work_mode;
+
+	struct mpp_dma_buffer *table_array;
+	struct list_head unused_list;
+	struct list_head used_list;
+	u32 timeout_flag;
+};
+
+extern struct rkvdec_link_info rkvdec_link_rk356x_hw_info;
+extern struct rkvdec_link_info rkvdec_link_v2_hw_info;
+extern struct rkvdec_link_info rkvdec_link_vdpu382_hw_info;
+extern struct rkvdec_link_info rkvdec_link_vdpu383_hw_info;
+
+int rkvdec_link_dump(struct mpp_dev *mpp);
+
+int rkvdec2_link_init(struct platform_device *pdev, struct rkvdec2_dev *dec);
+int rkvdec2_link_procfs_init(struct mpp_dev *mpp);
+int rkvdec2_link_remove(struct mpp_dev *mpp, struct rkvdec_link_dev *link_dec);
+
+irqreturn_t rkvdec2_link_irq_proc(int irq, void *param);
+int rkvdec2_link_process_task(struct mpp_session *session,
+			      struct mpp_task_msgs *msgs);
+int rkvdec2_link_wait_result(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs);
+void rkvdec2_link_worker(struct kthread_work *work_s);
+void rkvdec2_link_session_deinit(struct mpp_session *session);
+
+/* for ccu link */
+int rkvdec2_attach_ccu(struct device *dev, struct rkvdec2_dev *dec);
+int rkvdec2_ccu_link_init(struct platform_device *pdev, struct rkvdec2_dev *dec);
+void *rkvdec2_ccu_alloc_task(struct mpp_session *session, struct mpp_task_msgs *msgs);
+int rkvdec2_soft_ccu_iommu_fault_handle(struct iommu_domain *iommu,
+					struct device *iommu_dev,
+					unsigned long iova, int status, void *arg);
+irqreturn_t rkvdec2_soft_ccu_irq(int irq, void *param);
+void rkvdec2_soft_ccu_worker(struct kthread_work *work_s);
+
+int rkvdec2_ccu_alloc_table(struct rkvdec2_dev *dec,
+			    struct rkvdec_link_dev *link_dec);
+irqreturn_t rkvdec2_hard_ccu_irq(int irq, void *param);
+void rkvdec2_hard_ccu_worker(struct kthread_work *work_s);
+int rkvdec2_hard_ccu_iommu_fault_handle(struct iommu_domain *iommu,
+					struct device *iommu_dev,
+					unsigned long iova, int status, void *arg);
+
+/* for special handle */
+int rkvdec_vdpu383_link_irq(struct mpp_dev *mpp);
+
+#endif
diff --git a/drivers/video/rockchip/mpp/mpp_rkvenc.c b/drivers/video/rockchip/mpp/mpp_rkvenc.c
new file mode 100644
index 0000000000000..5ab7441b178f7
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvenc.c
@@ -0,0 +1,1466 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+
+#include <asm/cacheflush.h>
+#include <linux/delay.h>
+#include <linux/devfreq.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/regulator/consumer.h>
+#include <linux/proc_fs.h>
+#include <linux/nospec.h>
+#include <linux/workqueue.h>
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_iommu.h>
+#include <soc/rockchip/rockchip_ipa.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_system_monitor.h>
+
+#ifdef CONFIG_PM_DEVFREQ
+#include "../../../devfreq/governor.h"
+#endif
+
+#include "mpp_debug.h"
+#include "mpp_iommu.h"
+#include "mpp_common.h"
+
+#define RKVENC_DRIVER_NAME			"mpp_rkvenc"
+
+#define IOMMU_GET_BUS_ID(x)			(((x) >> 6) & 0x1f)
+#define IOMMU_PAGE_SIZE				SZ_4K
+
+#define	RKVENC_SESSION_MAX_BUFFERS		40
+/* The maximum registers number of all the version */
+#define RKVENC_REG_L1_NUM			780
+#define RKVENC_REG_L2_NUM			320
+#define RKVENC_REG_START_INDEX			0
+#define RKVENC_REG_END_INDEX			131
+/* rkvenc register info */
+#define RKVENC_REG_NUM				112
+#define RKVENC_REG_HW_ID_INDEX			0
+#define RKVENC_REG_CLR_CACHE_BASE		0x884
+
+#define RKVENC_ENC_START_INDEX			1
+#define RKVENC_ENC_START_BASE			0x004
+#define RKVENC_LKT_NUM(x)			((x) & 0xff)
+#define RKVENC_CMD(x)				(((x) & 0x3) << 8)
+#define RKVENC_CLK_GATE_EN			BIT(16)
+#define RKVENC_CLR_BASE				0x008
+#define RKVENC_SAFE_CLR_BIT			BIT(0)
+#define RKVENC_FORCE_CLR_BIT			BIT(1)
+#define RKVENC_LKT_ADDR_BASE			0x00c
+
+#define RKVENC_INT_EN_INDEX			4
+#define RKVENC_INT_EN_BASE			0x010
+#define RKVENC_INT_MSK_BASE			0x014
+#define RKVENC_INT_CLR_BASE			0x018
+#define RKVENC_INT_STATUS_INDEX			7
+#define RKVENC_INT_STATUS_BASE			0x01c
+/* bit for int mask clr status */
+#define RKVENC_BIT_ONE_FRAME			BIT(0)
+#define RKVENC_BIT_LINK_TABLE			BIT(1)
+#define RKVENC_BIT_SAFE_CLEAR			BIT(2)
+#define RKVENC_BIT_ONE_SLICE			BIT(3)
+#define RKVENC_BIT_STREAM_OVERFLOW		BIT(4)
+#define RKVENC_BIT_AXI_WRITE_FIFO_FULL		BIT(5)
+#define RKVENC_BIT_AXI_WRITE_CHANNEL		BIT(6)
+#define RKVENC_BIT_AXI_READ_CHANNEL		BIT(7)
+#define RKVENC_BIT_TIMEOUT			BIT(8)
+#define RKVENC_INT_ERROR_BITS	((RKVENC_BIT_STREAM_OVERFLOW) |\
+				(RKVENC_BIT_AXI_WRITE_FIFO_FULL) |\
+				(RKVENC_BIT_AXI_WRITE_CHANNEL) |\
+				(RKVENC_BIT_AXI_READ_CHANNEL) |\
+				(RKVENC_BIT_TIMEOUT))
+#define RKVENC_ENC_RSL_INDEX			12
+#define RKVENC_ENC_PIC_INDEX			13
+#define RKVENC_ENC_PIC_BASE			0x034
+#define RKVENC_GET_FORMAT(x)			((x) & 0x1)
+#define RKVENC_ENC_PIC_NODE_INT_EN		BIT(31)
+#define RKVENC_ENC_WDG_BASE			0x038
+#define RKVENC_PPLN_ENC_LMT(x)			((x) & 0xf)
+#define RKVENC_OSD_CFG_BASE			0x1c0
+#define RKVENC_OSD_PLT_TYPE			BIT(17)
+#define RKVENC_OSD_CLK_SEL_BIT			BIT(16)
+#define RKVENC_STATUS_BASE(i)			(0x210 + (4 * (i)))
+#define RKVENC_BSL_STATUS_BASE			0x210
+#define RKVENC_BITSTREAM_LENGTH(x)		((x) & 0x7FFFFFF)
+#define RKVENC_ENC_STATUS_BASE			0x220
+#define RKVENC_ENC_STATUS_ENC(x)		(((x) >> 0) & 0x3)
+#define RKVENC_LKT_STATUS_BASE			0x224
+#define RKVENC_LKT_STATUS_FNUM_ENC(x)		(((x) >> 0) & 0xff)
+#define RKVENC_LKT_STATUS_FNUM_CFG(x)		(((x) >> 8) & 0xff)
+#define RKVENC_LKT_STATUS_FNUM_INT(x)		(((x) >> 16) & 0xff)
+#define RKVENC_OSD_PLT_BASE(i)			(0x400 + (4 * (i)))
+
+#define RKVENC_L2_OFFSET			(0x10000)
+#define RKVENC_L2_ADDR_BASE			(0x3f0)
+#define RKVENC_L2_WRITE_BASE			(0x3f4)
+#define RKVENC_L2_READ_BASE			(0x3f8)
+#define RKVENC_L2_BURST_TYPE			BIT(0)
+
+#define RKVENC_GET_WIDTH(x)			(((x & 0x1ff) + 1) << 3)
+#define RKVENC_GET_HEIGHT(x)			((((x >> 16) & 0x1ff) + 1) << 3)
+
+#define to_rkvenc_task(ctx)		\
+		container_of(ctx, struct rkvenc_task, mpp_task)
+#define to_rkvenc_dev(dev)		\
+		container_of(dev, struct rkvenc_dev, mpp)
+
+enum rkvenc_format_type {
+	RKVENC_FMT_H264E = 0,
+	RKVENC_FMT_H265E = 1,
+	RKVENC_FMT_BUTT,
+};
+
+enum RKVENC_MODE {
+	RKVENC_MODE_NONE,
+	RKVENC_MODE_ONEFRAME,
+	RKVENC_MODE_LINKTABLE_FIX,
+	RKVENC_MODE_LINKTABLE_UPDATE,
+	RKVENC_MODE_BUTT
+};
+
+struct rkvenc_task {
+	struct mpp_task mpp_task;
+
+	int link_flags;
+	int fmt;
+	enum RKVENC_MODE link_mode;
+
+	/* level 1 register setting */
+	u32 reg_offset;
+	u32 reg_num;
+	u32 reg[RKVENC_REG_L1_NUM];
+	u32 width;
+	u32 height;
+	u32 pixels;
+	/* level 2 register setting */
+	u32 reg_l2_offset;
+	u32 reg_l2_num;
+	u32 reg_l2[RKVENC_REG_L2_NUM];
+	/* register offset info */
+	struct reg_offset_info off_inf;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct rkvenc_session_priv {
+	struct rw_semaphore rw_sem;
+	/* codec info from user */
+	struct {
+		/* show mode */
+		u32 flag;
+		/* item data */
+		u64 val;
+	} codec_info[ENC_INFO_BUTT];
+};
+
+struct rkvenc_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info core_clk_info;
+	u32 default_max_load;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_core;
+
+#ifdef CONFIG_PM_DEVFREQ
+	struct regulator *vdd;
+	struct devfreq *devfreq;
+	unsigned long volt;
+	unsigned long core_rate_hz;
+	unsigned long core_last_rate_hz;
+	struct monitor_dev_info *mdev_info;
+	struct rockchip_opp_info opp_info;
+#endif
+	/* for iommu pagefault handle */
+	struct work_struct iommu_work;
+	struct workqueue_struct *iommu_wq;
+	struct page *aux_page;
+	unsigned long aux_iova;
+	unsigned long fault_iova;
+};
+
+struct link_table_elem {
+	dma_addr_t lkt_dma_addr;
+	void *lkt_cpu_addr;
+	u32 lkt_index;
+	struct list_head list;
+};
+
+static struct mpp_hw_info rkvenc_hw_info = {
+	.reg_num = RKVENC_REG_NUM,
+	.reg_id = RKVENC_REG_HW_ID_INDEX,
+	.reg_en = RKVENC_ENC_START_INDEX,
+	.reg_start = RKVENC_REG_START_INDEX,
+	.reg_end = RKVENC_REG_END_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_h264e[] = {
+	70, 71, 72, 73, 74, 75, 76, 77, 78, 79,
+	80, 81, 82, 83, 84, 85, 86, 124, 125,
+	126, 127, 128, 129, 130, 131
+};
+
+static const u16 trans_tbl_h265e[] = {
+	70, 71, 72, 73, 74, 75, 76, 77, 78, 79,
+	80, 81, 82, 83, 84, 85, 86, 124, 125,
+	126, 127, 128, 129, 130, 131, 95, 96
+};
+
+static struct mpp_trans_info trans_rk_rkvenc[] = {
+	[RKVENC_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e),
+		.table = trans_tbl_h264e,
+	},
+	[RKVENC_FMT_H265E] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e),
+		.table = trans_tbl_h265e,
+	},
+};
+
+static int rkvenc_extract_task_msg(struct rkvenc_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			int req_base;
+			int max_size;
+			u8 *dst = NULL;
+
+			if (req->offset >= RKVENC_L2_OFFSET) {
+				req_base = RKVENC_L2_OFFSET;
+				max_size = sizeof(task->reg_l2);
+				dst = (u8 *)task->reg_l2;
+			} else {
+				req_base = 0;
+				max_size = sizeof(task->reg);
+				dst = (u8 *)task->reg;
+			}
+
+			ret = mpp_check_req(req, req_base, max_size,
+					    0, max_size);
+			if (ret)
+				return ret;
+
+			dst += req->offset - req_base;
+			if (copy_from_user(dst, req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			int req_base;
+			int max_size;
+
+			if (req->offset >= RKVENC_L2_OFFSET) {
+				req_base = RKVENC_L2_OFFSET;
+				max_size = sizeof(task->reg_l2);
+			} else {
+				req_base = 0;
+				max_size = sizeof(task->reg);
+			}
+
+			ret = mpp_check_req(req, req_base, max_size,
+					    0, max_size);
+			if (ret)
+				return ret;
+
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt=%d, r_req_cnt=%d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *rkvenc_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct rkvenc_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = rkvenc_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	task->fmt = RKVENC_GET_FORMAT(task->reg[RKVENC_ENC_PIC_INDEX]);
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = mpp_translate_reg_address(session,
+						mpp_task, task->fmt,
+						task->reg, &task->off_inf);
+		if (ret)
+			goto fail;
+		mpp_translate_reg_offset_info(mpp_task,
+					      &task->off_inf, task->reg);
+	}
+	task->link_mode = RKVENC_MODE_ONEFRAME;
+	task->clk_mode = CLK_MODE_NORMAL;
+	/* get resolution info */
+	task->width = RKVENC_GET_WIDTH(task->reg[RKVENC_ENC_RSL_INDEX]);
+	task->height = RKVENC_GET_HEIGHT(task->reg[RKVENC_ENC_RSL_INDEX]);
+	task->pixels = task->width * task->height;
+	mpp_debug(DEBUG_TASK_INFO, "width=%d, height=%d\n", task->width, task->height);
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int rkvenc_write_req_l2(struct mpp_dev *mpp,
+			       u32 *regs,
+			       u32 start_idx, u32 end_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++) {
+		int reg = i * sizeof(u32);
+
+		mpp_debug(DEBUG_SET_REG_L2, "reg[%03d]: %04x: 0x%08x\n", i, reg, regs[i]);
+		writel_relaxed(reg, mpp->reg_base + RKVENC_L2_ADDR_BASE);
+		writel_relaxed(regs[i], mpp->reg_base + RKVENC_L2_WRITE_BASE);
+	}
+
+	return 0;
+}
+
+static int rkvenc_read_req_l2(struct mpp_dev *mpp,
+			      u32 *regs,
+			      u32 start_idx, u32 end_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++) {
+		int reg = i * sizeof(u32);
+
+		writel_relaxed(reg, mpp->reg_base + RKVENC_L2_ADDR_BASE);
+		regs[i] = readl_relaxed(mpp->reg_base + RKVENC_L2_READ_BASE);
+		mpp_debug(DEBUG_GET_REG_L2, "reg[%03d]: %04x: 0x%08x\n", i, reg, regs[i]);
+	}
+
+	return 0;
+}
+
+static int rkvenc_write_req_backward(struct mpp_dev *mpp, u32 *regs,
+				     s32 start_idx, s32 end_idx, s32 en_idx)
+{
+	int i;
+
+	for (i = end_idx - 1; i >= start_idx; i--) {
+		if (i == en_idx)
+			continue;
+		mpp_write_relaxed(mpp, i * sizeof(u32), regs[i]);
+	}
+
+	return 0;
+}
+
+static int rkvenc_run(struct mpp_dev *mpp,
+		      struct mpp_task *mpp_task)
+{
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, RKVENC_REG_CLR_CACHE_BASE, 1);
+	switch (task->link_mode) {
+	case RKVENC_MODE_ONEFRAME: {
+		int i;
+		struct mpp_request *req;
+		u32 reg_en = mpp_task->hw_info->reg_en;
+		u32 timing_en = mpp->srv->timing_en;
+
+		/*
+		 * Tips: ensure osd plt clock is 0 before setting register,
+		 * otherwise, osd setting will not work
+		 */
+		mpp_write_relaxed(mpp, RKVENC_OSD_CFG_BASE, 0);
+		/* ensure clear finish */
+		wmb();
+		for (i = 0; i < task->w_req_cnt; i++) {
+			int s, e;
+
+			req = &task->w_reqs[i];
+			/* set register L2 */
+			if (req->offset >= RKVENC_L2_OFFSET) {
+				int off = req->offset - RKVENC_L2_OFFSET;
+
+				s = off / sizeof(u32);
+				e = s + req->size / sizeof(u32);
+				rkvenc_write_req_l2(mpp, task->reg_l2, s, e);
+			} else {
+				/* set register L1 */
+				s = req->offset / sizeof(u32);
+				e = s + req->size / sizeof(u32);
+				/* NOTE: for rkvenc, register should set backward */
+				rkvenc_write_req_backward(mpp, task->reg, s, e, reg_en);
+			}
+		}
+
+		/* flush tlb before starting hardware */
+		mpp_iommu_flush_tlb(mpp->iommu_info);
+
+		/* init current task */
+		mpp->cur_task = mpp_task;
+
+		mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+		/* Flush the register before the start the device */
+		wmb();
+		mpp_write(mpp, RKVENC_ENC_START_BASE, task->reg[reg_en]);
+
+		mpp_task_run_end(mpp_task, timing_en);
+	} break;
+	case RKVENC_MODE_LINKTABLE_FIX:
+	case RKVENC_MODE_LINKTABLE_UPDATE:
+	default: {
+		mpp_err("link_mode %d failed.\n", task->link_mode);
+	} break;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_irq(struct mpp_dev *mpp)
+{
+	mpp_debug_enter();
+
+	mpp->irq_status = mpp_read(mpp, RKVENC_INT_STATUS_BASE);
+	if (!mpp->irq_status)
+		return IRQ_NONE;
+
+	mpp_write(mpp, RKVENC_INT_MSK_BASE, 0x100);
+	mpp_write(mpp, RKVENC_INT_CLR_BASE, 0xffffffff);
+	mpp_write(mpp, RKVENC_INT_STATUS_BASE, 0);
+
+	mpp_debug_leave();
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int rkvenc_isr(struct mpp_dev *mpp)
+{
+	struct rkvenc_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	mpp_debug_enter();
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_rkvenc_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", task->irq_status);
+
+	if (task->irq_status & RKVENC_INT_ERROR_BITS) {
+		atomic_inc(&mpp->reset_request);
+		if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG)) {
+			/* dump error register */
+			mpp_debug(DEBUG_DUMP_ERR_REG, "irq_status: %08x\n", task->irq_status);
+			mpp_task_dump_hw_reg(mpp);
+		}
+	}
+
+	/* unmap reserve buffer */
+	if (enc->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, enc->aux_iova, IOMMU_PAGE_SIZE);
+		enc->aux_iova = -1;
+	}
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int rkvenc_finish(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	switch (task->link_mode) {
+	case RKVENC_MODE_ONEFRAME: {
+		u32 i;
+		struct mpp_request *req;
+
+		for (i = 0; i < task->r_req_cnt; i++) {
+			int s, e;
+
+			req = &task->r_reqs[i];
+			if (req->offset >= RKVENC_L2_OFFSET) {
+				int off = req->offset - RKVENC_L2_OFFSET;
+
+				s = off / sizeof(u32);
+				e = s + req->size / sizeof(u32);
+				rkvenc_read_req_l2(mpp, task->reg_l2, s, e);
+			} else {
+				s = req->offset / sizeof(u32);
+				e = s + req->size / sizeof(u32);
+				mpp_read_req(mpp, task->reg, s, e);
+			}
+		}
+		task->reg[RKVENC_INT_STATUS_INDEX] = task->irq_status;
+	} break;
+	case RKVENC_MODE_LINKTABLE_FIX:
+	case RKVENC_MODE_LINKTABLE_UPDATE:
+	default: {
+		mpp_err("link_mode %d failed.\n", task->link_mode);
+	} break;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	switch (task->link_mode) {
+	case RKVENC_MODE_ONEFRAME: {
+		u32 i;
+		struct mpp_request *req;
+
+		for (i = 0; i < task->r_req_cnt; i++) {
+			req = &task->r_reqs[i];
+			/* set register L2 */
+			if (req->offset >= RKVENC_L2_OFFSET) {
+				int off = req->offset - RKVENC_L2_OFFSET;
+
+				if (copy_to_user(req->data,
+						 (u8 *)task->reg_l2 + off,
+						 req->size)) {
+					mpp_err("copy_to_user reg_l2 fail\n");
+					return -EIO;
+				}
+			} else {
+				if (copy_to_user(req->data,
+						 (u8 *)task->reg + req->offset,
+						 req->size)) {
+					mpp_err("copy_to_user reg fail\n");
+					return -EIO;
+				}
+			}
+		}
+	} break;
+	case RKVENC_MODE_LINKTABLE_FIX:
+	case RKVENC_MODE_LINKTABLE_UPDATE:
+	default: {
+		mpp_err("link_mode %d failed.\n", task->link_mode);
+	} break;
+	}
+
+	return 0;
+}
+
+static int rkvenc_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+static int rkvenc_control(struct mpp_session *session, struct mpp_request *req)
+{
+	switch (req->cmd) {
+	case MPP_CMD_SEND_CODEC_INFO: {
+		int i;
+		int cnt;
+		struct codec_info_elem elem;
+		struct rkvenc_session_priv *priv;
+
+		if (!session || !session->priv) {
+			mpp_err("session info null\n");
+			return -EINVAL;
+		}
+		priv = session->priv;
+
+		cnt = req->size / sizeof(elem);
+		cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+		mpp_debug(DEBUG_IOCTL, "codec info count %d\n", cnt);
+		for (i = 0; i < cnt; i++) {
+			if (copy_from_user(&elem, req->data + i * sizeof(elem), sizeof(elem))) {
+				mpp_err("copy_from_user failed\n");
+				continue;
+			}
+			if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT &&
+			    elem.flag > CODEC_INFO_FLAG_NULL && elem.flag < CODEC_INFO_FLAG_BUTT) {
+				elem.type = array_index_nospec(elem.type, ENC_INFO_BUTT);
+				priv->codec_info[elem.type].flag = elem.flag;
+				priv->codec_info[elem.type].val = elem.data;
+			} else {
+				mpp_err("codec info invalid, type %d, flag %d\n",
+					elem.type, elem.flag);
+			}
+		}
+	} break;
+	default: {
+		mpp_err("unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+static int rkvenc_free_session(struct mpp_session *session)
+{
+	if (session && session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvenc_init_session(struct mpp_session *session)
+{
+	struct rkvenc_session_priv *priv;
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	init_rwsem(&priv->rw_sem);
+	session->priv = priv;
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int rkvenc_procfs_remove(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	if (enc->procfs) {
+		proc_remove(enc->procfs);
+		enc->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvenc_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	int i;
+	struct rkvenc_session_priv *priv = session->priv;
+
+	down_read(&priv->rw_sem);
+	/* item name */
+	seq_puts(seq, "------------------------------------------------------");
+	seq_puts(seq, "------------------------------------------------------\n");
+	seq_printf(seq, "|%8s|", (const char *)"session");
+	seq_printf(seq, "%8s|", (const char *)"device");
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		bool show = priv->codec_info[i].flag;
+
+		if (show)
+			seq_printf(seq, "%8s|", enc_info_item_name[i]);
+	}
+	seq_puts(seq, "\n");
+	/* item data*/
+	seq_printf(seq, "|%8d|", session->index);
+	seq_printf(seq, "%8s|", mpp_device_name[session->device_type]);
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		u32 flag = priv->codec_info[i].flag;
+
+		if (!flag)
+			continue;
+		if (flag == CODEC_INFO_FLAG_NUMBER) {
+			u32 data = priv->codec_info[i].val;
+
+			seq_printf(seq, "%8d|", data);
+		} else if (flag == CODEC_INFO_FLAG_STRING) {
+			const char *name = (const char *)&priv->codec_info[i].val;
+
+			seq_printf(seq, "%8s|", name);
+		} else {
+			seq_printf(seq, "%8s|", (const char *)"null");
+		}
+	}
+	seq_puts(seq, "\n");
+	up_read(&priv->rw_sem);
+
+	return 0;
+}
+
+static int rkvenc_show_session_info(struct seq_file *seq, void *offset)
+{
+	struct mpp_session *session = NULL, *n;
+	struct mpp_dev *mpp = seq->private;
+
+	mutex_lock(&mpp->srv->session_lock);
+	list_for_each_entry_safe(session, n,
+				 &mpp->srv->session_list,
+				 service_link) {
+		if (session->device_type != MPP_DEVICE_RKVENC)
+			continue;
+		if (!session->priv)
+			continue;
+		if (mpp->dev_ops->dump_session)
+			mpp->dev_ops->dump_session(session, seq);
+	}
+	mutex_unlock(&mpp->srv->session_lock);
+
+	return 0;
+}
+
+static int rkvenc_procfs_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	enc->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(enc->procfs)) {
+		mpp_err("failed on open procfs\n");
+		enc->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(enc->procfs, mpp);
+
+	/* for debug */
+	mpp_procfs_create_u32("aclk", 0644,
+			      enc->procfs, &enc->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_core", 0644,
+			      enc->procfs, &enc->core_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      enc->procfs, &mpp->session_max_buffers);
+	/* for show session info */
+	proc_create_single_data("sessions-info", 0444,
+				enc->procfs, rkvenc_show_session_info, mpp);
+
+	return 0;
+}
+#else
+static inline int rkvenc_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvenc_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvenc_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_PM_DEVFREQ
+static int rkvenc_devfreq_target(struct device *dev,
+				 unsigned long *freq, u32 flags)
+{
+	struct dev_pm_opp *opp;
+	unsigned long target_volt, target_freq;
+	int ret = 0;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct devfreq *devfreq = enc->devfreq;
+	struct devfreq_dev_status *stat = &devfreq->last_status;
+	unsigned long old_clk_rate = stat->current_frequency;
+
+	opp = devfreq_recommended_opp(dev, freq, flags);
+	if (IS_ERR(opp)) {
+		dev_err(dev, "Failed to find opp for %lu Hz\n", *freq);
+		return PTR_ERR(opp);
+	}
+	target_freq = dev_pm_opp_get_freq(opp);
+	target_volt = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	if (old_clk_rate == target_freq) {
+		enc->core_last_rate_hz = target_freq;
+		if (enc->volt == target_volt)
+			return ret;
+		ret = regulator_set_voltage(enc->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "Cannot set voltage %lu uV\n",
+				target_volt);
+			return ret;
+		}
+		enc->volt = target_volt;
+		return 0;
+	}
+
+	if (old_clk_rate < target_freq) {
+		ret = regulator_set_voltage(enc->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set voltage %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+
+	dev_dbg(dev, "%lu-->%lu\n", old_clk_rate, target_freq);
+	clk_set_rate(enc->core_clk_info.clk, target_freq);
+	stat->current_frequency = target_freq;
+	enc->core_last_rate_hz = target_freq;
+
+	if (old_clk_rate > target_freq) {
+		ret = regulator_set_voltage(enc->vdd, target_volt, INT_MAX);
+		if (ret) {
+			dev_err(dev, "set vol %lu uV\n", target_volt);
+			return ret;
+		}
+	}
+	enc->volt = target_volt;
+
+	return ret;
+}
+
+static int rkvenc_devfreq_get_dev_status(struct device *dev,
+					 struct devfreq_dev_status *stat)
+{
+	return 0;
+}
+
+static int rkvenc_devfreq_get_cur_freq(struct device *dev,
+				       unsigned long *freq)
+{
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	*freq = enc->core_last_rate_hz;
+
+	return 0;
+}
+
+static struct devfreq_dev_profile rkvenc_devfreq_profile = {
+	.target	= rkvenc_devfreq_target,
+	.get_dev_status	= rkvenc_devfreq_get_dev_status,
+	.get_cur_freq = rkvenc_devfreq_get_cur_freq,
+	.is_cooling_device = true,
+};
+
+static int devfreq_venc_ondemand_func(struct devfreq *df, unsigned long *freq)
+{
+	struct rkvenc_dev *enc = df->data;
+
+	if (enc)
+		*freq = enc->core_rate_hz;
+	else
+		*freq = df->previous_freq;
+
+	return 0;
+}
+
+static int devfreq_venc_ondemand_handler(struct devfreq *devfreq,
+					 unsigned int event, void *data)
+{
+	return 0;
+}
+
+static struct devfreq_governor devfreq_venc_ondemand = {
+	.name = "venc_ondemand",
+	.get_target_freq = devfreq_venc_ondemand_func,
+	.event_handler = devfreq_venc_ondemand_handler,
+};
+
+static struct monitor_dev_profile enc_mdevp = {
+	.type = MONITOR_TYPE_DEV,
+	.low_temp_adjust = rockchip_monitor_dev_low_temp_adjust,
+	.high_temp_adjust = rockchip_monitor_dev_high_temp_adjust,
+};
+
+static int __maybe_unused rv1126_get_soc_info(struct device *dev,
+					      struct device_node *np,
+					      int *bin, int *process)
+{
+	int ret = 0;
+	u8 value = 0;
+
+	if (of_property_match_string(np, "nvmem-cell-names", "performance") >= 0) {
+		ret = rockchip_nvmem_cell_read_u8(np, "performance", &value);
+		if (ret) {
+			dev_err(dev, "Failed to get soc performance value\n");
+			return ret;
+		}
+		if (value == 0x1)
+			*bin = 1;
+		else
+			*bin = 0;
+	}
+	if (*bin >= 0)
+		dev_info(dev, "bin=%d\n", *bin);
+
+	return ret;
+}
+
+static const struct rockchip_opp_data __maybe_unused rv1126_rkvenc_opp_data = {
+	.get_soc_info = rv1126_get_soc_info,
+};
+
+static const struct of_device_id rockchip_rkvenc_of_match[] = {
+#ifdef CONFIG_CPU_RV1126
+	{
+		.compatible = "rockchip,rv1109",
+		.data = (void *)&rv1126_rkvenc_opp_data,
+	},
+	{
+		.compatible = "rockchip,rv1126",
+		.data = (void *)&rv1126_rkvenc_opp_data,
+	},
+#endif
+	{},
+};
+
+static int rkvenc_devfreq_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct clk *clk_core = enc->core_clk_info.clk;
+	struct rockchip_opp_info *opp_info = &enc->opp_info;
+	int ret = 0;
+
+	if (!clk_core)
+		return 0;
+
+	enc->vdd = devm_regulator_get_optional(mpp->dev, "venc");
+	if (IS_ERR_OR_NULL(enc->vdd)) {
+		if (PTR_ERR(enc->vdd) == -EPROBE_DEFER) {
+			dev_warn(mpp->dev, "venc regulator not ready, retry\n");
+
+			return -EPROBE_DEFER;
+		}
+		dev_info(mpp->dev, "no regulator, devfreq is disabled\n");
+
+		return 0;
+	}
+
+	rockchip_get_opp_data(rockchip_rkvenc_of_match, opp_info);
+	ret = rockchip_init_opp_table(mpp->dev, opp_info, NULL, "venc");
+	if (ret) {
+		dev_err(mpp->dev, "failed to init_opp_table\n");
+		return ret;
+	}
+
+	ret = devfreq_add_governor(&devfreq_venc_ondemand);
+	if (ret) {
+		dev_err(mpp->dev, "failed to add venc_ondemand governor\n");
+		goto governor_err;
+	}
+
+	rkvenc_devfreq_profile.initial_freq = clk_get_rate(clk_core);
+
+	enc->devfreq = devm_devfreq_add_device(mpp->dev,
+					       &rkvenc_devfreq_profile,
+					       "venc_ondemand", (void *)enc);
+	if (IS_ERR(enc->devfreq)) {
+		ret = PTR_ERR(enc->devfreq);
+		enc->devfreq = NULL;
+		goto devfreq_err;
+	}
+	enc->devfreq->last_status.total_time = 1;
+	enc->devfreq->last_status.busy_time = 1;
+
+	devfreq_register_opp_notifier(mpp->dev, enc->devfreq);
+
+	enc_mdevp.data = enc->devfreq;
+	enc_mdevp.opp_info = opp_info;
+	enc->mdev_info = rockchip_system_monitor_register(mpp->dev, &enc_mdevp);
+	if (IS_ERR(enc->mdev_info)) {
+		dev_dbg(mpp->dev, "without system monitor\n");
+		enc->mdev_info = NULL;
+	}
+
+	return 0;
+
+devfreq_err:
+	devfreq_remove_governor(&devfreq_venc_ondemand);
+governor_err:
+	dev_pm_opp_of_remove_table(mpp->dev);
+
+	return ret;
+}
+
+static int rkvenc_devfreq_remove(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	if (enc->mdev_info)
+		rockchip_system_monitor_unregister(enc->mdev_info);
+	if (enc->devfreq)
+		devfreq_unregister_opp_notifier(mpp->dev, enc->devfreq);
+	devfreq_remove_governor(&devfreq_venc_ondemand);
+	rockchip_uninit_opp_table(mpp->dev, &enc->opp_info);
+
+	return 0;
+}
+#endif
+
+static void rkvenc_iommu_handle_work(struct work_struct *work_s)
+{
+	int ret = 0;
+	struct rkvenc_dev *enc = container_of(work_s, struct rkvenc_dev, iommu_work);
+	struct mpp_dev *mpp = &enc->mpp;
+	unsigned long page_iova = 0;
+
+	mpp_debug_enter();
+
+	/* avoid another page fault occur after page fault */
+	mpp_iommu_down_write(mpp->iommu_info);
+
+	if (enc->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, enc->aux_iova, IOMMU_PAGE_SIZE);
+		enc->aux_iova = -1;
+	}
+
+	page_iova = round_down(enc->fault_iova, SZ_4K);
+	ret = iommu_map(mpp->iommu_info->domain, page_iova,
+			page_to_phys(enc->aux_page), IOMMU_PAGE_SIZE,
+			IOMMU_READ | IOMMU_WRITE);
+	if (ret)
+		mpp_err("iommu_map iova %lx error.\n", page_iova);
+	else
+		enc->aux_iova = page_iova;
+
+	rockchip_iommu_unmask_irq(mpp->dev);
+	mpp_iommu_up_write(mpp->iommu_info);
+
+	mpp_debug_leave();
+}
+
+static int rkvenc_iommu_fault_handle(struct iommu_domain *iommu,
+				     struct device *iommu_dev,
+				     unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	mpp_debug_enter();
+	mpp_debug(DEBUG_IOMMU, "IOMMU_GET_BUS_ID(status)=%d\n", IOMMU_GET_BUS_ID(status));
+	if (IOMMU_GET_BUS_ID(status)) {
+		enc->fault_iova = iova;
+		rockchip_iommu_mask_irq(mpp->dev);
+		queue_work(enc->iommu_wq, &enc->iommu_work);
+	}
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	int ret = 0;
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_RKVENC];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &enc->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->core_clk_info, "clk_core");
+	if (ret)
+		mpp_err("failed on clk_get clk_core\n");
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load",
+			     &enc->default_max_load);
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&enc->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+	mpp_set_clk_info_rate_hz(&enc->core_clk_info, CLK_MODE_DEFAULT, 600 * MHZ);
+
+	/* Get reset control from dtsi */
+	enc->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!enc->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	enc->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!enc->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	enc->rst_core = mpp_reset_control_get(mpp, RST_TYPE_CORE, "video_core");
+	if (!enc->rst_core)
+		mpp_err("No core reset resource define\n");
+
+#ifdef CONFIG_PM_DEVFREQ
+	ret = rkvenc_devfreq_init(mpp);
+	if (ret)
+		mpp_err("failed to add venc devfreq\n");
+#endif
+
+	/* for mmu pagefault */
+	enc->aux_page = alloc_page(GFP_KERNEL);
+	if (!enc->aux_page) {
+		dev_err(mpp->dev, "allocate a page for auxiliary usage\n");
+		return -ENOMEM;
+	}
+	enc->aux_iova = -1;
+
+	enc->iommu_wq = create_singlethread_workqueue("iommu_wq");
+	if (!enc->iommu_wq) {
+		mpp_err("failed to create workqueue\n");
+		return -ENOMEM;
+	}
+	INIT_WORK(&enc->iommu_work, rkvenc_iommu_handle_work);
+
+	mpp->fault_handler = rkvenc_iommu_fault_handle;
+
+	return ret;
+}
+
+static int rkvenc_exit(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+#ifdef CONFIG_PM_DEVFREQ
+	rkvenc_devfreq_remove(mpp);
+#endif
+
+	if (enc->aux_page)
+		__free_page(enc->aux_page);
+
+	if (enc->aux_iova != -1) {
+		iommu_unmap(mpp->iommu_info->domain, enc->aux_iova, IOMMU_PAGE_SIZE);
+		enc->aux_iova = -1;
+	}
+
+	if (enc->iommu_wq) {
+		destroy_workqueue(enc->iommu_wq);
+		enc->iommu_wq = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvenc_reset(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	mpp_debug_enter();
+
+#ifdef CONFIG_PM_DEVFREQ
+	if (enc->devfreq)
+		mutex_lock(&enc->devfreq->lock);
+#endif
+	mpp_clk_set_rate(&enc->aclk_info, CLK_MODE_REDUCE);
+	mpp_clk_set_rate(&enc->core_clk_info, CLK_MODE_REDUCE);
+	/* safe reset */
+	mpp_write(mpp, RKVENC_INT_MSK_BASE, 0x1FF);
+	mpp_write(mpp, RKVENC_CLR_BASE, RKVENC_SAFE_CLR_BIT);
+	udelay(5);
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n", mpp_read(mpp, RKVENC_INT_STATUS_BASE));
+	mpp_write(mpp, RKVENC_INT_CLR_BASE, 0xffffffff);
+	mpp_write(mpp, RKVENC_INT_STATUS_BASE, 0);
+	/* cru reset */
+	if (enc->rst_a && enc->rst_h && enc->rst_core) {
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(enc->rst_a);
+		mpp_safe_reset(enc->rst_h);
+		mpp_safe_reset(enc->rst_core);
+		udelay(5);
+		mpp_safe_unreset(enc->rst_a);
+		mpp_safe_unreset(enc->rst_h);
+		mpp_safe_unreset(enc->rst_core);
+		mpp_pmu_idle_request(mpp, false);
+	}
+#ifdef CONFIG_PM_DEVFREQ
+	if (enc->devfreq)
+		mutex_unlock(&enc->devfreq->lock);
+#endif
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_clk_on(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	mpp_clk_safe_enable(enc->aclk_info.clk);
+	mpp_clk_safe_enable(enc->hclk_info.clk);
+	mpp_clk_safe_enable(enc->core_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvenc_clk_off(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	clk_disable_unprepare(enc->aclk_info.clk);
+	clk_disable_unprepare(enc->hclk_info.clk);
+	clk_disable_unprepare(enc->core_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvenc_get_freq(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	u32 task_cnt;
+	u32 workload;
+	struct mpp_task *loop = NULL, *n;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	/* if not set max load, consider not have advanced mode */
+	if (!enc->default_max_load)
+		return 0;
+
+	task_cnt = 1;
+	workload = task->pixels;
+	/* calc workload in pending list */
+	mutex_lock(&mpp->queue->pending_lock);
+	list_for_each_entry_safe(loop, n,
+				 &mpp->queue->pending_list,
+				 queue_link) {
+		struct rkvenc_task *loop_task = to_rkvenc_task(loop);
+
+		task_cnt++;
+		workload += loop_task->pixels;
+	}
+	mutex_unlock(&mpp->queue->pending_lock);
+
+	if (workload > enc->default_max_load)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	mpp_debug(DEBUG_TASK_INFO, "pending task %d, workload %d, clk_mode=%d\n",
+		  task_cnt, workload, task->clk_mode);
+
+	return 0;
+}
+
+static int rkvenc_set_freq(struct mpp_dev *mpp,
+			   struct mpp_task *mpp_task)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_clk_set_rate(&enc->aclk_info, task->clk_mode);
+
+#ifdef CONFIG_PM_DEVFREQ
+	if (enc->devfreq) {
+		unsigned long core_rate_hz;
+
+		mutex_lock(&enc->devfreq->lock);
+		core_rate_hz = mpp_get_clk_info_rate_hz(&enc->core_clk_info, task->clk_mode);
+		if (enc->core_rate_hz != core_rate_hz) {
+			enc->core_rate_hz = core_rate_hz;
+			update_devfreq(enc->devfreq);
+		} else {
+			/*
+			 * Restore frequency when frequency is changed by
+			 * rkvenc_reduce_freq()
+			 */
+			clk_set_rate(enc->core_clk_info.clk, enc->core_last_rate_hz);
+		}
+		mutex_unlock(&enc->devfreq->lock);
+		return 0;
+	}
+#endif
+	mpp_clk_set_rate(&enc->core_clk_info, task->clk_mode);
+
+	return 0;
+}
+
+static struct mpp_hw_ops rkvenc_hw_ops = {
+	.init = rkvenc_init,
+	.exit = rkvenc_exit,
+	.clk_on = rkvenc_clk_on,
+	.clk_off = rkvenc_clk_off,
+	.get_freq = rkvenc_get_freq,
+	.set_freq = rkvenc_set_freq,
+	.reset = rkvenc_reset,
+};
+
+static struct mpp_dev_ops rkvenc_dev_ops = {
+	.alloc_task = rkvenc_alloc_task,
+	.run = rkvenc_run,
+	.irq = rkvenc_irq,
+	.isr = rkvenc_isr,
+	.finish = rkvenc_finish,
+	.result = rkvenc_result,
+	.free_task = rkvenc_free_task,
+	.ioctl = rkvenc_control,
+	.init_session = rkvenc_init_session,
+	.free_session = rkvenc_free_session,
+	.dump_session = rkvenc_dump_session,
+};
+
+static const struct mpp_dev_var rkvenc_v1_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_hw_info,
+	.trans_info = trans_rk_rkvenc,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &rkvenc_dev_ops,
+};
+
+static const struct of_device_id mpp_rkvenc_dt_match[] = {
+	{
+		.compatible = "rockchip,rkv-encoder-v1",
+		.data = &rkvenc_v1_data,
+	},
+	{},
+};
+
+static int rkvenc_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct rkvenc_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+
+	dev_info(dev, "probing start\n");
+
+	enc = devm_kzalloc(dev, sizeof(*enc), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_rkvenc_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret)
+		return ret;
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		goto failed_get_irq;
+	}
+
+	mpp->session_max_buffers = RKVENC_SESSION_MAX_BUFFERS;
+	rkvenc_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+
+failed_get_irq:
+	mpp_dev_remove(mpp);
+
+	return ret;
+}
+
+static int rkvenc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	rkvenc_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_rkvenc_driver = {
+	.probe = rkvenc_probe,
+	.remove = rkvenc_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = RKVENC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_rkvenc_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
diff --git a/drivers/video/rockchip/mpp/mpp_rkvenc2.c b/drivers/video/rockchip/mpp/mpp_rkvenc2.c
new file mode 100644
index 0000000000000..7098e338803fa
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_rkvenc2.c
@@ -0,0 +1,2978 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2021 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+
+#include <asm/cacheflush.h>
+#include <linux/delay.h>
+#include <linux/devfreq.h>
+#include <linux/devfreq_cooling.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/slab.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/regulator/consumer.h>
+#include <linux/proc_fs.h>
+#include <linux/pm_runtime.h>
+#include <linux/nospec.h>
+#include <linux/workqueue.h>
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_ipa.h>
+#include <soc/rockchip/rockchip_opp_select.h>
+#include <soc/rockchip/rockchip_system_monitor.h>
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "mpp_debug.h"
+#include "mpp_iommu.h"
+#include "mpp_common.h"
+
+#define RKVENC_DRIVER_NAME			"mpp_rkvenc2"
+
+#define	RKVENC_SESSION_MAX_BUFFERS		40
+#define RKVENC_MAX_CORE_NUM			4
+#define RKVENC_MAX_DCHS_ID			4
+#define RKVENC_MAX_SLICE_FIFO_LEN		256
+#define RKVENC_SCLR_DONE_STA			BIT(2)
+#define RKVENC_WDG				0x38
+
+#define to_rkvenc_info(info)		\
+		container_of(info, struct rkvenc_hw_info, hw)
+#define to_rkvenc_task(ctx)		\
+		container_of(ctx, struct rkvenc_task, mpp_task)
+#define to_rkvenc_dev(dev)		\
+		container_of(dev, struct rkvenc_dev, mpp)
+
+
+enum RKVENC_FORMAT_TYPE {
+	RKVENC_FMT_BASE		= 0x0000,
+	RKVENC_FMT_H264E	= RKVENC_FMT_BASE + 0,
+	RKVENC_FMT_H265E	= RKVENC_FMT_BASE + 1,
+	RKVENC_FMT_JPEGE	= RKVENC_FMT_BASE + 2,
+
+	RKVENC_FMT_OSD_BASE	= 0x1000,
+	RKVENC_FMT_H264E_OSD	= RKVENC_FMT_OSD_BASE + 0,
+	RKVENC_FMT_H265E_OSD	= RKVENC_FMT_OSD_BASE + 1,
+	RKVENC_FMT_JPEGE_OSD	= RKVENC_FMT_OSD_BASE + 2,
+	RKVENC_FMT_BUTT,
+};
+
+enum RKVENC_CLASS_TYPE {
+	RKVENC_CLASS_BASE	= 0,	/* base */
+	RKVENC_CLASS_PIC	= 1,	/* picture configure */
+	RKVENC_CLASS_RC		= 2,	/* rate control */
+	RKVENC_CLASS_PAR	= 3,	/* parameter */
+	RKVENC_CLASS_SQI	= 4,	/* subjective Adjust */
+	RKVENC_CLASS_SCL	= 5,	/* scaling list */
+	RKVENC_CLASS_OSD	= 6,	/* osd */
+	RKVENC_CLASS_ST		= 7,	/* status */
+	RKVENC_CLASS_DEBUG	= 8,	/* debug */
+	RKVENC_CLASS_BUTT,
+};
+
+enum RKVENC_CLASS_FD_TYPE {
+	RKVENC_CLASS_FD_BASE	= 0,	/* base */
+	RKVENC_CLASS_FD_OSD	= 1,	/* osd */
+	RKVENC_CLASS_FD_BUTT,
+};
+
+enum RKVENC_VEPU_TYPE {
+	RKVENC_VEPU_580		= 0,
+	RKVENC_VEPU_540C	= 1,
+	RKVENC_VEPU_510		= 2,
+	RKVENC_VEPU_BUTT,
+};
+
+struct rkvenc_reg_msg {
+	u32 base_s;
+	u32 base_e;
+};
+
+struct rkvenc_hw_info {
+	struct mpp_hw_info hw;
+	/* for register range check */
+	u32 reg_class;
+	struct rkvenc_reg_msg reg_msg[RKVENC_CLASS_BUTT];
+	/* for fd translate */
+	u32 fd_class;
+	struct {
+		u32 class;
+		u32 base_fmt;
+	} fd_reg[RKVENC_CLASS_FD_BUTT];
+	/* for get format */
+	struct {
+		u32 class;
+		u32 base;
+		u32 bitpos;
+		u32 bitlen;
+	} fmt_reg;
+	/* register info */
+	u32 enc_start_base;
+	u32 enc_clr_base;
+	u32 int_en_base;
+	u32 int_mask_base;
+	u32 int_clr_base;
+	u32 int_sta_base;
+	u32 enc_wdg_base;
+	u32 err_mask;
+	u32 enc_rsl;
+	u32 dcsh_class_ofst;
+	u32 vepu_type;
+};
+
+#define INT_STA_ENC_DONE_STA	BIT(0)
+#define INT_STA_SCLR_DONE_STA	BIT(2)
+#define INT_STA_SLC_DONE_STA	BIT(3)
+#define INT_STA_BSF_OFLW_STA	BIT(4)
+#define INT_STA_BRSP_OTSD_STA	BIT(5)
+#define INT_STA_WBUS_ERR_STA	BIT(6)
+#define INT_STA_RBUS_ERR_STA	BIT(7)
+#define INT_STA_WDG_STA		BIT(8)
+
+#define INT_STA_ERROR		(INT_STA_BRSP_OTSD_STA | \
+				INT_STA_WBUS_ERR_STA | \
+				INT_STA_RBUS_ERR_STA | \
+				INT_STA_WDG_STA)
+
+#define DCHS_REG_OFFSET		(0x304)
+#define DCHS_TXE		(0x10)
+#define DCHS_RXE		(0x20)
+
+/* dual core hand-shake info */
+union rkvenc2_dual_core_handshake_id {
+	u64 val[2];
+	struct {
+		u32 txid	: 2;
+		u32 rxid	: 2;
+		u32 txe		: 1;
+		u32 rxe		: 1;
+		u32 reserve0	: 2;
+		u32 dly		: 8;
+		u32 offset	: 11;
+		u32 reserve1	: 5;
+		u32 session_id;
+		u32 working	: 1;
+		u32 txe_orig	: 1;
+		u32 rxe_orig	: 1;
+		u32 txid_orig	: 2;
+		u32 rxid_orig	: 2;
+		u32 txe_map	: 1;
+		u32 rxe_map	: 1;
+		u32 txid_map	: 2;
+		u32 rxid_map	: 2;
+		u32 reserve2	: 19;
+		u32 reserve3	: 32;
+	};
+};
+
+#define RKVENC2_REG_INT_EN		(8)
+#define RKVENC2_BIT_SLICE_DONE_EN	BIT(3)
+
+#define RKVENC2_REG_INT_MASK		(9)
+#define RKVENC2_BIT_SLICE_DONE_MASK	BIT(3)
+
+#define RKVENC2_REG_EXT_LINE_BUF_BASE	(22)
+
+#define RKVENC2_REG_ENC_PIC		(32)
+#define RKVENC510_REG_ENC_PIC		(36)
+#define RKVENC2_BIT_ENC_STND		BIT(0)
+#define RKVENC2_BIT_VAL_H264		0
+#define RKVENC2_BIT_VAL_H265		1
+#define RKVENC2_BIT_SLEN_FIFO		BIT(30)
+#define RKVENC2_BIT_REC_FBC_DIS		BIT(31)
+
+#define RKVENC2_REG_SLI_SPLIT		(56)
+#define RKVENC510_REG_SLI_SPLIT		(60)
+#define RKVENC2_BIT_SLI_SPLIT		BIT(0)
+#define RKVENC2_BIT_SLI_FLUSH		BIT(15)
+
+#define RKVENC2_REG_SLICE_NUM_BASE	(0x4034)
+#define RKVENC2_REG_SLICE_LEN_BASE	(0x4038)
+
+#define RKVENC2_REG_ST_BSB		(0x402c)
+#define RKVENC2_REG_ADR_BSBT		(0x2b0)
+#define RKVENC2_REG_ADR_BSBB		(0x2b4)
+#define RKVENC2_REG_ADR_BSBS		(0x2b8)
+#define RKVENC2_REG_ADR_BSBR		(0x2bc)
+#define RKVENC580_REG_ADR_BSBR		(0x2b8)
+#define RKVENC580_REG_ADR_BSBS		(0x2bc)
+
+union rkvenc2_slice_len_info {
+	u32 val;
+
+	struct {
+		u32 slice_len	: 31;
+		u32 last	: 1;
+	};
+};
+
+union rkvenc2_frame_resolution {
+	u32 val;
+
+	struct {
+		u32 pic_wd8	: 11;
+		u32 reserve0	: 5;
+		u32 pic_hd8	: 11;
+		u32 reserve1	: 5;
+	};
+};
+
+struct rkvenc_poll_slice_cfg {
+	s32 poll_type;
+	s32 poll_ret;
+	s32 count_max;
+	s32 count_ret;
+	union rkvenc2_slice_len_info slice_info[];
+};
+
+struct rkvenc_task {
+	struct mpp_task mpp_task;
+	int fmt;
+	struct rkvenc_hw_info *hw_info;
+
+	/* class register */
+	struct {
+		u32 valid;
+		u32 *data;
+		u32 size;
+	} reg[RKVENC_CLASS_BUTT];
+	/* register offset info */
+	struct reg_offset_info off_inf;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	struct mpp_dma_buffer *table;
+
+	union rkvenc2_dual_core_handshake_id dchs_id;
+
+	/* split output / slice mode info */
+	u32 task_split;
+	u32 task_split_done;
+	u32 last_slice_found;
+	u32 slice_wr_cnt;
+	u32 slice_rd_cnt;
+	DECLARE_KFIFO(slice_info, union rkvenc2_slice_len_info, RKVENC_MAX_SLICE_FIFO_LEN);
+
+	/* jpege bitstream */
+	struct mpp_dma_buffer *bs_buf;
+	u32 offset_bs;
+	u32 rec_fbc_dis;
+};
+
+#define RKVENC_MAX_RCB_NUM		(4)
+
+struct rcb_info_elem {
+	u32 index;
+	u32 size;
+};
+
+struct rkvenc2_rcb_info {
+	u32 cnt;
+	struct rcb_info_elem elem[RKVENC_MAX_RCB_NUM];
+};
+
+struct rkvenc2_session_priv {
+	struct rw_semaphore rw_sem;
+	/* codec info from user */
+	struct {
+		/* show mode */
+		u32 flag;
+		/* item data */
+		u64 val;
+	} codec_info[ENC_INFO_BUTT];
+	/* rcb_info for sram */
+	struct rkvenc2_rcb_info rcb_inf;
+};
+
+struct rkvenc_dev {
+	struct mpp_dev mpp;
+	struct rkvenc_hw_info *hw_info;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info core_clk_info;
+	u32 default_max_load;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_core;
+	/* for ccu */
+	struct rkvenc_ccu *ccu;
+	struct list_head core_link;
+
+	/* internal rcb-memory */
+	u32 sram_size;
+	u32 sram_used;
+	dma_addr_t sram_iova;
+	u32 sram_enabled;
+	struct page *rcb_page;
+
+	u32 bs_overflow;
+
+#ifdef CONFIG_PM_DEVFREQ
+	struct rockchip_opp_info opp_info;
+	struct monitor_dev_info *mdev_info;
+#endif
+};
+
+struct rkvenc_ccu {
+	u32 core_num;
+	/* lock for core attach */
+	struct mutex lock;
+	struct list_head core_list;
+	struct mpp_dev *main_core;
+
+	spinlock_t lock_dchs;
+	union rkvenc2_dual_core_handshake_id dchs[RKVENC_MAX_CORE_NUM];
+};
+
+static struct rkvenc_hw_info rkvenc_v2_hw_info = {
+	.hw = {
+		.reg_num = 254,
+		.reg_id = 0,
+		.reg_en = 4,
+		.reg_start = 160,
+		.reg_end = 253,
+	},
+	.reg_class = RKVENC_CLASS_BUTT,
+	.reg_msg[RKVENC_CLASS_BASE] = {
+		.base_s = 0x0000,
+		.base_e = 0x0058,
+	},
+	.reg_msg[RKVENC_CLASS_PIC] = {
+		.base_s = 0x0280,
+		.base_e = 0x03f4,
+	},
+	.reg_msg[RKVENC_CLASS_RC] = {
+		.base_s = 0x1000,
+		.base_e = 0x10e0,
+	},
+	.reg_msg[RKVENC_CLASS_PAR] = {
+		.base_s = 0x1700,
+		.base_e = 0x1cd4,
+	},
+	.reg_msg[RKVENC_CLASS_SQI] = {
+		.base_s = 0x2000,
+		.base_e = 0x21e4,
+	},
+	.reg_msg[RKVENC_CLASS_SCL] = {
+		.base_s = 0x2200,
+		.base_e = 0x2c98,
+	},
+	.reg_msg[RKVENC_CLASS_OSD] = {
+		.base_s = 0x3000,
+		.base_e = 0x347c,
+	},
+	.reg_msg[RKVENC_CLASS_ST] = {
+		.base_s = 0x4000,
+		.base_e = 0x42cc,
+	},
+	.reg_msg[RKVENC_CLASS_DEBUG] = {
+		.base_s = 0x5000,
+		.base_e = 0x5354,
+	},
+	.fd_class = RKVENC_CLASS_FD_BUTT,
+	.fd_reg[RKVENC_CLASS_FD_BASE] = {
+		.class = RKVENC_CLASS_PIC,
+		.base_fmt = RKVENC_FMT_BASE,
+	},
+	.fd_reg[RKVENC_CLASS_FD_OSD] = {
+		.class = RKVENC_CLASS_OSD,
+		.base_fmt = RKVENC_FMT_OSD_BASE,
+	},
+	.fmt_reg = {
+		.class = RKVENC_CLASS_PIC,
+		.base = 0x0300,
+		.bitpos = 0,
+		.bitlen = 1,
+	},
+	.enc_start_base = 0x0010,
+	.enc_clr_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_mask_base = 0x0024,
+	.int_clr_base = 0x0028,
+	.int_sta_base = 0x002c,
+	.enc_wdg_base = 0x0038,
+	.err_mask = 0x03f0,
+	.enc_rsl = 0x0310,
+	.dcsh_class_ofst = 33,
+	.vepu_type = RKVENC_VEPU_580,
+};
+
+static struct rkvenc_hw_info rkvenc_510_hw_info = {
+	.hw = {
+		.reg_num = 254,
+		.reg_id = 0,
+		.reg_en = 4,
+		.reg_start = 160,
+		.reg_end = 253,
+	},
+	.reg_class = RKVENC_CLASS_BUTT,
+	.reg_msg[RKVENC_CLASS_BASE] = {
+		.base_s = 0x0000,
+		.base_e = 0x0120,
+	},
+	.reg_msg[RKVENC_CLASS_PIC] = {
+		.base_s = 0x0270,
+		.base_e = 0x0480,
+	},
+	.reg_msg[RKVENC_CLASS_RC] = {
+		.base_s = 0x1000,
+		.base_e = 0x110c,
+	},
+	.reg_msg[RKVENC_CLASS_PAR] = {
+		.base_s = 0x1700,
+		.base_e = 0x19cc,
+	},
+	.reg_msg[RKVENC_CLASS_SQI] = {
+		.base_s = 0x2000,
+		.base_e = 0x212c,
+	},
+	.reg_msg[RKVENC_CLASS_SCL] = {
+		.base_s = 0x2200,
+		.base_e = 0x2584,
+	},
+	.reg_msg[RKVENC_CLASS_OSD] = {
+		.base_s = 0x3000,
+		.base_e = 0x326c,
+	},
+	.reg_msg[RKVENC_CLASS_ST] = {
+		.base_s = 0x4000,
+		.base_e = 0x424c,
+	},
+	.reg_msg[RKVENC_CLASS_DEBUG] = {
+		.base_s = 0x5000,
+		.base_e = 0x5230,
+	},
+	.fd_class = RKVENC_CLASS_FD_BUTT,
+	.fd_reg[RKVENC_CLASS_FD_BASE] = {
+		.class = RKVENC_CLASS_PIC,
+		.base_fmt = RKVENC_FMT_BASE,
+	},
+	.fd_reg[RKVENC_CLASS_FD_OSD] = {
+		.class = RKVENC_CLASS_OSD,
+		.base_fmt = RKVENC_FMT_OSD_BASE,
+	},
+	.fmt_reg = {
+		.class = RKVENC_CLASS_PIC,
+		.base = 0x0300,
+		.bitpos = 0,
+		.bitlen = 2,
+	},
+	.enc_start_base = 0x0010,
+	.enc_clr_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_mask_base = 0x0024,
+	.int_clr_base = 0x0028,
+	.int_sta_base = 0x002c,
+	.enc_wdg_base = 0x0038,
+	.err_mask = 0x27d0,
+	.enc_rsl = 0x0310,
+	.dcsh_class_ofst = 37,
+	.vepu_type =  RKVENC_VEPU_510,
+};
+
+static struct rkvenc_hw_info rkvenc_540c_hw_info = {
+	.hw = {
+		.reg_num = 254,
+		.reg_id = 0,
+		.reg_en = 4,
+		.reg_start = 160,
+		.reg_end = 253,
+	},
+	.reg_class = RKVENC_CLASS_BUTT,
+	.reg_msg[RKVENC_CLASS_BASE] = {
+		.base_s = 0x0000,
+		.base_e = 0x0120,
+	},
+	.reg_msg[RKVENC_CLASS_PIC] = {
+		.base_s = 0x0270,
+		.base_e = 0x0480,
+	},
+	.reg_msg[RKVENC_CLASS_RC] = {
+		.base_s = 0x1000,
+		.base_e = 0x110c,
+	},
+	.reg_msg[RKVENC_CLASS_PAR] = {
+		.base_s = 0x1700,
+		.base_e = 0x19cc,
+	},
+	.reg_msg[RKVENC_CLASS_SQI] = {
+		.base_s = 0x2000,
+		.base_e = 0x20fc,
+	},
+	.reg_msg[RKVENC_CLASS_SCL] = {
+		.base_s = 0x21e0,
+		.base_e = 0x2dfc,
+	},
+	.reg_msg[RKVENC_CLASS_OSD] = {
+		.base_s = 0x3000,
+		.base_e = 0x326c,
+	},
+	.reg_msg[RKVENC_CLASS_ST] = {
+		.base_s = 0x4000,
+		.base_e = 0x424c,
+	},
+	.reg_msg[RKVENC_CLASS_DEBUG] = {
+		.base_s = 0x5000,
+		.base_e = 0x5354,
+	},
+	.fd_class = RKVENC_CLASS_FD_BUTT,
+	.fd_reg[RKVENC_CLASS_FD_BASE] = {
+		.class = RKVENC_CLASS_PIC,
+		.base_fmt = RKVENC_FMT_BASE,
+	},
+	.fd_reg[RKVENC_CLASS_FD_OSD] = {
+		.class = RKVENC_CLASS_OSD,
+		.base_fmt = RKVENC_FMT_OSD_BASE,
+	},
+	.fmt_reg = {
+		.class = RKVENC_CLASS_PIC,
+		.base = 0x0300,
+		.bitpos = 0,
+		.bitlen = 2,
+	},
+	.enc_start_base = 0x0010,
+	.enc_clr_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_mask_base = 0x0024,
+	.int_clr_base = 0x0028,
+	.int_sta_base = 0x002c,
+	.enc_wdg_base = 0x0038,
+	.err_mask = 0x27d0,
+	.enc_rsl = 0x0310,
+	.dcsh_class_ofst = 0,
+	.vepu_type = RKVENC_VEPU_540C,
+};
+/*
+ * file handle translate information for v2
+ */
+static const u16 trans_tbl_h264e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_h264e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_h265e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_h265e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+/*
+ * file handle translate information for 540c
+ */
+static const u16 trans_tbl_h264e_540c[] = {
+	4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
+	14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
+	// /* renc and ref wrap */
+	// 24, 25, 26, 27,
+	28, 29, 30
+};
+
+static const u16 trans_tbl_h264e_540c_osd[] = {
+	3, 4, 12, 13, 21, 22, 30, 31,
+	39, 40, 48, 49, 57, 58, 66, 67,
+};
+
+static const u16 trans_tbl_h265e_540c[] = {
+	4, 5, 6, 7, 8, 9, 10, 11, 12, 13,
+	14, 15, 16, 17, 18, 19, 20, 21, 22, 23,
+	28, 29, 30
+};
+
+static const u16 trans_tbl_h265e_540c_osd[] = {
+	3, 4, 12, 13, 21, 22, 30, 31,
+	39, 40, 48, 49, 57, 58, 66, 67,
+};
+
+static const u16 trans_tbl_jpege[] = {
+	100, 101, 102, 103, 104, 105, 106, 107,
+	108, 109, 110,
+};
+
+static const u16 trans_tbl_jpege_osd[] = {
+	81, 82, 90, 91, 99, 100, 108, 109,
+	117, 118, 126, 127, 135, 136, 144, 145,
+};
+
+static const u32 rkvenc2_timeout_thd_by_rsl[5][2] = {
+	{1920*1088,	50},
+	{2560*1440,	100},
+	{4096*2304,	200},
+	{8192*8192,	400},
+	{15360*8640,	800}
+};
+
+static struct mpp_trans_info trans_rkvenc_v2[] = {
+	[RKVENC_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2),
+		.table = trans_tbl_h264e_v2,
+	},
+	[RKVENC_FMT_H264E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2_osd),
+		.table = trans_tbl_h264e_v2_osd,
+	},
+	[RKVENC_FMT_H265E] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2),
+		.table = trans_tbl_h265e_v2,
+	},
+	[RKVENC_FMT_H265E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2_osd),
+		.table = trans_tbl_h265e_v2_osd,
+	},
+};
+
+static struct mpp_trans_info trans_rkvenc_540c[] = {
+	[RKVENC_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_540c),
+		.table = trans_tbl_h264e_540c,
+	},
+	[RKVENC_FMT_H264E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_540c_osd),
+		.table = trans_tbl_h264e_540c_osd,
+	},
+	[RKVENC_FMT_H265E] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_540c),
+		.table = trans_tbl_h265e_540c,
+	},
+	[RKVENC_FMT_H265E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_540c_osd),
+		.table = trans_tbl_h265e_540c_osd,
+	},
+	[RKVENC_FMT_JPEGE] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege),
+		.table = trans_tbl_jpege,
+	},
+	[RKVENC_FMT_JPEGE_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege_osd),
+		.table = trans_tbl_jpege_osd,
+	},
+};
+
+static int rkvenc_soft_reset(struct mpp_dev *mpp);
+
+static bool req_over_class(struct mpp_request *req,
+			   struct rkvenc_task *task, int class)
+{
+	bool ret;
+	u32 base_s, base_e, req_e;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	base_s = hw->reg_msg[class].base_s;
+	base_e = hw->reg_msg[class].base_e;
+	req_e = req->offset + req->size - sizeof(u32);
+
+	ret = (req->offset <= base_e && req_e >= base_s) ? true : false;
+
+	return ret;
+}
+
+static int rkvenc_free_class_msg(struct rkvenc_task *task)
+{
+	u32 i;
+	u32 reg_class = task->hw_info->reg_class;
+
+	for (i = 0; i < reg_class; i++) {
+		kfree(task->reg[i].data);
+		task->reg[i].data = NULL;
+		task->reg[i].size = 0;
+	}
+
+	return 0;
+}
+
+static int rkvenc_alloc_class_msg(struct rkvenc_task *task, int class)
+{
+	u32 *data;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	if (!task->reg[class].data) {
+		u32 base_s = hw->reg_msg[class].base_s;
+		u32 base_e = hw->reg_msg[class].base_e;
+		u32 class_size = base_e - base_s + sizeof(u32);
+
+		data = kzalloc(class_size, GFP_KERNEL);
+		if (!data)
+			return -ENOMEM;
+		task->reg[class].data = data;
+		task->reg[class].size = class_size;
+	}
+
+	return 0;
+}
+
+static int rkvenc_update_req(struct rkvenc_task *task, int class,
+			     struct mpp_request *req_in,
+			     struct mpp_request *req_out)
+{
+	u32 base_s, base_e, req_e, s, e;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	base_s = hw->reg_msg[class].base_s;
+	base_e = hw->reg_msg[class].base_e;
+	req_e = req_in->offset + req_in->size - sizeof(u32);
+	s = max(req_in->offset, base_s);
+	e = min(req_e, base_e);
+
+	req_out->offset = s;
+	req_out->size = e - s + sizeof(u32);
+	req_out->data = (u8 *)req_in->data + (s - req_in->offset);
+
+	return 0;
+}
+
+static int rkvenc_get_class_msg(struct rkvenc_task *task,
+				u32 addr, struct mpp_request *msg)
+{
+	int i;
+	bool found = false;
+	u32 base_s, base_e;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	if (!msg)
+		return -EINVAL;
+
+	memset(msg, 0, sizeof(*msg));
+	for (i = 0; i < hw->reg_class; i++) {
+		base_s = hw->reg_msg[i].base_s;
+		base_e = hw->reg_msg[i].base_e;
+		if (addr >= base_s && addr < base_e) {
+			found = true;
+			msg->offset = base_s;
+			msg->size = task->reg[i].size;
+			msg->data = task->reg[i].data;
+			break;
+		}
+	}
+
+	return (found ? 0 : (-EINVAL));
+}
+
+static u32 *rkvenc_get_class_reg(struct rkvenc_task *task, u32 addr)
+{
+	int i;
+	u8 *reg = NULL;
+	u32 base_s, base_e;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	for (i = 0; i < hw->reg_class; i++) {
+		base_s = hw->reg_msg[i].base_s;
+		base_e = hw->reg_msg[i].base_e;
+		if (addr >= base_s && addr < base_e) {
+			reg = (u8 *)task->reg[i].data + (addr - base_s);
+			break;
+		}
+	}
+
+	return (u32 *)reg;
+}
+
+static int rkvenc2_extract_rcb_info(struct rkvenc2_rcb_info *rcb_inf,
+				    struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(rcb_inf->elem);
+	int cnt = req->size / sizeof(rcb_inf->elem[0]);
+
+	if (req->size > sizeof(rcb_inf->elem)) {
+		mpp_err("count %d,max_size %d\n", cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(rcb_inf->elem, req->data, req->size)) {
+		mpp_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	rcb_inf->cnt = cnt;
+
+	return 0;
+}
+
+static int rkvenc_extract_task_msg(struct mpp_session *session,
+				   struct rkvenc_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	int ret;
+	u32 i, j;
+	struct mpp_request *req;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			void *data;
+			struct mpp_request *wreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					mpp_err("alloc class msg %d fail.\n", j);
+					goto fail;
+				}
+				wreq = &task->w_reqs[task->w_req_cnt];
+				rkvenc_update_req(task, j, req, wreq);
+				data = rkvenc_get_class_reg(task, wreq->offset);
+				if (!data) {
+					mpp_err("get class reg fail, offset %08x\n", wreq->offset);
+					ret = -EINVAL;
+					goto fail;
+				}
+				if (copy_from_user(data, wreq->data, wreq->size)) {
+					mpp_err("copy_from_user fail, offset %08x\n", wreq->offset);
+					ret = -EIO;
+					goto fail;
+				}
+				task->reg[j].valid = 1;
+				task->w_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			struct mpp_request *rreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					mpp_err("alloc class msg reg %d fail.\n", j);
+					goto fail;
+				}
+				rreq = &task->r_reqs[task->r_req_cnt];
+				rkvenc_update_req(task, j, req, rreq);
+				task->reg[j].valid = 1;
+				task->r_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		case MPP_CMD_SET_RCB_INFO: {
+			struct rkvenc2_session_priv *priv = session->priv;
+
+			if (priv)
+				rkvenc2_extract_rcb_info(&priv->rcb_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt=%d, r_req_cnt=%d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	mpp_debug_enter();
+	return 0;
+
+fail:
+	rkvenc_free_class_msg(task);
+
+	mpp_debug_enter();
+	return ret;
+}
+
+static int rkvenc_task_get_format(struct mpp_dev *mpp,
+				  struct rkvenc_task *task)
+{
+	u32 offset, val;
+
+	struct rkvenc_hw_info *hw = task->hw_info;
+	u32 class = hw->fmt_reg.class;
+	u32 *class_reg = task->reg[class].data;
+	u32 class_size = task->reg[class].size;
+	u32 class_base = hw->reg_msg[class].base_s;
+	u32 bitpos = hw->fmt_reg.bitpos;
+	u32 bitlen = hw->fmt_reg.bitlen;
+
+	if (!class_reg || !class_size)
+		return -EINVAL;
+
+	offset = hw->fmt_reg.base - class_base;
+	val = class_reg[offset/sizeof(u32)];
+	task->fmt = (val >> bitpos) & ((1 << bitlen) - 1);
+
+	return 0;
+}
+
+static int rkvenc2_set_rcbbuf(struct mpp_dev *mpp, struct mpp_session *session,
+			      struct rkvenc_task *task)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc2_session_priv *priv = session->priv;
+	u32 sram_enabled = 0;
+
+	mpp_debug_enter();
+
+	if (priv && enc->sram_iova) {
+		int i;
+		u32 *reg;
+		u32 reg_idx, rcb_size, rcb_offset;
+		struct rkvenc2_rcb_info *rcb_inf = &priv->rcb_inf;
+
+		rcb_offset = 0;
+		for (i = 0; i < rcb_inf->cnt; i++) {
+			reg_idx = rcb_inf->elem[i].index;
+			rcb_size = rcb_inf->elem[i].size;
+
+			if (rcb_offset > enc->sram_size ||
+			    (rcb_offset + rcb_size) > enc->sram_used)
+				continue;
+
+			mpp_debug(DEBUG_SRAM_INFO, "rcb: reg %d offset %d, size %d\n",
+				  reg_idx, rcb_offset, rcb_size);
+
+			reg = rkvenc_get_class_reg(task, reg_idx * sizeof(u32));
+			if (reg)
+				*reg = enc->sram_iova + rcb_offset;
+
+			rcb_offset += rcb_size;
+			sram_enabled = 1;
+		}
+	}
+	if (enc->sram_enabled != sram_enabled) {
+		mpp_debug(DEBUG_SRAM_INFO, "sram %s\n", sram_enabled ? "enabled" : "disabled");
+		enc->sram_enabled = sram_enabled;
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static void rkvenc2_setup_task_id(u32 session_id, struct rkvenc_task *task)
+{
+	u32 val = 0;
+	struct rkvenc_hw_info *hw = task->hw_info;
+
+	/* always enable tx */
+	val = task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] | DCHS_TXE;
+	if (hw->dcsh_class_ofst)
+		task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = val;
+	task->dchs_id.val[0] = (((u64)session_id << 32) | val);
+
+	task->dchs_id.txid_orig = task->dchs_id.txid;
+	task->dchs_id.rxid_orig = task->dchs_id.rxid;
+	task->dchs_id.txid_map = task->dchs_id.txid;
+	task->dchs_id.rxid_map = task->dchs_id.rxid;
+
+	task->dchs_id.txe_orig = task->dchs_id.txe;
+	task->dchs_id.rxe_orig = task->dchs_id.rxe;
+	task->dchs_id.txe_map = task->dchs_id.txe;
+	task->dchs_id.rxe_map = task->dchs_id.rxe;
+}
+
+static void rkvenc2_check_split_task(struct mpp_dev *mpp, struct rkvenc_task *task)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 slen_fifo_en	= 0;
+	u32 sli_split_en	= 0;
+	u32 reg_enc_pic		= 0;
+	u32 reg_slt_split	= 0;
+
+	if (hw->vepu_type == RKVENC_VEPU_510) {
+		reg_enc_pic = RKVENC510_REG_ENC_PIC;
+		reg_slt_split = RKVENC510_REG_SLI_SPLIT;
+	} else {
+		reg_enc_pic = RKVENC2_REG_ENC_PIC;
+		reg_slt_split = RKVENC2_REG_SLI_SPLIT;
+	}
+
+	if (task->reg[RKVENC_CLASS_PIC].valid) {
+		u32 *reg = task->reg[RKVENC_CLASS_PIC].data;
+		u32 enc_stnd = reg[reg_enc_pic] & RKVENC2_BIT_ENC_STND;
+
+		slen_fifo_en = (reg[reg_enc_pic] & RKVENC2_BIT_SLEN_FIFO) ? 1 : 0;
+		sli_split_en = (reg[reg_slt_split] & RKVENC2_BIT_SLI_SPLIT) ? 1 : 0;
+
+		/*
+		 * FIXUP: rkvenc2 hardware bug:
+		 * H.264 encoding has bug when external line buffer and slice flush both
+		 * are enabled.
+		 */
+		if (sli_split_en && slen_fifo_en &&
+		    enc_stnd == RKVENC2_BIT_VAL_H264 &&
+		    reg[RKVENC2_REG_EXT_LINE_BUF_BASE])
+			reg[reg_slt_split] &= ~RKVENC2_BIT_SLI_FLUSH;
+	}
+
+	task->task_split = sli_split_en && slen_fifo_en;
+
+	if (task->task_split)
+		INIT_KFIFO(task->slice_info);
+}
+
+static void *rkvenc_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct rkvenc_task *task;
+	struct mpp_task *mpp_task;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	task->hw_info = to_rkvenc_info(mpp_task->hw_info);
+	/* extract reqs for current task */
+	ret = rkvenc_extract_task_msg(session, task, msgs);
+	if (ret)
+		goto free_task;
+	mpp_task->reg = task->reg[0].data;
+	/* get format */
+	ret = rkvenc_task_get_format(mpp, task);
+	if (ret)
+		goto free_task;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		u32 i, j;
+		int cnt;
+		u32 off;
+		const u16 *tbl;
+		struct rkvenc_hw_info *hw = task->hw_info;
+		int fd_bs = -1;
+
+		for (i = 0; i < hw->fd_class; i++) {
+			u32 class = hw->fd_reg[i].class;
+			u32 fmt = hw->fd_reg[i].base_fmt + task->fmt;
+			u32 *reg = task->reg[class].data;
+			u32 ss = hw->reg_msg[class].base_s / sizeof(u32);
+
+			if (!reg)
+				continue;
+
+			if (fmt == RKVENC_FMT_JPEGE && class == RKVENC_CLASS_PIC && fd_bs == -1) {
+				int bs_index;
+
+				bs_index = mpp->var->trans_info[fmt].table[2];
+				fd_bs = reg[bs_index];
+				task->offset_bs = mpp_query_reg_offset_info(&task->off_inf,
+									    bs_index + ss);
+			}
+
+			ret = mpp_translate_reg_address(session, mpp_task, fmt, reg, NULL);
+			if (ret)
+				goto fail;
+
+			cnt = mpp->var->trans_info[fmt].count;
+			tbl = mpp->var->trans_info[fmt].table;
+			for (j = 0; j < cnt; j++) {
+				off = mpp_query_reg_offset_info(&task->off_inf, tbl[j] + ss);
+				mpp_debug(DEBUG_IOMMU, "reg[%d] + offset %d\n", tbl[j] + ss, off);
+				reg[tbl[j]] += off;
+			}
+		}
+
+		if (fd_bs >= 0) {
+			struct mpp_dma_buffer *bs_buf =
+					mpp_dma_find_buffer_fd(session->dma, fd_bs);
+
+			if (bs_buf && task->offset_bs > 0)
+				mpp_dma_buf_sync(bs_buf, 0, task->offset_bs, DMA_TO_DEVICE, false);
+			task->bs_buf = bs_buf;
+		}
+	}
+	rkvenc2_setup_task_id(session->index, task);
+	task->clk_mode = CLK_MODE_NORMAL;
+	rkvenc2_check_split_task(mpp, task);
+
+	/* check whether the current task is rec_fbc_dis = 1 */
+	if (task->hw_info->vepu_type == RKVENC_VEPU_510) {
+		if (task->reg[RKVENC_CLASS_PIC].valid) {
+			u32 *reg = task->reg[RKVENC_CLASS_PIC].data;
+
+			task->rec_fbc_dis = reg[RKVENC510_REG_ENC_PIC] & RKVENC2_BIT_REC_FBC_DIS;
+			reg[RKVENC510_REG_ENC_PIC] &= ~(RKVENC2_BIT_REC_FBC_DIS);
+		}
+	}
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	/* free class register buffer */
+	rkvenc_free_class_msg(task);
+free_task:
+	kfree(task);
+
+	return NULL;
+}
+
+static void *rkvenc2_prepare(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct mpp_taskqueue *queue = mpp->queue;
+	unsigned long core_idle;
+	unsigned long flags;
+	u32 core_id_max;
+	s32 core_id;
+	u32 i;
+
+	spin_lock_irqsave(&queue->running_lock, flags);
+
+	core_idle = queue->core_idle;
+	core_id_max = queue->core_id_max;
+
+	for (i = 0; i <= core_id_max; i++) {
+		struct mpp_dev *mpp = queue->cores[i];
+
+		if (mpp && mpp->disable)
+			clear_bit(i, &core_idle);
+	}
+
+	core_id = find_first_bit(&core_idle, core_id_max + 1);
+
+	if (core_id >= core_id_max + 1 || !queue->cores[core_id]) {
+		mpp_task = NULL;
+		mpp_dbg_core("core %d all busy %lx\n", core_id, core_idle);
+	} else {
+		struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+		clear_bit(core_id, &queue->core_idle);
+		mpp_task->mpp = queue->cores[core_id];
+		mpp_task->core_id = core_id;
+		rkvenc2_set_rcbbuf(mpp_task->mpp, mpp_task->session, task);
+		mpp_dbg_core("core %d set idle %lx -> %lx\n", core_id,
+			     core_idle, queue->core_idle);
+	}
+
+	spin_unlock_irqrestore(&queue->running_lock, flags);
+
+	return mpp_task;
+}
+
+static void rkvenc2_patch_dchs(struct rkvenc_dev *enc, struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu;
+	union rkvenc2_dual_core_handshake_id *dchs;
+	union rkvenc2_dual_core_handshake_id *task_dchs = &task->dchs_id;
+	struct rkvenc_hw_info *hw = task->hw_info;
+	int core_num;
+	int core_id = enc->mpp.core_id;
+	unsigned long flags;
+	int i;
+
+	if (!enc->ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->mpp.dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	ccu = enc->ccu;
+	dchs = ccu->dchs;
+	core_num = ccu->core_num;
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+
+	if (dchs[core_id].working) {
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+
+		mpp_err("can not config when core %d is still working\n", core_id);
+		return;
+	}
+
+	if (mpp_debug_unlikely(DEBUG_CORE))
+		pr_info("core tx:rx 0 %s %d:%d %d:%d -- 1 %s %d:%d %d:%d -- task %d %d:%d %d:%d\n",
+			dchs[0].working ? "work" : "idle",
+			dchs[0].txid, dchs[0].txe, dchs[0].rxid, dchs[0].rxe,
+			dchs[1].working ? "work" : "idle",
+			dchs[1].txid, dchs[1].txe, dchs[1].rxid, dchs[1].rxe,
+			core_id, task_dchs->txid, task_dchs->txe,
+			task_dchs->rxid, task_dchs->rxe);
+
+	/* always use new id as  */
+	{
+		struct mpp_task *mpp_task = &task->mpp_task;
+		unsigned long id_valid = (unsigned long)-1;
+		int txid_map = -1;
+		int rxid_map = -1;
+
+		/* scan all used id */
+		for (i = 0; i < core_num; i++) {
+			if (!dchs[i].working)
+				continue;
+
+			clear_bit(dchs[i].txid_map, &id_valid);
+			clear_bit(dchs[i].rxid_map, &id_valid);
+		}
+
+		if (task_dchs->rxe) {
+			for (i = 0; i < core_num; i++) {
+				if (i == core_id)
+					continue;
+
+				if (!dchs[i].working)
+					continue;
+
+				if (task_dchs->session_id != dchs[i].session_id)
+					continue;
+
+				if (task_dchs->rxid_orig != dchs[i].txid_orig)
+					continue;
+
+				rxid_map = dchs[i].txid_map;
+				break;
+			}
+		}
+
+		txid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+		if (txid_map == RKVENC_MAX_DCHS_ID) {
+			spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+
+			mpp_err("task %d:%d on core %d failed to find a txid\n",
+				mpp_task->session->pid, mpp_task->task_id,
+				mpp_task->core_id);
+			return;
+		}
+
+		clear_bit(txid_map, &id_valid);
+		task_dchs->txid_map = txid_map;
+
+		if (rxid_map < 0) {
+			rxid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+			if (rxid_map == RKVENC_MAX_DCHS_ID) {
+				spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+
+				mpp_err("task %d:%d on core %d failed to find a rxid\n",
+					mpp_task->session->pid, mpp_task->task_id,
+					mpp_task->core_id);
+				return;
+			}
+
+			task_dchs->rxe_map = 0;
+		}
+
+		task_dchs->rxid_map = rxid_map;
+	}
+
+	task_dchs->txid = task_dchs->txid_map;
+	task_dchs->rxid = task_dchs->rxid_map;
+	task_dchs->rxe = task_dchs->rxe_map;
+
+	dchs[core_id].val[0] = task_dchs->val[0];
+	dchs[core_id].val[1] = task_dchs->val[1];
+	task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = task_dchs->val[0];
+
+	dchs[core_id].working = 1;
+
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+static void rkvenc2_update_dchs(struct rkvenc_dev *enc, struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu = enc->ccu;
+	int core_id = enc->mpp.core_id;
+	unsigned long flags;
+
+	if (!ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->mpp.dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+	ccu->dchs[core_id].val[0] = 0;
+	ccu->dchs[core_id].val[1] = 0;
+
+	if (mpp_debug_unlikely(DEBUG_CORE)) {
+		union rkvenc2_dual_core_handshake_id *dchs = ccu->dchs;
+		union rkvenc2_dual_core_handshake_id *task_dchs = &task->dchs_id;
+
+		pr_info("core %d task done\n", core_id);
+		pr_info("core tx:rx 0 %s %d:%d %d:%d -- 1 %s %d:%d %d:%d -- task %d %d:%d %d:%d\n",
+			dchs[0].working ? "work" : "idle",
+			dchs[0].txid, dchs[0].txe, dchs[0].rxid, dchs[0].rxe,
+			dchs[1].working ? "work" : "idle",
+			dchs[1].txid, dchs[1].txe, dchs[1].rxid, dchs[1].rxe,
+			core_id, task_dchs->txid, task_dchs->txe,
+			task_dchs->rxid, task_dchs->rxe);
+	}
+
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+static void rkvenc2_calc_timeout_thd(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	union rkvenc2_frame_resolution frm_rsl;
+	u32 timeout_ms		= 0;
+	u32 timeout_thd		= 0;
+	u32 timeout_thd_cnt	= 0;
+
+	timeout_thd = mpp_read(mpp, RKVENC_WDG) & 0xff000000;
+	frm_rsl.val = mpp_read(mpp, hw->enc_rsl);
+	frm_rsl.val = (frm_rsl.pic_wd8 + 1) * (frm_rsl.pic_hd8 + 1) * 64;
+
+	/* Assign appropriate timeout thresholds for videos of different resolutions */
+	timeout_thd_cnt = ARRAY_SIZE(rkvenc2_timeout_thd_by_rsl);
+	for (u32 i = 0; i < timeout_thd_cnt; i++) {
+		if (frm_rsl.val <= rkvenc2_timeout_thd_by_rsl[i][0]) {
+			timeout_ms = rkvenc2_timeout_thd_by_rsl[i][1];
+			break;
+		}
+	}
+
+	/*
+	 * When vepu_type is RKVENC_VEPU_510, multiplied by 256 core clock cycles,
+	 * else use x1024 core clk cycles
+	 */
+	if (hw->vepu_type == RKVENC_VEPU_510)
+		timeout_thd |= timeout_ms * clk_get_rate(enc->core_clk_info.clk) / 256000;
+	else
+		timeout_thd |= timeout_ms * clk_get_rate(enc->core_clk_info.clk) / 1024000;
+
+	mpp_write(mpp, RKVENC_WDG, timeout_thd);
+}
+
+static int rkvenc_run(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 start_val = 0;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* Add force clear to avoid pagefault */
+	if (hw->vepu_type == RKVENC_VEPU_580) {
+		mpp_write(mpp, hw->enc_clr_base, 0x2);
+		udelay(5);
+		mpp_write(mpp, hw->enc_clr_base, 0x0);
+	}
+
+	/* clear hardware counter */
+	mpp_write_relaxed(mpp, 0x5300, 0x2);
+
+	rkvenc2_patch_dchs(enc, task);
+
+	for (i = 0; i < task->w_req_cnt; i++) {
+		int ret;
+		u32 s, e, off;
+		u32 *regs;
+
+		struct mpp_request msg;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			return -EINVAL;
+
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		regs = (u32 *)msg.data;
+		for (j = s; j < e; j++) {
+			off = msg.offset + j * sizeof(u32);
+			if (off == enc->hw_info->enc_start_base) {
+				start_val = regs[j];
+				continue;
+			}
+			mpp_write_relaxed(mpp, off, regs[j]);
+		}
+	}
+
+	if (mpp_debug_unlikely(DEBUG_CORE))
+		dev_info(mpp->dev, "core %d dchs %08x\n", mpp->core_id,
+			 mpp_read_relaxed(&enc->mpp, DCHS_REG_OFFSET));
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	rkvenc2_calc_timeout_thd(mpp);
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	if (hw->vepu_type == RKVENC_VEPU_510) {
+		u32 rec_fbc_dis = task->rec_fbc_dis;
+		u32 enc_pic = mpp_read(mpp, 0x300);
+
+		/*
+		 * Config dvbm special reg to expected that
+		 * vepu will hold when the encoding finish.
+		 */
+		mpp_write(mpp, 0x74, 0x23);
+		mpp_write(mpp, 0x308, BIT(18) | BIT(16));
+		/* Enable slice done interrupt and slice fifo info. */
+		mpp_write(mpp, 0x20, mpp_read(mpp, 0x20) | BIT(3));
+		/*
+		 * Fix bug:
+		 * Writing reg 0x300 BIT(31) may cause the DMA module to falsely
+		 * trigger writing data. It will case enc err.
+		 * So we need to disable the core clock before writing reg 0x300,
+		 * and re-enable the core clock after writing reg 0x300.
+		 */
+		if (rec_fbc_dis) {
+			mpp_clk_safe_disable(enc->core_clk_info.clk);
+			mpp_write(mpp, 0x300, enc_pic | BIT(30) | BIT(31));
+			mpp_clk_safe_enable(enc->core_clk_info.clk);
+		} else {
+			mpp_write(mpp, 0x300, enc_pic | BIT(30));
+		}
+	}
+
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, enc->hw_info->enc_start_base, start_val);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static void rkvenc2_read_slice_len(struct mpp_dev *mpp, struct rkvenc_task *task,
+				   u32 *irq_status)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 sli_num = mpp_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+	u32 new_irq_status = mpp_read(mpp, hw->int_sta_base);
+	union rkvenc2_slice_len_info slice_info;
+	u32 task_id = task->mpp_task.task_id;
+	u32 i;
+	u32 last = 0;
+	u32 split = task->task_split;
+
+	/* Need update irq status and slice number when enc done ready with new status*/
+	if ((new_irq_status != *irq_status) && (new_irq_status & INT_STA_ENC_DONE_STA)) {
+		*irq_status |= new_irq_status;
+		sli_num = mpp_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+		mpp_write(mpp, hw->int_clr_base, new_irq_status);
+	}
+
+	last = *irq_status & INT_STA_ENC_DONE_STA;
+
+	mpp_dbg_slice("task %d wr %3d len start %s\n", task_id,
+		      sli_num, last ? "last" : "");
+
+	for (i = 0; i < sli_num; i++) {
+		slice_info.val = mpp_read_relaxed(mpp, RKVENC2_REG_SLICE_LEN_BASE);
+		/* after vepu510, HW will return last slice flag in bit(31) */
+		last |= slice_info.last;
+		if (last && i == sli_num - 1) {
+			task->last_slice_found = 1;
+			slice_info.last = 1;
+		}
+
+		if (split) {
+			mpp_dbg_slice("task %d wr %3d len %d %s\n", task_id,
+				task->slice_wr_cnt, slice_info.slice_len,
+				slice_info.last ? "last" : "");
+
+			kfifo_in(&task->slice_info, &slice_info, 1);
+			task->slice_wr_cnt++;
+		}
+	}
+
+	if (split) {
+		/* Fixup for async between last flag and slice number register */
+		if (last && !task->last_slice_found) {
+			mpp_dbg_slice("task %d mark last slice\n", task_id);
+			slice_info.last = 1;
+			slice_info.slice_len = 0;
+			kfifo_in(&task->slice_info, &slice_info, 1);
+		}
+	}
+
+	/*
+	 * In vepu_510 the sli_done interrupt is triggered by slice_fifo not empty status.
+	 * Therefore when the sli_done_sta interrupt is cleared without reading all the
+	 * slice_len data in slice_fifo an extra interrupt will be triggered. So it is
+	 * better to clear the interrupt after reading all slice_len to avoid false irq reporting.
+	 * On the other hand, vepu_580 is triggered only after writing a slice,
+	 * without checking if fifo is non-empty, so there is no need to clear the
+	 * interrupt again after reading the slice.
+	 */
+	if (hw->vepu_type == RKVENC_VEPU_510) {
+		/*
+		 * Fix bug:
+		 * There is a hw bug, the encoder has probabilistically encodes
+		 * one frame repeatedlly and does not return enc done in time.
+		 * So use slice info to check if the frame is encoded.
+		 */
+		if (last) {
+			/* after config the register, the encoder will update enc done int status */
+			mpp_write(mpp, 0x308, 0);
+			udelay(5);
+			new_irq_status = mpp_read(mpp, hw->int_sta_base);
+			if (new_irq_status & INT_STA_ENC_DONE_STA)
+				*irq_status |= new_irq_status;
+		}
+		mpp_write(mpp, hw->int_clr_base, *irq_status);
+	}
+}
+
+static void rkvenc2_bs_overflow_handle(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	u32 bs_rd, bs_wr, bs_top, bs_bot;
+
+	if (hw->vepu_type == RKVENC_VEPU_580) {
+		bs_rd = mpp_read(mpp, RKVENC580_REG_ADR_BSBR);
+		bs_wr = mpp_read(mpp, RKVENC2_REG_ST_BSB);
+		bs_top = mpp_read(mpp, RKVENC2_REG_ADR_BSBT);
+		bs_bot = mpp_read(mpp, RKVENC2_REG_ADR_BSBB);
+
+		bs_wr += 128;
+		if (bs_wr >= bs_top)
+			bs_wr = bs_bot;
+		/* update write addr for enc continue */
+		mpp_write(mpp, RKVENC2_REG_ADR_BSBS, bs_wr);
+	} else {
+		bs_rd = mpp_read(mpp, RKVENC2_REG_ADR_BSBR);
+		bs_wr = mpp_read(mpp, RKVENC2_REG_ST_BSB);
+		bs_top = mpp_read(mpp, RKVENC2_REG_ADR_BSBT);
+		bs_bot = mpp_read(mpp, RKVENC2_REG_ADR_BSBB);
+
+		bs_wr += 128;
+		if (bs_wr >= bs_top)
+			bs_wr = bs_bot;
+		/* update rw addr for enc continue */
+		mpp_write(mpp, RKVENC2_REG_ADR_BSBS, bs_wr);
+		mpp_write(mpp, RKVENC2_REG_ADR_BSBR, bs_rd | 0xc);
+	}
+
+	if (mpp_task)
+		dev_err(mpp->dev, "task %d found bitstream overflow [%#08x %#08x %#08x %#08x]\n",
+			mpp_task->task_index, bs_top, bs_bot, bs_wr, bs_rd);
+}
+
+static int rkvenc_irq(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	struct mpp_task *mpp_task = NULL;
+	struct rkvenc_task *task = NULL;
+	u32 irq_status;
+	int ret = IRQ_NONE;
+
+	mpp_debug_enter();
+
+	irq_status = mpp_read(mpp, hw->int_sta_base);
+
+	mpp_debug(DEBUG_IRQ_STATUS, "%s irq_status: %08x\n",
+		  dev_name(mpp->dev), irq_status);
+
+	if (!irq_status)
+		return ret;
+
+	/* clear int first */
+	mpp_write(mpp, hw->int_clr_base, irq_status);
+
+	/*
+	 * prevent watch dog irq storm.
+	 * The encoder did not stop working when watchdog interrupt is triggered,
+	 * it still check timeout and trigger watch dog irq.
+	 */
+	if (irq_status & INT_STA_WDG_STA)
+		mpp_write(mpp, hw->int_mask_base, INT_STA_WDG_STA);
+
+	if (mpp->cur_task) {
+		mpp_task = mpp->cur_task;
+		task = to_rkvenc_task(mpp_task);
+	}
+
+	/* 1. read slice number and slice length */
+	if (hw->vepu_type == RKVENC_VEPU_510 && task) {
+		rkvenc2_read_slice_len(mpp, task, &irq_status);
+		if (task->task_split)
+			wake_up(&mpp_task->wait);
+	} else {
+		if (task && task->task_split &&
+		    (irq_status & (INT_STA_SLC_DONE_STA | INT_STA_ENC_DONE_STA))) {
+			mpp_time_part_diff(mpp_task);
+			rkvenc2_read_slice_len(mpp, task, &irq_status);
+			wake_up(&mpp_task->wait);
+		}
+	}
+
+	/* 2. process slice irq */
+	if (irq_status & INT_STA_SLC_DONE_STA)
+		ret = IRQ_HANDLED;
+
+	/* 3. process bitstream overflow */
+	if (irq_status & INT_STA_BSF_OFLW_STA) {
+		rkvenc2_bs_overflow_handle(mpp);
+		enc->bs_overflow = 1;
+		ret = IRQ_HANDLED;
+	}
+
+	/* 4. process frame irq */
+	if (irq_status & INT_STA_ENC_DONE_STA) {
+		mpp->irq_status = irq_status;
+
+		if (enc->bs_overflow) {
+			mpp->irq_status |= INT_STA_BSF_OFLW_STA;
+			enc->bs_overflow = 0;
+		}
+
+		ret = IRQ_WAKE_THREAD;
+	}
+
+	/* 5. process error irq */
+	if (irq_status & INT_STA_ERROR) {
+		mpp->irq_status = irq_status;
+
+		dev_err(mpp->dev, "found error status %08x\n", irq_status);
+
+		ret = IRQ_WAKE_THREAD;
+	}
+
+	mpp_debug_leave();
+
+	return ret;
+}
+
+static int vepu540c_irq(struct mpp_dev *mpp)
+{
+	return rkvenc_irq(mpp);
+}
+
+static int rkvenc_isr(struct mpp_dev *mpp)
+{
+	struct rkvenc_task *task;
+	struct mpp_task *mpp_task;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct mpp_taskqueue *queue = mpp->queue;
+	unsigned long core_idle;
+
+	mpp_debug_enter();
+
+	/* FIXME use a spin lock here */
+	if (!mpp->cur_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+
+	mpp_task = mpp->cur_task;
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+
+	if (mpp_task->mpp && mpp_task->mpp != mpp)
+		dev_err(mpp->dev, "mismatch core dev %p:%p\n", mpp_task->mpp, mpp);
+
+	task = to_rkvenc_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+
+	rkvenc2_update_dchs(enc, task);
+
+	if (task->irq_status & enc->hw_info->err_mask) {
+		atomic_inc(&mpp->reset_request);
+
+		/* dump register */
+		if (mpp_debug_unlikely(DEBUG_DUMP_ERR_REG))
+			mpp_task_dump_hw_reg(mpp);
+	}
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	core_idle = queue->core_idle;
+	set_bit(mpp->core_id, &queue->core_idle);
+
+	mpp_dbg_core("core %d isr idle %lx -> %lx\n", mpp->core_id, core_idle,
+		     queue->core_idle);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int rkvenc_finish(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 *reg;
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		int ret;
+		int s, e;
+		struct mpp_request msg;
+		struct mpp_request *req = &task->r_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			return -EINVAL;
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		reg = (u32 *)msg.data;
+		for (j = s; j < e; j++)
+			reg[j] = mpp_read_relaxed(mpp, msg.offset + j * sizeof(u32));
+
+	}
+
+	if (task->bs_buf) {
+		u32 bs_size = mpp_read(mpp, 0x4064);
+
+		mpp_dma_buf_sync(task->bs_buf, 0, bs_size + task->offset_bs,
+				 DMA_FROM_DEVICE, true);
+	}
+
+	/* revert hack for irq status */
+	reg = rkvenc_get_class_reg(task, task->hw_info->int_sta_base);
+	if (reg)
+		*reg = task->irq_status;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		struct mpp_request *req = &task->r_reqs[i];
+		u32 *reg = rkvenc_get_class_reg(task, req->offset);
+
+		if (!reg)
+			return -EINVAL;
+		if (copy_to_user(req->data, reg, req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	rkvenc_free_class_msg(task);
+	kfree(task);
+
+	return 0;
+}
+
+static int rkvenc_control(struct mpp_session *session, struct mpp_request *req)
+{
+	switch (req->cmd) {
+	case MPP_CMD_SEND_CODEC_INFO: {
+		int i;
+		int cnt;
+		struct codec_info_elem elem;
+		struct rkvenc2_session_priv *priv;
+
+		if (!session || !session->priv) {
+			mpp_err("session info null\n");
+			return -EINVAL;
+		}
+		priv = session->priv;
+
+		cnt = req->size / sizeof(elem);
+		cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+		mpp_debug(DEBUG_IOCTL, "codec info count %d\n", cnt);
+		for (i = 0; i < cnt; i++) {
+			if (copy_from_user(&elem, req->data + i * sizeof(elem), sizeof(elem))) {
+				mpp_err("copy_from_user failed\n");
+				continue;
+			}
+			if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT &&
+			    elem.flag > CODEC_INFO_FLAG_NULL && elem.flag < CODEC_INFO_FLAG_BUTT) {
+				elem.type = array_index_nospec(elem.type, ENC_INFO_BUTT);
+				priv->codec_info[elem.type].flag = elem.flag;
+				priv->codec_info[elem.type].val = elem.data;
+			} else {
+				mpp_err("codec info invalid, type %d, flag %d\n",
+					elem.type, elem.flag);
+			}
+		}
+	} break;
+	default: {
+		mpp_err("unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+static int rkvenc_free_session(struct mpp_session *session)
+{
+	if (session && session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvenc_init_session(struct mpp_session *session)
+{
+	struct rkvenc2_session_priv *priv;
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	init_rwsem(&priv->rw_sem);
+	session->priv = priv;
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int rkvenc_procfs_remove(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	if (enc->procfs) {
+		proc_remove(enc->procfs);
+		enc->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int rkvenc_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	int i;
+	struct rkvenc2_session_priv *priv = session->priv;
+
+	down_read(&priv->rw_sem);
+	/* item name */
+	seq_puts(seq, "------------------------------------------------------");
+	seq_puts(seq, "------------------------------------------------------\n");
+	seq_printf(seq, "|%8s|", (const char *)"session");
+	seq_printf(seq, "%8s|", (const char *)"device");
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		bool show = priv->codec_info[i].flag;
+
+		if (show)
+			seq_printf(seq, "%8s|", enc_info_item_name[i]);
+	}
+	seq_puts(seq, "\n");
+	/* item data*/
+	seq_printf(seq, "|%8d|", session->index);
+	seq_printf(seq, "%8s|", mpp_device_name[session->device_type]);
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		u32 flag = priv->codec_info[i].flag;
+
+		if (!flag)
+			continue;
+		if (flag == CODEC_INFO_FLAG_NUMBER) {
+			u32 data = priv->codec_info[i].val;
+
+			seq_printf(seq, "%8d|", data);
+		} else if (flag == CODEC_INFO_FLAG_STRING) {
+			const char *name = (const char *)&priv->codec_info[i].val;
+
+			seq_printf(seq, "%8s|", name);
+		} else {
+			seq_printf(seq, "%8s|", (const char *)"null");
+		}
+	}
+	seq_puts(seq, "\n");
+	up_read(&priv->rw_sem);
+
+	return 0;
+}
+
+static int rkvenc_show_session_info(struct seq_file *seq, void *offset)
+{
+	struct mpp_session *session = NULL, *n;
+	struct mpp_dev *mpp = seq->private;
+
+	mutex_lock(&mpp->srv->session_lock);
+	list_for_each_entry_safe(session, n,
+				 &mpp->srv->session_list,
+				 service_link) {
+		if (session->device_type != MPP_DEVICE_RKVENC)
+			continue;
+		if (!session->priv)
+			continue;
+		if (mpp->dev_ops->dump_session)
+			mpp->dev_ops->dump_session(session, seq);
+	}
+	mutex_unlock(&mpp->srv->session_lock);
+
+	return 0;
+}
+
+static int rkvenc_procfs_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	char name[32];
+
+	if (!mpp->dev || !mpp->dev->of_node || !mpp->dev->of_node->name ||
+	    !mpp->srv || !mpp->srv->procfs)
+		return -EINVAL;
+
+	snprintf(name, sizeof(name) - 1, "%s%d",
+		 mpp->dev->of_node->name, mpp->core_id);
+
+	enc->procfs = proc_mkdir(name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(enc->procfs)) {
+		mpp_err("failed on open procfs\n");
+		enc->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(enc->procfs, mpp);
+
+	/* for debug */
+	mpp_procfs_create_u32("aclk", 0644,
+			      enc->procfs, &enc->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("clk_core", 0644,
+			      enc->procfs, &enc->core_clk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      enc->procfs, &mpp->session_max_buffers);
+	/* for show session info */
+	proc_create_single_data("sessions-info", 0444,
+				enc->procfs, rkvenc_show_session_info, mpp);
+
+	return 0;
+}
+
+static int rkvenc_procfs_ccu_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	if (!enc->procfs)
+		goto done;
+
+done:
+	return 0;
+}
+#else
+static inline int rkvenc_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvenc_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int rkvenc_procfs_ccu_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_PM_DEVFREQ
+static int rk3588_venc_set_read_margin(struct device *dev,
+				       struct rockchip_opp_info *opp_info,
+				       u32 rm)
+{
+	if (!opp_info->grf || !opp_info->volt_rm_tbl)
+		return 0;
+
+	if (rm == opp_info->current_rm || rm == UINT_MAX)
+		return 0;
+
+	dev_dbg(dev, "set rm to %d\n", rm);
+
+	regmap_write(opp_info->grf, 0x214, 0x001c0000 | (rm << 2));
+	regmap_write(opp_info->grf, 0x218, 0x001c0000 | (rm << 2));
+	regmap_write(opp_info->grf, 0x220, 0x003c0000 | (rm << 2));
+	regmap_write(opp_info->grf, 0x224, 0x003c0000 | (rm << 2));
+
+	opp_info->current_rm = rm;
+
+	return 0;
+}
+
+static const struct rockchip_opp_data rk3588_venc_opp_data = {
+	.set_read_margin = rk3588_venc_set_read_margin,
+};
+
+static const struct of_device_id rockchip_rkvenc_of_match[] = {
+	{
+		.compatible = "rockchip,rk3588",
+		.data = (void *)&rk3588_venc_opp_data,
+	},
+	{},
+};
+
+static struct monitor_dev_profile venc_mdevp = {
+	.type = MONITOR_TYPE_DEV,
+	.check_rate_volt = rockchip_monitor_check_rate_volt,
+};
+
+static int rkvenc_devfreq_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct clk *clk_core = enc->core_clk_info.clk;
+	struct device *dev = mpp->dev;
+	struct rockchip_opp_info *opp_info = &enc->opp_info;
+	int ret = 0;
+
+	if (!clk_core)
+		return 0;
+
+	rockchip_get_opp_data(rockchip_rkvenc_of_match, opp_info);
+	ret = rockchip_init_opp_table(dev, opp_info, "clk_core", "venc");
+	if (ret) {
+		dev_err(dev, "failed to init_opp_table\n");
+		return ret;
+	}
+	venc_mdevp.opp_info = opp_info;
+	enc->mdev_info = rockchip_system_monitor_register(dev, &venc_mdevp);
+	if (IS_ERR(enc->mdev_info)) {
+		dev_dbg(dev, "without system monitor\n");
+		enc->mdev_info = NULL;
+	}
+
+	return ret;
+}
+
+static int rkvenc_devfreq_remove(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	if (enc->mdev_info) {
+		rockchip_system_monitor_unregister(enc->mdev_info);
+		enc->mdev_info = NULL;
+	}
+	rockchip_uninit_opp_table(mpp->dev, &enc->opp_info);
+
+	return 0;
+}
+#endif
+
+static int rkvenc_init(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	int ret = 0;
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_RKVENC];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &enc->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->core_clk_info, "clk_core");
+	if (ret)
+		mpp_err("failed on clk_get clk_core\n");
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load",
+			     &enc->default_max_load);
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&enc->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+	mpp_set_clk_info_rate_hz(&enc->core_clk_info, CLK_MODE_DEFAULT, 600 * MHZ);
+
+	/* Get reset control from dtsi */
+	enc->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!enc->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	enc->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!enc->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	enc->rst_core = mpp_reset_control_get(mpp, RST_TYPE_CORE, "video_core");
+	if (!enc->rst_core)
+		mpp_err("No core reset resource define\n");
+
+#ifdef CONFIG_PM_DEVFREQ
+	ret = rkvenc_devfreq_init(mpp);
+	if (ret)
+		mpp_err("failed to add venc devfreq\n");
+#endif
+
+	return 0;
+}
+
+static int rkvenc_exit(struct mpp_dev *mpp)
+{
+#ifdef CONFIG_PM_DEVFREQ
+	rkvenc_devfreq_remove(mpp);
+#endif
+
+	return 0;
+}
+
+static int rkvenc_soft_reset(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 rst_status = 0;
+	int ret = 0;
+
+	/* safe reset */
+	mpp_write(mpp, hw->int_mask_base, 0x3FF);
+	mpp_write(mpp, hw->enc_clr_base, 0x1);
+	ret = readl_relaxed_poll_timeout(mpp->reg_base + hw->int_sta_base,
+					 rst_status,
+					 rst_status & RKVENC_SCLR_DONE_STA,
+					 0, 5);
+	if (ret)
+		mpp_err("safe reset failed\n");
+	mpp_write(mpp, hw->enc_clr_base, 0x2);
+	udelay(5);
+	mpp_write(mpp, hw->enc_clr_base, 0);
+	mpp_write(mpp, hw->int_clr_base, 0xffffffff);
+	mpp_write(mpp, hw->int_sta_base, 0);
+
+	return ret;
+
+}
+
+static int rkvenc_reset(struct mpp_dev *mpp)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct mpp_taskqueue *queue = mpp->queue;
+	struct rkvenc_ccu *ccu = enc->ccu;
+
+	mpp_debug_enter();
+
+	/* safe reset first*/
+	ret = rkvenc_soft_reset(mpp);
+
+	/* cru reset */
+	if (ret && enc->rst_a && enc->rst_h && enc->rst_core) {
+		mpp_err("soft reset timeout, use cru reset\n");
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(enc->rst_a);
+		mpp_safe_reset(enc->rst_h);
+		mpp_safe_reset(enc->rst_core);
+		udelay(5);
+		mpp_safe_unreset(enc->rst_a);
+		mpp_safe_unreset(enc->rst_h);
+		mpp_safe_unreset(enc->rst_core);
+		mpp_pmu_idle_request(mpp, false);
+	}
+
+	set_bit(mpp->core_id, &queue->core_idle);
+
+	if (ccu) {
+		spin_lock_irqsave(&ccu->lock_dchs, flags);
+		ccu->dchs[mpp->core_id].val[0] = 0;
+		ccu->dchs[mpp->core_id].val[1] = 0;
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+	}
+
+	mpp_dbg_core("core %d reset idle %lx\n", mpp->core_id, queue->core_idle);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int rkvenc_clk_on(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	mpp_clk_safe_enable(enc->aclk_info.clk);
+	mpp_clk_safe_enable(enc->hclk_info.clk);
+	mpp_clk_safe_enable(enc->core_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvenc_clk_off(struct mpp_dev *mpp)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+	clk_disable_unprepare(enc->aclk_info.clk);
+	clk_disable_unprepare(enc->hclk_info.clk);
+	clk_disable_unprepare(enc->core_clk_info.clk);
+
+	return 0;
+}
+
+static int rkvenc_set_freq(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct rkvenc_task *task = to_rkvenc_task(mpp_task);
+
+	mpp_clk_set_rate(&enc->aclk_info, task->clk_mode);
+	mpp_clk_set_rate(&enc->core_clk_info, task->clk_mode);
+
+	return 0;
+}
+
+#define RKVENC2_WORK_TIMEOUT_DELAY		(200)
+#define RKVENC2_WAIT_TIMEOUT_DELAY		(2000)
+
+static void rkvenc2_task_pop_pending(struct mpp_task *task)
+{
+	struct mpp_session *session = task->session;
+
+	mutex_lock(&session->pending_lock);
+	list_del_init(&task->pending_link);
+	mutex_unlock(&session->pending_lock);
+
+	kref_put(&task->ref, mpp_free_task);
+}
+
+static int rkvenc2_task_default_process(struct mpp_dev *mpp,
+					struct mpp_task *task)
+{
+	int ret = 0;
+
+	if (mpp->dev_ops && mpp->dev_ops->result)
+		ret = mpp->dev_ops->result(mpp, task, NULL);
+
+	mpp_debug_func(DEBUG_TASK_INFO, "kref_read %d, ret %d\n",
+			kref_read(&task->ref), ret);
+
+	rkvenc2_task_pop_pending(task);
+
+	return ret;
+}
+
+#define RKVENC2_TIMEOUT_DUMP_REG_START	(0x5100)
+#define RKVENC2_TIMEOUT_DUMP_REG_END	(0x5160)
+
+static void rkvenc2_task_timeout_process(struct mpp_session *session,
+					 struct mpp_task *task)
+{
+	atomic_inc(&task->abort_request);
+	set_bit(TASK_STATE_ABORT, &task->state);
+
+	mpp_err("session %d:%d count %d task %d ref %d timeout\n",
+		session->pid, session->index, atomic_read(&session->task_count),
+		task->task_id, kref_read(&task->ref));
+
+	if (task->mpp) {
+		struct mpp_dev *mpp = task->mpp;
+		u32 start = RKVENC2_TIMEOUT_DUMP_REG_START;
+		u32 end = RKVENC2_TIMEOUT_DUMP_REG_END;
+		u32 offset;
+
+		dev_err(mpp->dev, "core %d dump timeout status:\n", mpp->core_id);
+
+		for (offset = start; offset < end; offset += sizeof(u32))
+			mpp_reg_show(mpp, offset);
+	}
+
+	rkvenc2_task_pop_pending(task);
+}
+
+static int rkvenc2_wait_result(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	struct rkvenc_poll_slice_cfg cfg;
+	struct rkvenc_task *enc_task;
+	struct mpp_request *req;
+	struct mpp_task *task;
+	struct mpp_dev *mpp;
+	union rkvenc2_slice_len_info slice_info;
+	u32 task_id;
+	int ret = 0;
+
+	mutex_lock(&session->pending_lock);
+	task = list_first_entry_or_null(&session->pending_list,
+					struct mpp_task,
+					pending_link);
+	mutex_unlock(&session->pending_lock);
+	if (!task) {
+		mpp_err("session %p pending list is empty!\n", session);
+		return -EIO;
+	}
+
+	mpp = mpp_get_task_used_device(task, session);
+	enc_task = to_rkvenc_task(task);
+	task_id = task->task_id;
+
+	req = cmpxchg(&msgs->poll_req, msgs->poll_req, NULL);
+
+	if (!enc_task->task_split || enc_task->task_split_done) {
+task_done_ret:
+		ret = wait_event_interruptible(task->wait, test_bit(TASK_STATE_DONE, &task->state));
+		if (ret == -ERESTARTSYS)
+			mpp_err("wait task break by signal in normal mode\n");
+
+		return rkvenc2_task_default_process(mpp, task);
+
+	}
+
+	/* not slice return just wait all slice length */
+	if (!req) {
+		do {
+			ret = wait_event_interruptible(task->wait, kfifo_out(&enc_task->slice_info,
+									     &slice_info, 1));
+			if (ret == -ERESTARTSYS) {
+				mpp_err("wait task break by signal in slice all mode\n");
+				return 0;
+			}
+			mpp_dbg_slice("task %d rd %3d len %d %s\n",
+					task_id, enc_task->slice_rd_cnt, slice_info.slice_len,
+					slice_info.last ? "last" : "");
+
+			enc_task->slice_rd_cnt++;
+
+			if (slice_info.last)
+				goto task_done_ret;
+		} while (1);
+	}
+
+	if (copy_from_user(&cfg, req->data, sizeof(cfg))) {
+		mpp_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+
+	mpp_dbg_slice("task %d poll irq %d:%d\n", task->task_id,
+		      cfg.count_max, cfg.count_ret);
+	cfg.count_ret = 0;
+
+	/* handle slice mode poll return */
+	do {
+		ret = wait_event_interruptible(task->wait, kfifo_out(&enc_task->slice_info,
+								     &slice_info, 1));
+		if (ret == -ERESTARTSYS) {
+			mpp_err("wait task break by signal in slice one mode\n");
+			return 0;
+		}
+		mpp_dbg_slice("core %d task %d rd %3d len %d %s\n", task_id,
+				mpp->core_id, enc_task->slice_rd_cnt, slice_info.slice_len,
+				slice_info.last ? "last" : "");
+		enc_task->slice_rd_cnt++;
+		if (cfg.count_ret < cfg.count_max) {
+			struct rkvenc_poll_slice_cfg __user *ucfg =
+				(struct rkvenc_poll_slice_cfg __user *)(req->data);
+			u32 __user *dst = (u32 __user *)(ucfg + 1);
+
+			/* Do NOT return here when put_user error. Just continue */
+			if (put_user(slice_info.val, dst + cfg.count_ret))
+				ret = -EFAULT;
+
+			cfg.count_ret++;
+			if (put_user(cfg.count_ret, &ucfg->count_ret))
+				ret = -EFAULT;
+		}
+
+		if (slice_info.last) {
+			enc_task->task_split_done = 1;
+			goto task_done_ret;
+		}
+
+		if (cfg.count_ret >= cfg.count_max)
+			return 0;
+
+		if (ret < 0)
+			return ret;
+	} while (!ret);
+
+	rkvenc2_task_timeout_process(session, task);
+
+	return ret;
+}
+
+static struct mpp_hw_ops rkvenc_hw_ops = {
+	.init = rkvenc_init,
+	.exit = rkvenc_exit,
+	.clk_on = rkvenc_clk_on,
+	.clk_off = rkvenc_clk_off,
+	.set_freq = rkvenc_set_freq,
+	.reset = rkvenc_reset,
+};
+
+static struct mpp_dev_ops rkvenc_dev_ops_v2 = {
+	.wait_result = rkvenc2_wait_result,
+	.alloc_task = rkvenc_alloc_task,
+	.run = rkvenc_run,
+	.irq = rkvenc_irq,
+	.isr = rkvenc_isr,
+	.finish = rkvenc_finish,
+	.result = rkvenc_result,
+	.free_task = rkvenc_free_task,
+	.ioctl = rkvenc_control,
+	.init_session = rkvenc_init_session,
+	.free_session = rkvenc_free_session,
+	.dump_session = rkvenc_dump_session,
+};
+
+static struct mpp_dev_ops rkvenc_ccu_dev_ops = {
+	.wait_result = rkvenc2_wait_result,
+	.alloc_task = rkvenc_alloc_task,
+	.prepare = rkvenc2_prepare,
+	.run = rkvenc_run,
+	.irq = rkvenc_irq,
+	.isr = rkvenc_isr,
+	.finish = rkvenc_finish,
+	.result = rkvenc_result,
+	.free_task = rkvenc_free_task,
+	.ioctl = rkvenc_control,
+	.init_session = rkvenc_init_session,
+	.free_session = rkvenc_free_session,
+	.dump_session = rkvenc_dump_session,
+};
+
+static struct mpp_dev_ops vepu540c_dev_ops_v2 = {
+	.wait_result = rkvenc2_wait_result,
+	.alloc_task = rkvenc_alloc_task,
+	.run = rkvenc_run,
+	.irq = vepu540c_irq,
+	.isr = rkvenc_isr,
+	.finish = rkvenc_finish,
+	.result = rkvenc_result,
+	.free_task = rkvenc_free_task,
+	.ioctl = rkvenc_control,
+	.init_session = rkvenc_init_session,
+	.free_session = rkvenc_free_session,
+	.dump_session = rkvenc_dump_session,
+};
+
+static const struct mpp_dev_var rkvenc_v2_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_v2_hw_info.hw,
+	.trans_info = trans_rkvenc_v2,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &rkvenc_dev_ops_v2,
+};
+
+static const struct mpp_dev_var rkvenc_540c_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_540c_hw_info.hw,
+	.trans_info = trans_rkvenc_540c,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &vepu540c_dev_ops_v2,
+};
+
+static const struct mpp_dev_var rkvenc_510_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_510_hw_info.hw,
+	.trans_info = trans_rkvenc_540c,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &rkvenc_dev_ops_v2,
+};
+
+static const struct mpp_dev_var rkvenc_ccu_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_v2_hw_info.hw,
+	.trans_info = trans_rkvenc_v2,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &rkvenc_ccu_dev_ops,
+};
+
+static const struct mpp_dev_var rkvenc_rk3576_ccu_data = {
+	.device_type = MPP_DEVICE_RKVENC,
+	.hw_info = &rkvenc_510_hw_info.hw,
+	.trans_info = trans_rkvenc_540c,
+	.hw_ops = &rkvenc_hw_ops,
+	.dev_ops = &rkvenc_ccu_dev_ops,
+};
+
+static const struct of_device_id mpp_rkvenc_dt_match[] = {
+	{
+		.compatible = "rockchip,rkv-encoder-v2",
+		.data = &rkvenc_v2_data,
+	},
+#ifdef CONFIG_CPU_RK3576
+	{
+		.compatible = "rockchip,rkv-encoder-rk3576-core",
+		.data = &rkvenc_rk3576_ccu_data,
+	},
+	{
+		.compatible = "rockchip,rkv-encoder-rk3576-ccu",
+	},
+	{
+		.compatible = "rockchip,rkv-encoder-rk3576",
+		.data = &rkvenc_510_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3528
+	{
+		.compatible = "rockchip,rkv-encoder-rk3528",
+		.data = &rkvenc_540c_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3562
+	{
+		.compatible = "rockchip,rkv-encoder-rk3562",
+		.data = &rkvenc_540c_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3588
+	{
+		.compatible = "rockchip,rkv-encoder-v2-core",
+		.data = &rkvenc_ccu_data,
+	},
+	{
+		.compatible = "rockchip,rkv-encoder-v2-ccu",
+	},
+#endif
+	{},
+};
+
+static int rkvenc_ccu_probe(struct platform_device *pdev)
+{
+	struct rkvenc_ccu *ccu;
+	struct device *dev = &pdev->dev;
+
+	ccu = devm_kzalloc(dev, sizeof(*ccu), GFP_KERNEL);
+	if (!ccu)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ccu);
+
+	mutex_init(&ccu->lock);
+	INIT_LIST_HEAD(&ccu->core_list);
+	spin_lock_init(&ccu->lock_dchs);
+
+	return 0;
+}
+
+static int rkvenc_attach_ccu(struct device *dev, struct rkvenc_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_ccu *ccu;
+
+	mpp_debug_enter();
+
+	np = of_parse_phandle(dev->of_node, "rockchip,ccu", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	ccu = platform_get_drvdata(pdev);
+	if (!ccu)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&enc->core_link);
+	mutex_lock(&ccu->lock);
+	ccu->core_num++;
+	list_add_tail(&enc->core_link, &ccu->core_list);
+	mutex_unlock(&ccu->lock);
+
+	/* attach the ccu-domain to current core */
+	if (!ccu->main_core) {
+		/**
+		 * set the first device for the main-core,
+		 * then the domain of the main-core named ccu-domain
+		 */
+		ccu->main_core = &enc->mpp;
+	} else {
+		struct mpp_iommu_info *ccu_info, *cur_info;
+
+		/* set the ccu-domain for current device */
+		ccu_info = ccu->main_core->iommu_info;
+		cur_info = enc->mpp.iommu_info;
+
+		if (cur_info) {
+			cur_info->domain = ccu_info->domain;
+			cur_info->rw_sem = ccu_info->rw_sem;
+		}
+		mpp_iommu_attach(cur_info);
+
+		/* increase main core message capacity */
+		ccu->main_core->msgs_cap++;
+		enc->mpp.msgs_cap = 0;
+	}
+	enc->ccu = ccu;
+
+	dev_info(dev, "attach ccu as core %d\n", enc->mpp.core_id);
+	mpp_debug_enter();
+
+	return 0;
+}
+
+static int rkvenc2_alloc_rcbbuf(struct platform_device *pdev, struct rkvenc_dev *enc)
+{
+	int ret;
+	u32 vals[2];
+	dma_addr_t iova;
+	u32 sram_used, sram_size;
+	struct device_node *sram_np;
+	struct resource sram_res;
+	resource_size_t sram_start, sram_end;
+	struct iommu_domain *domain;
+	struct device *dev = &pdev->dev;
+
+	/* get rcb iova start and size */
+	ret = device_property_read_u32_array(dev, "rockchip,rcb-iova", vals, 2);
+	if (ret)
+		return ret;
+
+	iova = PAGE_ALIGN(vals[0]);
+	sram_used = PAGE_ALIGN(vals[1]);
+	if (!sram_used) {
+		dev_err(dev, "sram rcb invalid.\n");
+		return -EINVAL;
+	}
+	/* alloc reserve iova for rcb */
+	ret = mpp_iommu_reserve_iova(enc->mpp.iommu_info, iova, sram_used);
+	if (ret) {
+		dev_err(dev, "alloc rcb iova error.\n");
+		return ret;
+	}
+	/* get sram device node */
+	sram_np = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	if (!sram_np) {
+		dev_err(dev, "could not find phandle sram\n");
+		return -ENODEV;
+	}
+	/* get sram start and size */
+	ret = of_address_to_resource(sram_np, 0, &sram_res);
+	of_node_put(sram_np);
+	if (ret) {
+		dev_err(dev, "find sram res error\n");
+		return ret;
+	}
+	/* check sram start and size is PAGE_SIZE align */
+	sram_start = round_up(sram_res.start, PAGE_SIZE);
+	sram_end = round_down(sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (sram_end <= sram_start) {
+		dev_err(dev, "no available sram, phy_start %pa, phy_end %pa\n",
+			&sram_start, &sram_end);
+		return -ENOMEM;
+	}
+	sram_size = sram_end - sram_start;
+	sram_size = sram_used < sram_size ? sram_used : sram_size;
+	/* iova map to sram */
+	domain = enc->mpp.iommu_info->domain;
+	ret = iommu_map(domain, iova, sram_start, sram_size, IOMMU_READ | IOMMU_WRITE);
+	if (ret) {
+		dev_err(dev, "sram iommu_map error.\n");
+		return ret;
+	}
+	/* alloc dma for the remaining buffer, sram + dma */
+	if (sram_size < sram_used) {
+		struct page *page;
+		size_t page_size = PAGE_ALIGN(sram_used - sram_size);
+
+		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, get_order(page_size));
+		if (!page) {
+			dev_err(dev, "unable to allocate pages\n");
+			ret = -ENOMEM;
+			goto err_sram_map;
+		}
+		/* iova map to dma */
+		ret = iommu_map(domain, iova + sram_size, page_to_phys(page),
+				page_size, IOMMU_READ | IOMMU_WRITE);
+		if (ret) {
+			dev_err(dev, "page iommu_map error.\n");
+			__free_pages(page, get_order(page_size));
+			goto err_sram_map;
+		}
+		enc->rcb_page = page;
+	}
+
+	enc->sram_size = sram_size;
+	enc->sram_used = sram_used;
+	enc->sram_iova = iova;
+	enc->sram_enabled = -1;
+	dev_info(dev, "sram_start %pa\n", &sram_start);
+	dev_info(dev, "sram_iova %pad\n", &enc->sram_iova);
+	dev_info(dev, "sram_size %u\n", enc->sram_size);
+	dev_info(dev, "sram_used %u\n", enc->sram_used);
+
+	return 0;
+
+err_sram_map:
+	iommu_unmap(domain, iova, sram_size);
+
+	return ret;
+}
+
+static int rkvenc2_iommu_fault_handle(struct iommu_domain *iommu,
+				      struct device *iommu_dev,
+				      unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+	struct mpp_task *mpp_task;
+	struct rkvenc_ccu *ccu = enc->ccu;
+
+	if (ccu) {
+		struct rkvenc_dev *core = NULL, *n;
+
+		list_for_each_entry_safe(core, n, &ccu->core_list, core_link) {
+			if (core->mpp.iommu_info &&
+			    (&core->mpp.iommu_info->pdev->dev == iommu_dev)) {
+				mpp = &core->mpp;
+				break;
+			}
+		}
+	}
+
+	/*
+	 * Mask iommu irq, in order for iommu not repeatedly trigger pagefault.
+	 * Until the pagefault task finish by hw timeout.
+	 */
+	rockchip_iommu_mask_irq(mpp->dev);
+
+	mpp_task = mpp->cur_task;
+	dev_info(mpp->dev, "core %d page fault found dchs %08x\n",
+		 mpp->core_id, mpp_read_relaxed(&enc->mpp, DCHS_REG_OFFSET));
+
+	if (mpp_task)
+		mpp_task_dump_mem_region(mpp, mpp_task);
+
+	return 0;
+}
+
+static int rkvenc_core_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct rkvenc_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+
+	enc = devm_kzalloc(dev, sizeof(*enc), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		struct device_node *np = pdev->dev.of_node;
+		const struct of_device_id *match = NULL;
+
+		match = of_match_node(mpp_rkvenc_dt_match, np);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(np, "rkvenc");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret)
+		return ret;
+
+	/* attach core to ccu */
+	ret = rkvenc_attach_ccu(dev, enc);
+	if (ret) {
+		dev_err(dev, "attach ccu failed\n");
+		return ret;
+	}
+	rkvenc2_alloc_rcbbuf(pdev, enc);
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_ONESHOT,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+	mpp->session_max_buffers = RKVENC_SESSION_MAX_BUFFERS;
+	enc->hw_info = to_rkvenc_info(mpp->var->hw_info);
+	mpp->fault_handler = rkvenc2_iommu_fault_handle;
+	rkvenc_procfs_init(mpp);
+	rkvenc_procfs_ccu_init(mpp);
+
+	/* if current is main-core, register current device to mpp service */
+	if (mpp == enc->ccu->main_core)
+		mpp_dev_register_srv(mpp, mpp->srv);
+
+	return 0;
+}
+
+static int rkvenc_probe_default(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct rkvenc_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+
+	enc = devm_kzalloc(dev, sizeof(*enc), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_rkvenc_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret)
+		return ret;
+
+	rkvenc2_alloc_rcbbuf(pdev, enc);
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		goto failed_get_irq;
+	}
+	mpp->session_max_buffers = RKVENC_SESSION_MAX_BUFFERS;
+	enc->hw_info = to_rkvenc_info(mpp->var->hw_info);
+	rkvenc_procfs_init(mpp);
+	mpp_dev_register_srv(mpp, mpp->srv);
+
+	return 0;
+
+failed_get_irq:
+	mpp_dev_remove(mpp);
+
+	return ret;
+}
+
+static int rkvenc_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "probing start\n");
+
+	if (strstr(np->name, "ccu"))
+		ret = rkvenc_ccu_probe(pdev);
+	else if (strstr(np->name, "core"))
+		ret = rkvenc_core_probe(pdev);
+	else
+		ret = rkvenc_probe_default(pdev);
+
+	dev_info(dev, "probing finish\n");
+
+	return ret;
+}
+
+static int rkvenc2_free_rcbbuf(struct platform_device *pdev, struct rkvenc_dev *enc)
+{
+	struct iommu_domain *domain;
+
+	if (enc->rcb_page) {
+		size_t page_size = PAGE_ALIGN(enc->sram_used - enc->sram_size);
+		int order = min(get_order(page_size), MAX_ORDER);
+
+		__free_pages(enc->rcb_page, order);
+	}
+	if (enc->sram_iova) {
+		domain = enc->mpp.iommu_info->domain;
+		iommu_unmap(domain, enc->sram_iova, enc->sram_used);
+	}
+
+	return 0;
+}
+
+static int rkvenc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	if (strstr(np->name, "ccu")) {
+		dev_info(dev, "remove ccu\n");
+	} else if (strstr(np->name, "core")) {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+		struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+		dev_info(dev, "remove core\n");
+		if (enc->ccu) {
+			mutex_lock(&enc->ccu->lock);
+			list_del_init(&enc->core_link);
+			enc->ccu->core_num--;
+			mutex_unlock(&enc->ccu->lock);
+		}
+		rkvenc2_free_rcbbuf(pdev, enc);
+		mpp_dev_remove(&enc->mpp);
+		rkvenc_procfs_remove(&enc->mpp);
+	} else {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+		struct rkvenc_dev *enc = to_rkvenc_dev(mpp);
+
+		dev_info(dev, "remove device\n");
+		rkvenc2_free_rcbbuf(pdev, enc);
+		mpp_dev_remove(mpp);
+		rkvenc_procfs_remove(mpp);
+	}
+
+	return 0;
+}
+
+static void rkvenc_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	if (!strstr(dev_name(dev), "ccu"))
+		mpp_dev_shutdown(pdev);
+}
+
+struct platform_driver rockchip_rkvenc2_driver = {
+	.probe = rkvenc_probe,
+	.remove = rkvenc_remove,
+	.shutdown = rkvenc_shutdown,
+	.driver = {
+		.name = RKVENC_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_rkvenc_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
diff --git a/drivers/video/rockchip/mpp/mpp_service.c b/drivers/video/rockchip/mpp/mpp_service.c
new file mode 100644
index 0000000000000..882002e20097b
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_service.c
@@ -0,0 +1,533 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/completion.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/nospec.h>
+#include <linux/mfd/syscon.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define MPP_CLASS_NAME		"mpp_class"
+#define MPP_SERVICE_NAME	"mpp_service"
+
+#define HAS_RKVDEC	IS_ENABLED(CONFIG_ROCKCHIP_MPP_RKVDEC)
+#define HAS_RKVENC	IS_ENABLED(CONFIG_ROCKCHIP_MPP_RKVENC)
+#define HAS_VDPU1	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VDPU1)
+#define HAS_VEPU1	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VEPU1)
+#define HAS_VDPU2	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VDPU2)
+#define HAS_VEPU2	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VEPU2)
+#define HAS_VEPU22	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VEPU22)
+#define HAS_IEP2	IS_ENABLED(CONFIG_ROCKCHIP_MPP_IEP2)
+#define HAS_JPGDEC	IS_ENABLED(CONFIG_ROCKCHIP_MPP_JPGDEC)
+#define HAS_JPGENC	IS_ENABLED(CONFIG_ROCKCHIP_MPP_JPGENC)
+#define HAS_RKVDEC2	IS_ENABLED(CONFIG_ROCKCHIP_MPP_RKVDEC2)
+#define HAS_RKVENC2	IS_ENABLED(CONFIG_ROCKCHIP_MPP_RKVENC2)
+#define HAS_AV1DEC	IS_ENABLED(CONFIG_ROCKCHIP_MPP_AV1DEC)
+#define HAS_VDPP	IS_ENABLED(CONFIG_ROCKCHIP_MPP_VDPP)
+
+#define MPP_REGISTER_DRIVER(srv, flag, X, x) {\
+	if (flag)\
+		mpp_add_driver(srv, MPP_DRIVER_##X, &rockchip_##x##_driver, "grf_"#x);\
+	}
+
+unsigned int mpp_dev_debug;
+module_param(mpp_dev_debug, uint, 0644);
+MODULE_PARM_DESC(mpp_dev_debug, "bit switch for mpp debug information");
+
+static const char mpp_version[] = MPP_VERSION;
+
+static int mpp_init_grf(struct device_node *np,
+			struct mpp_grf_info *grf_info,
+			const char *grf_name)
+{
+	int ret;
+	int index;
+	u32 grf_offset = 0;
+	u32 grf_value = 0;
+	struct regmap *grf;
+
+	grf = syscon_regmap_lookup_by_phandle(np, "rockchip,grf");
+	if (IS_ERR_OR_NULL(grf))
+		return -EINVAL;
+
+	ret = of_property_read_u32(np, "rockchip,grf-offset", &grf_offset);
+	if (ret)
+		return -ENODATA;
+
+	index = of_property_match_string(np, "rockchip,grf-names", grf_name);
+	if (index < 0)
+		return -ENODATA;
+
+	ret = of_property_read_u32_index(np, "rockchip,grf-values",
+					 index, &grf_value);
+	if (ret)
+		return -ENODATA;
+
+	grf_info->grf = grf;
+	grf_info->offset = grf_offset;
+	grf_info->val = grf_value;
+
+	mpp_set_grf(grf_info);
+
+	return 0;
+}
+
+static int mpp_add_driver(struct mpp_service *srv,
+			  enum MPP_DRIVER_TYPE type,
+			  struct platform_driver *driver,
+			  const char *grf_name)
+{
+	int ret;
+
+	mpp_init_grf(srv->dev->of_node,
+		     &srv->grf_infos[type],
+		     grf_name);
+
+	ret = platform_driver_register(driver);
+	if (ret)
+		return ret;
+
+	srv->sub_drivers[type] = driver;
+
+	return 0;
+}
+
+static int mpp_remove_driver(struct mpp_service *srv, int i)
+{
+	if (srv && srv->sub_drivers[i]) {
+		mpp_set_grf(&srv->grf_infos[i]);
+		platform_driver_unregister(srv->sub_drivers[i]);
+		srv->sub_drivers[i] = NULL;
+	}
+
+	return 0;
+}
+
+static int mpp_register_service(struct mpp_service *srv,
+				const char *service_name)
+{
+	int ret;
+	struct device *dev = srv->dev;
+
+	/* create a device */
+	ret = alloc_chrdev_region(&srv->dev_id, 0, 1, service_name);
+	if (ret) {
+		dev_err(dev, "alloc dev_t failed\n");
+		return ret;
+	}
+
+	cdev_init(&srv->mpp_cdev, &rockchip_mpp_fops);
+	srv->mpp_cdev.owner = THIS_MODULE;
+	srv->mpp_cdev.ops = &rockchip_mpp_fops;
+
+	ret = cdev_add(&srv->mpp_cdev, srv->dev_id, 1);
+	if (ret) {
+		unregister_chrdev_region(srv->dev_id, 1);
+		dev_err(dev, "add device failed\n");
+		return ret;
+	}
+
+	srv->child_dev = device_create(srv->cls, dev, srv->dev_id,
+				       NULL, "%s", service_name);
+
+	return 0;
+}
+
+static int mpp_remove_service(struct mpp_service *srv)
+{
+	device_destroy(srv->cls, srv->dev_id);
+	cdev_del(&srv->mpp_cdev);
+	unregister_chrdev_region(srv->dev_id, 1);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int mpp_procfs_remove(struct mpp_service *srv)
+{
+	if (srv->procfs) {
+		proc_remove(srv->procfs);
+		srv->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int mpp_show_version(struct seq_file *seq, void *offset)
+{
+	seq_printf(seq, "%s\n", mpp_version);
+
+	return 0;
+}
+
+static int mpp_dump_session(struct mpp_session *session, struct seq_file *s)
+{
+	struct mpp_dma_session *dma = session->dma;
+	struct mpp_dma_buffer *n;
+	struct mpp_dma_buffer *buffer;
+	phys_addr_t end;
+	unsigned long z = 0, t = 0;
+	int i = 0;
+#define K(size) ((unsigned long)((size) >> 10))
+
+	if (!dma)
+		return 0;
+
+	seq_puts(s, "session iova range dump:\n");
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n, &dma->used_list, link) {
+		end = buffer->iova + buffer->size - 1;
+		z = (unsigned long)buffer->size;
+		t += z;
+
+		seq_printf(s, "%4d: ", i++);
+		seq_printf(s, "%pa..%pa (%10lu %s)\n", &buffer->iova, &end,
+			   (z >= 1024) ? (K(z)) : z,
+			   (z >= 1024) ? "KiB" : "Bytes");
+	}
+	i = 0;
+	list_for_each_entry_safe(buffer, n, &dma->unused_list, link) {
+		if (!buffer->dmabuf)
+			continue;
+
+		end = buffer->iova + buffer->size - 1;
+		z = (unsigned long)buffer->size;
+		t += z;
+
+		seq_printf(s, "%4d: ", i++);
+		seq_printf(s, "%pa..%pa (%10lu %s)\n", &buffer->iova, &end,
+			   (z >= 1024) ? (K(z)) : z,
+			   (z >= 1024) ? "KiB" : "Bytes");
+	}
+
+	mutex_unlock(&dma->list_mutex);
+	seq_printf(s, "session: pid=%d index=%d\n", session->pid, session->index);
+	seq_printf(s, " device: %s\n", dev_name(session->mpp->dev));
+	seq_printf(s, " memory: %lu MiB\n", K(K(t)));
+
+	return 0;
+}
+
+static int mpp_show_session_summary(struct seq_file *seq, void *offset)
+{
+	struct mpp_session *session = NULL, *n;
+	struct mpp_service *srv = seq->private;
+
+	mutex_lock(&srv->session_lock);
+	list_for_each_entry_safe(session, n,
+				 &srv->session_list,
+				 service_link) {
+		struct  mpp_dev *mpp;
+
+		if (!session->priv)
+			continue;
+
+		if (!session->mpp)
+			continue;
+		mpp = session->mpp;
+
+		mpp_dump_session(session, seq);
+
+		if (mpp->dev_ops->dump_session)
+			mpp->dev_ops->dump_session(session, seq);
+	}
+	mutex_unlock(&srv->session_lock);
+
+	return 0;
+}
+
+static int mpp_show_support_cmd(struct seq_file *file, void *v)
+{
+	seq_puts(file, "------------- SUPPORT CMD -------------\n");
+	seq_printf(file, "QUERY_HW_SUPPORT:     0x%08x\n", MPP_CMD_QUERY_HW_SUPPORT);
+	seq_printf(file, "QUERY_HW_ID:          0x%08x\n", MPP_CMD_QUERY_HW_ID);
+	seq_printf(file, "QUERY_CMD_SUPPORT:    0x%08x\n", MPP_CMD_QUERY_CMD_SUPPORT);
+	seq_printf(file, "QUERY_BUTT:           0x%08x\n", MPP_CMD_QUERY_BUTT);
+	seq_puts(file, "----\n");
+	seq_printf(file, "INIT_CLIENT_TYPE:     0x%08x\n", MPP_CMD_INIT_CLIENT_TYPE);
+	seq_printf(file, "INIT_TRANS_TABLE:     0x%08x\n", MPP_CMD_INIT_TRANS_TABLE);
+	seq_printf(file, "INIT_BUTT:            0x%08x\n", MPP_CMD_INIT_BUTT);
+	seq_puts(file, "----\n");
+	seq_printf(file, "SET_REG_WRITE:        0x%08x\n", MPP_CMD_SET_REG_WRITE);
+	seq_printf(file, "SET_REG_READ:         0x%08x\n", MPP_CMD_SET_REG_READ);
+	seq_printf(file, "SET_REG_ADDR_OFFSET:  0x%08x\n", MPP_CMD_SET_REG_ADDR_OFFSET);
+	seq_printf(file, "SEND_BUTT:            0x%08x\n", MPP_CMD_SEND_BUTT);
+	seq_puts(file, "----\n");
+	seq_printf(file, "POLL_HW_FINISH:       0x%08x\n", MPP_CMD_POLL_HW_FINISH);
+	seq_printf(file, "POLL_BUTT:            0x%08x\n", MPP_CMD_POLL_BUTT);
+	seq_puts(file, "----\n");
+	seq_printf(file, "RESET_SESSION:        0x%08x\n", MPP_CMD_RESET_SESSION);
+	seq_printf(file, "TRANS_FD_TO_IOVA:     0x%08x\n", MPP_CMD_TRANS_FD_TO_IOVA);
+	seq_printf(file, "RELEASE_FD:           0x%08x\n", MPP_CMD_RELEASE_FD);
+	seq_printf(file, "SEND_CODEC_INFO:      0x%08x\n", MPP_CMD_SEND_CODEC_INFO);
+	seq_printf(file, "CONTROL_BUTT:         0x%08x\n", MPP_CMD_CONTROL_BUTT);
+
+	return 0;
+}
+
+static int mpp_show_support_device(struct seq_file *file, void *v)
+{
+	u32 i;
+	struct mpp_service *srv = file->private;
+
+	seq_puts(file, "---- SUPPORT DEVICES ----\n");
+	for (i = 0; i < MPP_DEVICE_BUTT; i++) {
+		struct mpp_dev *mpp;
+		struct mpp_hw_info *hw_info;
+
+		if (test_bit(i, &srv->hw_support)) {
+			mpp = srv->sub_devices[array_index_nospec(i, MPP_DEVICE_BUTT)];
+			if (!mpp)
+				continue;
+
+			seq_printf(file, "DEVICE[%2d]:%-10s", i, mpp_device_name[i]);
+			hw_info = mpp->var->hw_info;
+			if (hw_info->hw_id)
+				seq_printf(file, "HW_ID:0x%08x", hw_info->hw_id);
+			seq_puts(file, "\n");
+		}
+	}
+
+	return 0;
+}
+
+static int mpp_show_device_load(struct seq_file *file, void *v)
+{
+	u32 i, j;
+	struct mpp_service *srv = file->private;
+
+	if (!srv->load_interval) {
+		seq_puts(file, "please set load_interval first!!!\n");
+		seq_puts(file, "e.g. set 1000ms to load_interval:\n");
+		seq_puts(file, "echo 1000 > /proc/mpp_service/load_interval\n");
+		return 0;
+	}
+
+	for (i = 0; i < MPP_DEVICE_BUTT; i++) {
+		struct mpp_taskqueue *queue = srv->task_queues[i];
+
+		if (!queue)
+			continue;
+
+		for (j = 0; j < MPP_MAX_CORE_NUM; j++) {
+			struct mpp_dev *mpp = queue->cores[j];
+
+			if (!mpp)
+				continue;
+			seq_printf(file, "%-25s load: %3d.%02d%% utilization: %3d.%02d%%\n",
+				   dev_name(mpp->dev),
+				   mpp->load_info.load, mpp->load_info.load_frac,
+				   mpp->load_info.utilization, mpp->load_info.utilization_frac);
+		}
+	}
+
+	return 0;
+}
+
+static int mpp_procfs_init(struct mpp_service *srv)
+{
+	srv->procfs = proc_mkdir(MPP_SERVICE_NAME, NULL);
+	if (IS_ERR_OR_NULL(srv->procfs)) {
+		mpp_err("failed on mkdir /proc/%s\n", MPP_SERVICE_NAME);
+		srv->procfs = NULL;
+		return -EIO;
+	}
+	/* show version */
+	proc_create_single("version", 0444, srv->procfs, mpp_show_version);
+	/* for show session info */
+	proc_create_single_data("sessions-summary", 0444,
+				srv->procfs, mpp_show_session_summary, srv);
+	/* show support dev cmd */
+	proc_create_single("supports-cmd", 0444, srv->procfs, mpp_show_support_cmd);
+	/* show support devices */
+	proc_create_single_data("supports-device", 0444,
+				srv->procfs, mpp_show_support_device, srv);
+	srv->timing_en = 1;
+	mpp_procfs_create_u32("timing_en", 0644, srv->procfs, &srv->timing_en);
+	/* show per device load info */
+	proc_create_single_data("load", 0444, srv->procfs, mpp_show_device_load, srv);
+	srv->load_interval = 0;
+	mpp_procfs_create_u32("load_interval", 0644, srv->procfs, &srv->load_interval);
+
+	return 0;
+}
+#else
+static inline int mpp_procfs_remove(struct mpp_service *srv)
+{
+	return 0;
+}
+
+static inline int mpp_procfs_init(struct mpp_service *srv)
+{
+	return 0;
+}
+#endif
+
+static int mpp_service_probe(struct platform_device *pdev)
+{
+	int ret, i;
+	struct mpp_service *srv = NULL;
+	struct mpp_taskqueue *queue;
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "%s\n", mpp_version);
+	dev_info(dev, "probe start\n");
+	srv = devm_kzalloc(dev, sizeof(*srv), GFP_KERNEL);
+	if (!srv)
+		return -ENOMEM;
+
+	srv->dev = dev;
+	atomic_set(&srv->shutdown_request, 0);
+	platform_set_drvdata(pdev, srv);
+
+	srv->cls = class_create(THIS_MODULE, MPP_CLASS_NAME);
+	if (PTR_ERR_OR_ZERO(srv->cls))
+		return PTR_ERR(srv->cls);
+
+	of_property_read_u32(np, "rockchip,taskqueue-count",
+			     &srv->taskqueue_cnt);
+	if (srv->taskqueue_cnt > MPP_DEVICE_BUTT) {
+		dev_err(dev, "rockchip,taskqueue-count %d must less than %d\n",
+			srv->taskqueue_cnt, MPP_DEVICE_BUTT);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < srv->taskqueue_cnt; i++) {
+		queue = mpp_taskqueue_init(dev);
+		if (!queue)
+			continue;
+
+		kthread_init_worker(&queue->worker);
+		queue->kworker_task = kthread_run(kthread_worker_fn, &queue->worker,
+						  "mpp_worker_%d", i);
+		srv->task_queues[i] = queue;
+	}
+
+	of_property_read_u32(np, "rockchip,resetgroup-count",
+			     &srv->reset_group_cnt);
+	if (srv->reset_group_cnt > MPP_DEVICE_BUTT) {
+		dev_err(dev, "rockchip,resetgroup-count %d must less than %d\n",
+			srv->reset_group_cnt, MPP_DEVICE_BUTT);
+		return -EINVAL;
+	}
+
+	if (srv->reset_group_cnt) {
+		u32 i = 0;
+		struct mpp_reset_group *group;
+
+		for (i = 0; i < srv->reset_group_cnt; i++) {
+			group = devm_kzalloc(dev, sizeof(*group), GFP_KERNEL);
+			if (!group)
+				continue;
+
+			init_rwsem(&group->rw_sem);
+			srv->reset_groups[i] = group;
+		}
+	}
+
+	ret = mpp_register_service(srv, MPP_SERVICE_NAME);
+	if (ret) {
+		dev_err(dev, "register %s device\n", MPP_SERVICE_NAME);
+		goto fail_register;
+	}
+	mutex_init(&srv->session_lock);
+	INIT_LIST_HEAD(&srv->session_list);
+	mpp_procfs_init(srv);
+
+	/* register sub drivers */
+	MPP_REGISTER_DRIVER(srv, HAS_RKVDEC, RKVDEC, rkvdec);
+	MPP_REGISTER_DRIVER(srv, HAS_RKVENC, RKVENC, rkvenc);
+	MPP_REGISTER_DRIVER(srv, HAS_VDPU1, VDPU1, vdpu1);
+	MPP_REGISTER_DRIVER(srv, HAS_VEPU1, VEPU1, vepu1);
+	MPP_REGISTER_DRIVER(srv, HAS_VDPU2, VDPU2, vdpu2);
+	MPP_REGISTER_DRIVER(srv, HAS_VEPU2, VEPU2, vepu2);
+	MPP_REGISTER_DRIVER(srv, HAS_VEPU22, VEPU22, vepu22);
+	MPP_REGISTER_DRIVER(srv, HAS_IEP2, IEP2, iep2);
+	MPP_REGISTER_DRIVER(srv, HAS_JPGDEC, JPGDEC, jpgdec);
+	MPP_REGISTER_DRIVER(srv, HAS_JPGENC, JPGENC, jpgenc);
+	MPP_REGISTER_DRIVER(srv, HAS_RKVDEC2, RKVDEC2, rkvdec2);
+	MPP_REGISTER_DRIVER(srv, HAS_RKVENC2, RKVENC2, rkvenc2);
+	MPP_REGISTER_DRIVER(srv, HAS_AV1DEC, AV1DEC, av1dec);
+	MPP_REGISTER_DRIVER(srv, HAS_VDPP, VDPP, vdpp);
+
+	dev_info(dev, "probe success\n");
+
+	return 0;
+
+fail_register:
+	class_destroy(srv->cls);
+
+	return ret;
+}
+
+static int mpp_service_remove(struct platform_device *pdev)
+{
+	struct mpp_taskqueue *queue;
+	struct device *dev = &pdev->dev;
+	struct mpp_service *srv = platform_get_drvdata(pdev);
+	int i;
+
+	dev_info(dev, "remove device\n");
+
+	for (i = 0; i < srv->taskqueue_cnt; i++) {
+		queue = srv->task_queues[i];
+		if (queue && queue->kworker_task) {
+			kthread_flush_worker(&queue->worker);
+			kthread_stop(queue->kworker_task);
+			queue->kworker_task = NULL;
+		}
+	}
+
+	/* remove sub drivers */
+	for (i = 0; i < MPP_DRIVER_BUTT; i++)
+		mpp_remove_driver(srv, i);
+
+	mpp_remove_service(srv);
+	class_destroy(srv->cls);
+	mpp_procfs_remove(srv);
+
+	return 0;
+}
+
+static const struct of_device_id mpp_dt_ids[] = {
+	{
+		.compatible = "rockchip,mpp-service",
+	},
+	{ },
+};
+
+static struct platform_driver mpp_service_driver = {
+	.probe = mpp_service_probe,
+	.remove = mpp_service_remove,
+	.driver = {
+		.name = "mpp_service",
+		.of_match_table = of_match_ptr(mpp_dt_ids),
+	},
+};
+
+module_platform_driver(mpp_service_driver);
+
+MODULE_IMPORT_NS(DMA_BUF);
+MODULE_LICENSE("Dual MIT/GPL");
+MODULE_VERSION(MPP_VERSION);
+MODULE_AUTHOR("Ding Wei leo.ding@rock-chips.com");
+MODULE_DESCRIPTION("Rockchip mpp service driver");
diff --git a/drivers/video/rockchip/mpp/mpp_vdpp.c b/drivers/video/rockchip/mpp/mpp_vdpp.c
new file mode 100644
index 0000000000000..5c48c5de73097
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_vdpp.c
@@ -0,0 +1,828 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2023 Rockchip Electronics Co., Ltd.
+ *
+ * author:
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define VDPP_DRIVER_NAME		"mpp_vdpp"
+
+#define	VDPP_SESSION_MAX_BUFFERS	15
+#define VDPP_REG_WORK_MODE			0x0008
+#define VDPP_REG_VDPP_MODE			BIT(1)
+#define VDPP_GET_VDPP_MODE(x)			((x) & 0x2)
+
+#define VDPP_FMT_DEFAULT			(0)
+
+#define to_vdpp_info(info)	\
+		container_of(info, struct vdpp_hw_info, hw)
+#define to_vdpp_task(task)	\
+		container_of(task, struct vdpp_task, mpp_task)
+#define to_vdpp_dev(dev)	\
+		container_of(dev, struct vdpp_dev, mpp)
+
+struct vdpp_hw_info {
+	struct mpp_hw_info hw;
+
+	/* register info */
+	u32 start_base;
+	u32 cfg_base;
+	u32 work_mode_base;
+	u32 gate_base;
+	u32 rst_sta_base;
+	u32 int_en_base;
+	u32 int_clr_base;
+	u32 int_sta_base; // int_sta = int_raw_sta && int_en
+	u32 int_mask;
+	u32 err_mask;
+	/* register for zme */
+	u32 zme_reg_off;
+	u32 zme_reg_num;
+	/* for soft reset */
+	u32 bit_rst_en;
+	u32 bit_rst_done;
+};
+
+struct vdpp_task {
+	struct mpp_task mpp_task;
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 *reg;
+	u32 *zme_reg;
+
+	struct reg_offset_info off_inf;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct vdpp_dev {
+	struct mpp_dev mpp;
+	struct vdpp_hw_info *hw_info;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	struct mpp_clk_info sclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_s;
+	/* for zme */
+	void __iomem *zme_base;
+};
+
+// RK3528
+static struct vdpp_hw_info vdpp_v1_hw_info = {
+	.hw = {
+		.reg_num = 53,
+		.reg_id = 21,
+		.reg_en = 0,
+		.reg_start = 0,
+		.reg_end = 52,
+	},
+	.start_base = 0x0000,
+	.cfg_base = 0x0004,
+	.work_mode_base = 0x0008,
+	.gate_base = 0x0010,
+	.rst_sta_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_clr_base = 0x0024,
+	.int_sta_base = 0x0028,
+	.int_mask = 0x0073,
+	.err_mask = 0x0070,
+	.zme_reg_off = 0x2000,
+	.zme_reg_num = 530,
+	.bit_rst_en = BIT(21),
+	.bit_rst_done = BIT(0),
+};
+
+// RK3576
+static struct vdpp_hw_info vdpp_rk3576_hw_info = {
+	.hw = {
+		.reg_num = 300,
+		.reg_id = 21,
+		.reg_en = 0,
+		.reg_start = 0,
+		.reg_end = 299,
+	},
+	.start_base = 0x0000,
+	.cfg_base = 0x0004,
+	.work_mode_base = 0x0008,
+	.gate_base = 0x0010,
+	.rst_sta_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_clr_base = 0x0024,
+	.int_sta_base = 0x0028,
+	.int_mask = 0x0073,
+	.err_mask = 0x0070,
+	.zme_reg_off = 0x2000,
+	.zme_reg_num = 530,
+	.bit_rst_en = BIT(21),
+	.bit_rst_done = BIT(0),
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_vdpp_v1[] = {
+	24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_vdpp_rk3576[] = {
+	24, 25, 26, 27, 56, 60,
+};
+
+static struct mpp_trans_info vdpp_v1_trans[] = {
+	[VDPP_FMT_DEFAULT] = {
+		.count = ARRAY_SIZE(trans_tbl_vdpp_v1),
+		.table = trans_tbl_vdpp_v1,
+	},
+};
+
+static struct mpp_trans_info vdpp_rk3576_trans[] = {
+	[VDPP_FMT_DEFAULT] = {
+		.count = ARRAY_SIZE(trans_tbl_vdpp_rk3576),
+		.table = trans_tbl_vdpp_rk3576,
+	},
+};
+
+static int vdpp_process_reg_fd(struct mpp_session *session,
+				 struct vdpp_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					VDPP_FMT_DEFAULT, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+	return 0;
+}
+
+static int vdpp_extract_task_msg(struct vdpp_task *task,
+				   struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct vdpp_hw_info *hw_info = to_vdpp_info(task->mpp_task.hw_info);
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			int req_base;
+			int max_size;
+			u8 *dst = NULL;
+
+			if (req->offset >= hw_info->zme_reg_off) {
+				req_base = hw_info->zme_reg_off;
+				max_size = hw_info->zme_reg_num * sizeof(u32);
+				dst = (u8 *)task->zme_reg;
+			} else {
+				req_base = 0;
+				max_size = hw_info->hw.reg_num * sizeof(u32);
+				dst = (u8 *)task->reg;
+			}
+
+			ret = mpp_check_req(req, req_base, max_size, 0, max_size);
+			if (ret)
+				return ret;
+
+			dst += req->offset - req_base;
+			if (copy_from_user(dst, req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			int req_base;
+			int max_size;
+
+			if (req->offset >= hw_info->zme_reg_off) {
+				req_base = hw_info->zme_reg_off;
+				max_size = hw_info->zme_reg_num * sizeof(u32);
+			} else {
+				req_base = 0;
+				max_size = hw_info->hw.reg_num * sizeof(u32);
+			}
+
+			ret = mpp_check_req(req, req_base, max_size, 0, max_size);
+			if (ret)
+				return ret;
+
+			memcpy(&task->r_reqs[task->r_req_cnt++], req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *vdpp_alloc_task(struct mpp_session *session,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	u32 reg_num;
+	struct mpp_task *mpp_task = NULL;
+	struct vdpp_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+	struct vdpp_hw_info *hw_info = to_vdpp_info(mpp->var->hw_info);
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+	/* alloc reg buffer */
+	reg_num = hw_info->hw.reg_num + hw_info->zme_reg_num;
+	task->reg = kcalloc(reg_num, sizeof(u32), GFP_KERNEL);
+	if (!task->reg)
+		goto free_task;
+	task->zme_reg = task->reg + hw_info->hw.reg_num;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = vdpp_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = vdpp_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task->reg);
+free_task:
+	kfree(task);
+	return NULL;
+}
+
+static int vdpp_write_req_zme(void __iomem *reg_base,
+			      u32 *regs,
+			      u32 start_idx, u32 end_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++) {
+		int reg = i * sizeof(u32);
+
+		mpp_debug(DEBUG_SET_REG_L2, "zme_reg[%03d]: %04x: 0x%08x\n", i, reg, regs[i]);
+		writel_relaxed(regs[i], reg_base + reg);
+	}
+
+	return 0;
+}
+
+static int vdpp_read_req_zme(void __iomem *reg_base,
+			     u32 *regs,
+			     u32 start_idx, u32 end_idx)
+{
+	int i;
+
+	for (i = start_idx; i < end_idx; i++) {
+		int reg = i * sizeof(u32);
+
+		regs[i] = readl_relaxed(reg_base + reg);
+		mpp_debug(DEBUG_GET_REG_L2, "zme_reg[%03d]: %04x: 0x%08x\n", i, reg, regs[i]);
+	}
+
+	return 0;
+}
+
+static int vdpp_run(struct mpp_dev *mpp,
+		      struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct vdpp_task *task = to_vdpp_task(mpp_task);
+	struct vdpp_hw_info *hw_info = vdpp->hw_info;
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	reg_en = hw_info->hw.reg_en;
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+
+		if (req->offset >= hw_info->zme_reg_off) {
+			/* set registers for zme */
+			int off = req->offset - hw_info->zme_reg_off;
+			int s = off / sizeof(u32);
+			int e = s + req->size / sizeof(u32);
+
+			if (!vdpp->zme_base)
+				continue;
+			vdpp_write_req_zme(vdpp->zme_base, task->zme_reg, s, e);
+		} else {
+			/* set registers for vdpp */
+			int s = req->offset / sizeof(u32);
+			int e = s + req->size / sizeof(u32);
+
+			mpp_write_req(mpp, task->reg, s, e, reg_en);
+		}
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, hw_info->start_base, task->reg[reg_en]);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpp_finish(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	u32 i;
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct vdpp_task *task = to_vdpp_task(mpp_task);
+	struct vdpp_hw_info *hw_info = vdpp->hw_info;
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		struct mpp_request *req = &task->r_reqs[i];
+
+		if (req->offset >= hw_info->zme_reg_off) {
+			int off = req->offset - hw_info->zme_reg_off;
+			int s = off / sizeof(u32);
+			int e = s + req->size / sizeof(u32);
+
+			if (!vdpp->zme_base)
+				continue;
+			vdpp_read_req_zme(vdpp->zme_base, task->zme_reg, s, e);
+		} else {
+			int s = req->offset / sizeof(u32);
+			int e = s + req->size / sizeof(u32);
+
+			mpp_read_req(mpp, task->reg, s, e);
+		}
+	}
+
+	task->reg[hw_info->int_sta_base / sizeof(u32)] = task->irq_status;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpp_result(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task,
+			 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct vdpp_task *task = to_vdpp_task(mpp_task);
+	struct vdpp_hw_info *hw_info = to_vdpp_info(mpp_task->hw_info);
+
+	mpp_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		struct mpp_request *req;
+
+		req = &task->r_reqs[i];
+		/* set register L2 */
+		if (req->offset >= hw_info->zme_reg_off) {
+			struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+			int off = req->offset - hw_info->zme_reg_off;
+
+			if (!vdpp->zme_base)
+				continue;
+			if (copy_to_user(req->data,
+					 (u8 *)task->zme_reg + off,
+					 req->size)) {
+				mpp_err("copy_to_user reg_l2 fail\n");
+				return -EIO;
+			}
+		} else {
+			if (copy_to_user(req->data,
+					 (u8 *)task->reg + req->offset,
+					 req->size)) {
+				mpp_err("copy_to_user reg fail\n");
+				return -EIO;
+			}
+		}
+	}
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpp_free_task(struct mpp_session *session,
+			    struct mpp_task *mpp_task)
+{
+	struct vdpp_task *task = to_vdpp_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task->reg);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int vdpp_procfs_remove(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	if (vdpp->procfs) {
+		proc_remove(vdpp->procfs);
+		vdpp->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int vdpp_procfs_init(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	vdpp->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(vdpp->procfs)) {
+		mpp_err("failed on open procfs\n");
+		vdpp->procfs = NULL;
+		return -EIO;
+	}
+	mpp_procfs_create_u32("aclk", 0644,
+			      vdpp->procfs, &vdpp->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      vdpp->procfs, &mpp->session_max_buffers);
+	return 0;
+}
+#else
+static inline int vdpp_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vdpp_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int vdpp_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &vdpp->aclk_info, "aclk");
+	if (ret)
+		mpp_err("failed on clk_get aclk\n");
+	ret = mpp_get_clk_info(mpp, &vdpp->hclk_info, "hclk");
+	if (ret)
+		mpp_err("failed on clk_get hclk\n");
+	ret = mpp_get_clk_info(mpp, &vdpp->sclk_info, "sclk");
+	if (ret)
+		mpp_err("failed on clk_get sclk\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&vdpp->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	vdpp->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "rst_a");
+	if (!vdpp->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	vdpp->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "rst_h");
+	if (!vdpp->rst_h)
+		mpp_err("No hclk reset resource define\n");
+	vdpp->rst_s = mpp_reset_control_get(mpp, RST_TYPE_CORE, "rst_s");
+	if (!vdpp->rst_s)
+		mpp_err("No sclk reset resource define\n");
+
+	return 0;
+}
+
+static int vdpp_clk_on(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	mpp_clk_safe_enable(vdpp->aclk_info.clk);
+	mpp_clk_safe_enable(vdpp->hclk_info.clk);
+	mpp_clk_safe_enable(vdpp->sclk_info.clk);
+
+	return 0;
+}
+
+static int vdpp_clk_off(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	mpp_clk_safe_disable(vdpp->aclk_info.clk);
+	mpp_clk_safe_disable(vdpp->hclk_info.clk);
+	mpp_clk_safe_disable(vdpp->sclk_info.clk);
+
+	return 0;
+}
+
+static int vdpp_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct vdpp_task *task = to_vdpp_task(mpp_task);
+
+	mpp_clk_set_rate(&vdpp->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int vdpp_reduce_freq(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+
+	mpp_clk_set_rate(&vdpp->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int vdpp_irq(struct mpp_dev *mpp)
+{
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct vdpp_hw_info *hw_info = vdpp->hw_info;
+	u32 work_mode = mpp_read(mpp, VDPP_REG_WORK_MODE);
+
+	mpp_debug_enter();
+
+	if (!(VDPP_GET_VDPP_MODE(work_mode) & VDPP_REG_VDPP_MODE))
+		return IRQ_NONE;
+	mpp->irq_status = mpp_read(mpp, hw_info->int_sta_base);
+	if (!(mpp->irq_status & hw_info->int_mask))
+		return IRQ_NONE;
+	mpp_write(mpp, hw_info->int_en_base, 0);
+	mpp_write(mpp, hw_info->int_clr_base, mpp->irq_status);
+
+	/* ensure hardware is being off status */
+	mpp_write(mpp, hw_info->start_base, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int vdpp_isr(struct mpp_dev *mpp)
+{
+	struct vdpp_task *task = NULL;
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct mpp_task *mpp_task = mpp->cur_task;
+
+	mpp_debug_enter();
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_vdpp_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	if (task->irq_status & vdpp->hw_info->err_mask)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int _vdpp_reset(struct mpp_dev *mpp, struct vdpp_dev *vdpp)
+{
+	if (vdpp->rst_a && vdpp->rst_h && vdpp->rst_s) {
+		mpp_debug(DEBUG_RESET, "reset in\n");
+
+		/* Don't skip this or iommu won't work after reset */
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(vdpp->rst_a);
+		mpp_safe_reset(vdpp->rst_h);
+		mpp_safe_reset(vdpp->rst_s);
+		udelay(5);
+		mpp_safe_unreset(vdpp->rst_a);
+		mpp_safe_unreset(vdpp->rst_h);
+		mpp_safe_unreset(vdpp->rst_s);
+		mpp_pmu_idle_request(mpp, false);
+
+		mpp_debug(DEBUG_RESET, "reset out\n");
+	}
+
+	return 0;
+}
+
+static int vdpp_reset(struct mpp_dev *mpp)
+{
+	int ret = 0;
+	u32 rst_status = 0;
+	struct vdpp_dev *vdpp = to_vdpp_dev(mpp);
+	struct vdpp_hw_info *hw_info = vdpp->hw_info;
+
+	/* soft rest first */
+	mpp_write(mpp, hw_info->cfg_base, hw_info->bit_rst_en);
+	ret = readl_relaxed_poll_timeout(mpp->reg_base + hw_info->rst_sta_base,
+					 rst_status,
+					 rst_status & hw_info->bit_rst_done,
+					 0, 5);
+	if (ret) {
+		mpp_err("soft reset timeout, use cru reset\n");
+		return _vdpp_reset(mpp, vdpp);
+	}
+
+	mpp_write(mpp, hw_info->rst_sta_base, 0);
+
+	/* ensure hardware is being off status */
+	mpp_write(mpp, hw_info->start_base, 0);
+
+	return 0;
+}
+
+static struct mpp_hw_ops vdpp_v1_hw_ops = {
+	.init = vdpp_init,
+	.clk_on = vdpp_clk_on,
+	.clk_off = vdpp_clk_off,
+	.set_freq = vdpp_set_freq,
+	.reduce_freq = vdpp_reduce_freq,
+	.reset = vdpp_reset,
+};
+
+static struct mpp_dev_ops vdpp_v1_dev_ops = {
+	.alloc_task = vdpp_alloc_task,
+	.run = vdpp_run,
+	.irq = vdpp_irq,
+	.isr = vdpp_isr,
+	.finish = vdpp_finish,
+	.result = vdpp_result,
+	.free_task = vdpp_free_task,
+};
+
+static const struct mpp_dev_var vdpp_v1_data = {
+	.device_type = MPP_DEVICE_VDPP,
+	.hw_info = &vdpp_v1_hw_info.hw,
+	.trans_info = vdpp_v1_trans,
+	.hw_ops = &vdpp_v1_hw_ops,
+	.dev_ops = &vdpp_v1_dev_ops,
+};
+
+static const struct mpp_dev_var vdpp_rk3576_data = {
+	.device_type = MPP_DEVICE_VDPP,
+	.hw_info = &vdpp_rk3576_hw_info.hw,
+	.trans_info = vdpp_rk3576_trans,
+	.hw_ops = &vdpp_v1_hw_ops,
+	.dev_ops = &vdpp_v1_dev_ops,
+};
+
+static const struct of_device_id mpp_vdpp_dt_match[] = {
+	{
+		.compatible = "rockchip,vdpp-v1",
+		.data = &vdpp_v1_data,
+	},
+#ifdef CONFIG_CPU_RK3576
+	{
+		.compatible = "rockchip,vdpp-rk3576",
+		.data = &vdpp_rk3576_data,
+	},
+#endif
+	{},
+};
+
+static int vdpp_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct vdpp_dev *vdpp = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+	struct resource *res;
+
+	dev_info(dev, "probe device\n");
+	vdpp = devm_kzalloc(dev, sizeof(struct vdpp_dev), GFP_KERNEL);
+	if (!vdpp)
+		return -ENOMEM;
+	mpp = &vdpp->mpp;
+	platform_set_drvdata(pdev, mpp);
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vdpp_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+		mpp->core_id = -1;
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+	/* map zme regs */
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "zme_regs");
+	if (res) {
+		vdpp->zme_base = devm_ioremap(dev, res->start, resource_size(res));
+		if (!vdpp->zme_base) {
+			dev_err(dev, "ioremap failed for resource %pR\n", res);
+			return -ENOMEM;
+		}
+	}
+	/* get irq */
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = VDPP_SESSION_MAX_BUFFERS;
+	vdpp->hw_info = to_vdpp_info(mpp->var->hw_info);
+	vdpp_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int vdpp_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = platform_get_drvdata(pdev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	vdpp_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_vdpp_driver = {
+	.probe = vdpp_probe,
+	.remove = vdpp_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = VDPP_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_vdpp_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_vdpp_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_vdpu1.c b/drivers/video/rockchip/mpp/mpp_vdpu1.c
new file mode 100644
index 0000000000000..5109bde2c2366
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_vdpu1.c
@@ -0,0 +1,973 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+#include <soc/rockchip/rockchip_iommu.h>
+
+#define VDPU1_DRIVER_NAME		"mpp_vdpu1"
+
+#define	VDPU1_SESSION_MAX_BUFFERS	40
+/* The maximum registers number of all the version */
+#define VDPU1_REG_NUM			60
+#define VDPU1_REG_HW_ID_INDEX		0
+#define VDPU1_REG_START_INDEX		0
+#define VDPU1_REG_END_INDEX		59
+
+#define VDPU1_REG_PP_NUM		101
+#define VDPU1_REG_PP_START_INDEX	0
+#define VDPU1_REG_PP_END_INDEX		100
+
+#define VDPU1_REG_DEC_INT_EN		0x004
+#define VDPU1_REG_DEC_INT_EN_INDEX	(1)
+/* B slice detected, used in 8190 decoder and later */
+#define	VDPU1_INT_PIC_INF		BIT(24)
+#define	VDPU1_INT_TIMEOUT		BIT(18)
+#define	VDPU1_INT_SLICE			BIT(17)
+#define	VDPU1_INT_STRM_ERROR		BIT(16)
+#define	VDPU1_INT_ASO_ERROR		BIT(15)
+#define	VDPU1_INT_BUF_EMPTY		BIT(14)
+#define	VDPU1_INT_BUS_ERROR		BIT(13)
+#define	VDPU1_DEC_INT			BIT(12)
+#define	VDPU1_DEC_INT_RAW		BIT(8)
+#define	VDPU1_DEC_IRQ_DIS		BIT(4)
+#define	VDPU1_DEC_START			BIT(0)
+
+/* NOTE: Don't enable it or decoding AVC would meet problem at rk3288 */
+#define VDPU1_REG_DEC_EN		0x008
+#define	VDPU1_CLOCK_GATE_EN		BIT(10)
+
+#define VDPU1_REG_SOFT_RESET		0x194
+#define VDPU1_REG_SOFT_RESET_INDEX	(101)
+
+#define VDPU1_REG_SYS_CTRL		0x00c
+#define VDPU1_REG_SYS_CTRL_INDEX	(3)
+#define VDPU1_RGE_WIDTH_INDEX		(4)
+#define	VDPU1_GET_FORMAT(x)		(((x) >> 28) & 0xf)
+#define VDPU1_GET_PROD_NUM(x)		(((x) >> 16) & 0xffff)
+#define VDPU1_GET_WIDTH(x)		(((x) & 0xff800000) >> 19)
+#define	VDPU1_FMT_H264D			0
+#define	VDPU1_FMT_MPEG4D		1
+#define	VDPU1_FMT_H263D			2
+#define	VDPU1_FMT_JPEGD			3
+#define	VDPU1_FMT_VC1D			4
+#define	VDPU1_FMT_MPEG2D		5
+#define	VDPU1_FMT_MPEG1D		6
+#define	VDPU1_FMT_VP6D			7
+#define	VDPU1_FMT_RESERVED		8
+#define	VDPU1_FMT_VP7D			9
+#define	VDPU1_FMT_VP8D			10
+#define	VDPU1_FMT_AVSD			11
+
+#define VDPU1_REG_STREAM_RLC_BASE	0x030
+#define VDPU1_REG_STREAM_RLC_BASE_INDEX	(12)
+
+#define VDPU1_REG_DIR_MV_BASE		0x0a4
+#define VDPU1_REG_DIR_MV_BASE_INDEX	(41)
+
+#define VDPU1_REG_CLR_CACHE_BASE	0x810
+
+#define to_vdpu_task(task)		\
+		container_of(task, struct vdpu_task, mpp_task)
+#define to_vdpu_dev(dev)		\
+		container_of(dev, struct vdpu_dev, mpp)
+
+enum VPUD1_HW_ID {
+	VDPU1_ID_0102 = 0x0102,
+	VDPU1_ID_9190 = 0x6731,
+};
+
+struct vdpu_task {
+	struct mpp_task mpp_task;
+	/* enable of post process */
+	bool pp_enable;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[VDPU1_REG_PP_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 strm_addr;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct vdpu_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+};
+
+static struct mpp_hw_info vdpu_v1_hw_info = {
+	.reg_num = VDPU1_REG_NUM,
+	.reg_id = VDPU1_REG_HW_ID_INDEX,
+	.reg_start = VDPU1_REG_START_INDEX,
+	.reg_end = VDPU1_REG_END_INDEX,
+	.reg_en = VDPU1_REG_DEC_INT_EN_INDEX,
+};
+
+static struct mpp_hw_info vdpu_pp_v1_hw_info = {
+	.reg_num = VDPU1_REG_PP_NUM,
+	.reg_id = VDPU1_REG_HW_ID_INDEX,
+	.reg_start = VDPU1_REG_PP_START_INDEX,
+	.reg_end = VDPU1_REG_PP_END_INDEX,
+	.reg_en = VDPU1_REG_DEC_INT_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_avsd[] = {
+	12, 13, 14, 15, 16, 17, 40, 41, 45
+};
+
+static const u16 trans_tbl_default[] = {
+	12, 13, 14, 15, 16, 17, 40, 41
+};
+
+static const u16 trans_tbl_jpegd[] = {
+	12, 13, 14, 40, 66, 67
+};
+
+static const u16 trans_tbl_h264d[] = {
+	12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,
+	28, 29, 40
+};
+
+static const u16 trans_tbl_vc1d[] = {
+	12, 13, 14, 15, 16, 17, 27, 41
+};
+
+static const u16 trans_tbl_vp6d[] = {
+	12, 13, 14, 18, 27, 40
+};
+
+static const u16 trans_tbl_vp8d[] = {
+	10, 12, 13, 14, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 40
+};
+
+static struct mpp_trans_info vdpu_v1_trans[] = {
+	[VDPU1_FMT_H264D] = {
+		.count = ARRAY_SIZE(trans_tbl_h264d),
+		.table = trans_tbl_h264d,
+	},
+	[VDPU1_FMT_H263D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU1_FMT_MPEG4D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU1_FMT_JPEGD] = {
+		.count = ARRAY_SIZE(trans_tbl_jpegd),
+		.table = trans_tbl_jpegd,
+	},
+	[VDPU1_FMT_VC1D] = {
+		.count = ARRAY_SIZE(trans_tbl_vc1d),
+		.table = trans_tbl_vc1d,
+	},
+	[VDPU1_FMT_MPEG2D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU1_FMT_MPEG1D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU1_FMT_VP6D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp6d),
+		.table = trans_tbl_vp6d,
+	},
+	[VDPU1_FMT_RESERVED] = {
+		.count = 0,
+		.table = NULL,
+	},
+	[VDPU1_FMT_VP7D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU1_FMT_VP8D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp8d),
+		.table = trans_tbl_vp8d,
+	},
+	[VDPU1_FMT_AVSD] = {
+		.count = ARRAY_SIZE(trans_tbl_avsd),
+		.table = trans_tbl_avsd,
+	},
+};
+
+static int vdpu_process_reg_fd(struct mpp_session *session,
+			       struct vdpu_task *task,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+	int fmt = VDPU1_GET_FORMAT(task->reg[VDPU1_REG_SYS_CTRL_INDEX]);
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					fmt, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+	/*
+	 * special offset scale case
+	 *
+	 * This translation is for fd + offset translation.
+	 * One register has 32bits. We need to transfer both buffer file
+	 * handle and the start address offset so we packet file handle
+	 * and offset together using below format.
+	 *
+	 *  0~9  bit for buffer file handle range 0 ~ 1023
+	 * 10~31 bit for offset range 0 ~ 4M
+	 *
+	 * But on 4K case the offset can be larger the 4M
+	 */
+	if (likely(fmt == VDPU1_FMT_H264D)) {
+		int fd;
+		u32 offset;
+		dma_addr_t iova = 0;
+		u32 idx = VDPU1_REG_DIR_MV_BASE_INDEX;
+		struct mpp_mem_region *mem_region = NULL;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			fd = task->reg[idx];
+			offset = 0;
+		} else {
+			fd = task->reg[idx] & 0x3ff;
+			offset = task->reg[idx] >> 10 << 4;
+		}
+		mem_region = mpp_task_attach_fd(&task->mpp_task, fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("reg[%03d]: %08x fd %d attach failed\n",
+				idx, task->reg[idx], fd);
+			goto fail;
+		}
+
+		iova = mem_region->iova;
+		mpp_debug(DEBUG_IOMMU, "DMV[%3d]: %3d => %pad + offset %10d\n",
+			  idx, fd, &iova, offset);
+		task->reg[idx] = iova + offset;
+	}
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+	return 0;
+fail:
+	return -EFAULT;
+}
+
+static int vdpu_extract_task_msg(struct vdpu_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *vdpu_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct vdpu_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	if (session->device_type == MPP_DEVICE_VDPU1_PP) {
+		task->pp_enable = true;
+		mpp_task->hw_info = &vdpu_pp_v1_hw_info;
+	} else {
+		mpp_task->hw_info = mpp->var->hw_info;
+	}
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = vdpu_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = vdpu_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->strm_addr = task->reg[VDPU1_REG_STREAM_RLC_BASE_INDEX];
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int vdpu_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, VDPU1_REG_CLR_CACHE_BASE, 1);
+	/* set registers for hardware */
+	reg_en = mpp_task->hw_info->reg_en;
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		int s = req->offset / sizeof(u32);
+		int e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the register before the start the device */
+	wmb();
+	mpp_write(mpp, VDPU1_REG_DEC_INT_EN,
+		  task->reg[reg_en] | VDPU1_DEC_START);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpu_finish(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	u32 dec_get;
+	s32 dec_length;
+	struct mpp_request *req;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+	/* revert hack for irq status */
+	task->reg[VDPU1_REG_DEC_INT_EN_INDEX] = task->irq_status;
+	/* revert hack for decoded length */
+	dec_get = mpp_read_relaxed(mpp, VDPU1_REG_STREAM_RLC_BASE);
+	dec_length = dec_get - task->strm_addr;
+	task->reg[VDPU1_REG_STREAM_RLC_BASE_INDEX] = dec_length << 10;
+	mpp_debug(DEBUG_REGISTER,
+		  "dec_get %08x dec_length %d\n", dec_get, dec_length);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpu_result(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task,
+		       struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int vdpu_free_task(struct mpp_session *session,
+			  struct mpp_task *mpp_task)
+{
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int vdpu_procfs_remove(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int vdpu_procfs_init(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	dec->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int vdpu_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vdpu_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int vdpu_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_VDPU1];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int vdpu_3036_init(struct mpp_dev *mpp)
+{
+	vdpu_init(mpp);
+	set_bit(mpp->var->device_type, &mpp->queue->dev_active_flags);
+	return 0;
+}
+
+static int vdpu_clk_on(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int vdpu_clk_off(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_safe_disable(dec->aclk_info.clk);
+	mpp_clk_safe_disable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int vdpu_3288_get_freq(struct mpp_dev *mpp,
+			      struct mpp_task *mpp_task)
+{
+	u32 width;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	width = VDPU1_GET_WIDTH(task->reg[VDPU1_RGE_WIDTH_INDEX]);
+	if (width > 2560)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	return 0;
+}
+
+static int vdpu_3368_get_freq(struct mpp_dev *mpp,
+			      struct mpp_task *mpp_task)
+{
+	u32 width;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	width = VDPU1_GET_WIDTH(task->reg[VDPU1_RGE_WIDTH_INDEX]);
+	if (width > 2560)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	return 0;
+}
+
+static int vdpu_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int vdpu_reduce_freq(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int vdpu_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, VDPU1_REG_DEC_INT_EN);
+	if (!(mpp->irq_status & VDPU1_DEC_INT_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, VDPU1_REG_DEC_INT_EN, 0);
+	/* set clock gating to save power */
+	mpp_write(mpp, VDPU1_REG_DEC_EN, VDPU1_CLOCK_GATE_EN);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int vdpu_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct vdpu_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_vdpu_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	err_mask = VDPU1_INT_TIMEOUT
+		| VDPU1_INT_STRM_ERROR
+		| VDPU1_INT_ASO_ERROR
+		| VDPU1_INT_BUF_EMPTY
+		| VDPU1_INT_BUS_ERROR;
+
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int vdpu_soft_reset(struct mpp_dev *mpp)
+{
+	u32 val;
+	u32 ret;
+
+	mpp_write(mpp, VDPU1_REG_SOFT_RESET, 1);
+	ret = readl_relaxed_poll_timeout(mpp->reg_base + VDPU1_REG_SOFT_RESET,
+					 val, !val, 0, 5);
+
+	return ret;
+}
+
+static int vdpu_reset(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+	u32 ret = 0;
+
+	/* soft reset first */
+	ret = vdpu_soft_reset(mpp);
+	if (ret && dec->rst_a && dec->rst_h) {
+		mpp_err("soft reset failed, use cru reset!\n");
+		mpp_debug(DEBUG_RESET, "reset in\n");
+
+		/* Don't skip this or iommu won't work after reset */
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+
+		mpp_debug(DEBUG_RESET, "reset out\n");
+	}
+	mpp_write(mpp, VDPU1_REG_DEC_INT_EN, 0);
+
+	return 0;
+}
+
+static int vdpu_3036_set_grf(struct mpp_dev *mpp)
+{
+	int grf_changed;
+	struct mpp_dev *loop = NULL, *n;
+	struct mpp_taskqueue *queue = mpp->queue;
+	bool pd_is_on;
+
+	grf_changed = mpp_grf_is_changed(mpp->grf_info);
+	if (grf_changed) {
+
+		/*
+		 * in this case, devices share the queue also share the same pd&clk,
+		 * so use mpp->dev's pd to control all the process is okay
+		 */
+		pd_is_on = rockchip_pmu_pd_is_on(mpp->dev);
+		if (!pd_is_on)
+			rockchip_pmu_pd_on(mpp->dev);
+		mpp->hw_ops->clk_on(mpp);
+
+		list_for_each_entry_safe(loop, n, &queue->dev_list, queue_link) {
+			if (test_bit(loop->var->device_type, &queue->dev_active_flags)) {
+				mpp_set_grf(loop->grf_info);
+				if (loop->hw_ops->clk_on)
+					loop->hw_ops->clk_on(loop);
+				if (loop->hw_ops->reset)
+					loop->hw_ops->reset(loop);
+				rockchip_iommu_disable(loop->dev);
+				if (loop->hw_ops->clk_off)
+					loop->hw_ops->clk_off(loop);
+				clear_bit(loop->var->device_type, &queue->dev_active_flags);
+			}
+		}
+
+		mpp_set_grf(mpp->grf_info);
+		rockchip_iommu_enable(mpp->dev);
+		set_bit(mpp->var->device_type, &queue->dev_active_flags);
+
+		mpp->hw_ops->clk_off(mpp);
+		if (!pd_is_on)
+			rockchip_pmu_pd_off(mpp->dev);
+	}
+
+	return 0;
+}
+
+static struct mpp_hw_ops vdpu_v1_hw_ops = {
+	.init = vdpu_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+	.set_grf = vdpu_3036_set_grf,
+};
+
+static struct mpp_hw_ops vdpu_3036_hw_ops = {
+	.init = vdpu_3036_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+	.set_grf = vdpu_3036_set_grf,
+};
+
+static struct mpp_hw_ops vdpu_3288_hw_ops = {
+	.init = vdpu_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.get_freq = vdpu_3288_get_freq,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+};
+
+static struct mpp_hw_ops vdpu_3368_hw_ops = {
+	.init = vdpu_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.get_freq = vdpu_3368_get_freq,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+};
+
+static struct mpp_dev_ops vdpu_v1_dev_ops = {
+	.alloc_task = vdpu_alloc_task,
+	.run = vdpu_run,
+	.irq = vdpu_irq,
+	.isr = vdpu_isr,
+	.finish = vdpu_finish,
+	.result = vdpu_result,
+	.free_task = vdpu_free_task,
+};
+
+static const struct mpp_dev_var vdpu_v1_data = {
+	.device_type = MPP_DEVICE_VDPU1,
+	.hw_info = &vdpu_v1_hw_info,
+	.trans_info = vdpu_v1_trans,
+	.hw_ops = &vdpu_v1_hw_ops,
+	.dev_ops = &vdpu_v1_dev_ops,
+};
+
+static const struct mpp_dev_var vdpu_3036_data = {
+	.device_type = MPP_DEVICE_VDPU1,
+	.hw_info = &vdpu_v1_hw_info,
+	.trans_info = vdpu_v1_trans,
+	.hw_ops = &vdpu_3036_hw_ops,
+	.dev_ops = &vdpu_v1_dev_ops,
+};
+
+static const struct mpp_dev_var vdpu_3288_data = {
+	.device_type = MPP_DEVICE_VDPU1,
+	.hw_info = &vdpu_v1_hw_info,
+	.trans_info = vdpu_v1_trans,
+	.hw_ops = &vdpu_3288_hw_ops,
+	.dev_ops = &vdpu_v1_dev_ops,
+};
+
+static const struct mpp_dev_var vdpu_3368_data = {
+	.device_type = MPP_DEVICE_VDPU1,
+	.hw_info = &vdpu_v1_hw_info,
+	.trans_info = vdpu_v1_trans,
+	.hw_ops = &vdpu_3368_hw_ops,
+	.dev_ops = &vdpu_v1_dev_ops,
+};
+
+static const struct mpp_dev_var avsd_plus_data = {
+	.device_type = MPP_DEVICE_AVSPLUS_DEC,
+	.hw_info = &vdpu_v1_hw_info,
+	.trans_info = vdpu_v1_trans,
+	.hw_ops = &vdpu_v1_hw_ops,
+	.dev_ops = &vdpu_v1_dev_ops,
+};
+
+static const struct of_device_id mpp_vdpu1_dt_match[] = {
+	{
+		.compatible = "rockchip,vpu-decoder-v1",
+		.data = &vdpu_v1_data,
+	},
+#ifdef CONFIG_CPU_RK3288
+	{
+		.compatible = "rockchip,vpu-decoder-rk3288",
+		.data = &vdpu_3288_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3036
+	{
+		.compatible = "rockchip,vpu-decoder-rk3036",
+		.data = &vdpu_3036_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3368
+	{
+		.compatible = "rockchip,vpu-decoder-rk3368",
+		.data = &vdpu_3368_data,
+	},
+#endif
+	{
+		.compatible = "rockchip,avs-plus-decoder",
+		.data = &avsd_plus_data,
+	},
+	{},
+};
+
+static int vdpu_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct vdpu_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	dev_info(dev, "probe device\n");
+	dec = devm_kzalloc(dev, sizeof(struct vdpu_dev), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vdpu1_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(pdev->dev.of_node, "vdpu");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	if (mpp->var->device_type == MPP_DEVICE_VDPU1) {
+		mpp->srv->sub_devices[MPP_DEVICE_VDPU1_PP] = mpp;
+		set_bit(MPP_DEVICE_VDPU1_PP, &mpp->srv->hw_support);
+	}
+
+	mpp->session_max_buffers = VDPU1_SESSION_MAX_BUFFERS;
+	vdpu_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int vdpu_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	vdpu_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_vdpu1_driver = {
+	.probe = vdpu_probe,
+	.remove = vdpu_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = VDPU1_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_vdpu1_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_vdpu1_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_vdpu2.c b/drivers/video/rockchip/mpp/mpp_vdpu2.c
new file mode 100644
index 0000000000000..3d6cd244b04b8
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_vdpu2.c
@@ -0,0 +1,809 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+#include "hack/mpp_hack_px30.h"
+
+#define VDPU2_DRIVER_NAME		"mpp_vdpu2"
+
+#define	VDPU2_SESSION_MAX_BUFFERS	40
+/* The maximum registers number of all the version */
+#define VDPU2_REG_NUM			159
+#define VDPU2_REG_HW_ID_INDEX		-1 /* INVALID */
+#define VDPU2_REG_START_INDEX		50
+#define VDPU2_REG_END_INDEX		158
+
+#define VDPU2_REG_SYS_CTRL			0x0d4
+#define VDPU2_REG_SYS_CTRL_INDEX		(53)
+#define VDPU2_GET_FORMAT(x)			((x) & 0xf)
+#define VDPU2_FMT_H264D				0
+#define VDPU2_FMT_MPEG4D			1
+#define VDPU2_FMT_H263D				2
+#define VDPU2_FMT_JPEGD				3
+#define VDPU2_FMT_VC1D				4
+#define VDPU2_FMT_MPEG2D			5
+#define VDPU2_FMT_MPEG1D			6
+#define VDPU2_FMT_VP6D				7
+#define VDPU2_FMT_RESERVED			8
+#define VDPU2_FMT_VP7D				9
+#define VDPU2_FMT_VP8D				10
+#define VDPU2_FMT_AVSD				11
+
+#define VDPU2_REG_DEC_INT			0x0dc
+#define VDPU2_REG_DEC_INT_INDEX			(55)
+#define VDPU2_INT_TIMEOUT			BIT(13)
+#define VDPU2_INT_STRM_ERROR			BIT(12)
+#define VDPU2_INT_SLICE				BIT(9)
+#define VDPU2_INT_ASO_ERROR			BIT(8)
+#define VDPU2_INT_BUF_EMPTY			BIT(6)
+#define VDPU2_INT_BUS_ERROR			BIT(5)
+#define	VDPU2_DEC_INT				BIT(4)
+#define VDPU2_DEC_IRQ_DIS			BIT(1)
+#define VDPU2_DEC_INT_RAW			BIT(0)
+
+#define VDPU2_REG_DEC_EN			0x0e4
+#define VDPU2_REG_DEC_EN_INDEX			(57)
+#define VDPU2_DEC_CLOCK_GATE_EN			BIT(4)
+#define VDPU2_DEC_START				BIT(0)
+
+#define VDPU2_REG_SOFT_RESET			0x0e8
+#define VDPU2_REG_SOFT_RESET_INDEX		(58)
+
+#define VDPU2_REG_DIR_MV_BASE			0x0f8
+#define VDPU2_REG_DIR_MV_BASE_INDEX		(62)
+
+#define VDPU2_REG_STREAM_RLC_BASE		0x100
+#define VDPU2_REG_STREAM_RLC_BASE_INDEX		(64)
+
+#define VDPU2_REG_CLR_CACHE_BASE		0x810
+
+#define to_vdpu_task(task)		\
+		container_of(task, struct vdpu_task, mpp_task)
+#define to_vdpu_dev(dev)		\
+		container_of(dev, struct vdpu_dev, mpp)
+
+struct vdpu_task {
+	struct mpp_task mpp_task;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[VDPU2_REG_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 strm_addr;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct vdpu_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+};
+
+static struct mpp_hw_info vdpu_v2_hw_info = {
+	.reg_num = VDPU2_REG_NUM,
+	.reg_id = VDPU2_REG_HW_ID_INDEX,
+	.reg_start = VDPU2_REG_START_INDEX,
+	.reg_end = VDPU2_REG_END_INDEX,
+	.reg_en = VDPU2_REG_DEC_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_default[] = {
+	61, 62, 63, 64, 131, 134, 135, 148
+};
+
+static const u16 trans_tbl_jpegd[] = {
+	21, 22, 61, 63, 64, 131
+};
+
+static const u16 trans_tbl_h264d[] = {
+	61, 63, 64, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97,
+	98, 99
+};
+
+static const u16 trans_tbl_vc1d[] = {
+	62, 63, 64, 131, 134, 135, 145, 148
+};
+
+static const u16 trans_tbl_vp6d[] = {
+	61, 63, 64, 131, 136, 145
+};
+
+static const u16 trans_tbl_vp8d[] = {
+	61, 63, 64, 131, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 149
+};
+
+static struct mpp_trans_info vdpu_v2_trans[] = {
+	[VDPU2_FMT_H264D] = {
+		.count = ARRAY_SIZE(trans_tbl_h264d),
+		.table = trans_tbl_h264d,
+	},
+	[VDPU2_FMT_H263D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU2_FMT_MPEG4D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU2_FMT_JPEGD] = {
+		.count = ARRAY_SIZE(trans_tbl_jpegd),
+		.table = trans_tbl_jpegd,
+	},
+	[VDPU2_FMT_VC1D] = {
+		.count = ARRAY_SIZE(trans_tbl_vc1d),
+		.table = trans_tbl_vc1d,
+	},
+	[VDPU2_FMT_MPEG2D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU2_FMT_MPEG1D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU2_FMT_VP6D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp6d),
+		.table = trans_tbl_vp6d,
+	},
+	[VDPU2_FMT_RESERVED] = {
+		.count = 0,
+		.table = NULL,
+	},
+	[VDPU2_FMT_VP7D] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VDPU2_FMT_VP8D] = {
+		.count = ARRAY_SIZE(trans_tbl_vp8d),
+		.table = trans_tbl_vp8d,
+	},
+	[VDPU2_FMT_AVSD] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+};
+
+static int vdpu_process_reg_fd(struct mpp_session *session,
+			       struct vdpu_task *task,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+	int fmt = VDPU2_GET_FORMAT(task->reg[VDPU2_REG_SYS_CTRL_INDEX]);
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					fmt, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	if (likely(fmt == VDPU2_FMT_H264D)) {
+		int fd;
+		u32 offset;
+		dma_addr_t iova = 0;
+		struct mpp_mem_region *mem_region = NULL;
+		int idx = VDPU2_REG_DIR_MV_BASE_INDEX;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			fd = task->reg[idx];
+			offset = 0;
+		} else {
+			fd = task->reg[idx] & 0x3ff;
+			offset = task->reg[idx] >> 10 << 4;
+		}
+		mem_region = mpp_task_attach_fd(&task->mpp_task, fd);
+		if (IS_ERR(mem_region)) {
+			mpp_err("reg[%3d]: %08x fd %d attach failed\n",
+				idx, task->reg[idx], fd);
+			return -EFAULT;
+		}
+
+		iova = mem_region->iova;
+		mpp_debug(DEBUG_IOMMU, "DMV[%3d]: %3d => %pad + offset %10d\n",
+			  idx, fd, &iova, offset);
+		task->reg[idx] = iova + offset;
+	}
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+	return 0;
+}
+
+static int vdpu_extract_task_msg(struct vdpu_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *vdpu_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct vdpu_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = vdpu_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = vdpu_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->strm_addr = task->reg[VDPU2_REG_STREAM_RLC_BASE_INDEX];
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int vdpu_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, VDPU2_REG_CLR_CACHE_BASE, 1);
+	/* set registers for hardware */
+	 reg_en = mpp_task->hw_info->reg_en;
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		int s = req->offset / sizeof(u32);
+		int e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Flush the registers */
+	wmb();
+	mpp_write(mpp, VDPU2_REG_DEC_EN,
+		  task->reg[reg_en] | VDPU2_DEC_START);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpu_px30_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	return vdpu_run(mpp, mpp_task);
+}
+
+static int vdpu_finish(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	u32 dec_get;
+	s32 dec_length;
+	struct mpp_request *req;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+	/* revert hack for irq status */
+	task->reg[VDPU2_REG_DEC_INT_INDEX] = task->irq_status;
+	/* revert hack for decoded length */
+	dec_get = mpp_read_relaxed(mpp, VDPU2_REG_STREAM_RLC_BASE);
+	dec_length = dec_get - task->strm_addr;
+	task->reg[VDPU2_REG_STREAM_RLC_BASE_INDEX] = dec_length << 10;
+	mpp_debug(DEBUG_REGISTER,
+		  "dec_get %08x dec_length %d\n", dec_get, dec_length);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vdpu_result(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task,
+		       struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int vdpu_free_task(struct mpp_session *session,
+			  struct mpp_task *mpp_task)
+{
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int vdpu_procfs_remove(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	if (dec->procfs) {
+		proc_remove(dec->procfs);
+		dec->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int vdpu_procfs_init(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	dec->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(dec->procfs)) {
+		mpp_err("failed on open procfs\n");
+		dec->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(dec->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      dec->procfs, &dec->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      dec->procfs, &mpp->session_max_buffers);
+
+	return 0;
+}
+#else
+static inline int vdpu_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vdpu_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+#endif
+
+static int vdpu_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_VDPU2];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &dec->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &dec->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&dec->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	dec->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!dec->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	dec->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!dec->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int vdpu_px30_init(struct mpp_dev *mpp)
+{
+	vdpu_init(mpp);
+	return px30_workaround_combo_init(mpp);
+}
+
+static int vdpu_clk_on(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_safe_enable(dec->aclk_info.clk);
+	mpp_clk_safe_enable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int vdpu_clk_off(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_safe_disable(dec->aclk_info.clk);
+	mpp_clk_safe_disable(dec->hclk_info.clk);
+
+	return 0;
+}
+
+static int vdpu_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+	struct vdpu_task *task = to_vdpu_task(mpp_task);
+
+	mpp_clk_set_rate(&dec->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int vdpu_reduce_freq(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+
+	mpp_clk_set_rate(&dec->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int vdpu_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, VDPU2_REG_DEC_INT);
+	if (!(mpp->irq_status & VDPU2_DEC_INT_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, VDPU2_REG_DEC_INT, 0);
+	/* set clock gating to save power */
+	mpp_write(mpp, VDPU2_REG_DEC_EN, VDPU2_DEC_CLOCK_GATE_EN);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int vdpu_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct vdpu_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_vdpu_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	err_mask = VDPU2_INT_TIMEOUT
+		| VDPU2_INT_STRM_ERROR
+		| VDPU2_INT_ASO_ERROR
+		| VDPU2_INT_BUF_EMPTY
+		| VDPU2_INT_BUS_ERROR;
+
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int vdpu_soft_reset(struct mpp_dev *mpp)
+{
+	u32 val;
+	u32 ret;
+
+	mpp_write(mpp, VDPU2_REG_SOFT_RESET, 1);
+	ret = readl_relaxed_poll_timeout(mpp->reg_base + VDPU2_REG_SOFT_RESET,
+					 val, !val, 0, 5);
+	return ret;
+}
+
+static int vdpu_reset(struct mpp_dev *mpp)
+{
+	struct vdpu_dev *dec = to_vdpu_dev(mpp);
+	u32 ret = 0;
+
+	mpp_write(mpp, VDPU2_REG_DEC_EN, 0);
+	mpp_write(mpp, VDPU2_REG_DEC_INT, 0);
+
+	/* soft reset first */
+	ret = vdpu_soft_reset(mpp);
+	if (ret && dec->rst_a && dec->rst_h) {
+		/* Don't skip this or iommu won't work after reset */
+		mpp_err("soft reset failed, use cru reset!\n");
+		mpp_debug(DEBUG_RESET, "reset in\n");
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(dec->rst_a);
+		mpp_safe_reset(dec->rst_h);
+		udelay(5);
+		mpp_safe_unreset(dec->rst_a);
+		mpp_safe_unreset(dec->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+		mpp_debug(DEBUG_RESET, "reset out\n");
+	}
+
+	return 0;
+}
+
+static struct mpp_hw_ops vdpu_v2_hw_ops = {
+	.init = vdpu_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+};
+
+static struct mpp_hw_ops vdpu_px30_hw_ops = {
+	.init = vdpu_px30_init,
+	.clk_on = vdpu_clk_on,
+	.clk_off = vdpu_clk_off,
+	.set_freq = vdpu_set_freq,
+	.reduce_freq = vdpu_reduce_freq,
+	.reset = vdpu_reset,
+	.set_grf = px30_workaround_combo_switch_grf,
+};
+
+static struct mpp_dev_ops vdpu_v2_dev_ops = {
+	.alloc_task = vdpu_alloc_task,
+	.run = vdpu_run,
+	.irq = vdpu_irq,
+	.isr = vdpu_isr,
+	.finish = vdpu_finish,
+	.result = vdpu_result,
+	.free_task = vdpu_free_task,
+};
+
+static struct mpp_dev_ops vdpu_px30_dev_ops = {
+	.alloc_task = vdpu_alloc_task,
+	.run = vdpu_px30_run,
+	.irq = vdpu_irq,
+	.isr = vdpu_isr,
+	.finish = vdpu_finish,
+	.result = vdpu_result,
+	.free_task = vdpu_free_task,
+};
+
+static const struct mpp_dev_var vdpu_v2_data = {
+	.device_type = MPP_DEVICE_VDPU2,
+	.hw_info = &vdpu_v2_hw_info,
+	.trans_info = vdpu_v2_trans,
+	.hw_ops = &vdpu_v2_hw_ops,
+	.dev_ops = &vdpu_v2_dev_ops,
+};
+
+static const struct mpp_dev_var vdpu_px30_data = {
+	.device_type = MPP_DEVICE_VDPU2,
+	.hw_info = &vdpu_v2_hw_info,
+	.trans_info = vdpu_v2_trans,
+	.hw_ops = &vdpu_px30_hw_ops,
+	.dev_ops = &vdpu_px30_dev_ops,
+};
+
+static const struct of_device_id mpp_vdpu2_dt_match[] = {
+	{
+		.compatible = "rockchip,vpu-decoder-v2",
+		.data = &vdpu_v2_data,
+	},
+#ifdef CONFIG_CPU_PX30
+	{
+		.compatible = "rockchip,vpu-decoder-px30",
+		.data = &vdpu_px30_data,
+	},
+#endif
+	{},
+};
+
+static int vdpu_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct vdpu_dev *dec = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+
+	dev_info(dev, "probe device\n");
+	dec = devm_kzalloc(dev, sizeof(struct vdpu_dev), GFP_KERNEL);
+	if (!dec)
+		return -ENOMEM;
+	mpp = &dec->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vdpu2_dt_match,
+				      pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(pdev->dev.of_node, "vdpu");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	if (mpp->var->device_type == MPP_DEVICE_VDPU2) {
+		mpp->srv->sub_devices[MPP_DEVICE_VDPU2_PP] = mpp;
+		set_bit(MPP_DEVICE_VDPU2_PP, &mpp->srv->hw_support);
+	}
+
+	mpp->session_max_buffers = VDPU2_SESSION_MAX_BUFFERS;
+	vdpu_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int vdpu_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	vdpu_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_vdpu2_driver = {
+	.probe = vdpu_probe,
+	.remove = vdpu_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = VDPU2_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_vdpu2_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_vdpu2_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_vepu1.c b/drivers/video/rockchip/mpp/mpp_vepu1.c
new file mode 100644
index 0000000000000..59b4f9ffc5423
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_vepu1.c
@@ -0,0 +1,796 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <linux/nospec.h>
+#include <soc/rockchip/pm_domains.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+
+#define VEPU1_DRIVER_NAME		"mpp_vepu1"
+
+#define	VEPU1_SESSION_MAX_BUFFERS	20
+/* The maximum registers number of all the version */
+#define VEPU1_REG_NUM			164
+#define VEPU1_REG_HW_ID_INDEX		0
+#define VEPU1_REG_START_INDEX		0
+#define VEPU1_REG_END_INDEX		163
+
+#define VEPU1_REG_INT			0x004
+#define VEPU1_REG_INT_INDEX		(1)
+#define VEPU1_INT_SLICE			BIT(8)
+#define VEPU1_INT_TIMEOUT		BIT(6)
+#define VEPU1_INT_BUF_FULL		BIT(5)
+#define VEPU1_INT_RESET			BIT(4)
+#define VEPU1_INT_BUS_ERROR		BIT(3)
+#define VEPU1_INT_RDY			BIT(2)
+#define VEPU1_IRQ_DIS			BIT(1)
+#define VEPU1_INT_RAW			BIT(0)
+
+#define VEPU1_REG_ENC_EN		0x038
+#define VEPU1_REG_ENC_EN_INDEX		(14)
+#define VEPU1_INT_TIMEOUT_EN		BIT(31)
+#define VEPU1_INT_SLICE_EN		BIT(28)
+#define VEPU1_ENC_START			BIT(0)
+
+#define VEPU1_GET_FORMAT(x)		(((x) >> 1) & 0x3)
+#define VEPU1_FORMAT_MASK		(0x06)
+
+#define VEPU1_FMT_RESERVED		(0)
+#define VEPU1_FMT_VP8E			(1)
+#define VEPU1_FMT_JPEGE			(2)
+#define VEPU1_FMT_H264E			(3)
+
+#define VEPU1_REG_CLR_CACHE_BASE	0xc10
+
+#define to_vepu_task(task)		\
+		container_of(task, struct vepu_task, mpp_task)
+#define to_vepu_dev(dev)		\
+		container_of(dev, struct vepu_dev, mpp)
+
+struct vepu_task {
+	struct mpp_task mpp_task;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[VEPU1_REG_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+};
+
+struct vepu_session_priv {
+	struct rw_semaphore rw_sem;
+	/* codec info from user */
+	struct {
+		/* show mode */
+		u32 flag;
+		/* item data */
+		u64 val;
+	} codec_info[ENC_INFO_BUTT];
+};
+
+struct vepu_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+};
+
+static struct mpp_hw_info vepu_v1_hw_info = {
+	.reg_num = VEPU1_REG_NUM,
+	.reg_id = VEPU1_REG_HW_ID_INDEX,
+	.reg_start = VEPU1_REG_START_INDEX,
+	.reg_end = VEPU1_REG_END_INDEX,
+	.reg_en = VEPU1_REG_ENC_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_default[] = {
+	5, 6, 7, 8, 9, 10, 11, 12, 13, 51
+};
+
+static const u16 trans_tbl_vp8e[] = {
+	5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 26, 51, 52, 58, 59, 71
+};
+
+static struct mpp_trans_info trans_rk_vepu1[] = {
+	[VEPU1_FMT_RESERVED] = {
+		.count = 0,
+		.table = NULL,
+	},
+	[VEPU1_FMT_VP8E] = {
+		.count = ARRAY_SIZE(trans_tbl_vp8e),
+		.table = trans_tbl_vp8e,
+	},
+	[VEPU1_FMT_JPEGE] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VEPU1_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+};
+
+static int vepu_process_reg_fd(struct mpp_session *session,
+			       struct vepu_task *task,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret = 0;
+	int fmt = VEPU1_GET_FORMAT(task->reg[VEPU1_REG_ENC_EN_INDEX]);
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					fmt, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+
+	return 0;
+}
+
+static int vepu_extract_task_msg(struct vepu_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *vepu_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct vepu_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = vepu_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = vepu_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static int vepu_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, VEPU1_REG_CLR_CACHE_BASE, 1);
+	/* set registers for hardware */
+	reg_en = mpp_task->hw_info->reg_en;
+	/* First, flush correct encoder format */
+	mpp_write_relaxed(mpp, VEPU1_REG_ENC_EN,
+			  task->reg[reg_en] & VEPU1_FORMAT_MASK);
+	/* Second, flush others register */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		int s = req->offset / sizeof(u32);
+		int e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Last, flush start registers */
+	wmb();
+	mpp_write(mpp, VEPU1_REG_ENC_EN,
+		  task->reg[reg_en] | VEPU1_ENC_START);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vepu_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, VEPU1_REG_INT);
+	if (!(mpp->irq_status & VEPU1_INT_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, VEPU1_REG_INT, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int vepu_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct vepu_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_vepu_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	err_mask = VEPU1_INT_TIMEOUT
+		| VEPU1_INT_BUF_FULL
+		| VEPU1_INT_BUS_ERROR;
+
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+
+	mpp_debug_leave();
+	return IRQ_HANDLED;
+}
+
+static int vepu_finish(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	struct mpp_request *req;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+	/* revert hack for irq status */
+	task->reg[VEPU1_REG_INT_INDEX] = task->irq_status;
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vepu_result(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task,
+		       struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+	return 0;
+}
+
+static int vepu_free_task(struct mpp_session *session,
+			  struct mpp_task *mpp_task)
+{
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+static int vepu_control(struct mpp_session *session, struct mpp_request *req)
+{
+	switch (req->cmd) {
+	case MPP_CMD_SEND_CODEC_INFO: {
+		int i;
+		int cnt;
+		struct codec_info_elem elem;
+		struct vepu_session_priv *priv;
+
+		if (!session || !session->priv) {
+			mpp_err("session info null\n");
+			return -EINVAL;
+		}
+		priv = session->priv;
+
+		cnt = req->size / sizeof(elem);
+		cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+		mpp_debug(DEBUG_IOCTL, "codec info count %d\n", cnt);
+		down_write(&priv->rw_sem);
+		for (i = 0; i < cnt; i++) {
+			if (copy_from_user(&elem, req->data + i * sizeof(elem), sizeof(elem))) {
+				mpp_err("copy_from_user failed\n");
+				continue;
+			}
+			if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT &&
+			    elem.flag > CODEC_INFO_FLAG_NULL && elem.flag < CODEC_INFO_FLAG_BUTT) {
+				elem.type = array_index_nospec(elem.type, ENC_INFO_BUTT);
+				priv->codec_info[elem.type].flag = elem.flag;
+				priv->codec_info[elem.type].val = elem.data;
+			} else {
+				mpp_err("codec info invalid, type %d, flag %d\n",
+					elem.type, elem.flag);
+			}
+		}
+		up_write(&priv->rw_sem);
+	} break;
+	default: {
+		mpp_err("unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+static int vepu_free_session(struct mpp_session *session)
+{
+	if (session && session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	return 0;
+}
+
+static int vepu_init_session(struct mpp_session *session)
+{
+	struct vepu_session_priv *priv;
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	init_rwsem(&priv->rw_sem);
+	session->priv = priv;
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int vepu_procfs_remove(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	if (enc->procfs) {
+		proc_remove(enc->procfs);
+		enc->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int vepu_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	int i;
+	struct vepu_session_priv *priv = session->priv;
+
+	down_read(&priv->rw_sem);
+	/* item name */
+	seq_puts(seq, "------------------------------------------------------");
+	seq_puts(seq, "------------------------------------------------------\n");
+	seq_printf(seq, "|%8s|", (const char *)"session");
+	seq_printf(seq, "%8s|", (const char *)"device");
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		bool show = priv->codec_info[i].flag;
+
+		if (show)
+			seq_printf(seq, "%8s|", enc_info_item_name[i]);
+	}
+	seq_puts(seq, "\n");
+	/* item data*/
+	seq_printf(seq, "|%8d|", session->index);
+	seq_printf(seq, "%8s|", mpp_device_name[session->device_type]);
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		u32 flag = priv->codec_info[i].flag;
+
+		if (!flag)
+			continue;
+		if (flag == CODEC_INFO_FLAG_NUMBER) {
+			u32 data = priv->codec_info[i].val;
+
+			seq_printf(seq, "%8d|", data);
+		} else if (flag == CODEC_INFO_FLAG_STRING) {
+			const char *name = (const char *)&priv->codec_info[i].val;
+
+			seq_printf(seq, "%8s|", name);
+		} else {
+			seq_printf(seq, "%8s|", (const char *)"null");
+		}
+	}
+	seq_puts(seq, "\n");
+	up_read(&priv->rw_sem);
+
+	return 0;
+}
+
+static int vepu_show_session_info(struct seq_file *seq, void *offset)
+{
+	struct mpp_session *session = NULL, *n;
+	struct mpp_dev *mpp = seq->private;
+
+	mutex_lock(&mpp->srv->session_lock);
+	list_for_each_entry_safe(session, n,
+				 &mpp->srv->session_list,
+				 service_link) {
+		if (session->device_type != MPP_DEVICE_VEPU1)
+			continue;
+		if (!session->priv)
+			continue;
+		if (mpp->dev_ops->dump_session)
+			mpp->dev_ops->dump_session(session, seq);
+	}
+	mutex_unlock(&mpp->srv->session_lock);
+
+	return 0;
+}
+
+static int vepu_procfs_init(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	enc->procfs = proc_mkdir(mpp->dev->of_node->name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(enc->procfs)) {
+		mpp_err("failed on open procfs\n");
+		enc->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(enc->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      enc->procfs, &enc->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      enc->procfs, &mpp->session_max_buffers);
+	/* for show session info */
+	proc_create_single_data("sessions-info", 0444,
+				enc->procfs, vepu_show_session_info, mpp);
+
+	return 0;
+}
+#else
+static inline int vepu_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vepu_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vepu_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	return 0;
+}
+#endif
+
+static int vepu_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_VEPU1];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &enc->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&enc->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	enc->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!enc->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	enc->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!enc->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int vepu_clk_on(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_safe_enable(enc->aclk_info.clk);
+	mpp_clk_safe_enable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int vepu_clk_off(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_safe_disable(enc->aclk_info.clk);
+	mpp_clk_safe_disable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int vepu_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_clk_set_rate(&enc->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int vepu_reduce_freq(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_set_rate(&enc->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int vepu_reset(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	if (enc->rst_a && enc->rst_h) {
+		/* Don't skip this or iommu won't work after reset */
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(enc->rst_a);
+		mpp_safe_reset(enc->rst_h);
+		udelay(5);
+		mpp_safe_unreset(enc->rst_a);
+		mpp_safe_unreset(enc->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+	}
+	mpp_write(mpp, VEPU1_REG_ENC_EN, 0);
+
+	return 0;
+}
+
+static struct mpp_hw_ops vepu_v1_hw_ops = {
+	.init = vepu_init,
+	.clk_on = vepu_clk_on,
+	.clk_off = vepu_clk_off,
+	.set_freq = vepu_set_freq,
+	.reduce_freq = vepu_reduce_freq,
+	.reset = vepu_reset,
+};
+
+static struct mpp_dev_ops vepu_v1_dev_ops = {
+	.alloc_task = vepu_alloc_task,
+	.run = vepu_run,
+	.irq = vepu_irq,
+	.isr = vepu_isr,
+	.finish = vepu_finish,
+	.result = vepu_result,
+	.free_task = vepu_free_task,
+	.ioctl = vepu_control,
+	.init_session = vepu_init_session,
+	.free_session = vepu_free_session,
+	.dump_session = vepu_dump_session,
+};
+
+static const struct mpp_dev_var vepu_v1_data = {
+	.device_type = MPP_DEVICE_VEPU1,
+	.hw_info = &vepu_v1_hw_info,
+	.trans_info = trans_rk_vepu1,
+	.hw_ops = &vepu_v1_hw_ops,
+	.dev_ops = &vepu_v1_dev_ops,
+};
+
+static const struct of_device_id mpp_vepu1_dt_match[] = {
+	{
+		.compatible = "rockchip,vpu-encoder-v1",
+		.data = &vepu_v1_data,
+	},
+	{},
+};
+
+static int vepu_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct device *dev = &pdev->dev;
+	struct vepu_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+
+	dev_info(dev, "probe device\n");
+	enc = devm_kzalloc(dev, sizeof(struct vepu_dev), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vepu1_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(pdev->dev.of_node, "vepu");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->session_max_buffers = VEPU1_SESSION_MAX_BUFFERS;
+	vepu_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+	dev_info(dev, "probing finish\n");
+
+	return 0;
+}
+
+static int vepu_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+	dev_info(dev, "remove device\n");
+	mpp_dev_remove(mpp);
+	vepu_procfs_remove(mpp);
+
+	return 0;
+}
+
+struct platform_driver rockchip_vepu1_driver = {
+	.probe = vepu_probe,
+	.remove = vepu_remove,
+	.shutdown = mpp_dev_shutdown,
+	.driver = {
+		.name = VEPU1_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_vepu1_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_vepu1_driver);
diff --git a/drivers/video/rockchip/mpp/mpp_vepu2.c b/drivers/video/rockchip/mpp/mpp_vepu2.c
new file mode 100644
index 0000000000000..d92609ef25eaa
--- /dev/null
+++ b/drivers/video/rockchip/mpp/mpp_vepu2.c
@@ -0,0 +1,1281 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2019 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *	Randy Li, randy.li@rock-chips.com
+ *	Ding Wei, leo.ding@rock-chips.com
+ *
+ */
+#include <asm/cacheflush.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/proc_fs.h>
+#include <linux/nospec.h>
+#include <soc/rockchip/pm_domains.h>
+#include <soc/rockchip/rockchip_iommu.h>
+
+#include "mpp_debug.h"
+#include "mpp_common.h"
+#include "mpp_iommu.h"
+#include "hack/mpp_hack_px30.h"
+
+#define VEPU2_DRIVER_NAME		"mpp_vepu2"
+
+#define	VEPU2_SESSION_MAX_BUFFERS		20
+/* The maximum registers number of all the version */
+#define VEPU2_REG_NUM				184
+#define VEPU2_REG_HW_ID_INDEX		-1 /* INVALID */
+#define VEPU2_REG_START_INDEX			0
+#define VEPU2_REG_END_INDEX			183
+#define VEPU2_REG_OUT_INDEX			(77)
+#define VEPU2_REG_STRM_INDEX			(53)
+
+#define VEPU2_REG_ENC_EN			0x19c
+#define VEPU2_REG_ENC_EN_INDEX			(103)
+#define VEPU2_ENC_START				BIT(0)
+
+#define VEPU2_GET_FORMAT(x)			(((x) >> 4) & 0x3)
+#define VEPU2_FORMAT_MASK			(0x30)
+#define VEPU2_GET_WIDTH(x)			(((x >> 8) & 0x1ff) << 4)
+#define VEPU2_GET_HEIGHT(x)			(((x >> 20) & 0x1ff) << 4)
+
+#define VEPU2_FMT_RESERVED			(0)
+#define VEPU2_FMT_VP8E				(1)
+#define VEPU2_FMT_JPEGE				(2)
+#define VEPU2_FMT_H264E				(3)
+
+#define VEPU2_REG_MB_CTRL			0x1a0
+#define VEPU2_REG_MB_CTRL_INDEX			(104)
+
+#define VEPU2_REG_INT				0x1b4
+#define VEPU2_REG_INT_INDEX			(109)
+#define VEPU2_MV_SAD_WR_EN			BIT(24)
+#define VEPU2_ROCON_WRITE_DIS			BIT(20)
+#define VEPU2_INT_SLICE_EN			BIT(16)
+#define VEPU2_CLOCK_GATE_EN			BIT(12)
+#define VEPU2_INT_TIMEOUT_EN			BIT(10)
+#define VEPU2_INT_CLEAR				BIT(9)
+#define VEPU2_IRQ_DIS				BIT(8)
+#define VEPU2_INT_TIMEOUT			BIT(6)
+#define VEPU2_INT_BUF_FULL			BIT(5)
+#define VEPU2_INT_BUS_ERROR			BIT(4)
+#define VEPU2_INT_SLICE				BIT(2)
+#define VEPU2_INT_RDY				BIT(1)
+#define VEPU2_INT_RAW				BIT(0)
+
+#define RKVPUE2_REG_DMV_4P_1P(i)		(0x1e0 + ((i) << 4))
+#define RKVPUE2_REG_DMV_4P_1P_INDEX(i)		(120 + (i))
+
+#define VEPU2_REG_CLR_CACHE_BASE		0xc10
+
+#define to_vepu_task(task)		\
+		container_of(task, struct vepu_task, mpp_task)
+#define to_vepu_dev(dev)		\
+		container_of(dev, struct vepu_dev, mpp)
+
+struct vepu_task {
+	struct mpp_task mpp_task;
+
+	enum MPP_CLOCK_MODE clk_mode;
+	u32 reg[VEPU2_REG_NUM];
+
+	struct reg_offset_info off_inf;
+	u32 irq_status;
+	/* req for current task */
+	u32 w_req_cnt;
+	struct mpp_request w_reqs[MPP_MAX_MSG_NUM];
+	u32 r_req_cnt;
+	struct mpp_request r_reqs[MPP_MAX_MSG_NUM];
+	/* image info */
+	u32 width;
+	u32 height;
+	u32 pixels;
+	struct mpp_dma_buffer *bs_buf;
+	u32 offset_bs;
+};
+
+struct vepu_session_priv {
+	struct rw_semaphore rw_sem;
+	/* codec info from user */
+	struct {
+		/* show mode */
+		u32 flag;
+		/* item data */
+		u64 val;
+	} codec_info[ENC_INFO_BUTT];
+};
+
+struct vepu_dev {
+	struct mpp_dev mpp;
+
+	struct mpp_clk_info aclk_info;
+	struct mpp_clk_info hclk_info;
+	u32 default_max_load;
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+	struct proc_dir_entry *procfs;
+#endif
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	/* for ccu(central control unit) */
+	struct vepu_ccu *ccu;
+	bool disable_work;
+};
+
+struct vepu_ccu {
+	u32 core_num;
+	/* lock for core attach */
+	spinlock_t lock;
+	struct mpp_dev *main_core;
+	struct mpp_dev *cores[MPP_MAX_CORE_NUM];
+	unsigned long core_idle;
+};
+
+static struct mpp_hw_info vepu_v2_hw_info = {
+	.reg_num = VEPU2_REG_NUM,
+	.reg_id = VEPU2_REG_HW_ID_INDEX,
+	.reg_start = VEPU2_REG_START_INDEX,
+	.reg_end = VEPU2_REG_END_INDEX,
+	.reg_en = VEPU2_REG_ENC_EN_INDEX,
+};
+
+/*
+ * file handle translate information
+ */
+static const u16 trans_tbl_default[] = {
+	48, 49, 50, 56, 57, 63, 64, 77, 78, 81
+};
+
+static const u16 trans_tbl_vp8e[] = {
+	27, 44, 45, 48, 49, 50, 56, 57, 63, 64,
+	76, 77, 78, 80, 81, 106, 108,
+};
+
+static struct mpp_trans_info trans_rk_vepu2[] = {
+	[VEPU2_FMT_RESERVED] = {
+		.count = 0,
+		.table = NULL,
+	},
+	[VEPU2_FMT_VP8E] = {
+		.count = ARRAY_SIZE(trans_tbl_vp8e),
+		.table = trans_tbl_vp8e,
+	},
+	[VEPU2_FMT_JPEGE] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+	[VEPU2_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_default),
+		.table = trans_tbl_default,
+	},
+};
+
+static int vepu_process_reg_fd(struct mpp_session *session,
+			       struct vepu_task *task,
+			       struct mpp_task_msgs *msgs)
+{
+	int ret;
+	int fd_bs;
+	int fmt = VEPU2_GET_FORMAT(task->reg[VEPU2_REG_ENC_EN_INDEX]);
+
+	if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET)
+		fd_bs = task->reg[VEPU2_REG_OUT_INDEX];
+	else
+		fd_bs = task->reg[VEPU2_REG_OUT_INDEX] & 0x3ff;
+
+	ret = mpp_translate_reg_address(session, &task->mpp_task,
+					fmt, task->reg, &task->off_inf);
+	if (ret)
+		return ret;
+
+	mpp_translate_reg_offset_info(&task->mpp_task,
+				      &task->off_inf, task->reg);
+
+	if (fmt == VEPU2_FMT_JPEGE) {
+		struct mpp_dma_buffer *bs_buf = mpp_dma_find_buffer_fd(session->dma, fd_bs);
+
+		task->offset_bs = mpp_query_reg_offset_info(&task->off_inf, VEPU2_REG_OUT_INDEX);
+		if (bs_buf && task->offset_bs > 0)
+			mpp_dma_buf_sync(bs_buf, 0, task->offset_bs, DMA_TO_DEVICE, false);
+		task->bs_buf = bs_buf;
+	}
+
+	return 0;
+}
+
+static int vepu_extract_task_msg(struct vepu_task *task,
+				 struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	int ret;
+	struct mpp_request *req;
+	struct mpp_hw_info *hw_info = task->mpp_task.hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		u32 off_s, off_e;
+
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			if (copy_from_user((u8 *)task->reg + req->offset,
+					   req->data, req->size)) {
+				mpp_err("copy_from_user reg failed\n");
+				return -EIO;
+			}
+			memcpy(&task->w_reqs[task->w_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			off_s = hw_info->reg_start * sizeof(u32);
+			off_e = hw_info->reg_end * sizeof(u32);
+			ret = mpp_check_req(req, 0, sizeof(task->reg),
+					    off_s, off_e);
+			if (ret)
+				continue;
+			memcpy(&task->r_reqs[task->r_req_cnt++],
+			       req, sizeof(*req));
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			mpp_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+	mpp_debug(DEBUG_TASK_INFO, "w_req_cnt %d, r_req_cnt %d\n",
+		  task->w_req_cnt, task->r_req_cnt);
+
+	return 0;
+}
+
+static void *vepu_alloc_task(struct mpp_session *session,
+			     struct mpp_task_msgs *msgs)
+{
+	int ret;
+	struct mpp_task *mpp_task = NULL;
+	struct vepu_task *task = NULL;
+	struct mpp_dev *mpp = session->mpp;
+
+	mpp_debug_enter();
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	mpp_task_init(session, mpp_task);
+	mpp_task->hw_info = mpp->var->hw_info;
+	mpp_task->reg = task->reg;
+	/* extract reqs for current task */
+	ret = vepu_extract_task_msg(task, msgs);
+	if (ret)
+		goto fail;
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		ret = vepu_process_reg_fd(session, task, msgs);
+		if (ret)
+			goto fail;
+	}
+	task->clk_mode = CLK_MODE_NORMAL;
+	/* get resolution info */
+	task->width = VEPU2_GET_WIDTH(task->reg[VEPU2_REG_ENC_EN_INDEX]);
+	task->height = VEPU2_GET_HEIGHT(task->reg[VEPU2_REG_ENC_EN_INDEX]);
+	task->pixels = task->width * task->height;
+	mpp_debug(DEBUG_TASK_INFO, "width=%d, height=%d\n", task->width, task->height);
+
+	mpp_debug_leave();
+
+	return mpp_task;
+
+fail:
+	mpp_task_dump_mem_region(mpp, mpp_task);
+	mpp_task_dump_reg(mpp, mpp_task);
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+	return NULL;
+}
+
+static void *vepu_prepare(struct mpp_dev *mpp, struct mpp_task *mpp_task)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_ccu *ccu = enc->ccu;
+	unsigned long core_idle;
+	unsigned long flags;
+	s32 core_id;
+	u32 i;
+
+	spin_lock_irqsave(&ccu->lock, flags);
+
+	core_idle = ccu->core_idle;
+
+	for (i = 0; i < ccu->core_num; i++) {
+		struct mpp_dev *mpp = ccu->cores[i];
+
+		if (mpp && mpp->disable)
+			clear_bit(mpp->core_id, &core_idle);
+	}
+
+	core_id = find_first_bit(&core_idle, ccu->core_num);
+	if (core_id >= ARRAY_SIZE(ccu->cores)) {
+		mpp_task = NULL;
+		mpp_dbg_core("core %d all busy %lx\n", core_id, ccu->core_idle);
+		goto done;
+	}
+
+	core_id = array_index_nospec(core_id, MPP_MAX_CORE_NUM);
+	clear_bit(core_id, &ccu->core_idle);
+	mpp_task->mpp = ccu->cores[core_id];
+	mpp_task->core_id = core_id;
+
+	mpp_dbg_core("core cnt %d core %d set idle %lx -> %lx\n",
+		     ccu->core_num, core_id, core_idle, ccu->core_idle);
+
+done:
+	spin_unlock_irqrestore(&ccu->lock, flags);
+
+	return mpp_task;
+}
+
+static int vepu_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 reg_en;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+	u32 timing_en = mpp->srv->timing_en;
+
+	mpp_debug_enter();
+
+	/* clear cache */
+	mpp_write_relaxed(mpp, VEPU2_REG_CLR_CACHE_BASE, 1);
+
+	reg_en = mpp_task->hw_info->reg_en;
+	/* First, flush correct encoder format */
+	mpp_write_relaxed(mpp, VEPU2_REG_ENC_EN,
+			  task->reg[reg_en] & VEPU2_FORMAT_MASK);
+	/* Second, flush others register */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		struct mpp_request *req = &task->w_reqs[i];
+		int s = req->offset / sizeof(u32);
+		int e = s + req->size / sizeof(u32);
+
+		mpp_write_req(mpp, task->reg, s, e, reg_en);
+	}
+
+	/* flush tlb before starting hardware */
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+
+	/* init current task */
+	mpp->cur_task = mpp_task;
+
+	mpp_task_run_begin(mpp_task, timing_en, MPP_WORK_TIMEOUT_DELAY);
+
+	/* Last, flush the registers */
+	wmb();
+	mpp_write(mpp, VEPU2_REG_ENC_EN,
+		  task->reg[reg_en] | VEPU2_ENC_START);
+
+	mpp_task_run_end(mpp_task, timing_en);
+
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vepu_px30_run(struct mpp_dev *mpp,
+		    struct mpp_task *mpp_task)
+{
+	mpp_iommu_flush_tlb(mpp->iommu_info);
+	return vepu_run(mpp, mpp_task);
+}
+
+static int vepu_irq(struct mpp_dev *mpp)
+{
+	mpp->irq_status = mpp_read(mpp, VEPU2_REG_INT);
+	if (!(mpp->irq_status & VEPU2_INT_RAW))
+		return IRQ_NONE;
+
+	mpp_write(mpp, VEPU2_REG_INT, 0);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int vepu_isr(struct mpp_dev *mpp)
+{
+	u32 err_mask;
+	struct vepu_task *task = NULL;
+	struct mpp_task *mpp_task = mpp->cur_task;
+	unsigned long core_idle;
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_ccu *ccu = enc->ccu;
+
+	/* FIXME use a spin lock here */
+	if (!mpp_task) {
+		dev_err(mpp->dev, "no current task\n");
+		return IRQ_HANDLED;
+	}
+	mpp_time_diff(mpp_task);
+	mpp->cur_task = NULL;
+	task = to_vepu_task(mpp_task);
+	task->irq_status = mpp->irq_status;
+	mpp_debug(DEBUG_IRQ_STATUS, "irq_status: %08x\n",
+		  task->irq_status);
+
+	err_mask = VEPU2_INT_TIMEOUT
+		| VEPU2_INT_BUF_FULL
+		| VEPU2_INT_BUS_ERROR;
+
+	if (err_mask & task->irq_status)
+		atomic_inc(&mpp->reset_request);
+
+	mpp_task_finish(mpp_task->session, mpp_task);
+	/* the whole vepu has no ccu that manage multi core */
+	if (ccu) {
+		core_idle = ccu->core_idle;
+		set_bit(mpp->core_id, &ccu->core_idle);
+
+		mpp_dbg_core("core %d isr idle %lx -> %lx\n", mpp->core_id, core_idle,
+			ccu->core_idle);
+	}
+
+	mpp_debug_leave();
+
+	return IRQ_HANDLED;
+}
+
+static int vepu_finish(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task)
+{
+	u32 i;
+	u32 s, e;
+	struct mpp_request *req;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_debug_enter();
+
+	/* read register after running */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+		s = req->offset / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		mpp_read_req(mpp, task->reg, s, e);
+	}
+	/* revert hack for irq status */
+	task->reg[VEPU2_REG_INT_INDEX] = task->irq_status;
+
+	if (task->bs_buf)
+		mpp_dma_buf_sync(task->bs_buf, 0,
+				 task->reg[VEPU2_REG_STRM_INDEX] / 8 +
+				 task->offset_bs,
+				 DMA_FROM_DEVICE, true);
+	mpp_debug_leave();
+
+	return 0;
+}
+
+static int vepu_result(struct mpp_dev *mpp,
+		       struct mpp_task *mpp_task,
+		       struct mpp_task_msgs *msgs)
+{
+	u32 i;
+	struct mpp_request *req;
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	/* FIXME may overflow the kernel */
+	for (i = 0; i < task->r_req_cnt; i++) {
+		req = &task->r_reqs[i];
+
+		if (copy_to_user(req->data,
+				 (u8 *)task->reg + req->offset,
+				 req->size)) {
+			mpp_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int vepu_free_task(struct mpp_session *session,
+			  struct mpp_task *mpp_task)
+{
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_task_finalize(session, mpp_task);
+	kfree(task);
+
+	return 0;
+}
+
+static int vepu_control(struct mpp_session *session, struct mpp_request *req)
+{
+	switch (req->cmd) {
+	case MPP_CMD_SEND_CODEC_INFO: {
+		int i;
+		int cnt;
+		struct codec_info_elem elem;
+		struct vepu_session_priv *priv;
+
+		if (!session || !session->priv) {
+			mpp_err("session info null\n");
+			return -EINVAL;
+		}
+		priv = session->priv;
+
+		cnt = req->size / sizeof(elem);
+		cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+		mpp_debug(DEBUG_IOCTL, "codec info count %d\n", cnt);
+		for (i = 0; i < cnt; i++) {
+			if (copy_from_user(&elem, req->data + i * sizeof(elem), sizeof(elem))) {
+				mpp_err("copy_from_user failed\n");
+				continue;
+			}
+			if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT &&
+			    elem.flag > CODEC_INFO_FLAG_NULL && elem.flag < CODEC_INFO_FLAG_BUTT) {
+				elem.type = array_index_nospec(elem.type, ENC_INFO_BUTT);
+				priv->codec_info[elem.type].flag = elem.flag;
+				priv->codec_info[elem.type].val = elem.data;
+			} else {
+				mpp_err("codec info invalid, type %d, flag %d\n",
+					elem.type, elem.flag);
+			}
+		}
+	} break;
+	default: {
+		mpp_err("unknown mpp ioctl cmd %x\n", req->cmd);
+	} break;
+	}
+
+	return 0;
+}
+
+static int vepu_free_session(struct mpp_session *session)
+{
+	if (session && session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	return 0;
+}
+
+static int vepu_init_session(struct mpp_session *session)
+{
+	struct vepu_session_priv *priv;
+
+	if (!session) {
+		mpp_err("session is null\n");
+		return -EINVAL;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	init_rwsem(&priv->rw_sem);
+	session->priv = priv;
+
+	return 0;
+}
+
+#ifdef CONFIG_ROCKCHIP_MPP_PROC_FS
+static int vepu_procfs_remove(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	if (enc->procfs) {
+		proc_remove(enc->procfs);
+		enc->procfs = NULL;
+	}
+
+	return 0;
+}
+
+static int vepu_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	int i;
+	struct vepu_session_priv *priv = session->priv;
+
+	down_read(&priv->rw_sem);
+	/* item name */
+	seq_puts(seq, "------------------------------------------------------");
+	seq_puts(seq, "------------------------------------------------------\n");
+	seq_printf(seq, "|%8s|", (const char *)"session");
+	seq_printf(seq, "%8s|", (const char *)"device");
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		bool show = priv->codec_info[i].flag;
+
+		if (show)
+			seq_printf(seq, "%8s|", enc_info_item_name[i]);
+	}
+	seq_puts(seq, "\n");
+	/* item data*/
+	seq_printf(seq, "|%8d|", session->index);
+	seq_printf(seq, "%8s|", mpp_device_name[session->device_type]);
+	for (i = ENC_INFO_BASE; i < ENC_INFO_BUTT; i++) {
+		u32 flag = priv->codec_info[i].flag;
+
+		if (!flag)
+			continue;
+		if (flag == CODEC_INFO_FLAG_NUMBER) {
+			u32 data = priv->codec_info[i].val;
+
+			seq_printf(seq, "%8d|", data);
+		} else if (flag == CODEC_INFO_FLAG_STRING) {
+			const char *name = (const char *)&priv->codec_info[i].val;
+
+			seq_printf(seq, "%8s|", name);
+		} else {
+			seq_printf(seq, "%8s|", (const char *)"null");
+		}
+	}
+	seq_puts(seq, "\n");
+	up_read(&priv->rw_sem);
+
+	return 0;
+}
+
+static int vepu_show_session_info(struct seq_file *seq, void *offset)
+{
+	struct mpp_session *session = NULL, *n;
+	struct mpp_dev *mpp = seq->private;
+
+	mutex_lock(&mpp->srv->session_lock);
+	list_for_each_entry_safe(session, n,
+				 &mpp->srv->session_list,
+				 service_link) {
+		if (session->device_type != MPP_DEVICE_VEPU2 &&
+		    session->device_type != MPP_DEVICE_VEPU2_JPEG)
+			continue;
+		if (!session->priv)
+			continue;
+		if (mpp->dev_ops->dump_session)
+			mpp->dev_ops->dump_session(session, seq);
+	}
+	mutex_unlock(&mpp->srv->session_lock);
+
+	return 0;
+}
+
+static int vepu_procfs_init(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	char name[32];
+
+	if (!mpp->dev || !mpp->dev->of_node || !mpp->dev->of_node->name ||
+	    !mpp->srv || !mpp->srv->procfs)
+		return -EINVAL;
+	if (enc->ccu)
+		snprintf(name, sizeof(name) - 1, "%s%d",
+			mpp->dev->of_node->name, mpp->core_id);
+	else
+		snprintf(name, sizeof(name) - 1, "%s",
+			mpp->dev->of_node->name);
+
+	enc->procfs = proc_mkdir(name, mpp->srv->procfs);
+	if (IS_ERR_OR_NULL(enc->procfs)) {
+		mpp_err("failed on open procfs\n");
+		enc->procfs = NULL;
+		return -EIO;
+	}
+
+	/* for common mpp_dev options */
+	mpp_procfs_create_common(enc->procfs, mpp);
+
+	mpp_procfs_create_u32("aclk", 0644,
+			      enc->procfs, &enc->aclk_info.debug_rate_hz);
+	mpp_procfs_create_u32("session_buffers", 0644,
+			      enc->procfs, &mpp->session_max_buffers);
+	/* for show session info */
+	proc_create_single_data("sessions-info", 0444,
+				enc->procfs, vepu_show_session_info, mpp);
+
+	return 0;
+}
+
+static int vepu_procfs_ccu_init(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	if (!enc->procfs)
+		goto done;
+
+done:
+	return 0;
+}
+#else
+static inline int vepu_procfs_remove(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vepu_procfs_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vepu_procfs_ccu_init(struct mpp_dev *mpp)
+{
+	return 0;
+}
+
+static inline int vepu_dump_session(struct mpp_session *session, struct seq_file *seq)
+{
+	return 0;
+}
+#endif
+
+static int vepu_init(struct mpp_dev *mpp)
+{
+	int ret;
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp->grf_info = &mpp->srv->grf_infos[MPP_DRIVER_VEPU2];
+
+	/* Get clock info from dtsi */
+	ret = mpp_get_clk_info(mpp, &enc->aclk_info, "aclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get aclk_vcodec\n");
+	ret = mpp_get_clk_info(mpp, &enc->hclk_info, "hclk_vcodec");
+	if (ret)
+		mpp_err("failed on clk_get hclk_vcodec\n");
+	/* Get normal max workload from dtsi */
+	of_property_read_u32(mpp->dev->of_node,
+			     "rockchip,default-max-load", &enc->default_max_load);
+	/* Set default rates */
+	mpp_set_clk_info_rate_hz(&enc->aclk_info, CLK_MODE_DEFAULT, 300 * MHZ);
+
+	/* Get reset control from dtsi */
+	enc->rst_a = mpp_reset_control_get(mpp, RST_TYPE_A, "video_a");
+	if (!enc->rst_a)
+		mpp_err("No aclk reset resource define\n");
+	enc->rst_h = mpp_reset_control_get(mpp, RST_TYPE_H, "video_h");
+	if (!enc->rst_h)
+		mpp_err("No hclk reset resource define\n");
+
+	return 0;
+}
+
+static int vepu_px30_init(struct mpp_dev *mpp)
+{
+	vepu_init(mpp);
+	return px30_workaround_combo_init(mpp);
+}
+
+static int vepu_clk_on(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_safe_enable(enc->aclk_info.clk);
+	mpp_clk_safe_enable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int vepu_clk_off(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_safe_disable(enc->aclk_info.clk);
+	mpp_clk_safe_disable(enc->hclk_info.clk);
+
+	return 0;
+}
+
+static int vepu_get_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	u32 task_cnt;
+	u32 workload;
+	struct mpp_task *loop = NULL, *n;
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	/* if not set max load, consider not have advanced mode */
+	if (!enc->default_max_load)
+		return 0;
+
+	task_cnt = 1;
+	workload = task->pixels;
+	/* calc workload in pending list */
+	mutex_lock(&mpp->queue->pending_lock);
+	list_for_each_entry_safe(loop, n,
+				 &mpp->queue->pending_list,
+				 queue_link) {
+		struct vepu_task *loop_task = to_vepu_task(loop);
+
+		task_cnt++;
+		workload += loop_task->pixels;
+	}
+	mutex_unlock(&mpp->queue->pending_lock);
+
+	if (workload > enc->default_max_load)
+		task->clk_mode = CLK_MODE_ADVANCED;
+
+	mpp_debug(DEBUG_TASK_INFO, "pending task %d, workload %d, clk_mode=%d\n",
+		  task_cnt, workload, task->clk_mode);
+
+	return 0;
+}
+
+static int vepu_set_freq(struct mpp_dev *mpp,
+			 struct mpp_task *mpp_task)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_task *task = to_vepu_task(mpp_task);
+
+	mpp_clk_set_rate(&enc->aclk_info, task->clk_mode);
+
+	return 0;
+}
+
+static int vepu_reduce_freq(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+
+	mpp_clk_set_rate(&enc->aclk_info, CLK_MODE_REDUCE);
+
+	return 0;
+}
+
+static int vepu_reset(struct mpp_dev *mpp)
+{
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_ccu *ccu = enc->ccu;
+
+	mpp_write(mpp, VEPU2_REG_ENC_EN, 0);
+	udelay(5);
+	if (enc->rst_a && enc->rst_h) {
+		/* Don't skip this or iommu won't work after reset */
+		mpp_pmu_idle_request(mpp, true);
+		mpp_safe_reset(enc->rst_a);
+		mpp_safe_reset(enc->rst_h);
+		udelay(5);
+		mpp_safe_unreset(enc->rst_a);
+		mpp_safe_unreset(enc->rst_h);
+		mpp_pmu_idle_request(mpp, false);
+	}
+	mpp_write(mpp, VEPU2_REG_INT, VEPU2_INT_CLEAR);
+
+	if (ccu) {
+		set_bit(mpp->core_id, &ccu->core_idle);
+		mpp_dbg_core("core %d reset idle %lx\n", mpp->core_id, ccu->core_idle);
+	}
+
+	return 0;
+}
+
+static int vepu2_iommu_fault_handle(struct iommu_domain *iommu, struct device *iommu_dev,
+				    unsigned long iova, int status, void *arg)
+{
+	struct mpp_dev *mpp = (struct mpp_dev *)arg;
+	struct mpp_task *mpp_task;
+	struct vepu_dev *enc = to_vepu_dev(mpp);
+	struct vepu_ccu *ccu = enc->ccu;
+
+	if (ccu) {
+		int i;
+		struct mpp_dev *core;
+
+		for (i = 0; i < ccu->core_num; i++) {
+			core = ccu->cores[i];
+			if (core->iommu_info && (&core->iommu_info->pdev->dev == iommu_dev)) {
+				mpp = core;
+				break;
+			}
+		}
+	}
+
+	/*
+	 * Mask iommu irq, in order for iommu not repeatedly trigger pagefault.
+	 * Until the pagefault task finish by hw timeout.
+	 */
+	if (mpp)
+		rockchip_iommu_mask_irq(mpp->dev);
+
+	dev_err(iommu_dev, "fault addr 0x%08lx status %x arg %p\n",
+		iova, status, arg);
+
+	if (!mpp) {
+		dev_err(iommu_dev, "pagefault without device to handle\n");
+		return 0;
+	}
+	mpp_task = mpp->cur_task;
+	if (mpp_task)
+		mpp_task_dump_mem_region(mpp, mpp_task);
+
+	mpp_task_dump_hw_reg(mpp);
+
+	return 0;
+}
+
+static struct mpp_hw_ops vepu_v2_hw_ops = {
+	.init = vepu_init,
+	.clk_on = vepu_clk_on,
+	.clk_off = vepu_clk_off,
+	.get_freq = vepu_get_freq,
+	.set_freq = vepu_set_freq,
+	.reduce_freq = vepu_reduce_freq,
+	.reset = vepu_reset,
+};
+
+static struct mpp_hw_ops vepu_px30_hw_ops = {
+	.init = vepu_px30_init,
+	.clk_on = vepu_clk_on,
+	.clk_off = vepu_clk_off,
+	.set_freq = vepu_set_freq,
+	.reduce_freq = vepu_reduce_freq,
+	.reset = vepu_reset,
+	.set_grf = px30_workaround_combo_switch_grf,
+};
+
+static struct mpp_dev_ops vepu_v2_dev_ops = {
+	.alloc_task = vepu_alloc_task,
+	.run = vepu_run,
+	.irq = vepu_irq,
+	.isr = vepu_isr,
+	.finish = vepu_finish,
+	.result = vepu_result,
+	.free_task = vepu_free_task,
+	.ioctl = vepu_control,
+	.init_session = vepu_init_session,
+	.free_session = vepu_free_session,
+	.dump_session = vepu_dump_session,
+};
+
+static struct mpp_dev_ops vepu_px30_dev_ops = {
+	.alloc_task = vepu_alloc_task,
+	.run = vepu_px30_run,
+	.irq = vepu_irq,
+	.isr = vepu_isr,
+	.finish = vepu_finish,
+	.result = vepu_result,
+	.free_task = vepu_free_task,
+	.ioctl = vepu_control,
+	.init_session = vepu_init_session,
+	.free_session = vepu_free_session,
+	.dump_session = vepu_dump_session,
+};
+
+static struct mpp_dev_ops vepu_ccu_dev_ops = {
+	.alloc_task = vepu_alloc_task,
+	.prepare = vepu_prepare,
+	.run = vepu_run,
+	.irq = vepu_irq,
+	.isr = vepu_isr,
+	.finish = vepu_finish,
+	.result = vepu_result,
+	.free_task = vepu_free_task,
+	.ioctl = vepu_control,
+	.init_session = vepu_init_session,
+	.free_session = vepu_free_session,
+	.dump_session = vepu_dump_session,
+};
+
+
+static const struct mpp_dev_var vepu_v2_data = {
+	.device_type = MPP_DEVICE_VEPU2,
+	.hw_info = &vepu_v2_hw_info,
+	.trans_info = trans_rk_vepu2,
+	.hw_ops = &vepu_v2_hw_ops,
+	.dev_ops = &vepu_v2_dev_ops,
+};
+
+static const struct mpp_dev_var vepu_px30_data = {
+	.device_type = MPP_DEVICE_VEPU2,
+	.hw_info = &vepu_v2_hw_info,
+	.trans_info = trans_rk_vepu2,
+	.hw_ops = &vepu_px30_hw_ops,
+	.dev_ops = &vepu_px30_dev_ops,
+};
+
+static const struct mpp_dev_var vepu_ccu_data = {
+	.device_type = MPP_DEVICE_VEPU2_JPEG,
+	.hw_info = &vepu_v2_hw_info,
+	.trans_info = trans_rk_vepu2,
+	.hw_ops = &vepu_v2_hw_ops,
+	.dev_ops = &vepu_ccu_dev_ops,
+};
+
+static const struct of_device_id mpp_vepu2_dt_match[] = {
+	{
+		.compatible = "rockchip,vpu-encoder-v2",
+		.data = &vepu_v2_data,
+	},
+#ifdef CONFIG_CPU_PX30
+	{
+		.compatible = "rockchip,vpu-encoder-px30",
+		.data = &vepu_px30_data,
+	},
+#endif
+#ifdef CONFIG_CPU_RK3588
+	{
+		.compatible = "rockchip,vpu-jpege-core",
+		.data = &vepu_ccu_data,
+	},
+	{
+		.compatible = "rockchip,vpu-jpege-ccu",
+	},
+#endif
+	{},
+};
+
+static int vepu_ccu_probe(struct platform_device *pdev)
+{
+	struct vepu_ccu *ccu;
+	struct device *dev = &pdev->dev;
+
+	ccu = devm_kzalloc(dev, sizeof(*ccu), GFP_KERNEL);
+	if (!ccu)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ccu);
+	spin_lock_init(&ccu->lock);
+	return 0;
+}
+
+static int vepu_attach_ccu(struct device *dev, struct vepu_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct vepu_ccu *ccu;
+	unsigned long flags;
+
+	np = of_parse_phandle(dev->of_node, "rockchip,ccu", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	ccu = platform_get_drvdata(pdev);
+	if (!ccu)
+		return -ENOMEM;
+
+	spin_lock_irqsave(&ccu->lock, flags);
+	ccu->core_num++;
+	ccu->cores[enc->mpp.core_id] = &enc->mpp;
+	set_bit(enc->mpp.core_id, &ccu->core_idle);
+	spin_unlock_irqrestore(&ccu->lock, flags);
+
+	/* attach the ccu-domain to current core */
+	if (!ccu->main_core) {
+		/**
+		 * set the first device for the main-core,
+		 * then the domain of the main-core named ccu-domain
+		 */
+		ccu->main_core = &enc->mpp;
+	} else {
+		struct mpp_iommu_info *ccu_info, *cur_info;
+
+		/* set the ccu domain for current device */
+		ccu_info = ccu->main_core->iommu_info;
+		cur_info = enc->mpp.iommu_info;
+
+		if (cur_info)
+			cur_info->domain = ccu_info->domain;
+		mpp_iommu_attach(cur_info);
+	}
+	enc->ccu = ccu;
+
+	dev_info(dev, "attach ccu success\n");
+	return 0;
+}
+
+static int vepu_core_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct vepu_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	enc = devm_kzalloc(dev, sizeof(struct vepu_dev), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vepu2_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(pdev->dev.of_node, "jpege");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+	/* current device attach to ccu */
+	ret = vepu_attach_ccu(dev, enc);
+	if (ret)
+		return ret;
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->fault_handler = vepu2_iommu_fault_handle;
+	mpp->session_max_buffers = VEPU2_SESSION_MAX_BUFFERS;
+	vepu_procfs_init(mpp);
+	vepu_procfs_ccu_init(mpp);
+	/* if current is main-core, register current device to mpp service */
+	if (mpp == enc->ccu->main_core)
+		mpp_dev_register_srv(mpp, mpp->srv);
+
+	return 0;
+}
+
+static int vepu_probe_default(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct vepu_dev *enc = NULL;
+	struct mpp_dev *mpp = NULL;
+	const struct of_device_id *match = NULL;
+	int ret = 0;
+
+	enc = devm_kzalloc(dev, sizeof(struct vepu_dev), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	mpp = &enc->mpp;
+	platform_set_drvdata(pdev, mpp);
+
+	if (pdev->dev.of_node) {
+		match = of_match_node(mpp_vepu2_dt_match, pdev->dev.of_node);
+		if (match)
+			mpp->var = (struct mpp_dev_var *)match->data;
+
+		mpp->core_id = of_alias_get_id(pdev->dev.of_node, "vepu");
+	}
+
+	ret = mpp_dev_probe(mpp, pdev);
+	if (ret) {
+		dev_err(dev, "probe sub driver failed\n");
+		return -EINVAL;
+	}
+
+	ret = devm_request_threaded_irq(dev, mpp->irq,
+					mpp_dev_irq,
+					NULL,
+					IRQF_SHARED,
+					dev_name(dev), mpp);
+	if (ret) {
+		dev_err(dev, "register interrupter runtime failed\n");
+		return -EINVAL;
+	}
+
+	mpp->fault_handler = vepu2_iommu_fault_handle;
+	mpp->session_max_buffers = VEPU2_SESSION_MAX_BUFFERS;
+	vepu_procfs_init(mpp);
+	/* register current device to mpp service */
+	mpp_dev_register_srv(mpp, mpp->srv);
+
+	return 0;
+}
+
+static int vepu_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "probing start\n");
+
+	if (strstr(np->name, "ccu"))
+		ret = vepu_ccu_probe(pdev);
+	else if (strstr(np->name, "core"))
+		ret = vepu_core_probe(pdev);
+	else
+		ret = vepu_probe_default(pdev);
+
+	dev_info(dev, "probing finish\n");
+
+	return ret;
+}
+
+static int vepu_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	if (strstr(np->name, "ccu")) {
+		dev_info(dev, "remove ccu device\n");
+	} else if (strstr(np->name, "core")) {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+		struct vepu_dev *enc = to_vepu_dev(mpp);
+
+		dev_info(dev, "remove core\n");
+		if (enc->ccu) {
+			s32 core_id = mpp->core_id;
+			struct vepu_ccu *ccu = enc->ccu;
+			unsigned long flags;
+
+			spin_lock_irqsave(&ccu->lock, flags);
+			ccu->core_num--;
+			ccu->cores[core_id] = NULL;
+			clear_bit(core_id, &ccu->core_idle);
+			spin_unlock_irqrestore(&ccu->lock, flags);
+		}
+		mpp_dev_remove(&enc->mpp);
+		vepu_procfs_remove(&enc->mpp);
+	} else {
+		struct mpp_dev *mpp = dev_get_drvdata(dev);
+
+		dev_info(dev, "remove device\n");
+		mpp_dev_remove(mpp);
+		vepu_procfs_remove(mpp);
+	}
+
+	return 0;
+}
+
+static void vepu_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	if (!strstr(dev_name(dev), "ccu"))
+		mpp_dev_shutdown(pdev);
+}
+
+struct platform_driver rockchip_vepu2_driver = {
+	.probe = vepu_probe,
+	.remove = vepu_remove,
+	.shutdown = vepu_shutdown,
+	.driver = {
+		.name = VEPU2_DRIVER_NAME,
+		.of_match_table = of_match_ptr(mpp_vepu2_dt_match),
+		.pm = &mpp_common_pm_ops,
+	},
+};
+EXPORT_SYMBOL(rockchip_vepu2_driver);
diff --git a/drivers/video/rockchip/mpp/rockchip_iep2_regs.h b/drivers/video/rockchip/mpp/rockchip_iep2_regs.h
new file mode 100644
index 0000000000000..cacb38d864fa1
--- /dev/null
+++ b/drivers/video/rockchip/mpp/rockchip_iep2_regs.h
@@ -0,0 +1,184 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2020 Fuzhou Rockchip Electronics Co., Ltd
+ *
+ * author:
+ *	Alpha Lin, alpha.lin@rock-chips.com
+ *
+ */
+
+#ifndef __ROCKCHIP_IEP2_REGS_H__
+#define __ROCKCHIP_IEP2_REGS_H__
+
+#define IEP2_REG_FRM_START			0x0000
+#define     IEP2_REG_FRM_EN                             BIT(0)
+#define IEP2_REG_IEP_CONFIG0			0x0004
+#define     IEP2_REG_CCLK_SRESET_P			BIT(22)
+#define     IEP2_REG_ACLK_SRESET_P			BIT(21)
+#define     IEP2_REG_HANDSAVE_P				BIT(20)
+#define     IEP2_REG_RST_PROTECT_EN			BIT(19)
+#define     IEP2_REG_DEBUG_DATA_EN			BIT(16)
+#define     IEP2_REG_DST_YUV_SWAP(x)			(((x) & 3) << 12)
+#define     IEP2_REG_DST_FMT(x)				(((x) & 3) << 8)
+#define     IEP2_REG_SRC_YUV_SWAP(x)			(((x) & 3) << 4)
+#define     IEP2_REG_SRC_FMT(x)				((x) & 3)
+#define IEP2_REG_WORK_MODE			0x0008
+#define     IEP2_REG_IEP2_MODE				BIT(0)
+#define     IEP2_GET_IEP2_MODE(x)			((x) & 0x3)
+#define IEP2_REG_GATING_CTRL			0x0010
+#define     IEP2_REG_REG_CLK_ON				BIT(11)
+#define     IEP2_REG_DMA_CLK_ON				BIT(10)
+#define     IEP2_REG_RAM_CLK_ON				BIT(9)
+#define     IEP2_REG_CTRL_CLK_ON			BIT(8)
+#define     IEP2_REG_OUT_CLK_ON				BIT(7)
+#define     IEP2_REG_BLE_CLK_ON				BIT(6)
+#define     IEP2_REG_EEDI_CLK_ON			BIT(5)
+#define     IEP2_REG_MC_CLK_ON				BIT(4)
+#define     IEP2_REG_ME_CLK_ON				BIT(3)
+#define     IEP2_REG_DECT_CLK_ON			BIT(2)
+#define     IEP2_REG_MD_CLK_ON				BIT(1)
+#define     IEP2_REG_CLK_ON				BIT(0)
+#define IEP2_REG_STATUS				0x0014
+#define     IEP2_REG_ARST_FINISH_DONE                   BIT(0)
+#define IEP2_REG_INT_EN				0x0020
+#define     IEP2_REG_TIMEOUT_EN			        BIT(5)
+#define     IEP2_REG_BUS_ERROR_EN			BIT(4)
+#define     IEP2_REG_OSD_MAX_EN				BIT(1)
+#define     IEP2_REG_FRM_DONE_EN			BIT(0)
+#define IEP2_REG_INT_CLR			0x0024
+#define     IEP2_REG_TIMEOUT_CLR			BIT(5)
+#define     IEP2_REG_BUS_ERROR_CLR			BIT(4)
+#define     IEP2_REG_OSD_MAX_CLR			BIT(1)
+#define     IEP2_REG_FRM_DONE_CLR			BIT(0)
+#define IEP2_REG_INT_STS			0x0028
+#define     IEP2_REG_RO_TIMEOUT_STS(x)		        ((x) & BIT(5))
+#define     IEP2_REG_RO_BUS_ERROR_STS(x)		((x) & BIT(4))
+#define     IEP2_REG_RO_OSD_MAX_STS(x)			((x) & BIT(1))
+#define     IEP2_REG_RO_FRM_DONE_STS(x)			((x) & BIT(0))
+#define     IEP2_REG_RO_VALID_INT_STS(x)		((x) & (BIT(5) | BIT(4) | BIT(0)))
+#define IEP2_REG_INT_RAW_STS			0x002c
+#define IEP2_REG_VIR_SRC_IMG_WIDTH		0x0030
+#define     IEP2_REG_SRC_VIR_UV_STRIDE(x)		(((x) & 0xffff) << 16)
+#define     IEP2_REG_SRC_VIR_Y_STRIDE(x)		((x) & 0xffff)
+#define IEP2_REG_VIR_DST_IMG_WIDTH		0x0034
+#define     IEP2_REG_DST_VIR_STRIDE(x)			((x) & 0xffff)
+#define IEP2_REG_SRC_IMG_SIZE			0x0038
+#define     IEP2_REG_SRC_PIC_HEIGHT(x)			(((x) & 0x7ff) << 16)
+#define     IEP2_REG_SRC_PIC_WIDTH(x)			((x) & 0x7ff)
+#define IEP2_REG_DIL_CONFIG0			0x0040
+#define     IEP2_REG_DIL_MV_HIST_EN			BIT(17)
+#define     IEP2_REG_DIL_ROI_EN				BIT(16)
+#define     IEP2_REG_DIL_COMB_EN			BIT(15)
+#define     IEP2_REG_DIL_BLE_EN				BIT(14)
+#define     IEP2_REG_DIL_EEDI_EN			BIT(13)
+#define     IEP2_REG_DIL_MEMC_EN			BIT(12)
+#define     IEP2_REG_DIL_OSD_EN				BIT(11)
+#define     IEP2_REG_DIL_PD_EN				BIT(10)
+#define     IEP2_REG_DIL_FF_EN				BIT(9)
+#define     IEP2_REG_DIL_MD_PRE_EN			BIT(8)
+#define     IEP2_REG_DIL_FIELD_ORDER(x)			(((x) & 1) << 5)
+#define     IEP2_REG_DIL_OUT_MODE(x)			(((x) & 1) << 4)
+#define     IEP2_REG_DIL_MODE(x)			((x) & 0xf)
+#define IEP2_REG_TIMEOUT_CFG                    0x0050
+#define     IEP2_REG_TIMEOUT_CFG_EN			BIT(31)
+#define IEP2_REG_DBG_FRM_CNT			0x0058
+#define IEP2_REG_DBG_TIMEOUT_CNT		0x005c
+#define IEP2_REG_SRC_ADDR_CURY			0x0060
+#define IEP2_REG_SRC_ADDR_NXTY			0x0064
+#define IEP2_REG_SRC_ADDR_PREY			0x0068
+#define IEP2_REG_SRC_ADDR_CURUV			0x006c
+#define IEP2_REG_SRC_ADDR_CURV			0x0070
+#define IEP2_REG_SRC_ADDR_NXTUV			0x0074
+#define IEP2_REG_SRC_ADDR_NXTV			0x0078
+#define IEP2_REG_SRC_ADDR_PREUV			0x007c
+#define IEP2_REG_SRC_ADDR_PREV			0x0080
+#define IEP2_REG_SRC_ADDR_MD			0x0084
+#define IEP2_REG_SRC_ADDR_MV			0x0088
+#define IEP2_REG_ROI_ADDR			0x008c
+#define IEP2_REG_DST_ADDR_TOPY			0x00b0
+#define IEP2_REG_DST_ADDR_BOTY			0x00b4
+#define IEP2_REG_DST_ADDR_TOPC			0x00b8
+#define IEP2_REG_DST_ADDR_BOTC			0x00bc
+#define IEP2_REG_DST_ADDR_MD			0x00c0
+#define IEP2_REG_DST_ADDR_MV			0x00c4
+#define IEP2_REG_MD_CONFIG0			0x00e0
+#define     IEP2_REG_MD_THETA(x)			(((x) & 3) << 8)
+#define     IEP2_REG_MD_R(x)				(((x) & 0xf) << 4)
+#define     IEP2_REG_MD_LAMBDA(x)			((x) & 0xf)
+#define IEP2_REG_DECT_CONFIG0			0x00e4
+#define     IEP2_REG_OSD_GRADV_THR(x)			(((x) & 0xff) << 24)
+#define     IEP2_REG_OSD_GRADH_THR(x)			(((x) & 0xff) << 16)
+#define     IEP2_REG_OSD_AREA_NUM(x)			(((x) & 0xf) << 8)
+#define     IEP2_REG_DECT_RESI_THR(x)			((x) & 0xff)
+#define IEP2_REG_OSD_LIMIT_CONFIG		0x00f0
+#define     IEP2_REG_OSD_POS_LIMIT_NUM(x)		(((x) & 7) << 4)
+#define     IEP2_REG_OSD_POS_LIMIT_EN			BIT(0)
+#define IEP2_REG_OSD_LIMIT_AREA(i)		(0x00f4 + ((i) * 4))
+#define IEP2_REG_OSD_CONFIG0			0x00fc
+#define     IEP2_REG_OSD_LINE_NUM(x)			(((x) & 0x1ff) << 16)
+#define     IEP2_REG_OSD_PEC_THR(x)			((x) & 0x7ff)
+#define IEP2_REG_OSD_AREA_CONF(i)		(0x0100 + ((i) * 4))
+#define     IEP2_REG_OSD_Y_END(x)			(((x) & 0x1ff) << 23)
+#define     IEP2_REG_OSD_Y_STA(x)			(((x) & 0x1ff) << 14)
+#define     IEP2_REG_OSD_X_END(x)			(((x) & 0x7f) << 7)
+#define     IEP2_REG_OSD_X_STA(x)			((x) & 0x7f)
+#define IEP2_REG_ME_CONFIG0			0x0120
+#define     IEP2_REG_ME_THR_OFFSET(x)			(((x) & 0xff) << 16)
+#define     IEP2_REG_MV_SIMILAR_NUM_THR0(x)		(((x) & 0xf) << 12)
+#define     IEP2_REG_MV_SIMILAR_THR(x)			(((x) & 0xf) << 8)
+#define     IEP2_REG_MV_BONUS(x)			(((x) & 0xf) << 4)
+#define     IEP2_REG_ME_PENA(x)				((x) & 0xf)
+#define IEP2_REG_ME_LIMIT_CONFIG		0x0124
+#define     IEP2_REG_MV_RIGHT_LIMIT(x)			(((x) & 0x3f) << 8)
+#define     IEP2_REG_MV_LEFT_LIMIT(x)			((x) & 0x3f)
+#define IEP2_REG_MV_TRU_LIST(i)			(0x0128 + ((i) * 4))
+#define     IEP2_REG_MV_TRU_LIST3_7(x)			(((x) & 0x3f) << 26)
+#define     IEP2_REG_MV_TRU_LIST3_7_VLD			BIT(24)
+#define     IEP2_REG_MV_TRU_LIST2_6(x)			(((x) & 0x3f) << 18)
+#define     IEP2_REG_MV_TRU_LIST2_6_VLD			BIT(16)
+#define     IEP2_REG_MV_TRU_LIST1_5(x)			(((x) & 0x3f) << 10)
+#define     IEP2_REG_MV_TRU_LIST1_5_VLD			BIT(8)
+#define     IEP2_REG_MV_TRU_LIST0_4(x)			(((x) & 0x3f) << 2)
+#define     IEP2_REG_MV_TRU_LIST0_4_VLD			BIT(0)
+#define IEP2_REG_EEDI_CONFIG0			0x0130
+#define     IEP2_REG_EEDI_THR0(x)			((x) & 0x1f)
+#define IEP2_REG_BLE_CONFIG0			0x0134
+#define     IEP2_REG_BLE_BACKTOMA_NUM(x)		((x) & 7)
+#define IEP2_REG_COMB_CONFIG0			0x0138
+#define     IEP2_REG_COMB_CNT_THR(x)			(((x) & 0xf) << 24)
+#define     IEP2_REG_COMB_FEATRUE_THR(x)		(((x) & 0x3f) << 16)
+#define     IEP2_REG_COMB_T_THR(x)			(((x) & 0xff) << 8)
+#define     IEP2_REG_COMB_OSD_VLD(i)			BIT(i)
+#define IEP2_REG_DIL_MTN_TAB(i)			(0x0140 + ((i) * 4))
+#define     IEP2_REG_MTN_SUB_TAB3_7_11_15(x)		(((x) & 0x7f) << 24)
+#define     IEP2_REG_MTN_SUB_TAB2_6_10_14(x)		(((x) & 0x7f) << 16)
+#define     IEP2_REG_MTN_SUB_TAB1_5_9_13(x)		(((x) & 0x7f) << 8)
+#define     IEP2_REG_MTN_SUB_TAB0_4_8_12(x)		((x) & 0x7f)
+#define IEP2_REG_RO_PD_TCNT			0x0400
+#define IEP2_REG_RO_PD_BCNT			0x0404
+#define IEP2_REG_RO_FF_CUR_TCNT			0x0408
+#define IEP2_REG_RO_FF_CUR_BCNT			0x040c
+#define IEP2_REG_RO_FF_NXT_TCNT			0x0410
+#define IEP2_REG_RO_FF_NXT_BCNT			0x0414
+#define IEP2_REG_RO_FF_BLE_TCNT			0x0418
+#define IEP2_REG_RO_FF_BLE_BCNT			0x041c
+#define IEP2_REG_RO_FF_COMB_NZ			0x0420
+#define IEP2_REG_RO_FF_COMB_F			0x0424
+#define IEP2_REG_RO_OSD_NUM			0x0428
+#define IEP2_REG_RO_COMB_CNT			0x042c
+#define     IEP2_REG_RO_OUT_OSD_COMB_CNT(x)		((x) >> 16)
+#define     IEP2_REG_RO_OUT_COMB_CNT(x)			((x) & 0xffff)
+#define IEP2_REG_RO_FF_GRADT_TCNT		0x0430
+#define IEP2_REG_RO_FF_GRADT_BCNT		0x0434
+#define IEP2_REG_RO_OSD_AREA_X(i)		(0x0440 + ((i) * 8))
+#define     IEP2_REG_RO_X_END(x)			(((x) >> 16) & 0x7ff)
+#define     IEP2_REG_RO_X_STA(x)			((x) & 0x7ff)
+#define IEP2_REG_RO_OSD_AREA_Y(i)		(0x0444 + ((i) * 8))
+#define     IEP2_REG_RO_Y_END(x)			(((x) >> 16) & 0x7ff)
+#define     IEP2_REG_RO_Y_STA(x)			((x) & 0x7ff)
+#define IEP2_REG_RO_MV_HIST_BIN(i)		(0x480 + ((i) * 4))
+#define     IEP2_REG_RO_MV_HIST_ODD(x)			((x) >> 16)
+#define     IEP2_REG_RO_MV_HIST_EVEN(x)			((x) & 0xffff)
+
+#endif
+
diff --git a/drivers/video/rockchip/mpp_osal/Kconfig b/drivers/video/rockchip/mpp_osal/Kconfig
new file mode 100644
index 0000000000000..b2134484155c3
--- /dev/null
+++ b/drivers/video/rockchip/mpp_osal/Kconfig
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+config ROCKCHIP_MPP_OSAL
+	bool "mpp osal"
+	help
+	  rockchip mpp osal adapt for kmpp
diff --git a/drivers/video/rockchip/mpp_osal/Makefile b/drivers/video/rockchip/mpp_osal/Makefile
new file mode 100644
index 0000000000000..f4ca9643fff2c
--- /dev/null
+++ b/drivers/video/rockchip/mpp_osal/Makefile
@@ -0,0 +1,2 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+obj-$(CONFIG_ROCKCHIP_MPP_OSAL) += mpp_osal.o
diff --git a/drivers/video/rockchip/mpp_osal/mpp_osal.c b/drivers/video/rockchip/mpp_osal/mpp_osal.c
new file mode 100644
index 0000000000000..abdaf4be6a227
--- /dev/null
+++ b/drivers/video/rockchip/mpp_osal/mpp_osal.c
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Copyright (c) 2023 Rockchip Electronics Co., Ltd.
+ *
+ */
+#include "mpp_osal.h"
+#include <linux/platform_device.h>
+
+struct device_node *mpp_dev_of_node(struct device *dev)
+{
+	return dev_of_node(dev);
+}
+EXPORT_SYMBOL(mpp_dev_of_node);
+
+void mpp_pm_relax(struct device *dev)
+{
+	return pm_relax(dev);
+}
+EXPORT_SYMBOL(mpp_pm_relax);
+
+void mpp_pm_stay_awake(struct device *dev)
+{
+	return pm_stay_awake(dev);
+}
+EXPORT_SYMBOL(mpp_pm_stay_awake);
+
+int mpp_device_init_wakeup(struct device *dev, bool enable)
+{
+	return device_init_wakeup(dev, enable);
+}
+EXPORT_SYMBOL(mpp_device_init_wakeup);
+
+void mpp_device_add_driver(void *dev, void *drv)
+{
+#ifdef CONFIG_PM_SLEEP
+	struct device *kdev = (struct device *)dev;
+	struct platform_driver *mpi_driver = (struct platform_driver *)drv;
+
+	kdev->driver = &mpi_driver->driver;
+	kdev->power.no_pm_callbacks = 0;
+#endif
+}
+EXPORT_SYMBOL(mpp_device_add_driver);
diff --git a/drivers/video/rockchip/mpp_osal/mpp_osal.h b/drivers/video/rockchip/mpp_osal/mpp_osal.h
new file mode 100644
index 0000000000000..826f006d8552e
--- /dev/null
+++ b/drivers/video/rockchip/mpp_osal/mpp_osal.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2023 Rockchip Electronics Co., Ltd.
+ *
+ */
+
+#ifndef __ROCKCHIP_MPP_OSAL_H__
+#define __ROCKCHIP_MPP_OSAL_H__
+
+#include <linux/platform_device.h>
+#include <linux/pm_wakeup.h>
+
+struct device_node *mpp_dev_of_node(struct device *dev);
+void mpp_pm_relax(struct device *dev);
+void mpp_pm_stay_awake(struct device *dev);
+int mpp_device_init_wakeup(struct device *dev, bool enable);
+void mpp_device_add_driver(void *dev, void *drv);
+
+#endif
diff --git a/drivers/video/rockchip/rga/Kconfig b/drivers/video/rockchip/rga/Kconfig
new file mode 100644
index 0000000000000..6023b2eb23a72
--- /dev/null
+++ b/drivers/video/rockchip/rga/Kconfig
@@ -0,0 +1,10 @@
+# SPDX-License-Identifier: GPL-2.0
+menu "RGA"
+	depends on ARCH_ROCKCHIP
+
+config ROCKCHIP_RGA
+	tristate "ROCKCHIP_RGA"
+	help
+	  rk30 rga module.
+
+endmenu
diff --git a/drivers/video/rockchip/rga/Makefile b/drivers/video/rockchip/rga/Makefile
new file mode 100644
index 0000000000000..58dd4c6a3e422
--- /dev/null
+++ b/drivers/video/rockchip/rga/Makefile
@@ -0,0 +1,4 @@
+# SPDX-License-Identifier: GPL-2.0
+rga-y	:= rga_drv.o rga_mmu_info.o rga_reg_info.o RGA_API.o
+
+obj-$(CONFIG_ROCKCHIP_RGA)	+= rga.o
diff --git a/drivers/video/rockchip/rga/RGA_API.c b/drivers/video/rockchip/rga/RGA_API.c
new file mode 100644
index 0000000000000..4359a6d503791
--- /dev/null
+++ b/drivers/video/rockchip/rga/RGA_API.c
@@ -0,0 +1,201 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#include <linux/memory.h>
+#include "RGA_API.h"
+#include "rga.h"
+//#include "rga_angle.h"
+
+#define IS_YUV_420(format) \
+     ((format == RK_FORMAT_YCbCr_420_P) | (format == RK_FORMAT_YCbCr_420_SP) | \
+      (format == RK_FORMAT_YCrCb_420_P) | (format == RK_FORMAT_YCrCb_420_SP))
+
+#define IS_YUV_422(format) \
+     ((format == RK_FORMAT_YCbCr_422_P) | (format == RK_FORMAT_YCbCr_422_SP) | \
+      (format == RK_FORMAT_YCrCb_422_P) | (format == RK_FORMAT_YCrCb_422_SP))
+
+#define IS_YUV(format) \
+     ((format == RK_FORMAT_YCbCr_420_P) | (format == RK_FORMAT_YCbCr_420_SP) | \
+      (format == RK_FORMAT_YCrCb_420_P) | (format == RK_FORMAT_YCrCb_420_SP) | \
+      (format == RK_FORMAT_YCbCr_422_P) | (format == RK_FORMAT_YCbCr_422_SP) | \
+      (format == RK_FORMAT_YCrCb_422_P) | (format == RK_FORMAT_YCrCb_422_SP))
+
+
+extern rga_service_info rga_service;
+
+
+void
+matrix_cal(const struct rga_req *msg, TILE_INFO *tile)
+{
+    uint64_t x_time, y_time;
+    uint64_t sina, cosa;
+
+    int s_act_w, s_act_h, d_act_w, d_act_h;
+
+    s_act_w = msg->src.act_w;
+    s_act_h = msg->src.act_h;
+    d_act_w = msg->dst.act_w;
+    d_act_h = msg->dst.act_h;
+
+    if (s_act_w == 1) s_act_w += 1;
+    if (s_act_h == 1) s_act_h += 1;
+    if (d_act_h == 1) d_act_h += 1;
+    if (d_act_w == 1) d_act_w += 1;
+
+    x_time = ((s_act_w - 1)<<16) / (d_act_w - 1);
+    y_time = ((s_act_h - 1)<<16) / (d_act_h - 1);
+
+    sina = msg->sina;
+    cosa = msg->cosa;
+
+    switch(msg->rotate_mode)
+    {
+        /* 16.16 x 16.16 */
+        /* matrix[] is 64 bit wide */
+        case 1 :
+            tile->matrix[0] =  cosa*x_time;
+            tile->matrix[1] = -sina*y_time;
+            tile->matrix[2] =  sina*x_time;
+            tile->matrix[3] =  cosa*y_time;
+            break;
+        case 2 :
+            tile->matrix[0] = -(x_time<<16);
+            tile->matrix[1] = 0;
+            tile->matrix[2] = 0;
+            tile->matrix[3] = (y_time<<16);
+            break;
+        case 3 :
+            tile->matrix[0] = (x_time<<16);
+            tile->matrix[1] = 0;
+            tile->matrix[2] = 0;
+            tile->matrix[3] = -(y_time<<16);
+            break;
+        default :
+            tile->matrix[0] =  (uint64_t)1<<32;
+            tile->matrix[1] =  0;
+            tile->matrix[2] =  0;
+            tile->matrix[3] =  (uint64_t)1<<32;
+            break;
+    }
+}
+
+
+int32_t RGA_gen_two_pro(struct rga_req *msg, struct rga_req *msg1)
+{
+
+    struct rga_req *mp;
+    uint32_t w_ratio, h_ratio;
+    uint32_t stride;
+
+    uint32_t daw, dah;
+    uint32_t pl;
+
+    daw = dah = 0;
+
+    mp = msg1;
+
+    if(msg->dst.act_w == 0)
+    {
+        printk("%s, [%d] rga dst act_w is zero\n", __FUNCTION__, __LINE__);
+        return -EINVAL;
+    }
+
+    if (msg->dst.act_h == 0)
+    {
+        printk("%s, [%d] rga dst act_w is zero\n", __FUNCTION__, __LINE__);
+        return -EINVAL;
+    }
+    w_ratio = (msg->src.act_w << 16) / msg->dst.act_w;
+    h_ratio = (msg->src.act_h << 16) / msg->dst.act_h;
+
+    memcpy(msg1, msg, sizeof(struct rga_req));
+
+    msg->dst.format = msg->src.format;
+
+    /*pre_scale_w cal*/
+    if ((w_ratio >= (2<<16)) && (w_ratio < (4<<16))) {
+        daw = (msg->src.act_w + 1) >> 1;
+        if((IS_YUV_420(msg->dst.format)) && (daw & 1)) {
+            daw -= 1;
+            msg->src.act_w = daw << 1;
+        }
+    }
+    else if ((w_ratio >= (4<<16)) && (w_ratio < (8<<16))) {
+        daw = (msg->src.act_w + 3) >> 2;
+        if((IS_YUV_420(msg->dst.format)) && (daw & 1)) {
+            daw -= 1;
+            msg->src.act_w = daw << 2;
+        }
+    }
+    else if ((w_ratio >= (8<<16)) && (w_ratio < (16<<16))) {
+        daw = (msg->src.act_w + 7) >> 3;
+        if((IS_YUV_420(msg->dst.format)) && (daw & 1)) {
+            daw -= 1;
+            msg->src.act_w = daw << 3;
+        }
+    }
+    else
+    {
+        daw = msg->src.act_w;
+    }
+
+    pl = (RGA_pixel_width_init(msg->src.format));
+    stride = (pl * daw + 3) & (~3);
+    msg->dst.act_w = daw;
+    msg->dst.vir_w = stride / pl;
+
+    /*pre_scale_h cal*/
+    if ((h_ratio >= (2<<16)) && (h_ratio < (4<<16))) {
+        dah = (msg->src.act_h + 1) >> 1;
+        if((IS_YUV(msg->dst.format)) && (dah & 1)) {
+            dah -= 1;
+            msg->src.act_h = dah << 1;
+        }
+    }
+    else if ((h_ratio >= (4<<16)) && (h_ratio < (8<<16))) {
+        dah = (msg->src.act_h + 3) >> 2;
+        if((IS_YUV(msg->dst.format)) && (dah & 1)) {
+            dah -= 1;
+            msg->src.act_h = dah << 2;
+
+        }
+    }
+    else if ((h_ratio >= (8<<16)) && (h_ratio < (16<<16))) {
+        dah = (msg->src.act_h + 7) >> 3;
+        if((IS_YUV(msg->dst.format)) && (dah & 1)) {
+            dah -= 1;
+            msg->src.act_h = dah << 3;
+        }
+    }
+    else
+    {
+        dah = msg->src.act_h;
+    }
+
+    msg->dst.act_h = dah;
+    msg->dst.vir_h = dah;
+
+    msg->dst.x_offset = 0;
+    msg->dst.y_offset = 0;
+
+    msg->dst.yrgb_addr = (unsigned long)rga_service.pre_scale_buf;
+    msg->dst.uv_addr = msg->dst.yrgb_addr + stride * dah;
+    msg->dst.v_addr = msg->dst.uv_addr + ((stride * dah) >> 1);
+
+    msg->render_mode = pre_scaling_mode;
+
+    msg1->src.yrgb_addr = msg->dst.yrgb_addr;
+    msg1->src.uv_addr = msg->dst.uv_addr;
+    msg1->src.v_addr = msg->dst.v_addr;
+
+    msg1->src.act_w = msg->dst.act_w;
+    msg1->src.act_h = msg->dst.act_h;
+    msg1->src.vir_w = msg->dst.vir_w;
+    msg1->src.vir_h = msg->dst.vir_h;
+
+    msg1->src.x_offset = 0;
+    msg1->src.y_offset = 0;
+
+    return 0;
+}
+
+
diff --git a/drivers/video/rockchip/rga/RGA_API.h b/drivers/video/rockchip/rga/RGA_API.h
new file mode 100644
index 0000000000000..96ca5240ad834
--- /dev/null
+++ b/drivers/video/rockchip/rga/RGA_API.h
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_API_H__
+#define __RGA_API_H__
+
+#include <linux/miscdevice.h>
+#include <linux/wakelock.h>
+
+#include "rga_reg_info.h"
+#include "rga.h"
+
+#define ENABLE      1
+#define DISABLE     0
+
+struct rga_drvdata {
+	struct miscdevice miscdev;
+	struct device *dev;
+	void *rga_base;
+	int irq;
+
+	struct delayed_work power_off_work;
+	void (*rga_irq_callback)(int rga_retval);   //callback function used by aync call
+	struct wake_lock wake_lock;
+
+	struct clk *pd_rga;
+	struct clk *aclk_rga;
+	struct clk *hclk_rga;
+
+	//#if defined(CONFIG_ION_ROCKCHIP)
+	struct ion_client *ion_client;
+	//#endif
+	char *version;
+};
+
+int32_t RGA_gen_two_pro(struct rga_req *msg, struct rga_req *msg1);
+
+
+
+
+
+#endif
diff --git a/drivers/video/rockchip/rga/rga.h b/drivers/video/rockchip/rga/rga.h
new file mode 100644
index 0000000000000..0735a74ceb4d4
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga.h
@@ -0,0 +1,507 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _RGA_DRIVER_H_
+#define _RGA_DRIVER_H_
+
+#include <linux/mutex.h>
+#include <linux/scatterlist.h>
+
+
+#define RGA_BLIT_SYNC	0x5017
+#define RGA_BLIT_ASYNC  0x5018
+#define RGA_FLUSH       0x5019
+#define RGA_GET_RESULT  0x501a
+#define RGA_GET_VERSION 0x501b
+
+
+#define RGA_REG_CTRL_LEN    0x8    /* 8  */
+#define RGA_REG_CMD_LEN     0x20   /* 32 */
+#define RGA_CMD_BUF_SIZE    0x700  /* 16*28*4 */
+
+#define RGA_OUT_OF_RESOURCES    -10
+#define RGA_MALLOC_ERROR        -11
+
+#define RGA_BUF_GEM_TYPE_MASK	0xC0
+
+#define rgaIS_ERROR(status)			(status < 0)
+#define rgaNO_ERROR(status)			(status >= 0)
+#define rgaIS_SUCCESS(status)		(status == 0)
+
+#define RGA_DEBUGFS 1
+
+/* RGA process mode enum */
+enum
+{
+    bitblt_mode               = 0x0,
+    color_palette_mode        = 0x1,
+    color_fill_mode           = 0x2,
+    line_point_drawing_mode   = 0x3,
+    blur_sharp_filter_mode    = 0x4,
+    pre_scaling_mode          = 0x5,
+    update_palette_table_mode = 0x6,
+    update_patten_buff_mode   = 0x7,
+};
+
+
+enum
+{
+    rop_enable_mask          = 0x2,
+    dither_enable_mask       = 0x8,
+    fading_enable_mask       = 0x10,
+    PD_enbale_mask           = 0x20,
+};
+
+enum
+{
+    yuv2rgb_mode0            = 0x0,     /* BT.601 MPEG */
+    yuv2rgb_mode1            = 0x1,     /* BT.601 JPEG */
+    yuv2rgb_mode2            = 0x2,     /* BT.709      */
+};
+
+
+/* RGA rotate mode */
+enum
+{
+    rotate_mode0             = 0x0,     /* no rotate */
+    rotate_mode1             = 0x1,     /* rotate    */
+    rotate_mode2             = 0x2,     /* x_mirror  */
+    rotate_mode3             = 0x3,     /* y_mirror  */
+};
+
+enum
+{
+    color_palette_mode0      = 0x0,     /* 1K */
+    color_palette_mode1      = 0x1,     /* 2K */
+    color_palette_mode2      = 0x2,     /* 4K */
+    color_palette_mode3      = 0x3,     /* 8K */
+};
+
+
+
+/*
+//          Alpha    Red     Green   Blue
+{  4, 32, {{32,24,   8, 0,  16, 8,  24,16 }}, GGL_RGBA },   // RK_FORMAT_RGBA_8888
+{  4, 24, {{ 0, 0,   8, 0,  16, 8,  24,16 }}, GGL_RGB  },   // RK_FORMAT_RGBX_8888
+{  3, 24, {{ 0, 0,   8, 0,  16, 8,  24,16 }}, GGL_RGB  },   // RK_FORMAT_RGB_888
+{  4, 32, {{32,24,  24,16,  16, 8,   8, 0 }}, GGL_BGRA },   // RK_FORMAT_BGRA_8888
+{  2, 16, {{ 0, 0,  16,11,  11, 5,   5, 0 }}, GGL_RGB  },   // RK_FORMAT_RGB_565
+{  2, 16, {{ 1, 0,  16,11,  11, 6,   6, 1 }}, GGL_RGBA },   // RK_FORMAT_RGBA_5551
+{  2, 16, {{ 4, 0,  16,12,  12, 8,   8, 4 }}, GGL_RGBA },   // RK_FORMAT_RGBA_4444
+{  3, 24, {{ 0, 0,  24,16,  16, 8,   8, 0 }}, GGL_BGR  },   // RK_FORMAT_BGB_888
+
+*/
+enum
+{
+	RK_FORMAT_RGBA_8888    = 0x0,
+    RK_FORMAT_RGBX_8888    = 0x1,
+    RK_FORMAT_RGB_888      = 0x2,
+    RK_FORMAT_BGRA_8888    = 0x3,
+    RK_FORMAT_RGB_565      = 0x4,
+    RK_FORMAT_RGBA_5551    = 0x5,
+    RK_FORMAT_RGBA_4444    = 0x6,
+    RK_FORMAT_BGR_888      = 0x7,
+
+    RK_FORMAT_YCbCr_422_SP = 0x8,
+    RK_FORMAT_YCbCr_422_P  = 0x9,
+    RK_FORMAT_YCbCr_420_SP = 0xa,
+    RK_FORMAT_YCbCr_420_P  = 0xb,
+
+    RK_FORMAT_YCrCb_422_SP = 0xc,
+    RK_FORMAT_YCrCb_422_P  = 0xd,
+    RK_FORMAT_YCrCb_420_SP = 0xe,
+    RK_FORMAT_YCrCb_420_P  = 0xf,
+
+    RK_FORMAT_BPP1         = 0x10,
+    RK_FORMAT_BPP2         = 0x11,
+    RK_FORMAT_BPP4         = 0x12,
+    RK_FORMAT_BPP8         = 0x13,
+    RK_FORMAT_YCbCr_420_SP_10B = 0x20,
+    RK_FORMAT_YCrCb_420_SP_10B = 0x21,
+};
+
+
+typedef struct rga_img_info_t
+{
+    unsigned long yrgb_addr;      /* yrgb    mem addr         */
+    unsigned long uv_addr;        /* cb/cr   mem addr         */
+    unsigned long v_addr;         /* cr      mem addr         */
+    unsigned int format;         //definition by RK_FORMAT
+
+    unsigned short act_w;
+    unsigned short act_h;
+    unsigned short x_offset;
+    unsigned short y_offset;
+
+    unsigned short vir_w;
+    unsigned short vir_h;
+
+    unsigned short endian_mode; //for BPP
+    unsigned short alpha_swap;
+}
+rga_img_info_t;
+
+
+typedef struct mdp_img_act
+{
+    unsigned short w;         // width
+    unsigned short h;         // height
+    short x_off;     // x offset for the vir
+    short y_off;     // y offset for the vir
+}
+mdp_img_act;
+
+
+
+typedef struct RANGE
+{
+    unsigned short min;
+    unsigned short max;
+}
+RANGE;
+
+typedef struct POINT
+{
+    unsigned short x;
+    unsigned short y;
+}
+POINT;
+
+typedef struct RECT
+{
+    unsigned short xmin;
+    unsigned short xmax; // width - 1
+    unsigned short ymin;
+    unsigned short ymax; // height - 1
+} RECT;
+
+typedef struct RGB
+{
+    unsigned char r;
+    unsigned char g;
+    unsigned char b;
+    unsigned char res;
+}RGB;
+
+
+typedef struct MMU
+{
+    unsigned char mmu_en;
+    unsigned long base_addr;
+	uint32_t mmu_flag;
+} MMU;
+
+
+
+
+typedef struct COLOR_FILL
+{
+    short gr_x_a;
+    short gr_y_a;
+    short gr_x_b;
+    short gr_y_b;
+    short gr_x_g;
+    short gr_y_g;
+    short gr_x_r;
+    short gr_y_r;
+
+    //u8  cp_gr_saturation;
+}
+COLOR_FILL;
+
+typedef struct FADING
+{
+    uint8_t b;
+    uint8_t g;
+    uint8_t r;
+    uint8_t res;
+}
+FADING;
+
+
+typedef struct line_draw_t
+{
+    POINT start_point;              /* LineDraw_start_point                */
+    POINT end_point;                /* LineDraw_end_point                  */
+    uint32_t   color;               /* LineDraw_color                      */
+    uint32_t   flag;                /* (enum) LineDrawing mode sel         */
+    uint32_t   line_width;          /* range 1~16 */
+}
+line_draw_t;
+
+
+
+struct rga_req {
+    uint8_t render_mode;            /* (enum) process mode sel */
+
+    rga_img_info_t src;             /* src image info */
+    rga_img_info_t dst;             /* dst image info */
+    rga_img_info_t pat;             /* patten image info */
+
+    unsigned long rop_mask_addr;         /* rop4 mask addr */
+    unsigned long LUT_addr;              /* LUT addr */
+
+    RECT clip;                      /* dst clip window default value is dst_vir */
+                                    /* value from [0, w-1] / [0, h-1]*/
+
+    int32_t sina;                   /* dst angle  default value 0  16.16 scan from table */
+    int32_t cosa;                   /* dst angle  default value 0  16.16 scan from table */
+
+    uint16_t alpha_rop_flag;        /* alpha rop process flag           */
+                                    /* ([0] = 1 alpha_rop_enable)       */
+                                    /* ([1] = 1 rop enable)             */
+                                    /* ([2] = 1 fading_enable)          */
+                                    /* ([3] = 1 PD_enable)              */
+                                    /* ([4] = 1 alpha cal_mode_sel)     */
+                                    /* ([5] = 1 dither_enable)          */
+                                    /* ([6] = 1 gradient fill mode sel) */
+                                    /* ([7] = 1 AA_enable)              */
+
+    uint8_t  scale_mode;            /* 0 nearst / 1 bilnear / 2 bicubic */
+
+    uint32_t color_key_max;         /* color key max */
+    uint32_t color_key_min;         /* color key min */
+
+    uint32_t fg_color;              /* foreground color */
+    uint32_t bg_color;              /* background color */
+
+    COLOR_FILL gr_color;            /* color fill use gradient */
+
+    line_draw_t line_draw_info;
+
+    FADING fading;
+
+    uint8_t PD_mode;                /* porter duff alpha mode sel */
+
+    uint8_t alpha_global_value;     /* global alpha value */
+
+    uint16_t rop_code;              /* rop2/3/4 code  scan from rop code table*/
+
+    uint8_t bsfilter_flag;          /* [2] 0 blur 1 sharp / [1:0] filter_type*/
+
+    uint8_t palette_mode;           /* (enum) color palatte  0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+
+    uint8_t yuv2rgb_mode;           /* (enum) BT.601 MPEG / BT.601 JPEG / BT.709  */
+
+    uint8_t endian_mode;            /* 0/big endian 1/little endian*/
+
+    uint8_t rotate_mode;            /* (enum) rotate mode  */
+                                    /* 0x0,     no rotate  */
+                                    /* 0x1,     rotate     */
+                                    /* 0x2,     x_mirror   */
+                                    /* 0x3,     y_mirror   */
+
+    uint8_t color_fill_mode;        /* 0 solid color / 1 patten color */
+
+    MMU mmu_info;                   /* mmu information */
+
+    uint8_t  alpha_rop_mode;        /* ([0~1] alpha mode)       */
+                                    /* ([2~3] rop   mode)       */
+                                    /* ([4]   zero  mode en)    */
+                                    /* ([5]   dst   alpha mode) */
+
+    uint8_t  src_trans_mode;
+
+    struct sg_table *sg_src;
+	struct sg_table *sg_dst;
+	struct dma_buf_attachment *attach_src;
+	struct dma_buf_attachment *attach_dst;
+};
+
+
+typedef struct TILE_INFO
+{
+    int64_t matrix[4];
+
+    uint16_t tile_x_num;     /* x axis tile num / tile size is 8x8 pixel */
+    uint16_t tile_y_num;     /* y axis tile num */
+
+    int16_t dst_x_tmp;      /* dst pos x = (xstart - xoff) default value 0 */
+    int16_t dst_y_tmp;      /* dst pos y = (ystart - yoff) default value 0 */
+
+    uint16_t tile_w;
+    uint16_t tile_h;
+    int16_t tile_start_x_coor;
+    int16_t tile_start_y_coor;
+    int32_t tile_xoff;
+    int32_t tile_yoff;
+
+    int32_t tile_temp_xstart;
+    int32_t tile_temp_ystart;
+
+    /* src tile incr */
+    int32_t x_dx;
+    int32_t x_dy;
+    int32_t y_dx;
+    int32_t y_dy;
+
+    mdp_img_act dst_ctrl;
+
+}
+TILE_INFO;
+
+struct rga_mmu_buf_t {
+    int32_t front;
+    int32_t back;
+    int32_t size;
+    int32_t curr;
+    unsigned int *buf;
+    unsigned int *buf_virtual;
+
+    struct page **pages;
+};
+
+/**
+ * struct for process session which connect to rga
+ *
+ * @author ZhangShengqin (2012-2-15)
+ */
+typedef struct rga_session {
+	/* a linked list of data so we can access them for debugging */
+	struct list_head    list_session;
+	/* a linked list of register data waiting for process */
+	struct list_head    waiting;
+	/* a linked list of register data in processing */
+	struct list_head    running;
+	/* all coommand this thread done */
+    atomic_t            done;
+	wait_queue_head_t   wait;
+	pid_t           pid;
+	atomic_t        task_running;
+    atomic_t        num_done;
+} rga_session;
+
+struct rga_reg {
+    rga_session 		*session;
+	struct list_head	session_link;		/* link to rga service session */
+	struct list_head	status_link;		/* link to register set list */
+	uint32_t  sys_reg[RGA_REG_CTRL_LEN];
+    uint32_t  cmd_reg[RGA_REG_CMD_LEN];
+
+    uint32_t *MMU_base;
+    uint32_t MMU_len;
+    //atomic_t int_enable;
+
+    //struct rga_req      req;
+
+	struct sg_table *sg_src;
+	struct sg_table *sg_dst;
+
+	struct dma_buf_attachment *attach_src;
+	struct dma_buf_attachment *attach_dst;
+};
+
+
+
+typedef struct rga_service_info {
+    struct mutex	lock;
+    struct timer_list	timer;			/* timer for power off */
+    struct list_head	waiting;		/* link to link_reg in struct vpu_reg */
+    struct list_head	running;		/* link to link_reg in struct vpu_reg */
+    struct list_head	done;			/* link to link_reg in struct vpu_reg */
+    struct list_head	session;		/* link to list_session in struct vpu_session */
+    atomic_t		total_running;
+
+    struct rga_reg        *reg;
+
+    uint32_t            cmd_buff[28*8];/* cmd_buff for rga */
+    uint32_t            *pre_scale_buf;
+	atomic_t            int_disable;     /* 0 int enable 1 int disable  */
+    atomic_t            cmd_num;
+	atomic_t src_format_swt;
+	int last_prc_src_format;
+	atomic_t            rga_working;
+    bool                enable;
+	u32 dev_mode;
+
+    //struct rga_req      req[10];
+
+    struct mutex	mutex;	// mutex
+} rga_service_info;
+
+
+
+#if defined(CONFIG_ARCH_RK2928) || defined(CONFIG_ARCH_RK3026) || defined(CONFIG_ARCH_RK312x)
+#define RGA_BASE                 0x1010c000
+#elif defined(CONFIG_ARCH_RK30)
+#define RGA_BASE                 0x10114000
+#endif
+
+//General Registers
+#define RGA_SYS_CTRL             0x000
+#define RGA_CMD_CTRL             0x004
+#define RGA_CMD_ADDR             0x008
+#define RGA_STATUS               0x00c
+#define RGA_INT                  0x010
+#define RGA_AXI_ID               0x014
+#define RGA_MMU_STA_CTRL         0x018
+#define RGA_MMU_STA              0x01c
+#define RGA_VERSION              0x028
+
+//Command code start
+#define RGA_MODE_CTRL            0x100
+
+//Source Image Registers
+#define RGA_SRC_Y_MST            0x104
+#define RGA_SRC_CB_MST           0x108
+#define RGA_MASK_READ_MST        0x108  //repeat
+#define RGA_SRC_CR_MST           0x10c
+#define RGA_SRC_VIR_INFO         0x110
+#define RGA_SRC_ACT_INFO         0x114
+#define RGA_SRC_X_PARA           0x118
+#define RGA_SRC_Y_PARA           0x11c
+#define RGA_SRC_TILE_XINFO       0x120
+#define RGA_SRC_TILE_YINFO       0x124
+#define RGA_SRC_TILE_H_INCR      0x128
+#define RGA_SRC_TILE_V_INCR      0x12c
+#define RGA_SRC_TILE_OFFSETX     0x130
+#define RGA_SRC_TILE_OFFSETY     0x134
+#define RGA_SRC_BG_COLOR         0x138
+#define RGA_SRC_FG_COLOR         0x13c
+#define RGA_LINE_DRAWING_COLOR   0x13c  //repeat
+#define RGA_SRC_TR_COLOR0        0x140
+#define RGA_CP_GR_A              0x140  //repeat
+#define RGA_SRC_TR_COLOR1        0x144
+#define RGA_CP_GR_B              0x144  //repeat
+
+#define RGA_LINE_DRAW            0x148
+#define RGA_PAT_START_POINT      0x148  //repeat
+
+//Destination Image Registers
+#define RGA_DST_MST              0x14c
+#define RGA_LUT_MST              0x14c  //repeat
+#define RGA_PAT_MST              0x14c  //repeat
+#define RGA_LINE_DRAWING_MST     0x14c  //repeat
+
+#define RGA_DST_VIR_INFO         0x150
+
+#define RGA_DST_CTR_INFO         0x154
+#define RGA_LINE_DRAW_XY_INFO    0x154  //repeat
+
+//Alpha/ROP Registers
+#define RGA_ALPHA_CON            0x158
+
+#define RGA_PAT_CON              0x15c
+#define RGA_DST_VIR_WIDTH_PIX    0x15c  //repeat
+
+#define RGA_ROP_CON0             0x160
+#define RGA_CP_GR_G              0x160  //repeat
+#define RGA_PRESCL_CB_MST        0x160  //repeat
+
+#define RGA_ROP_CON1             0x164
+#define RGA_CP_GR_R              0x164  //repeat
+#define RGA_PRESCL_CR_MST        0x164  //repeat
+
+//MMU Register
+#define RGA_FADING_CON           0x168
+#define RGA_MMU_CTRL             0x168  //repeat
+
+#define RGA_MMU_TBL              0x16c  //repeat
+
+#define RGA_YUV_OUT_CFG          0x170
+#define RGA_DST_UV_MST           0x174
+
+
+#define RGA_BLIT_COMPLETE_EVENT 1
+
+long rga_ioctl_kernel(struct rga_req *req);
+
+#endif /*_RK29_IPP_DRIVER_H_*/
diff --git a/drivers/video/rockchip/rga/rga_drv.c b/drivers/video/rockchip/rga/rga_drv.c
new file mode 100644
index 0000000000000..40061ff82497f
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_drv.c
@@ -0,0 +1,2574 @@
+/*
+ * Copyright (C) 2012 Rockchip Electronics Co., Ltd.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#define pr_fmt(fmt) "rga: " fmt
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/err.h>
+#include <linux/clk.h>
+#include <asm/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+//#include <mach/io.h>
+//#include <mach/irqs.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/miscdevice.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/syscalls.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <asm/cacheflush.h>
+#include <linux/slab.h>
+#include <linux/fb.h>
+#include <linux/wakelock.h>
+#include <linux/version.h>
+#include <linux/debugfs.h>
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+#include <linux/dma-buf.h>
+#include <linux/pm_runtime.h>
+#endif
+
+#if defined(CONFIG_ION_ROCKCHIP)
+#include <linux/rockchip_ion.h>
+#endif
+
+#include "rga.h"
+#include "rga_reg_info.h"
+#include "rga_mmu_info.h"
+#include "RGA_API.h"
+
+#define RGA_TEST_CASE 0
+
+#define RGA_TEST_FLUSH_TIME 0
+#define RGA_INFO_BUS_ERROR 1
+
+#define RGA_PRE_SCALE_BUF_SIZE (2048 * 2048 * 4)
+#define RGA_PRE_SCALE_PAGE_SIZE (RGA_PRE_SCALE_BUF_SIZE >> PAGE_SHIFT)
+
+#define RGA_POWER_OFF_DELAY	4*HZ /* 4s */
+#define RGA_TIMEOUT_DELAY	2*HZ /* 2s */
+
+#define RGA_MAJOR		255
+
+#if defined(CONFIG_ARCH_RK2928) || defined(CONFIG_ARCH_RK3026)
+#define RK30_RGA_PHYS		RK2928_RGA_PHYS
+#define RK30_RGA_SIZE		RK2928_RGA_SIZE
+#endif
+#define RGA_RESET_TIMEOUT	1000
+
+/* Driver information */
+#define DRIVER_DESC		"RGA Device Driver"
+#define DRIVER_NAME		"rga"
+
+
+ktime_t rga_start;
+ktime_t rga_end;
+
+static rga_session rga_session_global;
+
+long (*rga_ioctl_kernel_p)(struct rga_req *);
+
+#if RGA_DEBUGFS
+unsigned char RGA_TEST_REG;
+unsigned char RGA_TEST_MSG;
+unsigned char RGA_TEST_TIME;
+unsigned char RGA_CHECK_MODE;
+unsigned char RGA_NONUSE;
+unsigned char RGA_INT_FLAG;
+#endif
+
+struct rga_drvdata *rga_drvdata;
+rga_service_info rga_service;
+struct rga_mmu_buf_t rga_mmu_buf;
+
+
+#if defined(CONFIG_ION_ROCKCHIP)
+extern struct ion_client *rockchip_ion_client_create(const char * name);
+#endif
+
+static int rga_blit_async(rga_session *session, struct rga_req *req);
+static void rga_del_running_list(void);
+static void rga_del_running_list_timeout(void);
+static void rga_try_set_reg(void);
+
+
+/* Logging */
+#define RGA_DEBUG 1
+#if RGA_DEBUG
+#define DBG(format, args...) printk(KERN_DEBUG "%s: " format, DRIVER_NAME, ## args)
+#define ERR(format, args...) printk(KERN_ERR "%s: " format, DRIVER_NAME, ## args)
+#define WARNING(format, args...) printk(KERN_WARN "%s: " format, DRIVER_NAME, ## args)
+#define INFO(format, args...) printk(KERN_INFO "%s: " format, DRIVER_NAME, ## args)
+#else
+#define DBG(format, args...)
+#define ERR(format, args...)
+#define WARNING(format, args...)
+#define INFO(format, args...)
+#endif
+
+#if RGA_DEBUGFS
+static const char *rga_get_cmd_mode_str(u32 cmd)
+{
+	switch (cmd) {
+	case RGA_BLIT_SYNC:
+		return "RGA_BLIT_SYNC";
+	case RGA_BLIT_ASYNC:
+		return "RGA_BLIT_ASYNC";
+	case RGA_FLUSH:
+		return "RGA_FLUSH";
+	case RGA_GET_RESULT:
+		return "RGA_GET_RESULT";
+	case RGA_GET_VERSION:
+		return "RGA_GET_VERSION";
+	default:
+		return "UNF";
+	}
+}
+
+static const char *rga_get_blend_mode_str(u16 alpha_rop_flag)
+{
+	if (alpha_rop_flag == 0)
+		return "no blend";
+	else if (alpha_rop_flag == 0x19)
+		return "blend mode 105 src + (1 - src.a) * dst";
+	else if (alpha_rop_flag == 0x11)
+		return "blend mode 405 src.a * src + (1 - src.a) * dst";
+	else
+		return "check reg for more imformation";
+}
+
+static const char *rga_get_render_mode_str(u8 mode)
+{
+	switch (mode & 0x0F) {
+	case 0x0:
+		return "bitblt";
+	case 0x1:
+		return "color_palette";
+	case 0x2:
+		return "color_fill";
+	case 0x3:
+		return "line_point_drawing";
+	case 0x4:
+		return "blur_sharp_filter";
+	case 0x5:
+		return "pre_scaling";
+	case 0x6:
+		return "update_palette_table";
+	case 0x7:
+		return "update_patten_buff";
+	default:
+		return "UNF";
+	}
+}
+
+static const char *rga_get_rotate_mode_str(struct rga_req *req_rga)
+{
+	switch (req_rga->rotate_mode) {
+	case 0x0:
+		return "no rotate";
+	case 0x1:
+		if (req_rga->sina == 0 && req_rga->cosa == 65536)
+			/* rotate 0 */
+			return "rotate 0";
+		else if (req_rga->sina == 65536 && req_rga->cosa == 0)
+			/* rotate 90 */
+			return "rotate 90 ";
+		else if (req_rga->sina == 0 && req_rga->cosa == -65536)
+			/* rotate 180 */
+			return "rotate 180 ";
+		else if (req_rga->sina == -65536 && req_rga->cosa == 0)
+			/* totate 270 */
+			return "rotate 270 ";
+		return "UNF";
+	case 0x2:
+		return "xmirror";
+	case 0x3:
+		return "ymirror";
+	default:
+		return "UNF";
+	}
+}
+
+static bool rga_is_yuv10bit_format(uint32_t format)
+{
+	bool ret  = false;
+
+	switch (format) {
+	case RK_FORMAT_YCbCr_420_SP_10B:
+	case RK_FORMAT_YCrCb_420_SP_10B:
+		ret = true;
+		break;
+	}
+	return ret;
+}
+
+static bool rga_is_yuv8bit_format(uint32_t format)
+{
+	bool ret  = false;
+
+	switch (format) {
+	case RK_FORMAT_YCbCr_422_SP:
+	case RK_FORMAT_YCbCr_422_P:
+	case RK_FORMAT_YCbCr_420_SP:
+	case RK_FORMAT_YCbCr_420_P:
+	case RK_FORMAT_YCrCb_422_SP:
+	case RK_FORMAT_YCrCb_422_P:
+	case RK_FORMAT_YCrCb_420_SP:
+	case RK_FORMAT_YCrCb_420_P:
+		ret = true;
+		break;
+	}
+	return ret;
+}
+
+static const char *rga_get_format_name(uint32_t format)
+{
+	switch (format) {
+	case RK_FORMAT_RGBA_8888:
+		return "RGBA8888";
+	case RK_FORMAT_RGBX_8888:
+		return "RGBX8888";
+	case RK_FORMAT_RGB_888:
+		return "RGB888";
+	case RK_FORMAT_BGRA_8888:
+		return "BGRA8888";
+	case RK_FORMAT_RGB_565:
+		return "RGB565";
+	case RK_FORMAT_RGBA_5551:
+		return "RGBA5551";
+	case RK_FORMAT_RGBA_4444:
+		return "RGBA4444";
+	case RK_FORMAT_BGR_888:
+		return "BGR888";
+
+	case RK_FORMAT_YCbCr_422_SP:
+		return "YCbCr422SP";
+	case RK_FORMAT_YCbCr_422_P:
+		return "YCbCr422P";
+	case RK_FORMAT_YCbCr_420_SP:
+		return "YCbCr420SP";
+	case RK_FORMAT_YCbCr_420_P:
+		return "YCbCr420P";
+	case RK_FORMAT_YCrCb_422_SP:
+		return "YCrCb422SP";
+	case RK_FORMAT_YCrCb_422_P:
+		return "YCrCb422P";
+	case RK_FORMAT_YCrCb_420_SP:
+		return "YCrCb420SP";
+	case RK_FORMAT_YCrCb_420_P:
+		return "YCrCb420P";
+
+	case RK_FORMAT_BPP1:
+		return "BPP1";
+	case RK_FORMAT_BPP2:
+		return "BPP2";
+	case RK_FORMAT_BPP4:
+		return "BPP4";
+	case RK_FORMAT_BPP8:
+		return "BPP8";
+	case RK_FORMAT_YCbCr_420_SP_10B:
+		return "YCrCb420SP10B";
+	case RK_FORMAT_YCrCb_420_SP_10B:
+		return "YCbCr420SP10B";
+	default:
+		return "UNF";
+	}
+}
+
+static void print_debug_info(struct rga_req *req)
+{
+	DBG("render_mode %s, rotate_mode %s, blit mode %d\n",
+	    rga_get_render_mode_str(req->render_mode),
+	    rga_get_rotate_mode_str(req), req->bsfilter_flag);
+	DBG("src : y=%lx uv=%lx v=%lx format=%s aw=%d ah=%d vw=%d vh=%d xoff=%d yoff=%d\n",
+	    req->src.yrgb_addr, req->src.uv_addr, req->src.v_addr,
+	    rga_get_format_name(req->src.format),
+	    req->src.act_w, req->src.act_h, req->src.vir_w, req->src.vir_h,
+	    req->src.x_offset, req->src.y_offset);
+	DBG("dst : y=%lx uv=%lx v=%lx format=%s aw=%d ah=%d vw=%d vh=%d xoff=%d yoff=%d\n",
+	    req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+	    rga_get_format_name(req->dst.format),
+	    req->dst.act_w, req->dst.act_h, req->dst.vir_w, req->dst.vir_h,
+	    req->dst.x_offset, req->dst.y_offset);
+	DBG("mmuflg = %.8x, mmuen is %d\n", req->mmu_info.mmu_flag, req->mmu_info.mmu_en);
+	DBG("clip.xmin = %d, clip.xmax = %d, clip.ymin = %d, clip.ymax = %d\n",
+	    req->clip.xmin, req->clip.xmax, req->clip.ymin, req->clip.ymax);
+	DBG("alpha: flag %.8x mode=%.8x\n", req->alpha_rop_flag, req->alpha_rop_mode);
+	DBG("blend mode:%s\n", rga_get_blend_mode_str(req->alpha_rop_flag));
+	DBG("yuv2rgb mode:%x\n", req->yuv2rgb_mode);
+}
+
+static int rga_align_check(struct rga_req *req)
+{
+	if (rga_is_yuv10bit_format(req->src.format)) {
+		if ((req->src.vir_w % 16) || (req->src.x_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_h % 2) || (req->src.vir_h % 2))
+			DBG("err src wstride is not align to 16 or yuv not align to 2");
+	}
+	if (rga_is_yuv10bit_format(req->dst.format)) {
+		if ((req->dst.vir_w % 16) || (req->dst.x_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_h % 2) || (req->dst.vir_h % 2))
+			DBG("err dst wstride is not align to 16 or yuv not align to 2");
+	}
+	if (rga_is_yuv8bit_format(req->src.format)) {
+		if ((req->src.vir_w % 8) || (req->src.x_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_h % 2) || (req->src.vir_h % 2))
+			DBG("err src wstride is not align to 8 or yuv not align to 2");
+	}
+	if (rga_is_yuv8bit_format(req->dst.format)) {
+		if ((req->dst.vir_w % 8) || (req->dst.x_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_h % 2) || (req->dst.vir_h % 2))
+			DBG("err dst wstride is not align to 8 or yuv not align to 2");
+	}
+	DBG("rga align check over!\n");
+	return 0;
+}
+
+static int rga_memory_check(void *vaddr, u32 w, u32 h, u32 format, int fd)
+{
+	int bits = 32;
+	int temp_data = 0;
+	void *one_line = kzalloc(w * 4, GFP_KERNEL);
+
+	if (!one_line) {
+		pr_err("kzalloc fail %s[%d]\n", __func__, __LINE__);
+		return 0;
+	}
+
+	switch (format) {
+	case RK_FORMAT_RGBA_8888:
+	case RK_FORMAT_RGBX_8888:
+	case RK_FORMAT_BGRA_8888:
+		bits = 32;
+		break;
+	case RK_FORMAT_RGB_888:
+	case RK_FORMAT_BGR_888:
+		bits = 24;
+		break;
+	case RK_FORMAT_RGB_565:
+	case RK_FORMAT_RGBA_5551:
+	case RK_FORMAT_RGBA_4444:
+	case RK_FORMAT_YCbCr_422_SP:
+	case RK_FORMAT_YCbCr_422_P:
+	case RK_FORMAT_YCrCb_422_SP:
+	case RK_FORMAT_YCrCb_422_P:
+		bits = 16;
+		break;
+	case RK_FORMAT_YCbCr_420_SP:
+	case RK_FORMAT_YCbCr_420_P:
+	case RK_FORMAT_YCrCb_420_SP:
+	case RK_FORMAT_YCrCb_420_P:
+		bits = 12;
+		break;
+	case RK_FORMAT_YCbCr_420_SP_10B:
+	case RK_FORMAT_YCrCb_420_SP_10B:
+		bits = 15;
+		break;
+	default:
+		DBG("un know format\n");
+		kfree(one_line);
+		return -1;
+	}
+	temp_data = w * (h - 1) * bits / 8;
+	if (fd > 0) {
+		DBG("vaddr is%p, bits is %d, fd check\n", vaddr, bits);
+		memcpy(one_line, (char *)vaddr + temp_data, w * bits / 8);
+		DBG("fd check ok\n");
+	} else {
+		DBG("vir addr memory check.\n");
+		memcpy((void *)((char *)vaddr + temp_data), one_line, w * bits / 8);
+		DBG("vir addr check ok.\n");
+	}
+	kfree(one_line);
+	return 0;
+}
+#endif
+
+static inline void rga_write(u32 b, u32 r)
+{
+	__raw_writel(b, rga_drvdata->rga_base + r);
+}
+
+static inline u32 rga_read(u32 r)
+{
+	return __raw_readl(rga_drvdata->rga_base + r);
+}
+
+static void rga_soft_reset(void)
+{
+	u32 i;
+	u32 reg;
+
+	rga_write(1, RGA_SYS_CTRL); //RGA_SYS_CTRL
+
+	for(i = 0; i < RGA_RESET_TIMEOUT; i++)
+	{
+		reg = rga_read(RGA_SYS_CTRL) & 1; //RGA_SYS_CTRL
+
+		if(reg == 0)
+			break;
+
+		udelay(1);
+	}
+
+	if(i == RGA_RESET_TIMEOUT)
+		ERR("soft reset timeout.\n");
+}
+
+static void rga_dump(void)
+{
+	int running;
+    struct rga_reg *reg, *reg_tmp;
+    rga_session *session, *session_tmp;
+
+	running = atomic_read(&rga_service.total_running);
+	printk("rga total_running %d\n", running);
+
+    #if 0
+
+    /* Dump waiting list info */
+    if (!list_empty(&rga_service.waiting))
+    {
+        list_head	*next;
+
+        next = &rga_service.waiting;
+
+        printk("rga_service dump waiting list\n");
+
+        do
+        {
+            reg = list_entry(next->next, struct rga_reg, status_link);
+            running = atomic_read(&reg->session->task_running);
+            num_done = atomic_read(&reg->session->num_done);
+            printk("rga session pid %d, done %d, running %d\n", reg->session->pid, num_done, running);
+            next = next->next;
+        }
+        while(!list_empty(next));
+    }
+
+    /* Dump running list info */
+    if (!list_empty(&rga_service.running))
+    {
+        printk("rga_service dump running list\n");
+
+        list_head	*next;
+
+        next = &rga_service.running;
+        do
+        {
+            reg = list_entry(next->next, struct rga_reg, status_link);
+            running = atomic_read(&reg->session->task_running);
+            num_done = atomic_read(&reg->session->num_done);
+            printk("rga session pid %d, done %d, running %d:\n", reg->session->pid, num_done, running);
+            next = next->next;
+        }
+        while(!list_empty(next));
+    }
+    #endif
+
+	list_for_each_entry_safe(session, session_tmp, &rga_service.session, list_session)
+    {
+		printk("session pid %d:\n", session->pid);
+		running = atomic_read(&session->task_running);
+		printk("task_running %d\n", running);
+		list_for_each_entry_safe(reg, reg_tmp, &session->waiting, session_link)
+        {
+			printk("waiting register set 0x %.lu\n", (unsigned long)reg);
+		}
+		list_for_each_entry_safe(reg, reg_tmp, &session->running, session_link)
+        {
+			printk("running register set 0x %.lu\n", (unsigned long)reg);
+		}
+	}
+}
+
+static inline void rga_queue_power_off_work(void)
+{
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	queue_delayed_work(system_wq, &rga_drvdata->power_off_work, RGA_POWER_OFF_DELAY);
+#else
+	queue_delayed_work(system_nrt_wq, &rga_drvdata->power_off_work, RGA_POWER_OFF_DELAY);
+#endif
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_power_on(void)
+{
+	static ktime_t last;
+	ktime_t now = ktime_get();
+
+	if (ktime_to_ns(ktime_sub(now, last)) > NSEC_PER_SEC) {
+		cancel_delayed_work_sync(&rga_drvdata->power_off_work);
+		rga_queue_power_off_work();
+		last = now;
+	}
+	if (rga_service.enable)
+		return;
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	clk_prepare_enable(rga_drvdata->aclk_rga);
+	clk_prepare_enable(rga_drvdata->hclk_rga);
+	pm_runtime_get_sync(rga_drvdata->dev);
+#else
+	clk_prepare_enable(rga_drvdata->aclk_rga);
+	clk_prepare_enable(rga_drvdata->hclk_rga);
+	if (rga_drvdata->pd_rga)
+		clk_prepare_enable(rga_drvdata->pd_rga);
+#endif
+
+	wake_lock(&rga_drvdata->wake_lock);
+	rga_service.enable = true;
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_power_off(void)
+{
+	int total_running;
+
+	if (!rga_service.enable) {
+		return;
+	}
+
+	total_running = atomic_read(&rga_service.total_running);
+	if (total_running) {
+		pr_err("power off when %d task running!!\n", total_running);
+		mdelay(50);
+		pr_err("delay 50 ms for running task\n");
+		rga_dump();
+	}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_put(rga_drvdata->dev);
+	clk_disable_unprepare(rga_drvdata->aclk_rga);
+	clk_disable_unprepare(rga_drvdata->hclk_rga);
+#else
+	if (rga_drvdata->pd_rga)
+		clk_disable_unprepare(rga_drvdata->pd_rga);
+	clk_disable_unprepare(rga_drvdata->aclk_rga);
+	clk_disable_unprepare(rga_drvdata->hclk_rga);
+#endif
+	wake_unlock(&rga_drvdata->wake_lock);
+	rga_service.enable = false;
+}
+
+static void rga_power_off_work(struct work_struct *work)
+{
+	if (mutex_trylock(&rga_service.lock)) {
+		rga_power_off();
+		mutex_unlock(&rga_service.lock);
+	} else {
+		/* Come back later if the device is busy... */
+
+		rga_queue_power_off_work();
+	}
+}
+
+static int rga_flush(rga_session *session, unsigned long arg)
+{
+    int ret = 0;
+    int ret_timeout;
+
+    #if RGA_TEST_FLUSH_TIME
+    ktime_t start;
+    ktime_t end;
+    start = ktime_get();
+    #endif
+
+    ret_timeout = wait_event_timeout(session->wait, atomic_read(&session->done), RGA_TIMEOUT_DELAY);
+
+	if (unlikely(ret_timeout < 0)) {
+		//pr_err("flush pid %d wait task ret %d\n", session->pid, ret);
+        mutex_lock(&rga_service.lock);
+        rga_del_running_list();
+        mutex_unlock(&rga_service.lock);
+        ret = ret_timeout;
+	} else if (0 == ret_timeout) {
+		//pr_err("flush pid %d wait %d task done timeout\n", session->pid, atomic_read(&session->task_running));
+        //printk("bus  = %.8x\n", rga_read(RGA_INT));
+        mutex_lock(&rga_service.lock);
+        rga_del_running_list_timeout();
+        rga_try_set_reg();
+        mutex_unlock(&rga_service.lock);
+		ret = -ETIMEDOUT;
+	}
+
+#if RGA_TEST_FLUSH_TIME
+    end = ktime_get();
+    end = ktime_sub(end, start);
+    printk("one flush wait time %d\n", (int)ktime_to_us(end));
+#endif
+
+	return ret;
+}
+
+
+static int rga_get_result(rga_session *session, unsigned long arg)
+{
+	//printk("rga_get_result %d\n",rga_drvdata->rga_result);
+
+    int ret = 0;
+
+    int num_done;
+
+    num_done = atomic_read(&session->num_done);
+
+	if (unlikely(copy_to_user((void __user *)arg, &num_done, sizeof(int)))) {
+			printk("copy_to_user failed\n");
+			ret =  -EFAULT;
+		}
+	return ret;
+}
+
+
+static int rga_check_param(const struct rga_req *req)
+{
+	/*RGA can support up to 8192*8192 resolution in RGB format,but we limit the image size to 8191*8191 here*/
+	//check src width and height
+
+    if(!((req->render_mode == color_fill_mode) || (req->render_mode == line_point_drawing_mode)))
+    {
+    	if (unlikely((req->src.act_w <= 0) || (req->src.act_w > 8191) || (req->src.act_h <= 0) || (req->src.act_h > 8191)))
+        {
+    		printk("invalid source resolution act_w = %d, act_h = %d\n", req->src.act_w, req->src.act_h);
+    		return  -EINVAL;
+    	}
+    }
+
+    if(!((req->render_mode == color_fill_mode) || (req->render_mode == line_point_drawing_mode)))
+    {
+    	if (unlikely((req->src.vir_w <= 0) || (req->src.vir_w > 8191) || (req->src.vir_h <= 0) || (req->src.vir_h > 8191)))
+        {
+    		printk("invalid source resolution vir_w = %d, vir_h = %d\n", req->src.vir_w, req->src.vir_h);
+    		return  -EINVAL;
+    	}
+    }
+
+	//check dst width and height
+	if (unlikely((req->dst.act_w <= 0) || (req->dst.act_w > 2048) || (req->dst.act_h <= 0) || (req->dst.act_h > 2048)))
+    {
+		printk("invalid destination resolution act_w = %d, act_h = %d\n", req->dst.act_w, req->dst.act_h);
+		return	-EINVAL;
+	}
+
+    if (unlikely((req->dst.vir_w <= 0) || (req->dst.vir_w > 4096) || (req->dst.vir_h <= 0) || (req->dst.vir_h > 2048)))
+    {
+		printk("invalid destination resolution vir_w = %d, vir_h = %d\n", req->dst.vir_w, req->dst.vir_h);
+		return	-EINVAL;
+	}
+
+	//check src_vir_w
+	if(unlikely(req->src.vir_w < req->src.act_w)){
+		printk("invalid src_vir_w act_w = %d, vir_w = %d\n", req->src.act_w, req->src.vir_w);
+		return	-EINVAL;
+	}
+
+	//check dst_vir_w
+	if(unlikely(req->dst.vir_w < req->dst.act_w)){
+        if(req->rotate_mode != 1)
+        {
+		    printk("invalid dst_vir_w act_h = %d, vir_h = %d\n", req->dst.act_w, req->dst.vir_w);
+		    return	-EINVAL;
+        }
+	}
+
+	return 0;
+}
+
+static void rga_copy_reg(struct rga_reg *reg, uint32_t offset)
+{
+    uint32_t i;
+    uint32_t *cmd_buf;
+    uint32_t *reg_p;
+
+    if(atomic_read(&reg->session->task_running) != 0)
+    {
+        printk(KERN_ERR "task_running is no zero\n");
+    }
+
+    atomic_add(1, &rga_service.cmd_num);
+	atomic_add(1, &reg->session->task_running);
+
+    cmd_buf = (uint32_t *)rga_service.cmd_buff + offset*32;
+    reg_p = (uint32_t *)reg->cmd_reg;
+
+    for(i=0; i<32; i++)
+        cmd_buf[i] = reg_p[i];
+}
+
+static struct rga_reg * rga_reg_init(rga_session *session, struct rga_req *req)
+{
+    int32_t ret;
+	struct rga_reg *reg = kzalloc(sizeof(struct rga_reg), GFP_KERNEL);
+	if (NULL == reg) {
+		pr_err("kmalloc fail in rga_reg_init\n");
+		return NULL;
+	}
+
+    reg->session = session;
+	INIT_LIST_HEAD(&reg->session_link);
+	INIT_LIST_HEAD(&reg->status_link);
+
+    reg->MMU_base = NULL;
+
+    if (req->mmu_info.mmu_en)
+    {
+        ret = rga_set_mmu_info(reg, req);
+        if(ret < 0)
+        {
+            printk("%s, [%d] set mmu info error \n", __FUNCTION__, __LINE__);
+            if(reg != NULL)
+            {
+                kfree(reg);
+            }
+            return NULL;
+        }
+    }
+
+    if(RGA_gen_reg_info(req, (uint8_t *)reg->cmd_reg) == -1)
+    {
+        printk("gen reg info error\n");
+        if(reg != NULL)
+        {
+            kfree(reg);
+        }
+        return NULL;
+    }
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	reg->sg_src = req->sg_src;
+	reg->sg_dst = req->sg_dst;
+	reg->attach_src = req->attach_src;
+	reg->attach_dst = req->attach_dst;
+#endif
+
+    mutex_lock(&rga_service.lock);
+	list_add_tail(&reg->status_link, &rga_service.waiting);
+	list_add_tail(&reg->session_link, &session->waiting);
+	mutex_unlock(&rga_service.lock);
+
+    return reg;
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_reg_deinit(struct rga_reg *reg)
+{
+	list_del_init(&reg->session_link);
+	list_del_init(&reg->status_link);
+	kfree(reg);
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_reg_from_wait_to_run(struct rga_reg *reg)
+{
+	list_del_init(&reg->status_link);
+	list_add_tail(&reg->status_link, &rga_service.running);
+
+	list_del_init(&reg->session_link);
+	list_add_tail(&reg->session_link, &reg->session->running);
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_service_session_clear(rga_session *session)
+{
+	struct rga_reg *reg, *n;
+
+    list_for_each_entry_safe(reg, n, &session->waiting, session_link)
+    {
+		rga_reg_deinit(reg);
+	}
+
+    list_for_each_entry_safe(reg, n, &session->running, session_link)
+    {
+		rga_reg_deinit(reg);
+	}
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_try_set_reg(void)
+{
+    struct rga_reg *reg ;
+
+    if (list_empty(&rga_service.running))
+    {
+        if (!list_empty(&rga_service.waiting))
+        {
+            /* RGA is idle */
+            reg = list_entry(rga_service.waiting.next, struct rga_reg, status_link);
+
+            rga_power_on();
+            udelay(1);
+
+            rga_copy_reg(reg, 0);
+            rga_reg_from_wait_to_run(reg);
+			rga_dma_flush_range(&rga_service.cmd_buff[0], &rga_service.cmd_buff[32]);
+
+            rga_soft_reset();
+
+            rga_write(0x0, RGA_SYS_CTRL);
+            rga_write(0, RGA_MMU_CTRL);
+
+            /* CMD buff */
+            rga_write(virt_to_phys(rga_service.cmd_buff), RGA_CMD_ADDR);
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_REG) {
+                //printk(KERN_DEBUG "cmd_addr = %.8x\n", rga_read(RGA_CMD_ADDR));
+                uint32_t i;
+                uint32_t *p;
+                p = rga_service.cmd_buff;
+                printk("CMD_REG\n");
+                for (i=0; i<7; i++)
+                    printk("%.8x %.8x %.8x %.8x\n", p[0 + i*4], p[1+i*4], p[2 + i*4], p[3 + i*4]);
+                printk("%.8x %.8x\n", p[0 + i*4], p[1+i*4]);
+	}
+#endif
+
+            /* master mode */
+            rga_write((0x1<<2)|(0x1<<3), RGA_SYS_CTRL);
+
+            /* All CMD finish int */
+            rga_write(rga_read(RGA_INT)|(0x1<<10)|(0x1<<9)|(0x1<<8), RGA_INT);
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_TIME)
+		rga_start = ktime_get();
+#endif
+
+            /* Start proc */
+            atomic_set(&reg->session->done, 0);
+            rga_write(0x1, RGA_CMD_CTRL);
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_REG) {
+                uint32_t i;
+                printk("CMD_READ_BACK_REG\n");
+                for (i=0; i<7; i++)
+                    printk("%.8x %.8x %.8x %.8x\n", rga_read(0x100 + i*16 + 0),
+                            rga_read(0x100 + i*16 + 4), rga_read(0x100 + i*16 + 8), rga_read(0x100 + i*16 + 12));
+                printk("%.8x %.8x\n", rga_read(0x100 + i*16 + 0), rga_read(0x100 + i*16 + 4));
+	}
+#endif
+        }
+    }
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static int rga_put_dma_buf(struct rga_req *req, struct rga_reg *reg)
+{
+	struct dma_buf_attachment *attach = NULL;
+	struct sg_table *sgt = NULL;
+	struct dma_buf *dma_buf = NULL;
+
+	if (!req && !reg)
+		return -EINVAL;
+
+	attach = (!reg) ? req->attach_src : reg->attach_src;
+	sgt = (!reg) ? req->sg_src : reg->sg_src;
+	if (attach && sgt)
+		dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+	if (attach) {
+		dma_buf = attach->dmabuf;
+		dma_buf_detach(dma_buf, attach);
+		dma_buf_put(dma_buf);
+	}
+
+	attach = (!reg) ? req->attach_dst : reg->attach_dst;
+	sgt = (!reg) ? req->sg_dst : reg->sg_dst;
+	if (attach && sgt)
+		dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+	if (attach) {
+		dma_buf = attach->dmabuf;
+		dma_buf_detach(dma_buf, attach);
+		dma_buf_put(dma_buf);
+	}
+
+	return 0;
+}
+#endif
+/* Caller must hold rga_service.lock */
+static void rga_del_running_list(void)
+{
+    struct rga_reg *reg;
+
+    while(!list_empty(&rga_service.running))
+    {
+        reg = list_entry(rga_service.running.next, struct rga_reg, status_link);
+
+        if(reg->MMU_len != 0)
+        {
+            if (rga_mmu_buf.back + reg->MMU_len > 2*rga_mmu_buf.size)
+                rga_mmu_buf.back = reg->MMU_len + rga_mmu_buf.size;
+            else
+                rga_mmu_buf.back += reg->MMU_len;
+        }
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+		rga_put_dma_buf(NULL, reg);
+#endif
+
+        atomic_sub(1, &reg->session->task_running);
+        atomic_sub(1, &rga_service.total_running);
+
+        if(list_empty(&reg->session->waiting))
+        {
+            atomic_set(&reg->session->done, 1);
+            wake_up(&reg->session->wait);
+        }
+
+        rga_reg_deinit(reg);
+    }
+}
+
+/* Caller must hold rga_service.lock */
+static void rga_del_running_list_timeout(void)
+{
+    struct rga_reg *reg;
+
+    while(!list_empty(&rga_service.running))
+    {
+        reg = list_entry(rga_service.running.next, struct rga_reg, status_link);
+
+        if(reg->MMU_len != 0)
+        {
+            if (rga_mmu_buf.back + reg->MMU_len > 2*rga_mmu_buf.size)
+                rga_mmu_buf.back = reg->MMU_len + rga_mmu_buf.size;
+            else
+                rga_mmu_buf.back += reg->MMU_len;
+        }
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+		rga_put_dma_buf(NULL, reg);
+#endif
+        atomic_sub(1, &reg->session->task_running);
+        atomic_sub(1, &rga_service.total_running);
+
+        //printk("RGA soft reset for timeout process\n");
+        rga_soft_reset();
+
+
+        #if 0
+        printk("RGA_INT is %.8x\n", rga_read(RGA_INT));
+        printk("reg->session->task_running = %d\n", atomic_read(&reg->session->task_running));
+        printk("rga_service.total_running  = %d\n", atomic_read(&rga_service.total_running));
+
+        print_info(&reg->req);
+
+        {
+            uint32_t *p, i;
+            p = reg->cmd_reg;
+            for (i=0; i<7; i++)
+                printk("%.8x %.8x %.8x %.8x\n", p[0 + i*4], p[1+i*4], p[2 + i*4], p[3 + i*4]);
+
+        }
+        #endif
+
+        if(list_empty(&reg->session->waiting))
+        {
+            atomic_set(&reg->session->done, 1);
+            wake_up(&reg->session->wait);
+        }
+
+        rga_reg_deinit(reg);
+    }
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+static int rga_convert_dma_buf(struct rga_req *req)
+{
+	struct ion_handle *hdl;
+	ion_phys_addr_t phy_addr;
+	size_t len;
+	int ret;
+	u32 src_offset, dst_offset;
+	void *vaddr;
+
+	req->sg_src  = NULL;
+	req->sg_dst  = NULL;
+
+	src_offset = req->line_draw_info.flag;
+	dst_offset = req->line_draw_info.line_width;
+
+	if (req->src.yrgb_addr) {
+		hdl = ion_import_dma_buf(rga_drvdata->ion_client, req->src.yrgb_addr);
+		if (IS_ERR(hdl)) {
+		ret = PTR_ERR(hdl);
+		pr_err("RGA ERROR ion buf handle\n");
+		return ret;
+		}
+
+	if (req->src.uv_addr) {
+		if (RGA_TEST_MSG)
+			pr_err("WARNING : don't input viraddrs when already input fd !\n");
+		req->src.uv_addr = 0;
+	}
+
+#if RGA_DEBUGFS
+	if (RGA_CHECK_MODE) {
+		vaddr = ion_map_kernel(rga_drvdata->ion_client, hdl);
+		if (vaddr)
+			rga_memory_check(vaddr, req->src.vir_h, req->src.vir_w,
+					req->src.format, req->src.yrgb_addr);
+		ion_unmap_kernel(rga_drvdata->ion_client, hdl);
+	}
+#endif
+        if ((req->mmu_info.mmu_flag >> 8) & 1) {
+            req->sg_src = ion_sg_table(rga_drvdata->ion_client, hdl);
+            req->src.yrgb_addr = req->src.uv_addr;
+            req->src.uv_addr = req->src.yrgb_addr + (req->src.vir_w * req->src.vir_h);
+            req->src.v_addr = req->src.uv_addr + (req->src.vir_w * req->src.vir_h)/4;
+        }
+        else {
+            ion_phys(rga_drvdata->ion_client, hdl, &phy_addr, &len);
+            req->src.yrgb_addr = phy_addr + src_offset;
+            req->src.uv_addr = req->src.yrgb_addr + (req->src.vir_w * req->src.vir_h);
+            req->src.v_addr = req->src.uv_addr + (req->src.vir_w * req->src.vir_h)/4;
+        }
+        ion_free(rga_drvdata->ion_client, hdl);
+    }
+    else {
+        req->src.yrgb_addr = req->src.uv_addr;
+        req->src.uv_addr = req->src.yrgb_addr + (req->src.vir_w * req->src.vir_h);
+        req->src.v_addr = req->src.uv_addr + (req->src.vir_w * req->src.vir_h)/4;
+    }
+
+    if(req->dst.yrgb_addr) {
+        hdl = ion_import_dma_buf(rga_drvdata->ion_client, req->dst.yrgb_addr);
+        if (IS_ERR(hdl)) {
+            ret = PTR_ERR(hdl);
+            printk("RGA2 ERROR ion buf handle\n");
+            return ret;
+        }
+
+	if (req->dst.uv_addr) {
+		if (RGA_TEST_MSG)
+			pr_err("WARNING : don't input viraddrs when already input fd !\n");
+		req->dst.uv_addr = 0;
+	}
+
+#if RGA_DEBUGFS
+	if (RGA_CHECK_MODE) {
+		vaddr = ion_map_kernel(rga_drvdata->ion_client, hdl);
+		if (vaddr)
+			rga_memory_check(vaddr, req->src.vir_h, req->src.vir_w,
+				 req->src.format, req->src.yrgb_addr);
+		ion_unmap_kernel(rga_drvdata->ion_client, hdl);
+	}
+#endif
+        if ((req->mmu_info.mmu_flag >> 10) & 1) {
+            req->sg_dst = ion_sg_table(rga_drvdata->ion_client, hdl);
+            req->dst.yrgb_addr = req->dst.uv_addr;
+            req->dst.uv_addr = req->dst.yrgb_addr + (req->dst.vir_w * req->dst.vir_h);
+            req->dst.v_addr = req->dst.uv_addr + (req->dst.vir_w * req->dst.vir_h)/4;
+        }
+        else {
+            ion_phys(rga_drvdata->ion_client, hdl, &phy_addr, &len);
+            req->dst.yrgb_addr = phy_addr + dst_offset;
+            req->dst.uv_addr = req->dst.yrgb_addr + (req->dst.vir_w * req->dst.vir_h);
+            req->dst.v_addr = req->dst.uv_addr + (req->dst.vir_w * req->dst.vir_h)/4;
+        }
+        ion_free(rga_drvdata->ion_client, hdl);
+    }
+    else {
+        req->dst.yrgb_addr = req->dst.uv_addr;
+        req->dst.uv_addr = req->dst.yrgb_addr + (req->dst.vir_w * req->dst.vir_h);
+        req->dst.v_addr = req->dst.uv_addr + (req->dst.vir_w * req->dst.vir_h)/4;
+    }
+
+    return 0;
+}
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static int rga_get_img_info(rga_img_info_t *img,
+			     u8 mmu_flag,
+			     struct sg_table **psgt,
+			     struct dma_buf_attachment **pattach)
+{
+	struct dma_buf_attachment *attach = NULL;
+	struct device *rga_dev = NULL;
+	struct sg_table *sgt = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	struct iosys_map map;
+#endif
+	struct dma_buf *dma_buf = NULL;
+	u32 vir_w, vir_h;
+	int yrgb_addr = -1;
+	int ret = 0;
+	void *vaddr = NULL;
+
+	rga_dev = rga_drvdata->dev;
+	yrgb_addr = (int)img->yrgb_addr;
+	vir_w = img->vir_w;
+	vir_h = img->vir_h;
+
+	if (yrgb_addr > 0) {
+		dma_buf = dma_buf_get(img->yrgb_addr);
+		if (IS_ERR(dma_buf)) {
+			ret = -EINVAL;
+			pr_err("dma_buf_get fail fd[%d]\n", yrgb_addr);
+			return ret;
+		}
+
+		attach = dma_buf_attach(dma_buf, rga_dev);
+		if (IS_ERR(attach)) {
+			dma_buf_put(dma_buf);
+			ret = -EINVAL;
+			pr_err("Failed to attach dma_buf\n");
+			return ret;
+		}
+#if RGA_DEBUGFS
+	if (RGA_CHECK_MODE) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		ret = dma_buf_vmap(dma_buf, &map);
+		vaddr = ret ? NULL : map.vaddr;
+#else
+		vaddr = dma_buf_vmap(dma_buf);
+#endif
+		if (vaddr)
+			rga_memory_check(vaddr, img->vir_w, img->vir_h,
+					 img->format, img->yrgb_addr);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		dma_buf_vunmap(dma_buf, &map);
+#else
+		dma_buf_vunmap(dma_buf, vaddr);
+#endif
+	}
+#endif
+		*pattach = attach;
+		sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+		if (IS_ERR(sgt)) {
+			ret = -EINVAL;
+			pr_err("Failed to map src attachment\n");
+			goto err_get_sg;
+		}
+		if (!mmu_flag) {
+			ret = -EINVAL;
+			pr_err("Fix it please enable iommu flag\n");
+			goto err_get_sg;
+		}
+
+		if (mmu_flag) {
+			*psgt = sgt;
+			img->yrgb_addr = img->uv_addr;
+			img->uv_addr = img->yrgb_addr + (vir_w * vir_h);
+			img->v_addr = img->uv_addr + (vir_w * vir_h) / 4;
+		}
+	} else {
+		img->yrgb_addr = img->uv_addr;
+		img->uv_addr = img->yrgb_addr + (vir_w * vir_h);
+		img->v_addr = img->uv_addr + (vir_w * vir_h) / 4;
+	}
+
+	return ret;
+
+err_get_sg:
+	if (sgt)
+		dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+	if (attach) {
+		dma_buf = attach->dmabuf;
+		dma_buf_detach(dma_buf, attach);
+		*pattach = NULL;
+		dma_buf_put(dma_buf);
+	}
+	return ret;
+}
+
+static int rga_get_dma_buf(struct rga_req *req)
+{
+	struct dma_buf *dma_buf = NULL;
+	u8 mmu_flag = 0;
+	int ret = 0;
+
+	req->sg_src = NULL;
+	req->sg_dst = NULL;
+	req->attach_src = NULL;
+	req->attach_dst = NULL;
+	mmu_flag = (req->mmu_info.mmu_flag >> 8) & 1;
+	ret = rga_get_img_info(&req->src, mmu_flag, &req->sg_src,
+				&req->attach_src);
+	if (ret) {
+		pr_err("src:rga_get_img_info fail\n");
+		goto err_src;
+	}
+
+	mmu_flag = (req->mmu_info.mmu_flag >> 10) & 1;
+	ret = rga_get_img_info(&req->dst, mmu_flag, &req->sg_dst,
+				&req->attach_dst);
+	if (ret) {
+		pr_err("dst:rga_get_img_info fail\n");
+		goto err_dst;
+	}
+
+	return ret;
+
+err_dst:
+	if (req->sg_src && req->attach_src) {
+		dma_buf_unmap_attachment(req->attach_src,
+					 req->sg_src, DMA_BIDIRECTIONAL);
+		dma_buf = req->attach_src->dmabuf;
+		dma_buf_detach(dma_buf, req->attach_src);
+		dma_buf_put(dma_buf);
+	}
+err_src:
+
+	return ret;
+}
+#endif
+static struct rga_reg *rga_reg_init_2(rga_session *session, struct rga_req *req0,
+				      struct rga_req *req1)
+{
+	int32_t ret;
+	struct rga_reg *reg0, *reg1;
+
+	reg0 = NULL;
+	reg1 = NULL;
+
+	do {
+		reg0 = kzalloc(sizeof(*reg0), GFP_KERNEL);
+		if (!reg0) {
+			pr_err("%s [%d] kmalloc fail in rga_reg_init\n",
+			       __func__, __LINE__);
+			break;
+		}
+
+		reg1 = kzalloc(sizeof(*reg1), GFP_KERNEL);
+		if (!reg1) {
+			pr_err("%s [%d] kmalloc fail in rga_reg_init\n",
+			       __func__, __LINE__);
+			break;
+		}
+
+		reg0->session = session;
+		INIT_LIST_HEAD(&reg0->session_link);
+		INIT_LIST_HEAD(&reg0->status_link);
+
+		reg1->session = session;
+		INIT_LIST_HEAD(&reg1->session_link);
+		INIT_LIST_HEAD(&reg1->status_link);
+
+		req0->mmu_info.mmu_flag &= (~(1 << 10));
+		if (req0->mmu_info.mmu_en) {
+			ret = rga_set_mmu_info(reg0, req0);
+			if (ret < 0) {
+				pr_err("%s, [%d] set mmu info error\n",
+				       __func__, __LINE__);
+				break;
+			}
+		}
+
+		RGA_gen_reg_info(req0, (uint8_t *)reg0->cmd_reg);
+		req1->mmu_info.mmu_flag &= (~(1 << 8));
+		if (req1->mmu_info.mmu_en) {
+			ret = rga_set_mmu_info(reg1, req1);
+			if (ret < 0) {
+				pr_err("%s, [%d] set mmu info error\n",
+				       __func__, __LINE__);
+				break;
+			}
+		}
+		RGA_gen_reg_info(req1, (uint8_t *)reg1->cmd_reg);
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+		reg1->sg_src = req1->sg_src;
+		reg1->sg_dst = req1->sg_dst;
+		reg1->attach_src = req1->attach_src;
+		reg1->attach_dst = req1->attach_dst;
+#endif
+
+		mutex_lock(&rga_service.lock);
+		list_add_tail(&reg0->status_link, &rga_service.waiting);
+		list_add_tail(&reg0->session_link, &session->waiting);
+		list_add_tail(&reg1->status_link, &rga_service.waiting);
+		list_add_tail(&reg1->session_link, &session->waiting);
+		mutex_unlock(&rga_service.lock);
+
+		return reg1;
+
+	} while (0);
+
+	if (reg0)
+		kfree(reg0);
+	if (reg1)
+		kfree(reg1);
+	return NULL;
+}
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+static void rga_mem_addr_sel(struct rga_req *req)
+{
+	switch (req->src.format) {
+	case RK_FORMAT_YCbCr_422_SP:
+		break;
+	case RK_FORMAT_YCbCr_422_P:
+		break;
+	case RK_FORMAT_YCbCr_420_SP:
+		if ((req->src.yrgb_addr > 0xc0000000) && (req->src.uv_addr > 0xc0000000) &&
+		    (req->dst.yrgb_addr > 0xc0000000)) {
+			req->src.yrgb_addr = req->src.yrgb_addr - 0x60000000;
+			req->src.uv_addr = req->src.uv_addr - 0x60000000;
+			req->dst.yrgb_addr = req->dst.yrgb_addr - 0x60000000;
+			req->mmu_info.mmu_en = 0;
+			req->mmu_info.mmu_flag &= 0xfffe;
+	}
+		break;
+	case RK_FORMAT_YCbCr_420_P:
+		break;
+	case RK_FORMAT_YCrCb_422_SP:
+		break;
+	case RK_FORMAT_YCrCb_422_P:
+		break;
+	case RK_FORMAT_YCrCb_420_SP:
+		break;
+	case RK_FORMAT_YCrCb_420_P:
+		break;
+	default:
+		break;
+	}
+}
+#endif
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static int rga_blit(rga_session *session, struct rga_req *req)
+{
+	int ret = -1;
+	int num = 0;
+	struct rga_reg *reg;
+	struct rga_req req2;
+
+	uint32_t saw, sah, daw, dah;
+
+	saw = req->src.act_w;
+	sah = req->src.act_h;
+	daw = req->dst.act_w;
+	dah = req->dst.act_h;
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_MSG)
+		print_debug_info(req);
+	if (RGA_CHECK_MODE) {
+		rga_align_check(req);
+		/*rga_scale_check(req);*/
+	}
+#endif
+	if (rga_get_dma_buf(req)) {
+		pr_err("RGA : DMA buf copy error\n");
+		return -EFAULT;
+	}
+	req->render_mode &= (~RGA_BUF_GEM_TYPE_MASK);
+	do {
+	if ((req->render_mode == bitblt_mode) && (((saw >> 1) >= daw) || ((sah >> 1) >= dah))) {
+			/* generate 2 cmd for pre scale */
+		if (((saw >> 3) > daw) || ((sah >> 3) > dah)) {
+			pr_err("unsupported to scaling less than 1/8\n");
+			goto err_put_dma_buf;
+		}
+		if (((daw >> 3) > saw) || ((dah >> 3) > daw)) {
+			pr_err("unsupported to scaling more than 8\n");
+			goto err_put_dma_buf;
+		}
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req 0 argument is inval\n");
+			goto err_put_dma_buf;
+		}
+
+		ret = RGA_gen_two_pro(req, &req2);
+		if (ret == -EINVAL) {
+			pr_err("RGA_gen_two_pro err\n");
+			goto err_put_dma_buf;
+		}
+
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req 1 argument is inval\n");
+			goto err_put_dma_buf;
+		}
+
+		ret = rga_check_param(&req2);
+		if (ret == -EINVAL) {
+			pr_err("req 2 argument is inval\n");
+			goto err_put_dma_buf;
+		}
+
+		reg = rga_reg_init_2(session, req, &req2);
+		if (!reg) {
+			pr_err("init2 reg fail\n");
+			goto err_put_dma_buf;
+		}
+		num = 2;
+	} else {
+		/* check value if legal */
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req argument is inval\n");
+			goto err_put_dma_buf;
+		}
+
+		reg = rga_reg_init(session, req);
+		if (!reg) {
+			pr_err("init reg fail\n");
+			goto err_put_dma_buf;
+		}
+
+		num = 1;
+	}
+
+	mutex_lock(&rga_service.lock);
+	atomic_add(num, &rga_service.total_running);
+	rga_try_set_reg();
+	mutex_unlock(&rga_service.lock);
+	return 0;
+
+	} while (0);
+
+err_put_dma_buf:
+	rga_put_dma_buf(req, NULL);
+
+	return -EFAULT;
+}
+#else
+static int rga_blit(rga_session *session, struct rga_req *req)
+{
+	int ret = -1;
+	int num = 0;
+	struct rga_reg *reg;
+	struct rga_req req2;
+	uint32_t saw, sah, daw, dah;
+
+	saw = req->src.act_w;
+	sah = req->src.act_h;
+	daw = req->dst.act_w;
+	dah = req->dst.act_h;
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_MSG)
+		print_debug_info(req);
+	if (RGA_CHECK_MODE) {
+		rga_align_check(req);
+		/*rga_scale_check(req);*/
+	}
+#endif
+	if (rga_convert_dma_buf(req)) {
+		pr_err("RGA : DMA buf copy error\n");
+		return -EFAULT;
+	}
+	do {
+	if ((req->render_mode == bitblt_mode) && (((saw >> 1) >= daw) || ((sah >> 1) >= dah))) {
+		/* generate 2 cmd for pre scale */
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req 0 argument is inval\n");
+			break;
+		}
+
+		ret = RGA_gen_two_pro(req, &req2);
+		if (ret == -EINVAL)
+			break;
+
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req 1 argument is inval\n");
+			break;
+		}
+
+		ret = rga_check_param(&req2);
+		if (ret == -EINVAL) {
+			pr_err("req 2 argument is inval\n");
+			break;
+		}
+
+		reg = rga_reg_init_2(session, req, &req2);
+		if (!reg)
+			break;
+		num = 2;
+
+	} else {
+		/* check value if legal */
+		ret = rga_check_param(req);
+		if (ret == -EINVAL) {
+			pr_err("req argument is inval\n");
+			break;
+		}
+
+		if (req->render_mode == bitblt_mode)
+			rga_mem_addr_sel(req);
+
+		reg = rga_reg_init(session, req);
+		if (!reg)
+			break;
+		num = 1;
+	}
+
+	mutex_lock(&rga_service.lock);
+	atomic_add(num, &rga_service.total_running);
+	rga_try_set_reg();
+	mutex_unlock(&rga_service.lock);
+
+	return 0;
+	} while (0);
+
+	return -EFAULT;
+}
+#endif
+
+static int rga_blit_async(rga_session *session, struct rga_req *req)
+{
+	int ret = -1;
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_MSG)
+		DBG("*** rga_blit_async proc ***\n");
+#endif
+	atomic_set(&session->done, 0);
+	ret = rga_blit(session, req);
+	return ret;
+}
+
+static int rga_blit_sync(rga_session *session, struct rga_req *req)
+{
+    int ret = -1;
+    int ret_timeout = 0;
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_MSG)
+		DBG("*** rga_blit_sync proc ***\n");
+#endif
+
+    atomic_set(&session->done, 0);
+    ret = rga_blit(session, req);
+    if(ret < 0)
+        return ret;
+
+    ret_timeout = wait_event_timeout(session->wait, atomic_read(&session->done), RGA_TIMEOUT_DELAY);
+
+    if (unlikely(ret_timeout< 0)) {
+        mutex_lock(&rga_service.lock);
+        rga_del_running_list();
+        mutex_unlock(&rga_service.lock);
+        ret = ret_timeout;
+	}
+    else if (0 == ret_timeout) {
+        mutex_lock(&rga_service.lock);
+        rga_del_running_list_timeout();
+        rga_try_set_reg();
+        mutex_unlock(&rga_service.lock);
+		ret = -ETIMEDOUT;
+	}
+
+#if RGA_DEBUGFS
+	if (RGA_TEST_TIME) {
+		rga_end = ktime_get();
+		rga_end = ktime_sub(rga_end, rga_start);
+		DBG("sync one cmd end time %d us\n", (int)ktime_to_us(rga_end));
+	}
+#endif
+
+    return ret;
+}
+
+
+static long rga_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+    struct rga_req req;
+	int ret = 0;
+    rga_session *session;
+
+	memset(&req, 0x0, sizeof(req));
+    mutex_lock(&rga_service.mutex);
+
+    session = (rga_session *)file->private_data;
+
+	if (NULL == session) {
+        printk("%s [%d] rga thread session is null\n",__FUNCTION__,__LINE__);
+        mutex_unlock(&rga_service.mutex);
+		return -EINVAL;
+	}
+
+	memset(&req, 0x0, sizeof(req));
+#if RGA_DEBUGFS
+	if (RGA_TEST_MSG)
+		DBG("cmd is %s(0x%x)\n", rga_get_cmd_mode_str(cmd), cmd);
+	if (RGA_NONUSE) {
+		mutex_unlock(&rga_service.mutex);
+		return 0;
+	}
+#endif
+	switch (cmd) {
+		case RGA_BLIT_SYNC:
+    		if (unlikely(copy_from_user(&req, (struct rga_req*)arg, sizeof(struct rga_req))))
+            {
+        		ERR("copy_from_user failed\n");
+        		ret = -EFAULT;
+                break;
+        	}
+            ret = rga_blit_sync(session, &req);
+            break;
+		case RGA_BLIT_ASYNC:
+    		if (unlikely(copy_from_user(&req, (struct rga_req*)arg, sizeof(struct rga_req))))
+            {
+        		ERR("copy_from_user failed\n");
+        		ret = -EFAULT;
+                break;
+        	}
+
+            if((atomic_read(&rga_service.total_running) > 16))
+            {
+			    ret = rga_blit_sync(session, &req);
+            }
+            else
+            {
+                ret = rga_blit_async(session, &req);
+            }
+			break;
+		case RGA_FLUSH:
+			ret = rga_flush(session, arg);
+			break;
+        case RGA_GET_RESULT:
+            ret = rga_get_result(session, arg);
+            break;
+        case RGA_GET_VERSION:
+		if (!rga_drvdata->version) {
+			rga_drvdata->version = kzalloc(16, GFP_KERNEL);
+			if (!rga_drvdata->version) {
+				ret = -ENOMEM;
+				break;
+			}
+			rga_power_on();
+			udelay(1);
+			if (rga_read(RGA_VERSION) == 0x02018632)
+				snprintf(rga_drvdata->version, 16, "1.6");
+			else
+				snprintf(rga_drvdata->version, 16, "1.003");
+		}
+
+			ret = copy_to_user((void *)arg, rga_drvdata->version, 16);
+            break;
+		default:
+			ret = -EINVAL;
+			break;
+	}
+
+	mutex_unlock(&rga_service.mutex);
+
+	return ret;
+}
+
+
+long rga_ioctl_kernel(struct rga_req *req)
+{
+	int ret = 0;
+    if (!rga_ioctl_kernel_p) {
+        printk("rga_ioctl_kernel_p is NULL\n");
+        return -1;
+    }
+    else {
+        ret = (*rga_ioctl_kernel_p)(req);
+	    return ret;
+    }
+}
+
+
+long rga_ioctl_kernel_imp(struct rga_req *req)
+{
+	int ret = 0;
+    rga_session *session;
+
+    mutex_lock(&rga_service.mutex);
+
+    session = &rga_session_global;
+
+	if (NULL == session) {
+        printk("%s [%d] rga thread session is null\n",__FUNCTION__,__LINE__);
+        mutex_unlock(&rga_service.mutex);
+		return -EINVAL;
+	}
+
+    ret = rga_blit_sync(session, req);
+
+	mutex_unlock(&rga_service.mutex);
+
+	return ret;
+}
+
+
+static int rga_open(struct inode *inode, struct file *file)
+{
+    rga_session *session = kzalloc(sizeof(rga_session), GFP_KERNEL);
+	if (NULL == session) {
+		pr_err("unable to allocate memory for rga_session.");
+		return -ENOMEM;
+	}
+
+	session->pid = current->pid;
+    //printk(KERN_DEBUG  "+");
+
+	INIT_LIST_HEAD(&session->waiting);
+	INIT_LIST_HEAD(&session->running);
+	INIT_LIST_HEAD(&session->list_session);
+	init_waitqueue_head(&session->wait);
+	mutex_lock(&rga_service.lock);
+	list_add_tail(&session->list_session, &rga_service.session);
+	mutex_unlock(&rga_service.lock);
+	atomic_set(&session->task_running, 0);
+    atomic_set(&session->num_done, 0);
+
+	file->private_data = (void *)session;
+
+    //DBG("*** rga dev opened by pid %d *** \n", session->pid);
+	return nonseekable_open(inode, file);
+
+}
+
+static int rga_release(struct inode *inode, struct file *file)
+{
+    int task_running;
+	rga_session *session = (rga_session *)file->private_data;
+	if (NULL == session)
+		return -EINVAL;
+    //printk(KERN_DEBUG  "-");
+	task_running = atomic_read(&session->task_running);
+
+    if (task_running)
+    {
+		pr_err("rga_service session %d still has %d task running when closing\n", session->pid, task_running);
+		msleep(100);
+	}
+
+	wake_up(&session->wait);
+	mutex_lock(&rga_service.lock);
+	list_del(&session->list_session);
+	rga_service_session_clear(session);
+	kfree(session);
+	mutex_unlock(&rga_service.lock);
+
+    //DBG("*** rga dev close ***\n");
+	return 0;
+}
+
+static irqreturn_t rga_irq_thread(int irq, void *dev_id)
+{
+#if RGA_DEBUGFS
+	if (RGA_INT_FLAG)
+		DBG("irqthread INT[%x], STATS[%x]\n", rga_read(RGA_INT), rga_read(RGA_STATUS));
+#endif
+	mutex_lock(&rga_service.lock);
+	if (rga_service.enable) {
+		rga_del_running_list();
+		rga_try_set_reg();
+	}
+	mutex_unlock(&rga_service.lock);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t rga_irq(int irq,  void *dev_id)
+{
+#if RGA_DEBUGFS
+	if (RGA_INT_FLAG)
+		DBG("irq INT[%x], STATS[%x]\n", rga_read(RGA_INT), rga_read(RGA_STATUS));
+#endif
+	/*if error interrupt then soft reset hardware*/
+	if (rga_read(RGA_INT) & 0x03) {
+		pr_err("Err irq INT[%x], STATS[%x]\n", rga_read(RGA_INT), rga_read(RGA_STATUS));
+		rga_soft_reset();
+	}
+	/*clear INT */
+	rga_write(rga_read(RGA_INT) | (0x1<<6) | (0x1<<7) | (0x1<<5) | (0x1<<4), RGA_INT);
+
+	return IRQ_WAKE_THREAD;
+}
+
+struct file_operations rga_fops = {
+	.owner		= THIS_MODULE,
+	.open		= rga_open,
+	.release	= rga_release,
+	.unlocked_ioctl		= rga_ioctl,
+};
+
+static struct miscdevice rga_dev ={
+    .minor = RGA_MAJOR,
+    .name  = "rga",
+    .fops  = &rga_fops,
+};
+
+#if defined(CONFIG_OF)
+static const struct of_device_id rockchip_rga_dt_ids[] = {
+	{ .compatible = "rockchip,rk312x-rga", },
+	{},
+};
+#endif
+
+static int rga_drv_probe(struct platform_device *pdev)
+{
+	struct rga_drvdata *data;
+    struct resource *res;
+    //struct device_node *np = pdev->dev.of_node;
+	int ret = 0;
+
+	mutex_init(&rga_service.lock);
+	mutex_init(&rga_service.mutex);
+	atomic_set(&rga_service.total_running, 0);
+	rga_service.enable = false;
+
+    rga_ioctl_kernel_p = rga_ioctl_kernel_imp;
+
+	data = devm_kzalloc(&pdev->dev, sizeof(struct rga_drvdata), GFP_KERNEL);
+	if(! data) {
+		ERR("failed to allocate driver data.\n");
+		return -ENOMEM;
+	}
+
+	INIT_DELAYED_WORK(&data->power_off_work, rga_power_off_work);
+	wake_lock_init(&data->wake_lock, WAKE_LOCK_SUSPEND, "rga");
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+	data->pd_rga = devm_clk_get(&pdev->dev, "pd_rga");
+	if (IS_ERR(data->pd_rga)) {
+		dev_err(&pdev->dev, "Failed to get rga power domain");
+		data->pd_rga = NULL;
+	}
+#endif
+    data->aclk_rga = devm_clk_get(&pdev->dev, "aclk_rga");
+    data->hclk_rga = devm_clk_get(&pdev->dev, "hclk_rga");
+
+    /* map the registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	data->rga_base = devm_ioremap_resource(&pdev->dev, res);
+	if (!data->rga_base) {
+		ERR("rga ioremap failed\n");
+		ret = -ENOENT;
+		goto err_ioremap;
+	}
+
+	/* get the IRQ */
+	data->irq = ret = platform_get_irq(pdev, 0);
+	if (ret <= 0) {
+		ERR("failed to get rga irq resource (%d).\n", data->irq);
+		ret = data->irq;
+		goto err_irq;
+	}
+
+	/* request the IRQ */
+	//ret = request_threaded_irq(data->irq, rga_irq, rga_irq_thread, 0, "rga", pdev);
+    ret = devm_request_threaded_irq(&pdev->dev, data->irq, rga_irq, rga_irq_thread, 0, "rga", data);
+	if (ret)
+	{
+		ERR("rga request_irq failed (%d).\n", ret);
+		goto err_irq;
+	}
+
+	platform_set_drvdata(pdev, data);
+	data->dev = &pdev->dev;
+	rga_drvdata = data;
+
+    #if defined(CONFIG_ION_ROCKCHIP)
+	data->ion_client = rockchip_ion_client_create("rga");
+	if (IS_ERR(data->ion_client)) {
+		dev_err(&pdev->dev, "failed to create ion client for rga");
+		return PTR_ERR(data->ion_client);
+	} else {
+		dev_info(&pdev->dev, "rga ion client create success!\n");
+	}
+    #endif
+
+	ret = misc_register(&rga_dev);
+	if(ret)
+	{
+		ERR("cannot register miscdev (%d)\n", ret);
+		goto err_misc_register;
+	}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_enable(&pdev->dev);
+#endif
+
+	pr_info("Driver loaded successfully\n");
+
+	return 0;
+
+err_misc_register:
+	free_irq(data->irq, pdev);
+err_irq:
+	iounmap(data->rga_base);
+err_ioremap:
+	wake_lock_destroy(&data->wake_lock);
+	//kfree(data);
+
+	return ret;
+}
+
+static int rga_drv_remove(struct platform_device *pdev)
+{
+	struct rga_drvdata *data = platform_get_drvdata(pdev);
+	DBG("%s [%d]\n",__FUNCTION__,__LINE__);
+
+	wake_lock_destroy(&data->wake_lock);
+	misc_deregister(&(data->miscdev));
+	free_irq(data->irq, &data->miscdev);
+	iounmap((void __iomem *)(data->rga_base));
+	kfree(data->version);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	devm_clk_put(&pdev->dev, data->aclk_rga);
+	devm_clk_put(&pdev->dev, data->hclk_rga);
+	pm_runtime_disable(&pdev->dev);
+#else
+	if (data->pd_rga)
+		devm_clk_put(&pdev->dev, data->pd_rga);
+	devm_clk_put(&pdev->dev, data->aclk_rga);
+	devm_clk_put(&pdev->dev, data->hclk_rga);
+#endif
+	//clk_put(data->pd_rga);
+
+	//kfree(data);
+	return 0;
+}
+
+static struct platform_driver rga_driver = {
+	.probe		= rga_drv_probe,
+	.remove		= rga_drv_remove,
+	.driver		= {
+		.owner  = THIS_MODULE,
+		.name	= "rga",
+		.of_match_table = of_match_ptr(rockchip_rga_dt_ids),
+	},
+};
+
+#if RGA_DEBUGFS
+void rga_slt(void);
+
+static int rga_debug_show(struct seq_file *m, void *data)
+{
+	seq_puts(m, "echo reg > rga to open rga reg MSG\n");
+	seq_puts(m, "echo msg  > rga to open rga msg MSG\n");
+	seq_puts(m, "echo time > rga to open rga time MSG\n");
+	seq_puts(m, "echo check > rga to open rga check flag\n");
+	seq_puts(m, "echo int > rga to open rga int flag\n");
+	seq_puts(m, "echo stop > rga to stop using hardware\n");
+	return 0;
+}
+
+static ssize_t rga_debug_write(struct file *file, const char __user *ubuf,
+			      size_t len, loff_t *offp)
+{
+	char buf[14];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+	if (strncmp(buf, "reg", 4) == 0) {
+		if (RGA_TEST_REG) {
+			RGA_TEST_REG = 0;
+			DBG("close rga reg!\n");
+		} else {
+			RGA_TEST_REG = 1;
+			DBG("open rga reg!\n");
+		}
+	} else if (strncmp(buf, "msg", 3) == 0) {
+		if (RGA_TEST_MSG) {
+			RGA_TEST_MSG = 0;
+			DBG("close rga test MSG!\n");
+		} else {
+			RGA_TEST_MSG = 1;
+			DBG("open rga test MSG!\n");
+		}
+	} else if (strncmp(buf, "time", 4) == 0) {
+		if (RGA_TEST_TIME) {
+			RGA_TEST_TIME = 0;
+			DBG("close rga test time!\n");
+		} else {
+			RGA_TEST_TIME = 1;
+			DBG("open rga test time!\n");
+		}
+	} else if (strncmp(buf, "check", 5) == 0) {
+		if (RGA_CHECK_MODE) {
+			RGA_CHECK_MODE = 0;
+			DBG("close rga check mode!\n");
+		} else {
+			RGA_CHECK_MODE = 1;
+			DBG("open rga check mode!\n");
+		}
+	} else if (strncmp(buf, "stop", 4) == 0) {
+		if (RGA_NONUSE) {
+			RGA_NONUSE = 0;
+			DBG("stop using rga hardware!\n");
+		} else {
+			RGA_NONUSE = 1;
+			DBG("use  rga hardware!\n");
+		}
+	} else if (strncmp(buf, "int", 3) == 0) {
+		if (RGA_INT_FLAG) {
+			RGA_INT_FLAG = 0;
+			DBG("close rga interuppt mesg!\n");
+		} else {
+			RGA_INT_FLAG = 1;
+			DBG("open rga interuppt mesg!\n");
+		}
+	} else if (strncmp(buf, "slt", 3) == 0) {
+		rga_slt();
+	}
+	return len;
+}
+
+static int rga_debug_open(struct inode *inode, struct file *file)
+
+{
+	return single_open(file, rga_debug_show, NULL);
+}
+
+static const struct file_operations rga_debug_fops = {
+	.owner = THIS_MODULE,
+	.open = rga_debug_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.write = rga_debug_write,
+};
+
+static void rga_debugfs_add(void)
+{
+	struct dentry *rga_debug_root;
+	struct dentry *ent;
+
+	rga_debug_root = debugfs_create_dir("rga_debug", NULL);
+
+	ent = debugfs_create_file("rga", 0644, rga_debug_root,
+				  NULL, &rga_debug_fops);
+	if (!ent) {
+		pr_err("create rga_debugfs err\n");
+		debugfs_remove_recursive(rga_debug_root);
+	}
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+void rga_slt(void)
+{
+	struct rga_req req;
+	rga_session session;
+	void *src_vir, *dst_vir;
+	unsigned int *src, *dst;
+	ion_phys_addr_t src_phy, dst_phy;
+	int i;
+	unsigned int srcW, srcH, dstW, dstH;
+	struct ion_handle *src_handle;
+	struct ion_handle *dst_handle;
+	struct rga_drvdata *data;
+	unsigned int srclen, dstlen;
+	int err_count = 0;
+	int right_count = 0;
+	int size;
+	unsigned int *pstd;
+	unsigned int *pnow;
+
+	data = rga_drvdata;
+	srcW = 1280;
+	srcH = 720;
+	dstW = 1280;
+	dstH = 720;
+	src_handle = ion_alloc(data->ion_client, (size_t)srcW * srcH * 4, 0,
+		   ION_HEAP(ION_CMA_HEAP_ID), 0);
+
+	dst_handle = ion_alloc(data->ion_client, (size_t)dstW * dstH * 4, 0,
+		   ION_HEAP(ION_CMA_HEAP_ID), 0);
+
+	session.pid	= current->pid;
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	/* no need to protect */
+	list_add_tail(&session.list_session, &rga_service.session);
+	atomic_set(&session.task_running, 0);
+	atomic_set(&session.num_done, 0);
+
+	src_vir = ion_map_kernel(data->ion_client, src_handle);
+	dst_vir = ion_map_kernel(data->ion_client, dst_handle);
+
+	ion_phys(data->ion_client, src_handle, &src_phy, &srclen);
+	ion_phys(data->ion_client, dst_handle, &dst_phy, &dstlen);
+
+	memset(&req, 0, sizeof(struct rga_req));
+	src = (unsigned int *)src_vir;
+	dst = (unsigned int *)dst_vir;
+
+	memset(src_vir, 0x80, srcW * srcH * 4);
+
+	DBG("\n********************************\n");
+	DBG("************ RGA_TEST ************\n");
+	DBG("********************************\n\n");
+
+	req.src.act_w = srcW;
+	req.src.act_h = srcH;
+
+	req.src.vir_w = srcW;
+	req.src.vir_h = srcW;
+	req.src.yrgb_addr = 0;
+	req.src.uv_addr = src_phy;
+	req.src.v_addr = src_phy + srcH * srcW;
+	req.src.format = RK_FORMAT_RGBA_8888;
+
+	req.dst.act_w = dstW;
+	req.dst.act_h = dstH;
+
+	req.dst.vir_w = dstW;
+	req.dst.vir_h = dstH;
+	req.dst.x_offset = 0;
+	req.dst.y_offset = 0;
+
+	req.dst.yrgb_addr = 0;
+	req.dst.uv_addr = dst_phy;
+	req.dst.v_addr = dst_phy + dstH * dstW;
+
+	req.dst.format = RK_FORMAT_RGBA_8888;
+
+	req.clip.xmin = 0;
+	req.clip.xmax = dstW - 1;
+	req.clip.ymin = 0;
+	req.clip.ymax = dstH - 1;
+
+	rga_blit_sync(&session, &req);
+
+	size = dstW * dstH * 4;
+	pstd = (unsigned int *)src_vir;
+	pnow = (unsigned int *)dst_vir;
+
+	DBG("[  num   : srcInfo    dstInfo ]\n");
+	for (i = 0; i < size / 4; i++) {
+		if (*pstd != *pnow) {
+			DBG("[X%.8d:0x%x 0x%x]", i, *pstd, *pnow);
+			if (i % 4 == 0)
+				DBG("\n");
+			err_count++;
+		} else {
+			if (i % (640 * 1024) == 0)
+				DBG("[Y%.8d:0x%.8x 0x%.8x]\n", i,
+				    *pstd, *pnow);
+			right_count++;
+		}
+	pstd++;
+	pnow++;
+	if (err_count > 64)
+		break;
+	}
+
+	DBG("err_count=%d,right_count=%d\n", err_count, right_count);
+	if (err_count != 0)
+		DBG("rga slt err !!\n");
+	else
+		DBG("rga slt success !!\n");
+
+	ion_unmap_kernel(data->ion_client, src_handle);
+	ion_unmap_kernel(data->ion_client, dst_handle);
+
+	ion_free(data->ion_client, src_handle);
+	ion_free(data->ion_client, dst_handle);
+}
+#else
+unsigned long src1_buf[400 * 200];
+unsigned long dst1_buf[400 * 200];
+void rga_slt(void)
+{
+	struct rga_req req;
+	rga_session session;
+	unsigned long *src_vir, *dst_vir;
+	int i;
+	unsigned int srcW, srcH, dstW, dstH;
+	int err_count = 0;
+	int right_count = 0;
+	int size;
+	unsigned int *pstd;
+	unsigned int *pnow;
+
+	srcW = 400;
+	srcH = 200;
+	dstW = 400;
+	dstH = 200;
+
+	session.pid	= current->pid;
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	/* no need to protect */
+	list_add_tail(&session.list_session, &rga_service.session);
+	atomic_set(&session.task_running, 0);
+	atomic_set(&session.num_done, 0);
+
+	memset(&req, 0, sizeof(struct rga_req));
+	src_vir = src1_buf;
+	dst_vir = dst1_buf;
+
+	memset(src1_buf, 0x50, 400 * 200 * 4);
+	memset(dst1_buf, 0x00, 400 * 200 * 4);
+
+	rga_dma_flush_range(&src1_buf[0], &src1_buf[400 * 200]);
+
+	DBG("\n********************************\n");
+	DBG("************ RGA_TEST ************\n");
+	DBG("********************************\n\n");
+
+	req.src.act_w = srcW;
+	req.src.act_h = srcH;
+
+	req.src.vir_w = srcW;
+	req.src.vir_h = srcW;
+	req.src.yrgb_addr = 0;
+	req.src.uv_addr = (unsigned long)virt_to_phys(src_vir);
+	req.src.v_addr = req.src.uv_addr + srcH * srcW;
+	req.src.format = RK_FORMAT_RGBA_8888;
+
+	req.dst.act_w = dstW;
+	req.dst.act_h = dstH;
+
+	req.dst.vir_w = dstW;
+	req.dst.vir_h = dstH;
+	req.dst.x_offset = 0;
+	req.dst.y_offset = 0;
+
+	req.dst.yrgb_addr = 0;
+	req.dst.uv_addr = (unsigned long)virt_to_phys(dst_vir);
+	req.dst.v_addr = req.dst.uv_addr + dstH * dstW;
+
+	req.dst.format = RK_FORMAT_RGBA_8888;
+	rga_blit_sync(&session, &req);
+	size = dstW * dstH * 4;
+	pstd = (unsigned int *)src_vir;
+	pnow = (unsigned int *)dst_vir;
+
+	DBG("[  num   : srcInfo    dstInfo ]\n");
+	for (i = 0; i < size / 4; i++) {
+		if (*pstd != *pnow) {
+			DBG("[X%.8d:0x%x 0x%x]", i, *pstd, *pnow);
+			if (i % 4 == 0)
+				DBG("\n");
+			err_count++;
+		} else {
+			if (i % (640 * 1024) == 0)
+				DBG("[Y%.8d:0x%.8x 0x%.8x]\n", i,
+				    *pstd, *pnow);
+			right_count++;
+		}
+	pstd++;
+	pnow++;
+	if (err_count > 64)
+		break;
+	}
+
+	DBG("err_count=%d, right_count=%d\n", err_count, right_count);
+	if (err_count != 0)
+		DBG("rga slt err !!\n");
+	else
+		DBG("rga slt success !!\n");
+}
+#endif
+#endif
+
+void rga_test_0(void);
+void rga_test_1(void);
+
+static int __init rga_init(void)
+{
+	int i, ret;
+	void * pre_scale_page_buf;
+	uint32_t *pre_scale_page_table;
+	uint32_t *mmu_base;
+	struct page **pages;
+
+	/* malloc pre scale mid buf mmu table */
+	pre_scale_page_table = kzalloc(RGA_PRE_SCALE_PAGE_SIZE * sizeof(*pre_scale_page_table),
+				       GFP_KERNEL);
+	if(pre_scale_page_table == NULL) {
+		pr_err("RGA alloc pre-scale page table failed.\n");
+		return -ENOMEM;
+	}
+
+	/* alloc reserved pre-scale buf */
+	for(i = 0; i < RGA_PRE_SCALE_PAGE_SIZE; i++) {
+		pre_scale_page_buf = (void *)__get_free_page(GFP_KERNEL | __GFP_ZERO);
+		if(pre_scale_page_buf == NULL) {
+			printk(KERN_ERR "RGA init pre scale page_table[%d] falied\n", i);
+			ret = -ENOMEM;
+			goto free_pre_scale_page_table;
+		}
+		pre_scale_page_table[i] = (uint32_t)virt_to_phys(pre_scale_page_buf);
+	}
+
+	mmu_base = kmalloc(1024 * 256, GFP_KERNEL);
+	if (mmu_base == NULL) {
+		pr_err("RGA alloc mmu buffer failed.\n");
+		ret = -ENOMEM;
+		goto free_pre_scale_page_table;
+	}
+
+	pages = kmalloc((32768)* sizeof(struct page *), GFP_KERNEL);
+	if (pages == NULL) {
+		pr_err("RGA alloc pages buffer failed.\n");
+		ret = -ENOMEM;
+		goto free_mmu_base;
+	}
+
+	ret = platform_driver_register(&rga_driver);
+	if (ret != 0) {
+		printk(KERN_ERR "Platform device register failed (%d).\n", ret);
+		goto free_pages_buf;
+	}
+
+	rga_service.pre_scale_buf = pre_scale_page_table;
+
+	rga_mmu_buf.buf_virtual = mmu_base;
+#if (defined(CONFIG_ARM) && defined(CONFIG_ARM_LPAE))
+	rga_mmu_buf.buf = (uint32_t *)(uint32_t)virt_to_phys((void *)((unsigned long)mmu_base));
+#else
+	rga_mmu_buf.buf = (uint32_t *)virt_to_phys((void *)((unsigned long)mmu_base));
+#endif
+	rga_mmu_buf.front = 0;
+	rga_mmu_buf.back = 64*1024;
+	rga_mmu_buf.size = 64*1024;
+
+	rga_mmu_buf.pages = pages;
+
+	rga_session_global.pid = 0x0000ffff;
+	INIT_LIST_HEAD(&rga_session_global.waiting);
+	INIT_LIST_HEAD(&rga_session_global.running);
+	INIT_LIST_HEAD(&rga_session_global.list_session);
+
+	INIT_LIST_HEAD(&rga_service.waiting);
+	INIT_LIST_HEAD(&rga_service.running);
+	INIT_LIST_HEAD(&rga_service.done);
+	INIT_LIST_HEAD(&rga_service.session);
+
+	init_waitqueue_head(&rga_session_global.wait);
+	//mutex_lock(&rga_service.lock);
+	list_add_tail(&rga_session_global.list_session, &rga_service.session);
+	//mutex_unlock(&rga_service.lock);
+	atomic_set(&rga_session_global.task_running, 0);
+	atomic_set(&rga_session_global.num_done, 0);
+
+#if RGA_TEST_CASE
+	rga_test_0();
+#endif
+#if RGA_DEBUGFS
+	rga_debugfs_add();
+#endif
+
+	INFO("RGA Module initialized.\n");
+
+	return 0;
+
+free_pages_buf:
+	kfree(pages);
+
+free_mmu_base:
+	kfree(mmu_base);
+
+free_pre_scale_page_table:
+	for (i = 0; i < RGA_PRE_SCALE_PAGE_SIZE; i++)
+		if (pre_scale_page_table[i] != 0)
+			kfree(phys_to_virt((phys_addr_t)pre_scale_page_table[i]));
+
+	kfree(pre_scale_page_table);
+
+	return ret;
+}
+
+static void __exit rga_exit(void)
+{
+	phys_addr_t pre_scale_buf;
+
+	rga_power_off();
+
+	if (rga_service.pre_scale_buf != NULL) {
+		pre_scale_buf = (phys_addr_t)rga_service.pre_scale_buf[0];
+		if (pre_scale_buf)
+			kfree(phys_to_virt(pre_scale_buf));
+		kfree(rga_service.pre_scale_buf);
+	}
+	kfree(rga_mmu_buf.buf_virtual);
+	kfree(rga_mmu_buf.pages);
+
+	platform_driver_unregister(&rga_driver);
+}
+
+#if RGA_TEST_CASE
+
+extern struct fb_info * rk_get_fb(int fb_id);
+EXPORT_SYMBOL(rk_get_fb);
+
+extern void rk_direct_fb_show(struct fb_info * fbi);
+EXPORT_SYMBOL(rk_direct_fb_show);
+
+unsigned int src_buf[1920*1080];
+unsigned int dst_buf[1920*1080];
+//unsigned int tmp_buf[1920*1080 * 2];
+
+void rga_test_0(void)
+{
+    struct rga_req req;
+    rga_session session;
+    unsigned int *src, *dst;
+    uint32_t i, j;
+    uint8_t *p;
+    uint8_t t;
+    uint32_t *dst0, *dst1, *dst2;
+
+    struct fb_info *fb;
+
+    session.pid	= current->pid;
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	/* no need to protect */
+	list_add_tail(&session.list_session, &rga_service.session);
+	atomic_set(&session.task_running, 0);
+    atomic_set(&session.num_done, 0);
+	//file->private_data = (void *)session;
+
+    fb = rk_get_fb(0);
+
+    memset(&req, 0, sizeof(struct rga_req));
+    src = src_buf;
+    dst = dst_buf;
+
+    memset(src_buf, 0x80, 1024*600*4);
+
+    dmac_flush_range(&src_buf[0], &src_buf[1024*600]);
+    outer_flush_range(virt_to_phys(&src_buf[0]),virt_to_phys(&src_buf[1024*600]));
+
+
+    #if 0
+    memset(src_buf, 0x80, 800*480*4);
+    memset(dst_buf, 0xcc, 800*480*4);
+
+    dmac_flush_range(&dst_buf[0], &dst_buf[800*480]);
+    outer_flush_range(virt_to_phys(&dst_buf[0]),virt_to_phys(&dst_buf[800*480]));
+    #endif
+
+    dst0 = &dst_buf[0];
+    //dst1 = &dst_buf[1280*800*4];
+    //dst2 = &dst_buf[1280*800*4*2];
+
+    i = j = 0;
+
+    printk("\n********************************\n");
+    printk("************ RGA_TEST ************\n");
+    printk("********************************\n\n");
+
+    req.src.act_w = 1024;
+    req.src.act_h = 600;
+
+    req.src.vir_w = 1024;
+    req.src.vir_h = 600;
+    req.src.yrgb_addr = (uint32_t)virt_to_phys(src);
+    req.src.uv_addr = (uint32_t)(req.src.yrgb_addr + 1080*1920);
+    req.src.v_addr = (uint32_t)virt_to_phys(src);
+    req.src.format = RK_FORMAT_RGBA_8888;
+
+    req.dst.act_w = 600;
+    req.dst.act_h = 352;
+
+    req.dst.vir_w = 1280;
+    req.dst.vir_h = 800;
+    req.dst.x_offset = 600;
+    req.dst.y_offset = 0;
+
+    dst = dst0;
+
+    req.dst.yrgb_addr = ((uint32_t)virt_to_phys(dst));
+
+    //req.dst.format = RK_FORMAT_RGB_565;
+
+    req.clip.xmin = 0;
+    req.clip.xmax = 1279;
+    req.clip.ymin = 0;
+    req.clip.ymax = 799;
+
+    //req.render_mode = color_fill_mode;
+    //req.fg_color = 0x80ffffff;
+
+    req.rotate_mode = 1;
+    //req.scale_mode = 2;
+
+    //req.alpha_rop_flag = 0;
+    //req.alpha_rop_mode = 0x19;
+    //req.PD_mode = 3;
+
+    req.sina = 65536;
+    req.cosa = 0;
+
+    //req.mmu_info.mmu_flag = 0x21;
+    //req.mmu_info.mmu_en = 1;
+
+    //printk("src = %.8x\n", req.src.yrgb_addr);
+    //printk("src = %.8x\n", req.src.uv_addr);
+    //printk("dst = %.8x\n", req.dst.yrgb_addr);
+
+
+    rga_blit_sync(&session, &req);
+
+    #if 1
+    fb->var.bits_per_pixel = 32;
+
+    fb->var.xres = 1280;
+    fb->var.yres = 800;
+
+    fb->var.red.length = 8;
+    fb->var.red.offset = 0;
+    fb->var.red.msb_right = 0;
+
+    fb->var.green.length = 8;
+    fb->var.green.offset = 8;
+    fb->var.green.msb_right = 0;
+
+    fb->var.blue.length = 8;
+
+    fb->var.blue.offset = 16;
+    fb->var.blue.msb_right = 0;
+
+    fb->var.transp.length = 8;
+    fb->var.transp.offset = 24;
+    fb->var.transp.msb_right = 0;
+
+    fb->var.nonstd &= (~0xff);
+    fb->var.nonstd |= 1;
+
+    fb->fix.smem_start = virt_to_phys(dst);
+
+    rk_direct_fb_show(fb);
+    #endif
+
+}
+
+#endif
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 1, 0))
+fs_initcall(rga_init);
+#else
+module_init(rga_init);
+#endif
+module_exit(rga_exit);
+
+/* Module information */
+MODULE_AUTHOR("zsq@rock-chips.com");
+MODULE_DESCRIPTION("Driver for rga device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/video/rockchip/rga/rga_mmu_info.c b/drivers/video/rockchip/rga/rga_mmu_info.c
new file mode 100644
index 0000000000000..9dcffa50a1e22
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_mmu_info.c
@@ -0,0 +1,1325 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/pagemap.h>
+#include <linux/seq_file.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/memory.h>
+#include <linux/dma-mapping.h>
+#include <asm/memory.h>
+#include <asm/atomic.h>
+#include <asm/cacheflush.h>
+#include "rga_mmu_info.h"
+#include <linux/delay.h>
+
+extern rga_service_info rga_service;
+extern struct rga_mmu_buf_t rga_mmu_buf;
+
+#if RGA_DEBUGFS
+extern int RGA_CHECK_MODE;
+#endif
+
+#define KERNEL_SPACE_VALID    0xc0000000
+
+void rga_dma_flush_range(void *pstart, void *pend)
+{
+	dma_sync_single_for_device(rga_drvdata->dev, virt_to_phys(pstart), pend - pstart, DMA_TO_DEVICE);
+}
+
+static int rga_mmu_buf_get(struct rga_mmu_buf_t *t, uint32_t size)
+{
+    mutex_lock(&rga_service.lock);
+    t->front += size;
+    mutex_unlock(&rga_service.lock);
+
+    return 0;
+}
+
+static void rga_current_mm_read_lock(struct mm_struct *mm)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_lock(mm);
+#else
+	down_read(&mm->mmap_sem);
+#endif
+}
+
+static void rga_current_mm_read_unlock(struct mm_struct *mm)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_unlock(mm);
+#else
+	up_read(&mm->mmap_sem);
+#endif
+}
+
+static long rga_get_user_pages(struct page **pages, unsigned long Memory,
+			       uint32_t pageCount, int writeFlag,
+			       struct mm_struct *current_mm)
+{
+	#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 168) && \
+		LINUX_VERSION_CODE < KERNEL_VERSION(4, 5, 0)
+		return get_user_pages(current, current_mm, Memory << PAGE_SHIFT,
+				      pageCount, writeFlag ? FOLL_WRITE : 0, pages, NULL);
+	#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
+		return get_user_pages(current, current_mm, Memory << PAGE_SHIFT,
+				      pageCount, writeFlag ? FOLL_WRITE : 0, 0, pages, NULL);
+	#elif LINUX_VERSION_CODE < KERNEL_VERSION(5, 10, 0)
+		return get_user_pages_remote(current, current_mm, Memory << PAGE_SHIFT,
+					     pageCount, writeFlag ? FOLL_WRITE : 0, pages,
+					     NULL, NULL);
+	#else
+		return get_user_pages_remote(current_mm, Memory << PAGE_SHIFT,
+					     pageCount, writeFlag ? FOLL_WRITE : 0, pages,
+					     NULL, NULL);
+	#endif
+}
+
+static int rga_mmu_buf_get_try(struct rga_mmu_buf_t *t, uint32_t size)
+{
+	int ret = 0;
+
+	mutex_lock(&rga_service.lock);
+	if ((t->back - t->front) > t->size) {
+		if(t->front + size > t->back - t->size) {
+			ret = -ENOMEM;
+			goto out;
+		}
+	} else {
+		if ((t->front + size) > t->back) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		if (t->front + size > t->size) {
+			if (size > (t->back - t->size)) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			t->front = 0;
+		}
+	}
+
+out:
+	mutex_unlock(&rga_service.lock);
+	return ret;
+}
+
+static int rga_mem_size_cal(unsigned long Mem, uint32_t MemSize, unsigned long *StartAddr)
+{
+    unsigned long start, end;
+    uint32_t pageCount;
+
+    end = (Mem + (MemSize + PAGE_SIZE - 1)) >> PAGE_SHIFT;
+    start = Mem >> PAGE_SHIFT;
+    pageCount = end - start;
+    *StartAddr = start;
+    return pageCount;
+}
+
+static int rga_buf_size_cal(unsigned long yrgb_addr, unsigned long uv_addr, unsigned long v_addr,
+                                        int format, uint32_t w, uint32_t h, unsigned long *StartAddr )
+{
+    uint32_t size_yrgb = 0;
+    uint32_t size_uv = 0;
+    uint32_t size_v = 0;
+    uint32_t stride = 0;
+    unsigned long start, end;
+    uint32_t pageCount;
+
+    switch(format)
+    {
+        case RK_FORMAT_RGBA_8888 :
+            stride = (w * 4 + 3) & (~3);
+            size_yrgb = stride*h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_RGBX_8888 :
+            stride = (w * 4 + 3) & (~3);
+            size_yrgb = stride*h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_RGB_888 :
+            stride = (w * 3 + 3) & (~3);
+            size_yrgb = stride*h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_BGRA_8888 :
+            size_yrgb = w*h*4;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_RGB_565 :
+            stride = (w*2 + 3) & (~3);
+            size_yrgb = stride * h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_RGBA_5551 :
+            stride = (w*2 + 3) & (~3);
+            size_yrgb = stride * h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_RGBA_4444 :
+            stride = (w*2 + 3) & (~3);
+            size_yrgb = stride * h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+        case RK_FORMAT_BGR_888 :
+            stride = (w*3 + 3) & (~3);
+            size_yrgb = stride * h;
+            start = yrgb_addr >> PAGE_SHIFT;
+            pageCount = (size_yrgb + PAGE_SIZE - 1) >> PAGE_SHIFT;
+            break;
+
+        /* YUV FORMAT */
+        case RK_FORMAT_YCbCr_422_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = stride * h;
+            start = MIN(yrgb_addr, uv_addr);
+
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RK_FORMAT_YCbCr_422_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * h);
+            size_v = ((stride >> 1) * h);
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start = start >> PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RK_FORMAT_YCbCr_420_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = (stride * (h >> 1));
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RK_FORMAT_YCbCr_420_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * (h >> 1));
+            size_v = ((stride >> 1) * (h >> 1));
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+
+        case RK_FORMAT_YCrCb_422_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = stride * h;
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RK_FORMAT_YCrCb_422_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * h);
+            size_v = ((stride >> 1) * h);
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+
+        case RK_FORMAT_YCrCb_420_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = (stride * (h >> 1));
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RK_FORMAT_YCrCb_420_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * (h >> 1));
+            size_v = ((stride >> 1) * (h >> 1));
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        #if 0
+        case RK_FORMAT_BPP1 :
+            break;
+        case RK_FORMAT_BPP2 :
+            break;
+        case RK_FORMAT_BPP4 :
+            break;
+        case RK_FORMAT_BPP8 :
+            break;
+        #endif
+        default :
+            pageCount = 0;
+            start = 0;
+            break;
+    }
+
+    *StartAddr = start;
+    return pageCount;
+}
+
+#if RGA_DEBUGFS
+static int rga_usermemory_cheeck(struct page **pages, u32 w, u32 h, u32 format, int flag)
+{
+	int bits;
+	void *vaddr = NULL;
+	int taipage_num;
+	int taidata_num;
+	int *tai_vaddr = NULL;
+
+	switch (format) {
+	case RK_FORMAT_RGBA_8888:
+	case RK_FORMAT_RGBX_8888:
+	case RK_FORMAT_BGRA_8888:
+		bits = 32;
+		break;
+	case RK_FORMAT_RGB_888:
+	case RK_FORMAT_BGR_888:
+		bits = 24;
+		break;
+	case RK_FORMAT_RGB_565:
+	case RK_FORMAT_RGBA_5551:
+	case RK_FORMAT_RGBA_4444:
+	case RK_FORMAT_YCbCr_422_SP:
+	case RK_FORMAT_YCbCr_422_P:
+	case RK_FORMAT_YCrCb_422_SP:
+	case RK_FORMAT_YCrCb_422_P:
+		bits = 16;
+		break;
+	case RK_FORMAT_YCbCr_420_SP:
+	case RK_FORMAT_YCbCr_420_P:
+	case RK_FORMAT_YCrCb_420_SP:
+	case RK_FORMAT_YCrCb_420_P:
+		bits = 12;
+		break;
+	case RK_FORMAT_YCbCr_420_SP_10B:
+	case RK_FORMAT_YCrCb_420_SP_10B:
+		bits = 15;
+		break;
+	default:
+		printk(KERN_DEBUG "un know format\n");
+		return -1;
+	}
+	taipage_num = w * h * bits / 8 / (1024 * 4);
+	taidata_num = w * h * bits / 8 % (1024 * 4);
+	if (taidata_num == 0) {
+		vaddr = kmap(pages[taipage_num - 1]);
+		tai_vaddr = (int *)vaddr + 1023;
+	} else {
+		vaddr = kmap(pages[taipage_num]);
+		tai_vaddr = (int *)vaddr + taidata_num / 4 - 1;
+	}
+	if (flag == 1) {
+		printk(KERN_DEBUG "src user memory check\n");
+		printk(KERN_DEBUG "tai data is %d\n", *tai_vaddr);
+	} else {
+		printk(KERN_DEBUG "dst user memory check\n");
+		printk(KERN_DEBUG "tai data is %d\n", *tai_vaddr);
+	}
+	if (taidata_num == 0)
+		kunmap(pages[taipage_num - 1]);
+	else
+		kunmap(pages[taipage_num]);
+	return 0;
+}
+#endif
+
+static int rga_MapUserMemory(struct page **pages,
+                                            uint32_t *pageTable,
+                                            unsigned long Memory,
+                                            uint32_t pageCount)
+{
+    int32_t result;
+    uint32_t i;
+    uint32_t status;
+    unsigned long Address;
+
+    status = 0;
+    Address = 0;
+
+    do {
+        rga_current_mm_read_lock(current->mm);
+
+	result = rga_get_user_pages(pages, Memory, pageCount, 1, current->mm);
+
+        rga_current_mm_read_unlock(current->mm);
+
+        #if 0
+        if(result <= 0 || result < pageCount)
+        {
+            status = 0;
+
+            for(i=0; i<pageCount; i++)
+            {
+                temp = armv7_va_to_pa((Memory + i) << PAGE_SHIFT);
+                if (temp == 0xffffffff)
+                {
+                    printk("rga find mmu phy ddr error\n ");
+                    status = RGA_OUT_OF_RESOURCES;
+                    break;
+                }
+
+                pageTable[i] = temp;
+            }
+
+            return status;
+        }
+        #else
+        if(result <= 0 || result < pageCount)
+        {
+            struct vm_area_struct *vma;
+
+            if (result>0) {
+		rga_current_mm_read_lock(current->mm);
+
+		for (i = 0; i < result; i++)
+			put_page(pages[i]);
+
+		rga_current_mm_read_unlock(current->mm);
+	    }
+
+            for(i=0; i<pageCount; i++)
+            {
+                vma = find_vma(current->mm, (Memory + i) << PAGE_SHIFT);
+
+                if (vma)//&& (vma->vm_flags & VM_PFNMAP) )
+                {
+                    do
+                    {
+                        pte_t       * pte;
+                        spinlock_t  * ptl;
+                        unsigned long pfn;
+                        pgd_t * pgd;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+						p4d_t * p4d;
+#endif
+                        pud_t * pud;
+
+                        pgd = pgd_offset(current->mm, (Memory + i) << PAGE_SHIFT);
+
+                        if(pgd_val(*pgd) == 0)
+                        {
+                            //printk("rga pgd value is zero \n");
+                            break;
+                        }
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+						/* In the four-level page table, it will do nothing and return pgd. */
+						p4d = p4d_offset(pgd, (Memory + i) << PAGE_SHIFT);
+						if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d))) {
+							pr_err("RGA2 failed to get p4d, result = %d, pageCount = %d\n",
+								   result, pageCount);
+							status = RGA_OUT_OF_RESOURCES;
+							break;
+						}
+
+						pud = pud_offset(p4d, (Memory + i) << PAGE_SHIFT);
+#else
+						pud = pud_offset(pgd, (Memory + i) << PAGE_SHIFT);
+#endif
+                        if (pud)
+                        {
+                            pmd_t * pmd = pmd_offset(pud, (Memory + i) << PAGE_SHIFT);
+                            if (pmd)
+                            {
+                                pte = pte_offset_map_lock(current->mm, pmd, (Memory + i) << PAGE_SHIFT, &ptl);
+                                if (!pte)
+                                {
+                                    pte_unmap_unlock(pte, ptl);
+                                    break;
+                                }
+                            }
+                            else
+                            {
+                                break;
+                            }
+                        }
+                        else
+                        {
+                            break;
+                        }
+
+                        pfn = pte_pfn(*pte);
+                        Address = ((pfn << PAGE_SHIFT) | (((unsigned long)((Memory + i) << PAGE_SHIFT)) & ~PAGE_MASK));
+                        pte_unmap_unlock(pte, ptl);
+                    }
+                    while (0);
+
+                    pageTable[i] = Address;
+                }
+                else
+                {
+                    status = RGA_OUT_OF_RESOURCES;
+                    break;
+                }
+            }
+
+            return status;
+        }
+        #endif
+
+        /* Fill the page table. */
+        for(i=0; i<pageCount; i++)
+        {
+            /* Get the physical address from page struct. */
+            pageTable[i] = page_to_phys(pages[i]);
+        }
+
+	rga_current_mm_read_lock(current->mm);
+
+	for (i = 0; i < result; i++)
+		put_page(pages[i]);
+
+	rga_current_mm_read_unlock(current->mm);
+
+        return 0;
+    }
+    while(0);
+
+    return status;
+}
+
+static int rga_MapION(struct sg_table *sg,
+                               uint32_t *Memory,
+                               int32_t  pageCount,
+                               uint32_t offset)
+{
+    uint32_t i;
+    uint32_t status;
+    unsigned long Address;
+    uint32_t mapped_size = 0;
+    uint32_t len = 0;
+    struct scatterlist *sgl = sg->sgl;
+    uint32_t sg_num = 0;
+
+    status = 0;
+    Address = 0;
+    offset = offset >> PAGE_SHIFT;
+    if (offset != 0) {
+        do {
+            len += (sg_dma_len(sgl) >> PAGE_SHIFT);
+	        if (len == offset) {
+	    	    sg_num += 1;
+		    break;
+    	    }
+    	    else {
+                if (len > offset)
+                     break;
+    	    }
+                sg_num += 1;
+        }
+        while((sgl = sg_next(sgl)) && (mapped_size < pageCount) && (sg_num < sg->nents));
+
+        sgl = sg->sgl;
+    	len = 0;
+        do {
+            len += (sg_dma_len(sgl) >> PAGE_SHIFT);
+            sgl = sg_next(sgl);
+        }
+        while(--sg_num);
+
+        offset -= len;
+
+        len = sg_dma_len(sgl) >> PAGE_SHIFT;
+        Address = sg_phys(sgl);
+    	Address += offset;
+
+        for(i=offset; i<len; i++) {
+             Memory[i - offset] = Address + (i << PAGE_SHIFT);
+        }
+        mapped_size += (len - offset);
+        sg_num = 1;
+        sgl = sg_next(sgl);
+        do {
+            len = sg_dma_len(sgl) >> PAGE_SHIFT;
+            Address = sg_phys(sgl);
+
+            for(i=0; i<len; i++) {
+                Memory[mapped_size + i] = Address + (i << PAGE_SHIFT);
+            }
+
+            mapped_size += len;
+            sg_num += 1;
+        }
+        while((sgl = sg_next(sgl)) && (mapped_size < pageCount) && (sg_num < sg->nents));
+    }
+    else {
+        do {
+            len = sg_dma_len(sgl) >> PAGE_SHIFT;
+            Address = sg_phys(sgl);
+            for(i=0; i<len; i++) {
+                Memory[mapped_size + i] = Address + (i << PAGE_SHIFT);
+            }
+            mapped_size += len;
+            sg_num += 1;
+        }
+        while((sgl = sg_next(sgl)) && (mapped_size < pageCount) && (sg_num < sg->nents));
+    }
+    return 0;
+}
+
+
+static int rga_mmu_info_BitBlt_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int SrcMemSize, DstMemSize;
+    unsigned long SrcStart, DstStart;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p, *MMU_Base_phys;
+    int ret;
+    int status;
+    uint32_t uv_size, v_size;
+
+    struct page **pages = NULL;
+
+    MMU_Base = NULL;
+
+    SrcMemSize = 0;
+    DstMemSize = 0;
+
+    do {
+        /* cal src buf mmu info */
+        SrcMemSize = rga_buf_size_cal(req->src.yrgb_addr, req->src.uv_addr, req->src.v_addr,
+                                        req->src.format, req->src.vir_w, req->src.act_h + req->src.y_offset,
+                                        &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        /* cal dst buf mmu info */
+
+        DstMemSize = rga_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                        req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                        &DstStart);
+        if(DstMemSize == 0)
+            return -EINVAL;
+
+        /* Cal out the needed mem size */
+        SrcMemSize = (SrcMemSize + 15) & (~15);
+        DstMemSize = (DstMemSize + 15) & (~15);
+        AllSize = SrcMemSize + DstMemSize;
+
+        if (rga_mmu_buf_get_try(&rga_mmu_buf, AllSize + 16)) {
+            pr_err("RGA Get MMU mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        mutex_lock(&rga_service.lock);
+        MMU_Base = rga_mmu_buf.buf_virtual + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        MMU_Base_phys = rga_mmu_buf.buf + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        mutex_unlock(&rga_service.lock);
+
+        pages = rga_mmu_buf.pages;
+
+        if((req->mmu_info.mmu_flag >> 8) & 1) {
+            if (req->sg_src) {
+                ret = rga_MapION(req->sg_src, &MMU_Base[0], SrcMemSize, req->line_draw_info.flag);
+            }
+            else {
+                ret = rga_MapUserMemory(&pages[0], &MMU_Base[0], SrcStart, SrcMemSize);
+                if (ret < 0) {
+                    pr_err("rga map src memory failed\n");
+                    status = ret;
+                    break;
+                }
+
+#if RGA_DEBUGFS
+	if (RGA_CHECK_MODE)
+		rga_usermemory_cheeck(&pages[0], req->src.vir_w,
+				      req->src.vir_h, req->src.format, 1);
+#endif
+            }
+        }
+        else {
+            MMU_p = MMU_Base;
+
+            if(req->src.yrgb_addr == (unsigned long)rga_service.pre_scale_buf) {
+                for(i=0; i<SrcMemSize; i++)
+                    MMU_p[i] = rga_service.pre_scale_buf[i];
+            }
+            else {
+                for(i=0; i<SrcMemSize; i++)
+                    MMU_p[i] = (uint32_t)((SrcStart + i) << PAGE_SHIFT);
+            }
+        }
+
+        if ((req->mmu_info.mmu_flag >> 10) & 1) {
+            if (req->sg_dst) {
+                ret = rga_MapION(req->sg_dst, &MMU_Base[SrcMemSize], DstMemSize, req->line_draw_info.line_width);
+            }
+            else {
+                ret = rga_MapUserMemory(&pages[SrcMemSize], &MMU_Base[SrcMemSize], DstStart, DstMemSize);
+                if (ret < 0) {
+                    pr_err("rga map dst memory failed\n");
+                    status = ret;
+                    break;
+                }
+
+#if RGA_DEBUGFS
+	if (RGA_CHECK_MODE)
+		rga_usermemory_cheeck(&pages[0], req->src.vir_w,
+				      req->src.vir_h, req->src.format, 2);
+#endif
+            }
+        }
+        else {
+            MMU_p = MMU_Base + SrcMemSize;
+            for(i=0; i<DstMemSize; i++)
+                MMU_p[i] = (uint32_t)((DstStart + i) << PAGE_SHIFT);
+        }
+
+        MMU_Base[AllSize] = MMU_Base[AllSize-1];
+
+        /* zsq
+         * change the buf address in req struct
+         */
+
+        req->mmu_info.base_addr = (unsigned long)MMU_Base_phys >> 2;
+
+        uv_size = (req->src.uv_addr - (SrcStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+        v_size = (req->src.v_addr - (SrcStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK));
+        req->src.uv_addr = (req->src.uv_addr & (~PAGE_MASK)) | (uv_size << PAGE_SHIFT);
+        req->src.v_addr = (req->src.v_addr & (~PAGE_MASK)) | (v_size << PAGE_SHIFT);
+
+        uv_size = (req->dst.uv_addr - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+
+        req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK)) | (SrcMemSize << PAGE_SHIFT);
+        req->dst.uv_addr = (req->dst.uv_addr & (~PAGE_MASK)) | ((SrcMemSize + uv_size) << PAGE_SHIFT);
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize + 1));
+
+        rga_mmu_buf_get(&rga_mmu_buf, AllSize + 16);
+        reg->MMU_len = AllSize + 16;
+
+        status = 0;
+
+        return status;
+    }
+    while(0);
+
+    return status;
+}
+
+static int rga_mmu_info_color_palette_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int SrcMemSize, DstMemSize, CMDMemSize;
+    unsigned long SrcStart, DstStart, CMDStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base = NULL, *MMU_Base_phys = NULL;
+    uint32_t *MMU_p;
+    int ret, status = 0;
+    uint32_t stride;
+
+    uint8_t shift;
+    uint16_t sw, byte_num;
+
+    shift = 3 - (req->palette_mode & 3);
+    sw = req->src.vir_w;
+    byte_num = sw >> shift;
+    stride = (byte_num + 3) & (~3);
+
+    do {
+        SrcMemSize = rga_mem_size_cal(req->src.yrgb_addr, stride, &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        DstMemSize = rga_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                        req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                        &DstStart);
+        if(DstMemSize == 0) {
+            return -EINVAL;
+        }
+
+        CMDMemSize = rga_mem_size_cal((unsigned long)rga_service.cmd_buff, RGA_CMD_BUF_SIZE, &CMDStart);
+        if(CMDMemSize == 0) {
+            return -EINVAL;
+        }
+
+        SrcMemSize = (SrcMemSize + 15) & (~15);
+        DstMemSize = (DstMemSize + 15) & (~15);
+        CMDMemSize = (CMDMemSize + 15) & (~15);
+
+        AllSize = SrcMemSize + DstMemSize + CMDMemSize;
+
+        if (rga_mmu_buf_get_try(&rga_mmu_buf, AllSize + 16)) {
+            pr_err("RGA Get MMU mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        mutex_lock(&rga_service.lock);
+        MMU_Base = rga_mmu_buf.buf_virtual + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        MMU_Base_phys = rga_mmu_buf.buf + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        mutex_unlock(&rga_service.lock);
+
+        pages = rga_mmu_buf.pages;
+
+        /* map CMD addr */
+        for(i=0; i<CMDMemSize; i++) {
+            MMU_Base[i] = (uint32_t)virt_to_phys((uint32_t *)((CMDStart + i)<<PAGE_SHIFT));
+        }
+
+        /* map src addr */
+        if (req->src.yrgb_addr < KERNEL_SPACE_VALID) {
+            ret = rga_MapUserMemory(&pages[CMDMemSize], &MMU_Base[CMDMemSize], SrcStart, SrcMemSize);
+            if (ret < 0) {
+                pr_err("rga map src memory failed\n");
+                status = ret;
+                break;
+            }
+        }
+        else {
+            MMU_p = MMU_Base + CMDMemSize;
+
+            for(i=0; i<SrcMemSize; i++)
+            {
+                MMU_p[i] = (uint32_t)virt_to_phys((uint32_t *)((SrcStart + i) << PAGE_SHIFT));
+            }
+        }
+
+        /* map dst addr */
+        if (req->src.yrgb_addr < KERNEL_SPACE_VALID) {
+            ret = rga_MapUserMemory(&pages[CMDMemSize + SrcMemSize], &MMU_Base[CMDMemSize + SrcMemSize], DstStart, DstMemSize);
+            if (ret < 0) {
+                pr_err("rga map dst memory failed\n");
+                status = ret;
+                break;
+            }
+        }
+        else {
+            MMU_p = MMU_Base + CMDMemSize + SrcMemSize;
+            for(i=0; i<DstMemSize; i++)
+                MMU_p[i] = (uint32_t)virt_to_phys((uint32_t *)((DstStart + i) << PAGE_SHIFT));
+        }
+
+
+        /* zsq
+         * change the buf address in req struct
+         * for the reason of lie to MMU
+         */
+        req->mmu_info.base_addr = (virt_to_phys(MMU_Base)>>2);
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK)) | (CMDMemSize << PAGE_SHIFT);
+        req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK)) | ((CMDMemSize + SrcMemSize) << PAGE_SHIFT);
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize + 1));
+
+        rga_mmu_buf_get(&rga_mmu_buf, AllSize + 16);
+        reg->MMU_len = AllSize + 16;
+
+        return status;
+
+    }
+    while(0);
+
+    return 0;
+}
+
+static int rga_mmu_info_color_fill_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int DstMemSize;
+    unsigned long DstStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p, *MMU_Base_phys;
+    int ret;
+    int status;
+
+    MMU_Base = NULL;
+
+    do {
+        DstMemSize = rga_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                        req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                        &DstStart);
+        if(DstMemSize == 0) {
+            return -EINVAL;
+        }
+
+        AllSize = (DstMemSize + 15) & (~15);
+
+        pages = rga_mmu_buf.pages;
+
+        if (rga_mmu_buf_get_try(&rga_mmu_buf, AllSize + 16)) {
+            pr_err("RGA Get MMU mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        mutex_lock(&rga_service.lock);
+        MMU_Base = rga_mmu_buf.buf_virtual + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        MMU_Base_phys = rga_mmu_buf.buf + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        mutex_unlock(&rga_service.lock);
+
+        if (req->dst.yrgb_addr < KERNEL_SPACE_VALID) {
+            if (req->sg_dst) {
+                ret = rga_MapION(req->sg_dst, &MMU_Base[0], DstMemSize, req->line_draw_info.line_width);
+            }
+            else {
+                ret = rga_MapUserMemory(&pages[0], &MMU_Base[0], DstStart, DstMemSize);
+                if (ret < 0) {
+                    pr_err("rga map dst memory failed\n");
+                    status = ret;
+                    break;
+                }
+            }
+        }
+        else {
+            MMU_p = MMU_Base;
+            for(i=0; i<DstMemSize; i++)
+                MMU_p[i] = (uint32_t)((DstStart + i) << PAGE_SHIFT);
+        }
+
+        MMU_Base[AllSize] = MMU_Base[AllSize - 1];
+
+        /* zsq
+         * change the buf address in req struct
+         */
+
+        req->mmu_info.base_addr = ((unsigned long)(MMU_Base_phys)>>2);
+        req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK));
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize + 1));
+
+        rga_mmu_buf_get(&rga_mmu_buf, AllSize + 16);
+        reg->MMU_len = AllSize + 16;
+
+        return 0;
+    }
+    while(0);
+
+    return status;
+}
+
+
+static int rga_mmu_info_line_point_drawing_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    return 0;
+}
+
+static int rga_mmu_info_blur_sharp_filter_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    return 0;
+}
+
+
+
+static int rga_mmu_info_pre_scale_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int SrcMemSize, DstMemSize;
+    unsigned long SrcStart, DstStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p, *MMU_Base_phys;
+    int ret;
+    int status;
+    uint32_t uv_size, v_size;
+
+    MMU_Base = NULL;
+
+    do {
+        /* cal src buf mmu info */
+        SrcMemSize = rga_buf_size_cal(req->src.yrgb_addr, req->src.uv_addr, req->src.v_addr,
+                                        req->src.format, req->src.vir_w, req->src.vir_h,
+                                        &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        /* cal dst buf mmu info */
+        DstMemSize = rga_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                        req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                        &DstStart);
+        if(DstMemSize == 0) {
+            return -EINVAL;
+        }
+
+	    SrcMemSize = (SrcMemSize + 15) & (~15);
+	    DstMemSize = (DstMemSize + 15) & (~15);
+
+        AllSize = SrcMemSize + DstMemSize;
+
+        pages = rga_mmu_buf.pages;
+
+        if (rga_mmu_buf_get_try(&rga_mmu_buf, AllSize + 16)) {
+            pr_err("RGA Get MMU mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        mutex_lock(&rga_service.lock);
+        MMU_Base = rga_mmu_buf.buf_virtual + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        MMU_Base_phys = rga_mmu_buf.buf + (rga_mmu_buf.front & (rga_mmu_buf.size - 1));
+        mutex_unlock(&rga_service.lock);
+
+        /* map src pages */
+        if ((req->mmu_info.mmu_flag >> 8) & 1) {
+            if (req->sg_src) {
+                ret = rga_MapION(req->sg_src, &MMU_Base[0], SrcMemSize,req->line_draw_info.flag);
+            }
+            else {
+                ret = rga_MapUserMemory(&pages[0], &MMU_Base[0], SrcStart, SrcMemSize);
+                if (ret < 0) {
+                    pr_err("rga map src memory failed\n");
+                    status = ret;
+                    break;
+                }
+            }
+        }
+        else {
+            MMU_p = MMU_Base;
+
+            for(i=0; i<SrcMemSize; i++)
+                MMU_p[i] = (uint32_t)((SrcStart + i) << PAGE_SHIFT);
+        }
+
+        if((req->mmu_info.mmu_flag >> 10) & 1) {
+            if (req->sg_dst) {
+                ret = rga_MapION(req->sg_dst, &MMU_Base[SrcMemSize], DstMemSize, req->line_draw_info.line_width);
+            }
+            else {
+                ret = rga_MapUserMemory(&pages[SrcMemSize], &MMU_Base[SrcMemSize], DstStart, DstMemSize);
+                if (ret < 0) {
+                    pr_err("rga map dst memory failed\n");
+                    status = ret;
+                    break;
+                }
+            }
+        }
+        else
+        {
+            /* kernel space */
+            MMU_p = MMU_Base + SrcMemSize;
+
+            if(req->dst.yrgb_addr == (unsigned long)rga_service.pre_scale_buf) {
+                for(i=0; i<DstMemSize; i++)
+                    MMU_p[i] = rga_service.pre_scale_buf[i];
+            }
+            else {
+                for(i=0; i<DstMemSize; i++)
+                    MMU_p[i] = (uint32_t)((DstStart + i) << PAGE_SHIFT);
+            }
+        }
+
+        MMU_Base[AllSize] = MMU_Base[AllSize];
+
+        /* zsq
+         * change the buf address in req struct
+         * for the reason of lie to MMU
+         */
+
+        req->mmu_info.base_addr = ((unsigned long)(MMU_Base_phys)>>2);
+
+        uv_size = (req->src.uv_addr - (SrcStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+        v_size = (req->src.v_addr - (SrcStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK));
+        req->src.uv_addr = (req->src.uv_addr & (~PAGE_MASK)) | (uv_size << PAGE_SHIFT);
+        req->src.v_addr = (req->src.v_addr & (~PAGE_MASK)) | (v_size << PAGE_SHIFT);
+
+        uv_size = (req->dst.uv_addr - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+        v_size = (req->dst.v_addr - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+
+        req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK)) | ((SrcMemSize) << PAGE_SHIFT);
+        req->dst.uv_addr = (req->dst.uv_addr & (~PAGE_MASK)) | ((SrcMemSize + uv_size) << PAGE_SHIFT);
+        req->dst.v_addr = (req->dst.v_addr & (~PAGE_MASK)) | ((SrcMemSize + v_size) << PAGE_SHIFT);
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize + 1));
+
+	    rga_mmu_buf_get(&rga_mmu_buf, AllSize + 16);
+        reg->MMU_len = AllSize + 16;
+
+        return 0;
+    }
+    while(0);
+
+    return status;
+}
+
+
+static int rga_mmu_info_update_palette_table_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int SrcMemSize, CMDMemSize;
+    unsigned long SrcStart, CMDStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p;
+    int ret, status;
+
+    MMU_Base = NULL;
+
+    do {
+        /* cal src buf mmu info */
+        SrcMemSize = rga_mem_size_cal(req->src.yrgb_addr, req->src.vir_w * req->src.vir_h, &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        /* cal cmd buf mmu info */
+        CMDMemSize = rga_mem_size_cal((unsigned long)rga_service.cmd_buff, RGA_CMD_BUF_SIZE, &CMDStart);
+        if(CMDMemSize == 0) {
+            return -EINVAL;
+        }
+
+        AllSize = SrcMemSize + CMDMemSize;
+
+        pages = kzalloc(AllSize * sizeof(struct page *), GFP_KERNEL);
+        if(pages == NULL) {
+            pr_err("RGA MMU malloc pages mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        MMU_Base = kzalloc((AllSize + 1)* sizeof(uint32_t), GFP_KERNEL);
+        if(pages == NULL) {
+            pr_err("RGA MMU malloc MMU_Base point failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        for(i=0; i<CMDMemSize; i++) {
+            MMU_Base[i] = (uint32_t)virt_to_phys((uint32_t *)((CMDStart + i) << PAGE_SHIFT));
+        }
+
+        if (req->src.yrgb_addr < KERNEL_SPACE_VALID)
+        {
+            ret = rga_MapUserMemory(&pages[CMDMemSize], &MMU_Base[CMDMemSize], SrcStart, SrcMemSize);
+            if (ret < 0) {
+                pr_err("rga map src memory failed\n");
+                return -EINVAL;
+            }
+        }
+        else
+        {
+            MMU_p = MMU_Base + CMDMemSize;
+
+                for(i=0; i<SrcMemSize; i++)
+                {
+                    MMU_p[i] = (uint32_t)virt_to_phys((uint32_t *)((SrcStart + i) << PAGE_SHIFT));
+                }
+        }
+
+        /* zsq
+         * change the buf address in req struct
+         * for the reason of lie to MMU
+         */
+        req->mmu_info.base_addr = (virt_to_phys(MMU_Base) >> 2);
+
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK)) | (CMDMemSize << PAGE_SHIFT);
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+
+
+        if (pages != NULL) {
+            /* Free the page table */
+            kfree(pages);
+        }
+
+        return 0;
+    }
+    while(0);
+
+    if (pages != NULL)
+        kfree(pages);
+
+    if (MMU_Base != NULL)
+        kfree(MMU_Base);
+
+    return status;
+}
+
+static int rga_mmu_info_update_patten_buff_mode(struct rga_reg *reg, struct rga_req *req)
+{
+    int SrcMemSize, CMDMemSize;
+    unsigned long SrcStart, CMDStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p;
+    int ret, status;
+
+    MMU_Base = MMU_p = 0;
+
+    do
+    {
+
+        /* cal src buf mmu info */
+        SrcMemSize = rga_mem_size_cal(req->pat.yrgb_addr, req->pat.vir_w * req->pat.vir_h * 4, &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        /* cal cmd buf mmu info */
+        CMDMemSize = rga_mem_size_cal((unsigned long)rga_service.cmd_buff, RGA_CMD_BUF_SIZE, &CMDStart);
+        if(CMDMemSize == 0) {
+            return -EINVAL;
+        }
+
+        AllSize = SrcMemSize + CMDMemSize;
+
+        pages = kzalloc(AllSize * sizeof(struct page *), GFP_KERNEL);
+        if(pages == NULL) {
+            pr_err("RGA MMU malloc pages mem failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        MMU_Base = kzalloc(AllSize * sizeof(uint32_t), GFP_KERNEL);
+        if(MMU_Base == NULL) {
+            pr_err("RGA MMU malloc MMU_Base point failed\n");
+            status = RGA_MALLOC_ERROR;
+            break;
+        }
+
+        for(i=0; i<CMDMemSize; i++) {
+            MMU_Base[i] = virt_to_phys((uint32_t *)((CMDStart + i) << PAGE_SHIFT));
+        }
+
+        if (req->src.yrgb_addr < KERNEL_SPACE_VALID)
+        {
+            ret = rga_MapUserMemory(&pages[CMDMemSize], &MMU_Base[CMDMemSize], SrcStart, SrcMemSize);
+            if (ret < 0) {
+                pr_err("rga map src memory failed\n");
+                status = ret;
+                break;
+            }
+        }
+        else
+        {
+            MMU_p = MMU_Base + CMDMemSize;
+
+            for(i=0; i<SrcMemSize; i++)
+            {
+                MMU_p[i] = (uint32_t)virt_to_phys((uint32_t *)((SrcStart + i) << PAGE_SHIFT));
+            }
+        }
+
+        /* zsq
+         * change the buf address in req struct
+         * for the reason of lie to MMU
+         */
+        req->mmu_info.base_addr = (virt_to_phys(MMU_Base) >> 2);
+
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK)) | (CMDMemSize << PAGE_SHIFT);
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+
+        if (pages != NULL) {
+            /* Free the page table */
+            kfree(pages);
+        }
+
+        return 0;
+
+    }
+    while(0);
+
+    if (pages != NULL)
+        kfree(pages);
+
+    if (MMU_Base != NULL)
+        kfree(MMU_Base);
+
+    return status;
+}
+
+int rga_set_mmu_info(struct rga_reg *reg, struct rga_req *req)
+{
+    int ret;
+
+    switch (req->render_mode) {
+        case bitblt_mode :
+            ret = rga_mmu_info_BitBlt_mode(reg, req);
+            break;
+        case color_palette_mode :
+            ret = rga_mmu_info_color_palette_mode(reg, req);
+            break;
+        case color_fill_mode :
+            ret = rga_mmu_info_color_fill_mode(reg, req);
+            break;
+        case line_point_drawing_mode :
+            ret = rga_mmu_info_line_point_drawing_mode(reg, req);
+            break;
+        case blur_sharp_filter_mode :
+            ret = rga_mmu_info_blur_sharp_filter_mode(reg, req);
+            break;
+        case pre_scaling_mode :
+            ret = rga_mmu_info_pre_scale_mode(reg, req);
+            break;
+        case update_palette_table_mode :
+            ret = rga_mmu_info_update_palette_table_mode(reg, req);
+            break;
+        case update_patten_buff_mode :
+            ret = rga_mmu_info_update_patten_buff_mode(reg, req);
+            break;
+        default :
+            ret = -1;
+            break;
+    }
+
+    return ret;
+}
+
diff --git a/drivers/video/rockchip/rga/rga_mmu_info.h b/drivers/video/rockchip/rga/rga_mmu_info.h
new file mode 100644
index 0000000000000..502f6594c29d4
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_mmu_info.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_MMU_INFO_H__
+#define __RGA_MMU_INFO_H__
+
+#include "rga.h"
+#include "RGA_API.h"
+
+#ifndef MIN
+#define MIN(X, Y)           ((X)<(Y)?(X):(Y))
+#endif
+
+#ifndef MAX
+#define MAX(X, Y)           ((X)>(Y)?(X):(Y))
+#endif
+
+extern struct rga_drvdata *rga_drvdata;
+
+void rga_dma_flush_range(void *pstart, void *pend);
+int rga_set_mmu_info(struct rga_reg *reg, struct rga_req *req);
+
+
+#endif
+
+
diff --git a/drivers/video/rockchip/rga/rga_reg_info.c b/drivers/video/rockchip/rga/rga_reg_info.c
new file mode 100644
index 0000000000000..563eaab7bb9a9
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_reg_info.c
@@ -0,0 +1,1587 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+//#include <linux/kernel.h>
+#include <linux/memory.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/err.h>
+#include <linux/clk.h>
+#include <asm/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+//#include <mach/io.h>
+//#include <mach/irqs.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/miscdevice.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/syscalls.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <asm/cacheflush.h>
+#include <linux/slab.h>
+#include <linux/fb.h>
+#include <linux/wakelock.h>
+#include <linux/version.h>
+
+#include "rga_reg_info.h"
+#include "rga_rop.h"
+#include "rga.h"
+
+
+/*************************************************************
+Func:
+    RGA_pixel_width_init
+Description:
+    select pixel_width form data format
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+unsigned char
+RGA_pixel_width_init(unsigned int format)
+{
+    unsigned char pixel_width;
+
+    pixel_width = 0;
+
+    switch(format)
+    {
+        /* RGB FORMAT */
+        case RK_FORMAT_RGBA_8888 :   pixel_width = 4;   break;
+        case RK_FORMAT_RGBX_8888 :   pixel_width = 4;   break;
+        case RK_FORMAT_RGB_888   :   pixel_width = 3;   break;
+        case RK_FORMAT_BGRA_8888 :   pixel_width = 4;   break;
+        case RK_FORMAT_RGB_565   :   pixel_width = 2;   break;
+        case RK_FORMAT_RGBA_5551 :   pixel_width = 2;   break;
+        case RK_FORMAT_RGBA_4444 :   pixel_width = 2;   break;
+        case RK_FORMAT_BGR_888   :   pixel_width = 3;   break;
+
+        /* YUV FORMAT */
+        case RK_FORMAT_YCbCr_422_SP :   pixel_width = 1;  break;
+        case RK_FORMAT_YCbCr_422_P  :   pixel_width = 1;  break;
+        case RK_FORMAT_YCbCr_420_SP :   pixel_width = 1;  break;
+        case RK_FORMAT_YCbCr_420_P  :   pixel_width = 1;  break;
+        case RK_FORMAT_YCrCb_422_SP :   pixel_width = 1;  break;
+        case RK_FORMAT_YCrCb_422_P  :   pixel_width = 1;  break;
+        case RK_FORMAT_YCrCb_420_SP :   pixel_width = 1;  break;
+        case RK_FORMAT_YCrCb_420_P :    pixel_width = 1;  break;
+        //case default :                  pixel_width = 0;  break;
+    }
+
+    return pixel_width;
+}
+
+/*************************************************************
+Func:
+    dst_ctrl_cal
+Description:
+    calculate dst act window position / width / height
+    and set the tile struct
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+static void
+dst_ctrl_cal(const struct rga_req *msg, TILE_INFO *tile)
+{
+    u32 width   = msg->dst.act_w;
+    u32 height  = msg->dst.act_h;
+    s32 xoff    = msg->dst.x_offset;
+    s32 yoff    = msg->dst.y_offset;
+
+    s32 x0, y0, x1, y1, x2, y2;
+    s32 x00,y00,x10,y10,x20,y20;
+    s32 xx, xy, yx, yy;
+    s32 pos[8];
+
+    s32 xmax, xmin, ymax, ymin;
+
+    s32 sina = msg->sina; /* 16.16 */
+    s32 cosa = msg->cosa; /* 16.16 */
+
+    xmax = xmin = ymax = ymin = 0;
+
+    if((msg->rotate_mode == 0)||(msg->rotate_mode == 2)||(msg->rotate_mode == 3))
+    {
+        pos[0] = xoff;
+        pos[1] = yoff;
+
+        pos[2] = xoff;
+        pos[3] = yoff + height - 1;
+
+        pos[4] = xoff + width - 1;
+        pos[5] = yoff + height - 1;
+
+        pos[6] = xoff + width - 1;
+        pos[7] = yoff;
+
+        xmax = MIN(MAX(MAX(MAX(pos[0], pos[2]), pos[4]), pos[6]), msg->clip.xmax);
+        xmin = MAX(MIN(MIN(MIN(pos[0], pos[2]), pos[4]), pos[6]), msg->clip.xmin);
+
+        ymax = MIN(MAX(MAX(MAX(pos[1], pos[3]), pos[5]), pos[7]), msg->clip.ymax);
+        ymin = MAX(MIN(MIN(MIN(pos[1], pos[3]), pos[5]), pos[7]), msg->clip.ymin);
+
+        //printk("xmax = %d, xmin = %d, ymin = %d, ymax = %d\n", xmax, xmin, ymin, ymax);
+    }
+    else if(msg->rotate_mode == 1)
+    {
+        if((sina == 0) || (cosa == 0))
+        {
+            if((sina == 0) && (cosa == -65536))
+            {
+                /* 180 */
+                pos[0] = xoff - width + 1;
+                pos[1] = yoff - height + 1;
+
+                pos[2] = xoff - width  + 1;
+                pos[3] = yoff;
+
+                pos[4] = xoff;
+                pos[5] = yoff;
+
+                pos[6] = xoff;
+                pos[7] = yoff - height + 1;
+            }
+            else if((cosa == 0)&&(sina == 65536))
+            {
+                /* 90 */
+                pos[0] = xoff - height + 1;
+                pos[1] = yoff;
+
+                pos[2] = xoff - height + 1;
+                pos[3] = yoff + width - 1;
+
+                pos[4] = xoff;
+                pos[5] = yoff + width - 1;
+
+                pos[6] = xoff;
+                pos[7] = yoff;
+            }
+            else if((cosa == 0)&&(sina == -65536))
+            {
+                /* 270 */
+                pos[0] = xoff;
+                pos[1] = yoff - width + 1;
+
+                pos[2] = xoff;
+                pos[3] = yoff;
+
+                pos[4] = xoff + height - 1;
+                pos[5] = yoff;
+
+                pos[6] = xoff + height - 1;
+                pos[7] = yoff - width + 1;
+            }
+            else
+            {
+                /* 0 */
+                pos[0] = xoff;
+                pos[1] = yoff;
+
+                pos[2] = xoff;
+                pos[3] = yoff + height - 1;
+
+                pos[4] = xoff + width - 1;
+                pos[5] = yoff + height - 1;
+
+                pos[6] = xoff + width - 1;
+                pos[7] = yoff;
+            }
+
+            xmax = MIN(MAX(MAX(MAX(pos[0], pos[2]), pos[4]), pos[6]), msg->clip.xmax);
+            xmin = MAX(MIN(MIN(MIN(pos[0], pos[2]), pos[4]), pos[6]), msg->clip.xmin);
+
+            ymax = MIN(MAX(MAX(MAX(pos[1], pos[3]), pos[5]), pos[7]), msg->clip.ymax);
+            ymin = MAX(MIN(MIN(MIN(pos[1], pos[3]), pos[5]), pos[7]), msg->clip.ymin);
+        }
+        else
+        {
+            xx = msg->cosa;
+            xy = msg->sina;
+            yx = xy;
+            yy = xx;
+
+            x0 = width + xoff;
+            y0 = yoff;
+
+            x1 = xoff;
+            y1 = height + yoff;
+
+            x2 = width + xoff;
+            y2 = height + yoff;
+
+            pos[0] = xoff;
+            pos[1] = yoff;
+
+            pos[2] = x00 = (((x0 - xoff)*xx - (y0 - yoff)*xy)>>16) + xoff;
+            pos[3] = y00 = (((x0 - xoff)*yx + (y0 - yoff)*yy)>>16) + yoff;
+
+            pos[4] = x10 = (((x1 - xoff)*xx - (y1 - yoff)*xy)>>16) + xoff;
+            pos[5] = y10 = (((x1 - xoff)*yx + (y1 - yoff)*yy)>>16) + yoff;
+
+            pos[6] = x20 = (((x2 - xoff)*xx - (y2 - yoff)*xy)>>16) + xoff;
+            pos[7] = y20 = (((x2 - xoff)*yx + (y2 - yoff)*yy)>>16) + yoff;
+
+            xmax = MAX(MAX(MAX(x00, xoff), x10), x20) + 2;
+            xmin = MIN(MIN(MIN(x00, xoff), x10), x20) - 1;
+
+            ymax = MAX(MAX(MAX(y00, yoff), y10), y20) + 2;
+            ymin = MIN(MIN(MIN(y00, yoff), y10), y20) - 1;
+
+            xmax = MIN(xmax, msg->clip.xmax);
+            xmin = MAX(xmin, msg->clip.xmin);
+
+            ymax = MIN(ymax, msg->clip.ymax);
+            ymin = MAX(ymin, msg->clip.ymin);
+
+            //printk("xmin = %d, xmax = %d, ymin = %d, ymax = %d\n", xmin, xmax, ymin, ymax);
+        }
+    }
+
+    if ((xmax < xmin) || (ymax < ymin)) {
+        xmin = xmax;
+        ymin = ymax;
+    }
+
+    if ((xmin >= msg->dst.vir_w)||(xmax < 0)||(ymin >= msg->dst.vir_h)||(ymax < 0)) {
+        xmin = xmax = ymin = ymax = 0;
+    }
+
+    //printk("xmin = %d, xmax = %d, ymin = %d, ymax = %d\n", xmin, xmax, ymin, ymax);
+
+    tile->dst_ctrl.w = (xmax - xmin);
+    tile->dst_ctrl.h = (ymax - ymin);
+    tile->dst_ctrl.x_off = xmin;
+    tile->dst_ctrl.y_off = ymin;
+
+    //printk("tile->dst_ctrl.w = %x, tile->dst_ctrl.h = %x\n", tile->dst_ctrl.w, tile->dst_ctrl.h);
+
+    tile->tile_x_num = (xmax - xmin + 1 + 7)>>3;
+    tile->tile_y_num = (ymax - ymin + 1 + 7)>>3;
+
+    tile->dst_x_tmp = xmin - msg->dst.x_offset;
+    tile->dst_y_tmp = ymin - msg->dst.y_offset;
+}
+
+/*************************************************************
+Func:
+    src_tile_info_cal
+Description:
+    calculate src remap window position / width / height
+    and set the tile struct
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+src_tile_info_cal(const struct rga_req *msg, TILE_INFO *tile)
+{
+    s32 x0, x1, x2, x3, y0, y1, y2, y3;
+
+    int64_t xx, xy, yx, yy;
+
+    int64_t pos[8];
+    int64_t epos[8];
+
+    int64_t x_dx, x_dy, y_dx, y_dy;
+    int64_t x_temp_start, y_temp_start;
+    int64_t xmax, xmin, ymax, ymin;
+
+    int64_t t_xoff, t_yoff;
+
+    xx = tile->matrix[0]; /* 32.32 */
+    xy = tile->matrix[1]; /* 32.32 */
+    yx = tile->matrix[2]; /* 32.32 */
+    yy = tile->matrix[3]; /* 32.32 */
+
+    if(msg->rotate_mode == 1)
+    {
+        x0 = tile->dst_x_tmp;
+        y0 = tile->dst_y_tmp;
+
+        x1 = x0;
+        y1 = y0 + 8;
+
+        x2 = x0 + 8;
+        y2 = y0 + 8;
+
+        x3 = x0 + 8;
+        y3 = y0;
+
+        pos[0] = (x0*xx + y0*yx);
+        pos[1] = (x0*xy + y0*yy);
+
+        pos[2] = (x1*xx + y1*yx);
+        pos[3] = (x1*xy + y1*yy);
+
+        pos[4] = (x2*xx + y2*yx);
+        pos[5] = (x2*xy + y2*yy);
+
+        pos[6] = (x3*xx + y3*yx);
+        pos[7] = (x3*xy + y3*yy);
+
+        y1 = y0 + 7;
+        x2 = x0 + 7;
+        y2 = y0 + 7;
+        x3 = x0 + 7;
+
+        epos[0] = pos[0];
+        epos[1] = pos[1];
+
+        epos[2] = (x1*xx + y1*yx);
+        epos[3] = (x1*xy + y1*yy);
+
+        epos[4] = (x2*xx + y2*yx);
+        epos[5] = (x2*xy + y2*yy);
+
+        epos[6] = (x3*xx + y3*yx);
+        epos[7] = (x3*xy + y3*yy);
+
+        x_dx = pos[6] - pos[0];
+        x_dy = pos[7] - pos[1];
+
+        y_dx = pos[2] - pos[0];
+        y_dy = pos[3] - pos[1];
+
+        tile->x_dx = (s32)(x_dx >> 22 );
+        tile->x_dy = (s32)(x_dy >> 22 );
+        tile->y_dx = (s32)(y_dx >> 22 );
+        tile->y_dy = (s32)(y_dy >> 22 );
+
+        x_temp_start = x0*xx + y0*yx;
+        y_temp_start = x0*xy + y0*yy;
+
+        xmax = (MAX(MAX(MAX(epos[0], epos[2]), epos[4]), epos[6]));
+        xmin = (MIN(MIN(MIN(epos[0], epos[2]), epos[4]), epos[6]));
+
+        ymax = (MAX(MAX(MAX(epos[1], epos[3]), epos[5]), epos[7]));
+        ymin = (MIN(MIN(MIN(epos[1], epos[3]), epos[5]), epos[7]));
+
+        t_xoff = (x_temp_start - xmin)>>18;
+        t_yoff = (y_temp_start - ymin)>>18;
+
+        tile->tile_xoff = (s32)t_xoff;
+        tile->tile_yoff = (s32)t_yoff;
+
+        tile->tile_w = (u16)((xmax - xmin)>>21); //.11
+        tile->tile_h = (u16)((ymax - ymin)>>21); //.11
+
+        tile->tile_start_x_coor = (s16)(xmin>>29); //.3
+        tile->tile_start_y_coor = (s16)(ymin>>29); //.3
+    }
+    else if (msg->rotate_mode == 2)
+    {
+        tile->x_dx = (s32)((8*xx)>>22);
+        tile->x_dy = 0;
+        tile->y_dx = 0;
+        tile->y_dy = (s32)((8*yy)>>22);
+
+        tile->tile_w = ABS((s32)((7*xx)>>21));
+        tile->tile_h = ABS((s32)((7*yy)>>21));
+
+        tile->tile_xoff = ABS((s32)((7*xx)>>18));
+        tile->tile_yoff = 0;
+
+        tile->tile_start_x_coor = (((msg->src.act_w - 1)<<11) - (tile->tile_w))>>8;
+        tile->tile_start_y_coor = 0;
+    }
+    else if (msg->rotate_mode == 3)
+    {
+        tile->x_dx = (s32)((8*xx)>>22);
+        tile->x_dy = 0;
+        tile->y_dx = 0;
+        tile->y_dy = (s32)((8*yy)>>22);
+
+        tile->tile_w = ABS((s32)((7*xx)>>21));
+        tile->tile_h = ABS((s32)((7*yy)>>21));
+
+        tile->tile_xoff = 0;
+        tile->tile_yoff = ABS((s32)((7*yy)>>18));
+
+        tile->tile_start_x_coor = 0;
+        tile->tile_start_y_coor = (((msg->src.act_h - 1)<<11) - (tile->tile_h))>>8;
+    }
+
+    if ((msg->scale_mode == 2)||(msg->alpha_rop_flag >> 7))
+    {
+        tile->tile_start_x_coor -= (1<<3);
+        tile->tile_start_y_coor -= (1<<3);
+        tile->tile_w += (2 << 11);
+        tile->tile_h += (2 << 11);
+        tile->tile_xoff += (1<<14);
+        tile->tile_yoff += (1<<14);
+    }
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_mode_ctrl
+Description:
+    fill mode ctrl reg info
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+RGA_set_mode_ctrl(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_MODE_CTL;
+    u32 reg = 0;
+
+    u8 src_rgb_pack = 0;
+    u8 src_format = 0;
+    u8 src_rb_swp = 0;
+    u8 src_a_swp = 0;
+    u8 src_cbcr_swp = 0;
+
+    u8 dst_rgb_pack = 0;
+    u8 dst_format = 0;
+    u8 dst_rb_swp = 0;
+    u8 dst_a_swp = 0;
+
+    bRGA_MODE_CTL = (u32 *)(base + RGA_MODE_CTRL_OFFSET);
+
+    reg = ((reg & (~m_RGA_MODE_CTRL_2D_RENDER_MODE)) | (s_RGA_MODE_CTRL_2D_RENDER_MODE(msg->render_mode)));
+
+    /* src info set */
+
+    if (msg->render_mode == color_palette_mode || msg->render_mode == update_palette_table_mode)
+    {
+        src_format = 0x10 | (msg->palette_mode & 3);
+    }
+    else
+    {
+        switch (msg->src.format)
+        {
+            case RK_FORMAT_RGBA_8888    : src_format = 0x0; break;
+            case RK_FORMAT_RGBA_4444    : src_format = 0x3; break;
+            case RK_FORMAT_RGBA_5551    : src_format = 0x2; break;
+            case RK_FORMAT_BGRA_8888    : src_format = 0x0; src_rb_swp = 0x1; break;
+            case RK_FORMAT_RGBX_8888    : src_format = 0x0; break;
+            case RK_FORMAT_RGB_565      : src_format = 0x1; break;
+            case RK_FORMAT_RGB_888      : src_format = 0x0; src_rgb_pack = 1; break;
+            case RK_FORMAT_BGR_888      : src_format = 0x0; src_rgb_pack = 1; src_rb_swp = 1; break;
+
+            case RK_FORMAT_YCbCr_422_SP : src_format = 0x4; break;
+            case RK_FORMAT_YCbCr_422_P  : src_format = 0x5; break;
+            case RK_FORMAT_YCbCr_420_SP : src_format = 0x6; break;
+            case RK_FORMAT_YCbCr_420_P  : src_format = 0x7; break;
+
+            case RK_FORMAT_YCrCb_422_SP : src_format = 0x4; src_cbcr_swp = 1; break;
+            case RK_FORMAT_YCrCb_422_P  : src_format = 0x5; src_cbcr_swp = 1; break;
+            case RK_FORMAT_YCrCb_420_SP : src_format = 0x6; src_cbcr_swp = 1; break;
+            case RK_FORMAT_YCrCb_420_P  : src_format = 0x7; src_cbcr_swp = 1; break;
+        }
+    }
+
+    src_a_swp = msg->src.alpha_swap & 1;
+
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_RGB_PACK))      | (s_RGA_MODE_CTRL_SRC_RGB_PACK(src_rgb_pack)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_FORMAT))        | (s_RGA_MODE_CTRL_SRC_FORMAT(src_format)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_RB_SWAP))       | (s_RGA_MODE_CTRL_SRC_RB_SWAP(src_rb_swp)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_ALPHA_SWAP))    | (s_RGA_MODE_CTRL_SRC_ALPHA_SWAP(src_a_swp)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_UV_SWAP_MODE )) | (s_RGA_MODE_CTRL_SRC_UV_SWAP_MODE (src_cbcr_swp)));
+
+
+    /* YUV2RGB MODE */
+    reg = ((reg & (~m_RGA_MODE_CTRL_YUV2RGB_CON_MODE)) | (s_RGA_MODE_CTRL_YUV2RGB_CON_MODE(msg->yuv2rgb_mode)));
+
+    /* ROTATE MODE */
+    reg = ((reg & (~m_RGA_MODE_CTRL_ROTATE_MODE)) | (s_RGA_MODE_CTRL_ROTATE_MODE(msg->rotate_mode)));
+
+    /* SCALE MODE */
+    reg = ((reg & (~m_RGA_MODE_CTRL_SCALE_MODE)) | (s_RGA_MODE_CTRL_SCALE_MODE(msg->scale_mode)));
+
+    /* COLOR FILL MODE */
+    reg = ((reg & (~m_RGA_MODE_CTRL_PAT_SEL)) | (s_RGA_MODE_CTRL_PAT_SEL(msg->color_fill_mode)));
+
+
+    if ((msg->render_mode == update_palette_table_mode)||(msg->render_mode == update_patten_buff_mode))
+    {
+        dst_format = msg->pat.format;
+    }
+    else
+    {
+        dst_format = (u8)msg->dst.format;
+    }
+
+    /* dst info set */
+    switch (dst_format)
+    {
+        case RK_FORMAT_BGRA_8888 : dst_format = 0x0; dst_rb_swp = 0x1; break;
+        case RK_FORMAT_RGBA_4444 : dst_format = 0x3; break;
+        case RK_FORMAT_RGBA_5551 : dst_format = 0x2; break;
+        case RK_FORMAT_RGBA_8888 : dst_format = 0x0; break;
+        case RK_FORMAT_RGB_565   : dst_format = 0x1; break;
+        case RK_FORMAT_RGB_888   : dst_format = 0x0; dst_rgb_pack = 0x1; break;
+        case RK_FORMAT_BGR_888   : dst_format = 0x0; dst_rgb_pack = 0x1; dst_rb_swp = 1; break;
+        case RK_FORMAT_RGBX_8888 : dst_format = 0x0; break;
+    }
+
+    dst_a_swp = msg->dst.alpha_swap & 1;
+
+    reg = ((reg & (~m_RGA_MODE_CTRL_DST_FORMAT))       | (s_RGA_MODE_CTRL_DST_FORMAT(dst_format)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_DST_RGB_PACK))     | (s_RGA_MODE_CTRL_DST_RGB_PACK(dst_rgb_pack)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_DST_RB_SWAP))      | (s_RGA_MODE_CTRL_DST_RB_SWAP(dst_rb_swp)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_DST_ALPHA_SWAP))   | (s_RGA_MODE_CTRL_DST_ALPHA_SWAP(dst_a_swp)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_LUT_ENDIAN_MODE))  | (s_RGA_MODE_CTRL_LUT_ENDIAN_MODE(msg->endian_mode & 1)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_SRC_TRANS_MODE))   | (s_RGA_MODE_CTRL_SRC_TRANS_MODE(msg->src_trans_mode)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_ZERO_MODE_ENABLE)) | (s_RGA_MODE_CTRL_ZERO_MODE_ENABLE(msg->alpha_rop_mode >> 4)));
+    reg = ((reg & (~m_RGA_MODE_CTRL_DST_ALPHA_ENABLE)) | (s_RGA_MODE_CTRL_DST_ALPHA_ENABLE(msg->alpha_rop_mode >> 5)));
+
+    *bRGA_MODE_CTL = reg;
+
+}
+
+
+
+/*************************************************************
+Func:
+    RGA_set_src
+Description:
+    fill src relate reg info
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+RGA_set_src(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_SRC_VIR_INFO;
+    u32 *bRGA_SRC_ACT_INFO;
+    u32 *bRGA_SRC_Y_MST;
+    u32 *bRGA_SRC_CB_MST;
+    u32 *bRGA_SRC_CR_MST;
+
+    s16 x_off, y_off, stride;
+    s16 uv_x_off, uv_y_off, uv_stride;
+    u32 pixel_width;
+
+    uv_x_off = uv_y_off = uv_stride = 0;
+
+    bRGA_SRC_Y_MST = (u32 *)(base + RGA_SRC_Y_MST_OFFSET);
+    bRGA_SRC_CB_MST = (u32 *)(base + RGA_SRC_CB_MST_OFFSET);
+    bRGA_SRC_CR_MST = (u32 *)(base + RGA_SRC_CR_MST_OFFSET);
+    bRGA_SRC_VIR_INFO = (u32 *)(base + RGA_SRC_VIR_INFO_OFFSET);
+    bRGA_SRC_ACT_INFO = (u32 *)(base + RGA_SRC_ACT_INFO_OFFSET);
+
+    x_off  = msg->src.x_offset;
+    y_off  = msg->src.y_offset;
+
+    pixel_width = RGA_pixel_width_init(msg->src.format);
+
+    stride = ((msg->src.vir_w * pixel_width) + 3) & (~3);
+
+    switch(msg->src.format)
+    {
+        case RK_FORMAT_YCbCr_422_SP :
+            uv_stride = stride;
+            uv_x_off = x_off;
+            uv_y_off = y_off;
+            break;
+        case RK_FORMAT_YCbCr_422_P  :
+            uv_stride = stride >> 1;
+            uv_x_off = x_off >> 1;
+            uv_y_off = y_off;
+            break;
+        case RK_FORMAT_YCbCr_420_SP :
+            uv_stride = stride;
+            uv_x_off = x_off;
+            uv_y_off = y_off >> 1;
+            break;
+        case RK_FORMAT_YCbCr_420_P :
+            uv_stride = stride >> 1;
+            uv_x_off = x_off >> 1;
+            uv_y_off = y_off >> 1;
+            break;
+        case RK_FORMAT_YCrCb_422_SP :
+            uv_stride = stride;
+            uv_x_off = x_off;
+            uv_y_off = y_off;
+            break;
+        case RK_FORMAT_YCrCb_422_P  :
+            uv_stride = stride >> 1;
+            uv_x_off = x_off >> 1;
+            uv_y_off = y_off;
+            break;
+        case RK_FORMAT_YCrCb_420_SP :
+            uv_stride = stride;
+            uv_x_off = x_off;
+            uv_y_off = y_off >> 1;
+            break;
+        case RK_FORMAT_YCrCb_420_P :
+            uv_stride = stride >> 1;
+            uv_x_off = x_off >> 1;
+            uv_y_off = y_off >> 1;
+            break;
+    }
+
+
+    /* src addr set */
+    *bRGA_SRC_Y_MST = msg->src.yrgb_addr + (y_off * stride) + (x_off * pixel_width);
+    *bRGA_SRC_CB_MST = msg->src.uv_addr + uv_y_off * uv_stride + uv_x_off;
+    *bRGA_SRC_CR_MST = msg->src.v_addr + uv_y_off * uv_stride + uv_x_off;
+
+    if((msg->alpha_rop_flag >> 1) & 1)
+        *bRGA_SRC_CB_MST = (u32)msg->rop_mask_addr;
+
+    if (msg->render_mode == color_palette_mode)
+    {
+        u8 shift;
+        u16 sw, byte_num;
+        shift = 3 - (msg->palette_mode & 3);
+        sw = msg->src.vir_w;
+
+        byte_num = sw >> shift;
+        stride = (byte_num + 3) & (~3);
+    }
+
+    /* src act window / vir window set */
+    *bRGA_SRC_VIR_INFO = ((stride >> 2) | (msg->src.vir_h)<<16);
+    *bRGA_SRC_ACT_INFO = ((msg->src.act_w-1) | (msg->src.act_h-1)<<16);
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_dst
+Description:
+    fill dst relate reg info
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static s32 RGA_set_dst(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_DST_MST;
+    u32 *bRGA_DST_UV_MST;
+    u32 *bRGA_DST_VIR_INFO;
+    u32 *bRGA_DST_CTR_INFO;
+    u32 *bRGA_PRESCL_CB_MST;
+    u32 *bRGA_PRESCL_CR_MST;
+    u32 *bRGA_YUV_OUT_CFG;
+
+    u32 reg = 0;
+
+    u8 pw;
+    s16 x_off = msg->dst.x_offset;
+    s16 y_off = msg->dst.y_offset;
+    u16 stride, rop_mask_stride;
+
+    bRGA_DST_MST = (u32 *)(base + RGA_DST_MST_OFFSET);
+    bRGA_DST_UV_MST = (u32 *)(base + RGA_DST_UV_MST_OFFSET);
+    bRGA_DST_VIR_INFO = (u32 *)(base + RGA_DST_VIR_INFO_OFFSET);
+    bRGA_DST_CTR_INFO = (u32 *)(base + RGA_DST_CTR_INFO_OFFSET);
+    bRGA_PRESCL_CB_MST = (u32 *)(base + RGA_PRESCL_CB_MST_OFFSET);
+    bRGA_PRESCL_CR_MST = (u32 *)(base + RGA_PRESCL_CR_MST_OFFSET);
+    bRGA_YUV_OUT_CFG = (u32 *)(base + RGA_YUV_OUT_CFG_OFFSET);
+
+    pw = RGA_pixel_width_init(msg->dst.format);
+
+    stride = (msg->dst.vir_w * pw + 3) & (~3);
+
+    *bRGA_DST_MST = (u32)msg->dst.yrgb_addr + (y_off * stride) + (x_off * pw);
+
+    *bRGA_DST_UV_MST = 0;
+    *bRGA_YUV_OUT_CFG = 0;
+	if (msg->rotate_mode == 1) {
+		if (msg->sina == 65536 && msg->cosa == 0) {
+			/* rotate 90 */
+			x_off = msg->dst.x_offset - msg->dst.act_h + 1;
+		} else if (msg->sina == 0 && msg->cosa == -65536) {
+			/* rotate 180 */
+			x_off = msg->dst.x_offset - msg->dst.act_w + 1;
+			y_off = msg->dst.y_offset - msg->dst.act_h + 1;
+		} else if (msg->sina == -65536 && msg->cosa == 0) {
+			/* totate 270 */
+			y_off = msg->dst.y_offset - msg->dst.act_w + 1;
+		}
+	}
+
+    switch(msg->dst.format)
+    {
+        case RK_FORMAT_YCbCr_422_SP :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off) * stride) + ((x_off) * pw);
+			*bRGA_DST_UV_MST = (u32)msg->dst.uv_addr + (y_off * stride) + x_off;
+			*bRGA_YUV_OUT_CFG |= (((msg->yuv2rgb_mode >> 2) & 3) << 4) | (0 << 3) | (0 << 1) | 1;
+            break;
+        case RK_FORMAT_YCbCr_422_P  :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off) * stride) + ((x_off>>1) * pw);
+            *bRGA_PRESCL_CR_MST = (u32)msg->dst.v_addr  + ((y_off) * stride) + ((x_off>>1) * pw);
+            break;
+        case RK_FORMAT_YCbCr_420_SP :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + ((x_off) * pw);
+			*bRGA_DST_UV_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + x_off;
+			*bRGA_YUV_OUT_CFG |= (((msg->yuv2rgb_mode >> 2) & 3) << 4) | (0 << 3) | (1 << 1) | 1;
+            break;
+        case RK_FORMAT_YCbCr_420_P :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + ((x_off>>1) * pw);
+            *bRGA_PRESCL_CR_MST = (u32)msg->dst.v_addr  + ((y_off>>1) * stride) + ((x_off>>1) * pw);
+            break;
+        case RK_FORMAT_YCrCb_422_SP :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off) * stride) + ((x_off) * pw);
+			*bRGA_DST_UV_MST = (u32)msg->dst.uv_addr + (y_off * stride) + x_off;
+			*bRGA_YUV_OUT_CFG |= (((msg->yuv2rgb_mode >> 2) & 3) << 4) | (1 << 3) | (0 << 1) | 1;
+            break;
+        case RK_FORMAT_YCrCb_422_P  :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off) * stride) + ((x_off>>1) * pw);
+            *bRGA_PRESCL_CR_MST = (u32)msg->dst.v_addr  + ((y_off) * stride) + ((x_off>>1) * pw);
+            break;
+        case RK_FORMAT_YCrCb_420_SP :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + ((x_off) * pw);
+			*bRGA_DST_UV_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + x_off;
+			*bRGA_YUV_OUT_CFG |= (((msg->yuv2rgb_mode >> 2) & 3) << 4) | (1 << 3) | (1 << 1) | 1;
+            break;
+        case RK_FORMAT_YCrCb_420_P :
+            *bRGA_PRESCL_CB_MST = (u32)msg->dst.uv_addr + ((y_off>>1) * stride) + ((x_off>>1) * pw);
+            *bRGA_PRESCL_CR_MST = (u32)msg->dst.v_addr  + ((y_off>>1) * stride) + ((x_off>>1) * pw);
+            break;
+    }
+
+    rop_mask_stride = (((msg->src.vir_w + 7)>>3) + 3) & (~3);//not dst_vir.w,hxx,2011.7.21
+
+    reg = (stride >> 2) & 0xffff;
+    reg = reg | ((rop_mask_stride>>2) << 16);
+
+    #if defined(CONFIG_ARCH_RK2928) || defined(CONFIG_ARCH_RK3188)
+    //reg = reg | ((msg->alpha_rop_mode & 3) << 28);
+    reg = reg | (1 << 28);
+    #endif
+
+    if (msg->render_mode == line_point_drawing_mode)
+    {
+        reg &= 0xffff;
+        reg = reg | (msg->dst.vir_h << 16);
+    }
+
+    *bRGA_DST_VIR_INFO = reg;
+    *bRGA_DST_CTR_INFO = (msg->dst.act_w - 1) | ((msg->dst.act_h - 1) << 16);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 1, 0))
+    if (msg->render_mode == pre_scaling_mode) {
+        *bRGA_YUV_OUT_CFG &= 0xfffffffe;
+    }
+#endif
+    return 0;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_alpha_rop
+Description:
+    fill alpha rop some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+static void
+RGA_set_alpha_rop(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_ALPHA_CON;
+    u32 *bRGA_ROP_CON0;
+    u32 *bRGA_ROP_CON1;
+    u32 reg = 0;
+    u32 rop_con0, rop_con1;
+
+    u8 rop_mode = (msg->alpha_rop_mode) & 3;
+    u8 alpha_mode = msg->alpha_rop_mode & 3;
+
+    rop_con0 = rop_con1 = 0;
+
+    bRGA_ALPHA_CON = (u32 *)(base + RGA_ALPHA_CON_OFFSET);
+
+    reg = ((reg & (~m_RGA_ALPHA_CON_ENABLE) )| (s_RGA_ALPHA_CON_ENABLE(msg->alpha_rop_flag & 1)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_A_OR_R_SEL)) | (s_RGA_ALPHA_CON_A_OR_R_SEL((msg->alpha_rop_flag >> 1) & 1)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_ALPHA_MODE)) | (s_RGA_ALPHA_CON_ALPHA_MODE(alpha_mode)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_PD_MODE)) | (s_RGA_ALPHA_CON_PD_MODE(msg->PD_mode)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_SET_CONSTANT_VALUE)) | (s_RGA_ALPHA_CON_SET_CONSTANT_VALUE(msg->alpha_global_value)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_PD_M_SEL)) | (s_RGA_ALPHA_CON_PD_M_SEL(msg->alpha_rop_flag >> 3)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_FADING_ENABLE)) | (s_RGA_ALPHA_CON_FADING_ENABLE(msg->alpha_rop_flag >> 2)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_ROP_MODE_SEL)) | (s_RGA_ALPHA_CON_ROP_MODE_SEL(rop_mode)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_CAL_MODE_SEL)) | (s_RGA_ALPHA_CON_CAL_MODE_SEL(msg->alpha_rop_flag >> 4)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_DITHER_ENABLE)) | (s_RGA_ALPHA_CON_DITHER_ENABLE(msg->alpha_rop_flag >> 5)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_GRADIENT_CAL_MODE)) | (s_RGA_ALPHA_CON_GRADIENT_CAL_MODE(msg->alpha_rop_flag >> 6)));
+    reg = ((reg & (~m_RGA_ALPHA_CON_AA_SEL)) | (s_RGA_ALPHA_CON_AA_SEL(msg->alpha_rop_flag >> 7)));
+
+    *bRGA_ALPHA_CON = reg;
+
+    if(rop_mode == 0) {
+        rop_con0 =  ROP3_code[(msg->rop_code & 0xff)];
+    }
+    else if(rop_mode == 1) {
+        rop_con0 =  ROP3_code[(msg->rop_code & 0xff)];
+    }
+    else if(rop_mode == 2) {
+        rop_con0 =  ROP3_code[(msg->rop_code & 0xff)];
+        rop_con1 =  ROP3_code[(msg->rop_code & 0xff00)>>8];
+    }
+
+    bRGA_ROP_CON0 = (u32 *)(base + RGA_ROP_CON0_OFFSET);
+    bRGA_ROP_CON1 = (u32 *)(base + RGA_ROP_CON1_OFFSET);
+
+    *bRGA_ROP_CON0 = (u32)rop_con0;
+    *bRGA_ROP_CON1 = (u32)rop_con1;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_color
+Description:
+    fill color some relate reg bit
+    bg_color/fg_color
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+RGA_set_color(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_SRC_TR_COLOR0;
+    u32 *bRGA_SRC_TR_COLOR1;
+    u32 *bRGA_SRC_BG_COLOR;
+    u32 *bRGA_SRC_FG_COLOR;
+
+
+    bRGA_SRC_BG_COLOR  = (u32 *)(base + RGA_SRC_BG_COLOR_OFFSET);
+    bRGA_SRC_FG_COLOR  = (u32 *)(base + RGA_SRC_FG_COLOR_OFFSET);
+
+    *bRGA_SRC_BG_COLOR = msg->bg_color;    /* 1bpp 0 */
+    *bRGA_SRC_FG_COLOR = msg->fg_color;    /* 1bpp 1 */
+
+    bRGA_SRC_TR_COLOR0 = (u32 *)(base + RGA_SRC_TR_COLOR0_OFFSET);
+    bRGA_SRC_TR_COLOR1 = (u32 *)(base + RGA_SRC_TR_COLOR1_OFFSET);
+
+    *bRGA_SRC_TR_COLOR0 = msg->color_key_min;
+    *bRGA_SRC_TR_COLOR1 = msg->color_key_max;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_fading
+Description:
+    fill fading some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static s32
+RGA_set_fading(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_FADING_CON;
+    u8 r, g, b;
+    u32 reg = 0;
+
+    bRGA_FADING_CON = (u32 *)(base + RGA_FADING_CON_OFFSET);
+
+    b = msg->fading.b;
+    g = msg->fading.g;
+    r = msg->fading.r;
+
+    reg = (r<<8) | (g<<16) | (b<<24) | reg;
+
+    *bRGA_FADING_CON = reg;
+
+    return 0;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_pat
+Description:
+    fill patten some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static s32
+RGA_set_pat(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_PAT_CON;
+    u32 *bRGA_PAT_START_POINT;
+    u32 reg = 0;
+
+    bRGA_PAT_START_POINT = (u32 *)(base + RGA_PAT_START_POINT_OFFSET);
+
+    bRGA_PAT_CON = (u32 *)(base + RGA_PAT_CON_OFFSET);
+
+    *bRGA_PAT_START_POINT = (msg->pat.act_w * msg->pat.y_offset) + msg->pat.x_offset;
+
+    reg = (msg->pat.act_w - 1) | ((msg->pat.act_h - 1) << 8) | (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+    *bRGA_PAT_CON = reg;
+
+    return 0;
+}
+
+
+
+
+/*************************************************************
+Func:
+    RGA_set_bitblt_reg_info
+Description:
+    fill bitblt mode relate ren info
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+RGA_set_bitblt_reg_info(u8 *base, const struct rga_req * msg, TILE_INFO *tile)
+{
+    u32 *bRGA_SRC_Y_MST;
+    u32 *bRGA_SRC_CB_MST;
+    u32 *bRGA_SRC_CR_MST;
+    u32 *bRGA_SRC_X_PARA;
+    u32 *bRGA_SRC_Y_PARA;
+    u32 *bRGA_SRC_TILE_XINFO;
+    u32 *bRGA_SRC_TILE_YINFO;
+    u32 *bRGA_SRC_TILE_H_INCR;
+    u32 *bRGA_SRC_TILE_V_INCR;
+    u32 *bRGA_SRC_TILE_OFFSETX;
+    u32 *bRGA_SRC_TILE_OFFSETY;
+
+    u32 *bRGA_DST_MST;
+    u32 *bRGA_DST_CTR_INFO;
+
+    s32 m0, m1, m2, m3;
+    s32 pos[8];
+    //s32 x_dx, x_dy, y_dx, y_dy;
+    s32 xmin, xmax, ymin, ymax;
+    s32 xp, yp;
+    u32 y_addr, u_addr, v_addr;
+    u32 pixel_width, stride;
+
+    u_addr = v_addr = 0;
+
+    /* src info */
+
+    bRGA_SRC_Y_MST = (u32 *)(base + RGA_SRC_Y_MST_OFFSET);
+    bRGA_SRC_CB_MST = (u32 *)(base + RGA_SRC_CB_MST_OFFSET);
+    bRGA_SRC_CR_MST = (u32 *)(base + RGA_SRC_CR_MST_OFFSET);
+
+    bRGA_SRC_X_PARA = (u32 *)(base + RGA_SRC_X_PARA_OFFSET);
+    bRGA_SRC_Y_PARA = (u32 *)(base + RGA_SRC_Y_PARA_OFFSET);
+
+    bRGA_SRC_TILE_XINFO = (u32 *)(base + RGA_SRC_TILE_XINFO_OFFSET);
+    bRGA_SRC_TILE_YINFO = (u32 *)(base + RGA_SRC_TILE_YINFO_OFFSET);
+    bRGA_SRC_TILE_H_INCR = (u32 *)(base + RGA_SRC_TILE_H_INCR_OFFSET);
+    bRGA_SRC_TILE_V_INCR = (u32 *)(base + RGA_SRC_TILE_V_INCR_OFFSET);
+    bRGA_SRC_TILE_OFFSETX = (u32 *)(base + RGA_SRC_TILE_OFFSETX_OFFSET);
+    bRGA_SRC_TILE_OFFSETY = (u32 *)(base + RGA_SRC_TILE_OFFSETY_OFFSET);
+
+    bRGA_DST_MST = (u32 *)(base + RGA_DST_MST_OFFSET);
+    bRGA_DST_CTR_INFO = (u32 *)(base + RGA_DST_CTR_INFO_OFFSET);
+
+    /* Matrix reg fill */
+    m0 = (s32)(tile->matrix[0] >> 18);
+    m1 = (s32)(tile->matrix[1] >> 18);
+    m2 = (s32)(tile->matrix[2] >> 18);
+    m3 = (s32)(tile->matrix[3] >> 18);
+
+    *bRGA_SRC_X_PARA = (m0 & 0xffff) | (m2 << 16);
+    *bRGA_SRC_Y_PARA = (m1 & 0xffff) | (m3 << 16);
+
+    /* src tile information setting */
+    if(msg->rotate_mode != 0)//add by hxx,2011.7.12,for rtl0707,when line scanning ,do not calc src tile info
+    {
+        *bRGA_SRC_TILE_XINFO = (tile->tile_start_x_coor & 0xffff) | (tile->tile_w << 16);
+        *bRGA_SRC_TILE_YINFO = (tile->tile_start_y_coor & 0xffff) | (tile->tile_h << 16);
+
+        *bRGA_SRC_TILE_H_INCR = ((tile->x_dx) & 0xffff) | ((tile->x_dy) << 16);
+        *bRGA_SRC_TILE_V_INCR = ((tile->y_dx) & 0xffff) | ((tile->y_dy) << 16);
+
+        *bRGA_SRC_TILE_OFFSETX = tile->tile_xoff;
+        *bRGA_SRC_TILE_OFFSETY = tile->tile_yoff;
+    }
+
+    pixel_width = RGA_pixel_width_init(msg->src.format);
+
+    stride = ((msg->src.vir_w * pixel_width) + 3) & (~3);
+
+    if ((msg->rotate_mode == 1)||(msg->rotate_mode == 2)||(msg->rotate_mode == 3))
+    {
+        pos[0] = tile->tile_start_x_coor<<8;
+        pos[1] = tile->tile_start_y_coor<<8;
+
+        pos[2] = pos[0];
+        pos[3] = pos[1] + tile->tile_h;
+
+        pos[4] = pos[0] + tile->tile_w;
+        pos[5] = pos[1] + tile->tile_h;
+
+        pos[6] = pos[0] + tile->tile_w;
+        pos[7] = pos[1];
+
+        pos[0] >>= 11;
+        pos[1] >>= 11;
+
+        pos[2] >>= 11;
+        pos[3] >>= 11;
+
+        pos[4] >>= 11;
+        pos[5] >>= 11;
+
+        pos[6] >>= 11;
+        pos[7] >>= 11;
+
+        xmax = (MAX(MAX(MAX(pos[0], pos[2]), pos[4]), pos[6]) + 1);
+        xmin = (MIN(MIN(MIN(pos[0], pos[2]), pos[4]), pos[6]));
+
+        ymax = (MAX(MAX(MAX(pos[1], pos[3]), pos[5]), pos[7]) + 1);
+        ymin = (MIN(MIN(MIN(pos[1], pos[3]), pos[5]), pos[7]));
+
+        xp = xmin + msg->src.x_offset;
+        yp = ymin + msg->src.y_offset;
+
+        if (!((xmax < 0)||(xmin > msg->src.act_w - 1)||(ymax < 0)||(ymin > msg->src.act_h - 1)))
+        {
+            xp = CLIP(xp, msg->src.x_offset, msg->src.x_offset + msg->src.act_w - 1);
+            yp = CLIP(yp, msg->src.y_offset, msg->src.y_offset + msg->src.act_h - 1);
+        }
+
+        switch(msg->src.format)
+        {
+            case RK_FORMAT_YCbCr_420_P :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp>>1)*(stride>>1) + (xp>>1);
+                v_addr = msg->src.v_addr  + (yp>>1)*(stride>>1) + (xp>>1);
+                break;
+            case RK_FORMAT_YCbCr_420_SP :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp>>1)*stride + ((xp>>1)<<1);
+                break;
+            case RK_FORMAT_YCbCr_422_P :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp)*(stride>>1) + (xp>>1);
+                v_addr = msg->src.v_addr  + (yp)*(stride>>1) + (xp>>1);
+                break;
+            case RK_FORMAT_YCbCr_422_SP:
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr  + yp*stride + ((xp>>1)<<1);
+                break;
+            case RK_FORMAT_YCrCb_420_P :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp>>1)*(stride>>1) + (xp>>1);
+                v_addr = msg->src.v_addr  + (yp>>1)*(stride>>1) + (xp>>1);
+                break;
+            case RK_FORMAT_YCrCb_420_SP :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp>>1)*stride + ((xp>>1)<<1);
+                break;
+            case RK_FORMAT_YCrCb_422_P :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr + (yp)*(stride>>1) + (xp>>1);
+                v_addr = msg->src.v_addr  + (yp)*(stride>>1) + (xp>>1);
+                break;
+            case RK_FORMAT_YCrCb_422_SP:
+                y_addr = msg->src.yrgb_addr + yp*stride + xp;
+                u_addr = msg->src.uv_addr  + yp*stride + ((xp>>1)<<1);
+                break;
+            default :
+                y_addr = msg->src.yrgb_addr + yp*stride + xp*pixel_width;
+                break;
+        }
+
+        *bRGA_SRC_Y_MST = y_addr;
+        *bRGA_SRC_CB_MST = u_addr;
+        *bRGA_SRC_CR_MST = v_addr;
+    }
+
+    /*dst info*/
+    pixel_width = RGA_pixel_width_init(msg->dst.format);
+    stride = (msg->dst.vir_w * pixel_width + 3) & (~3);
+    *bRGA_DST_MST = (u32)msg->dst.yrgb_addr + (tile->dst_ctrl.y_off * stride) + (tile->dst_ctrl.x_off * pixel_width);
+    *bRGA_DST_CTR_INFO = (tile->dst_ctrl.w) | ((tile->dst_ctrl.h) << 16);
+
+    *bRGA_DST_CTR_INFO |= ((1<<29) | (1<<28));
+}
+
+
+
+
+/*************************************************************
+Func:
+    RGA_set_color_palette_reg_info
+Description:
+    fill color palette process some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static void
+RGA_set_color_palette_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_SRC_Y_MST;
+    u32 p;
+    s16 x_off, y_off;
+    u16 src_stride;
+    u8  shift;
+    u16 sw, byte_num;
+
+    x_off = msg->src.x_offset;
+    y_off = msg->src.y_offset;
+
+    sw = msg->src.vir_w;
+    shift = 3 - (msg->palette_mode & 3);
+    byte_num = sw >> shift;
+    src_stride = (byte_num + 3) & (~3);
+
+    p = msg->src.yrgb_addr;
+    p = p + (x_off>>shift) + y_off*src_stride;
+
+    bRGA_SRC_Y_MST = (u32 *)(base + RGA_SRC_Y_MST_OFFSET);
+    *bRGA_SRC_Y_MST = (u32)p;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_color_fill_reg_info
+Description:
+    fill color fill process some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+static void
+RGA_set_color_fill_reg_info(u8 *base, const struct rga_req *msg)
+{
+
+    u32 *bRGA_CP_GR_A;
+    u32 *bRGA_CP_GR_B;
+    u32 *bRGA_CP_GR_G;
+    u32 *bRGA_CP_GR_R;
+
+    u32 *bRGA_PAT_CON;
+
+    bRGA_CP_GR_A = (u32 *)(base + RGA_CP_GR_A_OFFSET);
+    bRGA_CP_GR_B = (u32 *)(base + RGA_CP_GR_B_OFFSET);
+    bRGA_CP_GR_G = (u32 *)(base + RGA_CP_GR_G_OFFSET);
+    bRGA_CP_GR_R = (u32 *)(base + RGA_CP_GR_R_OFFSET);
+
+    bRGA_PAT_CON = (u32 *)(base + RGA_PAT_CON_OFFSET);
+
+    *bRGA_CP_GR_A = (msg->gr_color.gr_x_a & 0xffff) | (msg->gr_color.gr_y_a << 16);
+    *bRGA_CP_GR_B = (msg->gr_color.gr_x_b & 0xffff) | (msg->gr_color.gr_y_b << 16);
+    *bRGA_CP_GR_G = (msg->gr_color.gr_x_g & 0xffff) | (msg->gr_color.gr_y_g << 16);
+    *bRGA_CP_GR_R = (msg->gr_color.gr_x_r & 0xffff) | (msg->gr_color.gr_y_r << 16);
+
+    *bRGA_PAT_CON = (msg->pat.vir_w-1) | ((msg->pat.vir_h-1) << 8) | (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_line_drawing_reg_info
+Description:
+    fill line drawing process some relate reg bit
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static s32 RGA_set_line_drawing_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_LINE_DRAW;
+    u32 *bRGA_DST_VIR_INFO;
+    u32 *bRGA_LINE_DRAW_XY_INFO;
+    u32 *bRGA_LINE_DRAW_WIDTH;
+    u32 *bRGA_LINE_DRAWING_COLOR;
+    u32 *bRGA_LINE_DRAWING_MST;
+
+    u32  reg = 0;
+
+    s16 x_width, y_width;
+    u16 abs_x, abs_y, delta;
+    u16 stride;
+    u8 pw;
+    u32 start_addr;
+    u8 line_dir, dir_major, dir_semi_major;
+    u16 major_width;
+
+    bRGA_LINE_DRAW = (u32 *)(base + RGA_LINE_DRAW_OFFSET);
+    bRGA_DST_VIR_INFO = (u32 *)(base + RGA_DST_VIR_INFO_OFFSET);
+    bRGA_LINE_DRAW_XY_INFO = (u32 *)(base + RGA_LINE_DRAW_XY_INFO_OFFSET);
+    bRGA_LINE_DRAW_WIDTH = (u32 *)(base + RGA_LINE_DRAWING_WIDTH_OFFSET);
+    bRGA_LINE_DRAWING_COLOR = (u32 *)(base + RGA_LINE_DRAWING_COLOR_OFFSET);
+    bRGA_LINE_DRAWING_MST = (u32 *)(base + RGA_LINE_DRAWING_MST_OFFSET);
+
+    pw = RGA_pixel_width_init(msg->dst.format);
+
+    stride = (msg->dst.vir_w * pw + 3) & (~3);
+
+    start_addr = msg->dst.yrgb_addr
+                + (msg->line_draw_info.start_point.y * stride)
+                + (msg->line_draw_info.start_point.x * pw);
+
+    x_width = msg->line_draw_info.start_point.x - msg->line_draw_info.end_point.x;
+    y_width = msg->line_draw_info.start_point.y - msg->line_draw_info.end_point.y;
+
+    abs_x = abs(x_width);
+    abs_y = abs(y_width);
+
+    if (abs_x >= abs_y)
+    {
+        if (y_width > 0)
+            dir_semi_major = 1;
+        else
+            dir_semi_major = 0;
+
+        if (x_width > 0)
+            dir_major = 1;
+        else
+            dir_major = 0;
+
+        if((abs_x == 0)||(abs_y == 0))
+            delta = 0;
+        else
+            delta = (abs_y<<12)/abs_x;
+
+        if (delta >> 12)
+            delta -= 1;
+
+        major_width = abs_x;
+        line_dir = 0;
+    }
+    else
+    {
+        if (x_width > 0)
+            dir_semi_major = 1;
+        else
+            dir_semi_major = 0;
+
+        if (y_width > 0)
+            dir_major = 1;
+        else
+            dir_major = 0;
+
+        delta = (abs_x<<12)/abs_y;
+        major_width = abs_y;
+        line_dir = 1;
+    }
+
+    reg = (reg & (~m_RGA_LINE_DRAW_MAJOR_WIDTH))     | (s_RGA_LINE_DRAW_MAJOR_WIDTH(major_width));
+    reg = (reg & (~m_RGA_LINE_DRAW_LINE_DIRECTION))  | (s_RGA_LINE_DRAW_LINE_DIRECTION(line_dir));
+    reg = (reg & (~m_RGA_LINE_DRAW_LINE_WIDTH))      | (s_RGA_LINE_DRAW_LINE_WIDTH(msg->line_draw_info.line_width - 1));
+    reg = (reg & (~m_RGA_LINE_DRAW_INCR_VALUE))      | (s_RGA_LINE_DRAW_INCR_VALUE(delta));
+    reg = (reg & (~m_RGA_LINE_DRAW_DIR_SEMI_MAJOR))  | (s_RGA_LINE_DRAW_DIR_SEMI_MAJOR(dir_semi_major));
+    reg = (reg & (~m_RGA_LINE_DRAW_DIR_MAJOR))       | (s_RGA_LINE_DRAW_DIR_MAJOR(dir_major));
+    reg = (reg & (~m_RGA_LINE_DRAW_LAST_POINT))      | (s_RGA_LINE_DRAW_LAST_POINT(msg->line_draw_info.flag >> 1));
+    reg = (reg & (~m_RGA_LINE_DRAW_ANTI_ALISING))    | (s_RGA_LINE_DRAW_ANTI_ALISING(msg->line_draw_info.flag));
+
+    *bRGA_LINE_DRAW = reg;
+
+    reg = (msg->line_draw_info.start_point.x & 0xfff) | ((msg->line_draw_info.start_point.y & 0xfff) << 16);
+    *bRGA_LINE_DRAW_XY_INFO = reg;
+
+    *bRGA_LINE_DRAW_WIDTH = msg->dst.vir_w;
+
+    *bRGA_LINE_DRAWING_COLOR = msg->line_draw_info.color;
+
+    *bRGA_LINE_DRAWING_MST = (u32)start_addr;
+
+    return 0;
+}
+
+
+/*full*/
+static s32
+RGA_set_filter_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_BLUR_SHARP_INFO;
+    u32  reg = 0;
+
+    bRGA_BLUR_SHARP_INFO = (u32 *)(base + RGA_ALPHA_CON_OFFSET);
+
+    reg = *bRGA_BLUR_SHARP_INFO;
+
+    reg = ((reg & (~m_RGA_BLUR_SHARP_FILTER_TYPE)) | (s_RGA_BLUR_SHARP_FILTER_TYPE(msg->bsfilter_flag & 3)));
+    reg = ((reg & (~m_RGA_BLUR_SHARP_FILTER_MODE)) | (s_RGA_BLUR_SHARP_FILTER_MODE(msg->bsfilter_flag >>2)));
+
+    *bRGA_BLUR_SHARP_INFO = reg;
+
+    return 0;
+}
+
+
+/*full*/
+static s32
+RGA_set_pre_scale_reg_info(u8 *base, const struct rga_req *msg)
+{
+   u32 *bRGA_PRE_SCALE_INFO;
+   u32 reg = 0;
+   u32 h_ratio = 0;
+   u32 v_ratio = 0;
+   u32 ps_yuv_flag = 0;
+   u32 src_width, src_height;
+   u32 dst_width, dst_height;
+
+   src_width = msg->src.act_w;
+   src_height = msg->src.act_h;
+
+   dst_width = msg->dst.act_w;
+   dst_height = msg->dst.act_h;
+
+   if((dst_width == 0) || (dst_height == 0))
+   {
+        printk("pre scale reg info error ratio is divide zero\n");
+        return -EINVAL;
+   }
+
+   h_ratio = (src_width <<16) / dst_width;
+   v_ratio = (src_height<<16) / dst_height;
+
+   if (h_ratio <= (1<<16))
+       h_ratio = 0;
+   else if (h_ratio <= (2<<16))
+       h_ratio = 1;
+   else if (h_ratio <= (4<<16))
+       h_ratio = 2;
+   else if (h_ratio <= (8<<16))
+       h_ratio = 3;
+
+   if (v_ratio <= (1<<16))
+       v_ratio = 0;
+   else if (v_ratio <= (2<<16))
+       v_ratio = 1;
+   else if (v_ratio <= (4<<16))
+       v_ratio = 2;
+   else if (v_ratio <= (8<<16))
+       v_ratio = 3;
+
+   if(msg->src.format == msg->dst.format)
+        ps_yuv_flag = 0;
+    else
+        ps_yuv_flag = 1;
+
+   bRGA_PRE_SCALE_INFO = (u32 *)(base + RGA_ALPHA_CON_OFFSET);
+
+   reg = *bRGA_PRE_SCALE_INFO;
+   reg = ((reg & (~m_RGA_PRE_SCALE_HOR_RATIO)) | (s_RGA_PRE_SCALE_HOR_RATIO((u8)h_ratio)));
+   reg = ((reg & (~m_RGA_PRE_SCALE_VER_RATIO)) | (s_RGA_PRE_SCALE_VER_RATIO((u8)v_ratio)));
+   reg = ((reg & (~m_RGA_PRE_SCALE_OUTPUT_FORMAT)) | (s_RGA_PRE_SCALE_OUTPUT_FORMAT(ps_yuv_flag)));
+
+   *bRGA_PRE_SCALE_INFO = reg;
+
+   return 0;
+}
+
+
+
+/*full*/
+static int
+RGA_set_update_palette_table_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_LUT_MST;
+
+    if (!msg->LUT_addr) {
+        return -1;
+    }
+
+    bRGA_LUT_MST  = (u32 *)(base + RGA_LUT_MST_OFFSET);
+
+    *bRGA_LUT_MST = (u32)msg->LUT_addr;
+
+    return 0;
+}
+
+
+
+/*full*/
+static int
+RGA_set_update_patten_buff_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *bRGA_PAT_MST;
+    u32 *bRGA_PAT_CON;
+    u32 *bRGA_PAT_START_POINT;
+    u32 reg = 0;
+    rga_img_info_t *pat;
+
+    pat = (rga_img_info_t *)&msg->pat;
+
+    bRGA_PAT_START_POINT = (u32 *)(base + RGA_PAT_START_POINT_OFFSET);
+    bRGA_PAT_MST = (u32 *)(base + RGA_PAT_MST_OFFSET);
+    bRGA_PAT_CON = (u32 *)(base + RGA_PAT_CON_OFFSET);
+
+    if ( !pat->yrgb_addr ) {
+        return -1;
+    }
+    *bRGA_PAT_MST = (u32)pat->yrgb_addr;
+
+    if ((pat->vir_w > 256)||(pat->x_offset > 256)||(pat->y_offset > 256)) {
+        return -1;
+    }
+    *bRGA_PAT_START_POINT = (pat->vir_w * pat->y_offset) + pat->x_offset;
+
+    reg = (pat->vir_w-1) | ((pat->vir_h-1) << 8) | (pat->x_offset << 16) | (pat->y_offset << 24);
+    *bRGA_PAT_CON = reg;
+
+    return 0;
+}
+
+
+/*************************************************************
+Func:
+    RGA_set_mmu_ctrl_reg_info
+Description:
+    fill mmu relate some reg info
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+
+static s32
+RGA_set_mmu_ctrl_reg_info(u8 *base, const struct rga_req *msg)
+{
+    u32 *RGA_MMU_TLB, *RGA_MMU_CTRL_ADDR;
+    u32  mmu_addr;
+    u8   TLB_size, mmu_enable, src_flag, dst_flag, CMD_flag;
+    u32  reg = 0;
+
+    mmu_addr = (u32)msg->mmu_info.base_addr;
+    TLB_size = (msg->mmu_info.mmu_flag >> 4) & 0x3;
+    mmu_enable = msg->mmu_info.mmu_flag & 0x1;
+
+    src_flag = (msg->mmu_info.mmu_flag >> 1) & 0x1;
+    dst_flag = (msg->mmu_info.mmu_flag >> 2) & 0x1;
+    CMD_flag = (msg->mmu_info.mmu_flag >> 3) & 0x1;
+
+    RGA_MMU_TLB = (u32 *)(base + RGA_MMU_TLB_OFFSET);
+    RGA_MMU_CTRL_ADDR = (u32 *)(base + RGA_FADING_CON_OFFSET);
+
+    reg = ((reg & (~m_RGA_MMU_CTRL_TLB_ADDR)) | s_RGA_MMU_CTRL_TLB_ADDR(mmu_addr));
+    *RGA_MMU_TLB = reg;
+
+    reg = *RGA_MMU_CTRL_ADDR;
+    reg = ((reg & (~m_RGA_MMU_CTRL_PAGE_TABLE_SIZE)) | s_RGA_MMU_CTRL_PAGE_TABLE_SIZE(TLB_size));
+    reg = ((reg & (~m_RGA_MMU_CTRL_MMU_ENABLE)) | s_RGA_MMU_CTRL_MMU_ENABLE(mmu_enable));
+    reg = ((reg & (~m_RGA_MMU_CTRL_SRC_FLUSH)) | s_RGA_MMU_CTRL_SRC_FLUSH(1));
+    reg = ((reg & (~m_RGA_MMU_CTRL_DST_FLUSH)) | s_RGA_MMU_CTRL_DST_FLUSH(1));
+    reg = ((reg & (~m_RGA_MMU_CTRL_CMD_CHAN_FLUSH)) | s_RGA_MMU_CTRL_CMD_CHAN_FLUSH(1));
+    *RGA_MMU_CTRL_ADDR = reg;
+
+    return 0;
+}
+
+
+
+/*************************************************************
+Func:
+    RGA_gen_reg_info
+Description:
+    Generate RGA command reg list from rga_req struct.
+Author:
+    ZhangShengqin
+Date:
+    20012-2-2 10:59:25
+**************************************************************/
+int
+RGA_gen_reg_info(const struct rga_req *msg, unsigned char *base)
+{
+    TILE_INFO tile;
+
+    memset(base, 0x0, 28*4);
+    RGA_set_mode_ctrl(base, msg);
+
+    switch(msg->render_mode)
+    {
+        case bitblt_mode :
+            RGA_set_alpha_rop(base, msg);
+            RGA_set_src(base, msg);
+            RGA_set_dst(base, msg);
+            RGA_set_color(base, msg);
+            RGA_set_fading(base, msg);
+            RGA_set_pat(base, msg);
+            matrix_cal(msg, &tile);
+            dst_ctrl_cal(msg, &tile);
+            src_tile_info_cal(msg, &tile);
+            RGA_set_bitblt_reg_info(base, msg, &tile);
+            break;
+        case color_palette_mode :
+            RGA_set_src(base, msg);
+            RGA_set_dst(base, msg);
+            RGA_set_color(base, msg);
+            RGA_set_color_palette_reg_info(base, msg);
+            break;
+        case color_fill_mode :
+            RGA_set_alpha_rop(base, msg);
+            RGA_set_dst(base, msg);
+            RGA_set_color(base, msg);
+            RGA_set_pat(base, msg);
+            RGA_set_color_fill_reg_info(base, msg);
+            break;
+        case line_point_drawing_mode :
+            RGA_set_alpha_rop(base, msg);
+            RGA_set_dst(base, msg);
+            RGA_set_color(base, msg);
+            RGA_set_line_drawing_reg_info(base, msg);
+            break;
+        case blur_sharp_filter_mode :
+            RGA_set_src(base, msg);
+            RGA_set_dst(base, msg);
+            RGA_set_filter_reg_info(base, msg);
+            break;
+        case pre_scaling_mode :
+            RGA_set_src(base, msg);
+            RGA_set_dst(base, msg);
+            if(RGA_set_pre_scale_reg_info(base, msg) == -EINVAL)
+                return -1;
+            break;
+        case update_palette_table_mode :
+            if (RGA_set_update_palette_table_reg_info(base, msg)) {
+                return -1;
+            }
+			break;
+        case update_patten_buff_mode:
+            if (RGA_set_update_patten_buff_reg_info(base, msg)){
+                return -1;
+            }
+
+            break;
+    }
+
+    RGA_set_mmu_ctrl_reg_info(base, msg);
+
+    return 0;
+}
+
+
+
diff --git a/drivers/video/rockchip/rga/rga_reg_info.h b/drivers/video/rockchip/rga/rga_reg_info.h
new file mode 100644
index 0000000000000..8edbd5c3d4194
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_reg_info.h
@@ -0,0 +1,467 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __REG_INFO_H__
+#define __REG_INFO_H__
+
+
+//#include "chip_register.h"
+
+//#include "rga_struct.h"
+#include "rga.h"
+
+#ifndef MIN
+#define MIN(X, Y)           ((X)<(Y)?(X):(Y))
+#endif
+
+#ifndef MAX
+#define MAX(X, Y)           ((X)>(Y)?(X):(Y))
+#endif
+
+#ifndef ABS
+#define ABS(X)              (((X) < 0) ? (-(X)) : (X))
+#endif
+
+#ifndef CLIP
+#define CLIP(x, a,  b)				((x) < (a)) ? (a) : (((x) > (b)) ? (b) : (x))
+#endif
+
+//RGA register map
+
+//General Registers
+#define rRGA_SYS_CTRL             (*(volatile uint32_t *)(RGA_BASE + RGA_SYS_CTRL))
+#define rRGA_CMD_CTRL             (*(volatile uint32_t *)(RGA_BASE + RGA_CMD_CTRL))
+#define rRGA_CMD_ADDR             (*(volatile uint32_t *)(RGA_BASE + RGA_CMD_ADDR))
+#define rRGA_STATUS               (*(volatile uint32_t *)(RGA_BASE + RGA_STATUS))
+#define rRGA_INT                  (*(volatile uint32_t *)(RGA_BASE + RGA_INT))
+#define rRGA_AXI_ID               (*(volatile uint32_t *)(RGA_BASE + RGA_AXI_ID))
+#define rRGA_MMU_STA_CTRL         (*(volatile uint32_t *)(RGA_BASE + RGA_MMU_STA_CTRL))
+#define rRGA_MMU_STA              (*(volatile uint32_t *)(RGA_BASE + RGA_MMU_STA))
+
+//Command code start
+#define rRGA_MODE_CTRL            (*(volatile uint32_t *)(RGA_BASE + RGA_MODE_CTRL))
+
+//Source Image Registers
+#define rRGA_SRC_Y_MST            (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_Y_MST))
+#define rRGA_SRC_CB_MST           (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_CB_MST))
+#define rRGA_MASK_READ_MST        (*(volatile uint32_t *)(RGA_BASE + RGA_MASK_READ_MST))  //repeat
+#define rRGA_SRC_CR_MST           (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_CR_MST))
+#define rRGA_SRC_VIR_INFO         (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_VIR_INFO))
+#define rRGA_SRC_ACT_INFO         (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_ACT_INFO))
+#define rRGA_SRC_X_PARA           (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_X_PARA))
+#define rRGA_SRC_Y_PARA           (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_Y_PARA))
+#define rRGA_SRC_TILE_XINFO       (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_XINFO))
+#define rRGA_SRC_TILE_YINFO       (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_YINFO))
+#define rRGA_SRC_TILE_H_INCR      (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_H_INCR))
+#define rRGA_SRC_TILE_V_INCR      (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_V_INCR))
+#define rRGA_SRC_TILE_OFFSETX     (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_OFFSETX))
+#define rRGA_SRC_TILE_OFFSETY     (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TILE_OFFSETY))
+#define rRGA_SRC_BG_COLOR         (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_BG_COLOR))
+#define rRGA_SRC_FG_COLOR         (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_FG_COLOR))
+#define rRGA_LINE_DRAWING_COLOR   (*(volatile uint32_t *)(RGA_BASE + RGA_LINE_DRAWING_COLOR))  //repeat
+#define rRGA_SRC_TR_COLOR0        (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TR_COLOR0))
+#define rRGA_CP_GR_A              (*(volatile uint32_t *)(RGA_BASE + RGA_CP_GR_A))  //repeat
+#define rRGA_SRC_TR_COLOR1        (*(volatile uint32_t *)(RGA_BASE + RGA_SRC_TR_COLOR1))
+#define rRGA_CP_GR_B              (*(volatile uint32_t *)(RGA_BASE + RGA_CP_GR_B))  //repeat
+
+#define rRGA_LINE_DRAW            (*(volatile uint32_t *)(RGA_BASE + RGA_LINE_DRAW))
+#define rRGA_PAT_START_POINT      (*(volatile uint32_t *)(RGA_BASE + RGA_PAT_START_POINT))  //repeat
+
+//Destination Image Registers
+#define rRGA_DST_MST              (*(volatile uint32_t *)(RGA_BASE + RGA_DST_MST))
+#define rRGA_LUT_MST              (*(volatile uint32_t *)(RGA_BASE + RGA_LUT_MST))  //repeat
+#define rRGA_PAT_MST              (*(volatile uint32_t *)(RGA_BASE + RGA_PAT_MST))  //repeat
+#define rRGA_LINE_DRAWING_MST     (*(volatile uint32_t *)(RGA_BASE + RGA_LINE_DRAWING_MST))  //repeat
+
+#define rRGA_DST_VIR_INFO         (*(volatile uint32_t *)(RGA_BASE + RGA_DST_VIR_INFO))
+
+#define rRGA_DST_CTR_INFO         (*(volatile uint32_t *)(RGA_BASE + RGA_DST_CTR_INFO))
+#define rRGA_LINE_DRAW_XY_INFO    (*(volatile uint32_t *)(RGA_BASE + RGA_LINE_DRAW_XY_INFO))  //repeat
+
+//Alpha/ROP Registers
+#define rRGA_ALPHA_CON            (*(volatile uint32_t *)(RGA_BASE + RGA_ALPHA_CON))
+#define rRGA_FADING_CON           (*(volatile uint32_t *)(RGA_BASE + RGA_FADING_CON))
+
+#define rRGA_PAT_CON              (*(volatile uint32_t *)(RGA_BASE + RGA_PAT_CON))
+#define rRGA_DST_VIR_WIDTH_PIX    (*(volatile uint32_t *)(RGA_BASE + RGA_DST_VIR_WIDTH_PIX))  //repeat
+
+#define rRGA_ROP_CON0             (*(volatile uint32_t *)(RGA_BASE + RGA_ROP_CON0))
+#define rRGA_CP_GR_G              (*(volatile uint32_t *)(RGA_BASE + RGA_CP_GR_G))  //repeat
+#define rRGA_PRESCL_CB_MST        (*(volatile uint32_t *)(RGA_BASE + RGA_PRESCL_CB_MST))  //repeat
+
+#define rRGA_ROP_CON1             (*(volatile uint32_t *)(RGA_BASE + RGA_ROP_CON1))
+#define rRGA_CP_GR_R              (*(volatile uint32_t *)(RGA_BASE + RGA_CP_GR_R))  //repeat
+#define rRGA_PRESCL_CR_MST        (*(volatile uint32_t *)(RGA_BASE + RGA_PRESCL_CR_MST))  //repeat
+
+//MMU Register
+#define rRGA_MMU_CTRL             (*(volatile uint32_t *)(RGA_BASE + RGA_MMU_CTRL))
+
+
+
+
+//-----------------------------------------------------------------
+//reg detail definition
+//-----------------------------------------------------------------
+/*RGA_SYS_CTRL*/
+#define m_RGA_SYS_CTRL_CMD_MODE                   ( 1<<2 )
+#define m_RGA_SYS_CTRL_OP_ST_SLV                  ( 1<<1 )
+#define m_RGA_sys_CTRL_SOFT_RESET                 ( 1<<0 )
+
+#define s_RGA_SYS_CTRL_CMD_MODE(x)                ( (x&0x1)<<2 )
+#define s_RGA_SYS_CTRL_OP_ST_SLV(x)               ( (x&0x1)<<1 )
+#define s_RGA_sys_CTRL_SOFT_RESET(x)              ( (x&0x1)<<0 )
+
+
+/*RGA_CMD_CTRL*/
+#define m_RGA_CMD_CTRL_CMD_INCR_NUM               ( 0x3ff<<3 )
+#define m_RGA_CMD_CTRL_CMD_STOP_MODE              (     1<<2 )
+#define m_RGA_CMD_CTRL_CMD_INCR_VALID             (     1<<1 )
+#define m_RGA_CMD_CTRL_CMD_LINE_FET_ST            (     1<<0 )
+
+#define s_RGA_CMD_CTRL_CMD_INCR_NUM(x)            ( (x&0x3ff)<<3 )
+#define s_RGA_CMD_CTRL_CMD_STOP_MODE(x)           (   (x&0x1)<<2 )
+#define s_RGA_CMD_CTRL_CMD_INCR_VALID(x)          (   (x&0x1)<<1 )
+#define s_RGA_CMD_CTRL_CMD_LINE_FET_ST(x)         (   (x*0x1)<<0 )
+
+
+/*RGA_STATUS*/
+#define m_RGA_CMD_STATUS_CMD_TOTAL_NUM            ( 0xfff<<20 )
+#define m_RGA_CMD_STATUS_NOW_CMD_NUM              ( 0xfff<<8  )
+#define m_RGA_CMD_STATUS_ENGINE_STATUS            (     1<<0  )
+
+
+/*RGA_INT*/
+#define m_RGA_INT_ALL_CMD_DONE_INT_EN             ( 1<<10 )
+#define m_RGA_INT_MMU_INT_EN                      ( 1<<9  )
+#define m_RGA_INT_ERROR_INT_EN                    ( 1<<8  )
+#define m_RGA_INT_NOW_CMD_DONE_INT_CLEAR          ( 1<<7  )
+#define m_RGA_INT_ALL_CMD_DONE_INT_CLEAR          ( 1<<6  )
+#define m_RGA_INT_MMU_INT_CLEAR                   ( 1<<5  )
+#define m_RGA_INT_ERROR_INT_CLEAR                 ( 1<<4  )
+#define m_RGA_INT_NOW_CMD_DONE_INT_FLAG           ( 1<<3  )
+#define m_RGA_INT_ALL_CMD_DONE_INT_FLAG           ( 1<<2  )
+#define m_RGA_INT_MMU_INT_FLAG                    ( 1<<1  )
+#define m_RGA_INT_ERROR_INT_FLAG                  ( 1<<0  )
+
+#define s_RGA_INT_ALL_CMD_DONE_INT_EN(x)          ( (x&0x1)<<10 )
+#define s_RGA_INT_MMU_INT_EN(x)                   ( (x&0x1)<<9  )
+#define s_RGA_INT_ERROR_INT_EN(x)                 ( (x&0x1)<<8  )
+#define s_RGA_INT_NOW_CMD_DONE_INT_CLEAR(x)       ( (x&0x1)<<7  )
+#define s_RGA_INT_ALL_CMD_DONE_INT_CLEAR(x)       ( (x&0x1)<<6  )
+#define s_RGA_INT_MMU_INT_CLEAR(x)                ( (x&0x1)<<5  )
+#define s_RGA_INT_ERROR_INT_CLEAR(x)              ( (x&0x1)<<4  )
+
+
+/*RGA_AXI_ID*/
+#define m_RGA_AXI_ID_MMU_READ                     ( 3<<30 )
+#define m_RGA_AXI_ID_MMU_WRITE                    ( 3<<28 )
+#define m_RGA_AXI_ID_MASK_READ                    ( 0xf<<24 )
+#define m_RGA_AXI_ID_CMD_FET                      ( 0xf<<20 )
+#define m_RGA_AXI_ID_DST_WRITE                    ( 0xf<<16 )
+#define m_RGA_AXI_ID_DST_READ                     ( 0xf<<12 )
+#define m_RGA_AXI_ID_SRC_CR_READ                  ( 0xf<<8  )
+#define m_RGA_AXI_ID_SRC_CB_READ                  ( 0xf<<4  )
+#define m_RGA_AXI_ID_SRC_Y_READ                   ( 0xf<<0  )
+
+#define s_RGA_AXI_ID_MMU_READ(x)                  ( (x&0x3)<<30 )
+#define s_RGA_AXI_ID_MMU_WRITE(x)                 ( (x&0x3)<<28 )
+#define s_RGA_AXI_ID_MASK_READ(x)                 ( (x&0xf)<<24 )
+#define s_RGA_AXI_ID_CMD_FET(x)                   ( (x&0xf)<<20 )
+#define s_RGA_AXI_ID_DST_WRITE(x)                 ( (x&0xf)<<16 )
+#define s_RGA_AXI_ID_DST_READ(x)                  ( (x&0xf)<<12 )
+#define s_RGA_AXI_ID_SRC_CR_READ(x)               ( (x&0xf)<<8  )
+#define s_RGA_AXI_ID_SRC_CB_READ(x)               ( (x&0xf)<<4  )
+#define s_RGA_AXI_ID_SRC_Y_READ(x)                ( (x&0xf)<<0  )
+
+
+/*RGA_MMU_STA_CTRL*/
+#define m_RGA_MMU_STA_CTRL_TLB_STA_CLEAR          ( 1<<3 )
+#define m_RGA_MMU_STA_CTRL_TLB_STA_RESUME         ( 1<<2 )
+#define m_RGA_MMU_STA_CTRL_TLB_STA_PAUSE          ( 1<<1 )
+#define m_RGA_MMU_STA_CTRL_TLB_STA_EN             ( 1<<0 )
+
+#define s_RGA_MMU_STA_CTRL_TLB_STA_CLEAR(x)       ( (x&0x1)<<3 )
+#define s_RGA_MMU_STA_CTRL_TLB_STA_RESUME(x)      ( (x&0x1)<<2 )
+#define s_RGA_MMU_STA_CTRL_TLB_STA_PAUSE(x)       ( (x&0x1)<<1 )
+#define s_RGA_MMU_STA_CTRL_TLB_STA_EN(x)          ( (x&0x1)<<0 )
+
+
+
+/* RGA_MODE_CTRL */
+#define m_RGA_MODE_CTRL_2D_RENDER_MODE            (  7<<0  )
+#define m_RGA_MODE_CTRL_SRC_RGB_PACK              (  1<<3  )
+#define m_RGA_MODE_CTRL_SRC_FORMAT                ( 15<<4  )
+#define m_RGA_MODE_CTRL_SRC_RB_SWAP               (  1<<8  )
+#define m_RGA_MODE_CTRL_SRC_ALPHA_SWAP            (  1<<9  )
+#define m_RGA_MODE_CTRL_SRC_UV_SWAP_MODE          (  1<<10 )
+#define m_RGA_MODE_CTRL_YUV2RGB_CON_MODE          (  3<<11 )
+#define m_RGA_MODE_CTRL_SRC_TRANS_MODE           (0x1f<<13 )
+#define m_RGA_MODE_CTRL_SRC_TR_MODE               (  1<<13 )
+#define m_RGA_MODE_CTRL_SRC_TR_R_EN               (  1<<14 )
+#define m_RGA_MODE_CTRL_SRC_TR_G_EN               (  1<<15 )
+#define m_RGA_MODE_CTRL_SRC_TR_B_EN               (  1<<16 )
+#define m_RGA_MODE_CTRL_SRC_TR_A_EN               (  1<<17 )
+#define m_RGA_MODE_CTRL_ROTATE_MODE               (  3<<18 )
+#define m_RGA_MODE_CTRL_SCALE_MODE                (  3<<20 )
+#define m_RGA_MODE_CTRL_PAT_SEL                   (  1<<22 )
+#define m_RGA_MODE_CTRL_DST_FORMAT                (  3<<23 )
+#define m_RGA_MODE_CTRL_DST_RGB_PACK              (  1<<25 )
+#define m_RGA_MODE_CTRL_DST_RB_SWAP               (  1<<26 )
+#define m_RGA_MODE_CTRL_DST_ALPHA_SWAP            (  1<<27 )
+#define m_RGA_MODE_CTRL_LUT_ENDIAN_MODE           (  1<<28 )
+#define m_RGA_MODE_CTRL_CMD_INT_ENABLE            (  1<<29 )
+#define m_RGA_MODE_CTRL_ZERO_MODE_ENABLE          (  1<<30 )
+#define m_RGA_MODE_CTRL_DST_ALPHA_ENABLE          (  1<<30 )
+
+
+
+#define s_RGA_MODE_CTRL_2D_RENDER_MODE(x)         (  (x&0x7)<<0  )
+#define s_RGA_MODE_CTRL_SRC_RGB_PACK(x)           (  (x&0x1)<<3  )
+#define s_RGA_MODE_CTRL_SRC_FORMAT(x)             (  (x&0xf)<<4  )
+#define s_RGA_MODE_CTRL_SRC_RB_SWAP(x)            (  (x&0x1)<<8  )
+#define s_RGA_MODE_CTRL_SRC_ALPHA_SWAP(x)         (  (x&0x1)<<9  )
+#define s_RGA_MODE_CTRL_SRC_UV_SWAP_MODE(x)       (  (x&0x1)<<10 )
+#define s_RGA_MODE_CTRL_YUV2RGB_CON_MODE(x)       (  (x&0x3)<<11 )
+#define s_RGA_MODE_CTRL_SRC_TRANS_MODE(x)         ( (x&0x1f)<<13 )
+#define s_RGA_MODE_CTRL_SRC_TR_MODE(x)            (  (x&0x1)<<13 )
+#define s_RGA_MODE_CTRL_SRC_TR_R_EN(x)            (  (x&0x1)<<14 )
+#define s_RGA_MODE_CTRL_SRC_TR_G_EN(x)            (  (x&0x1)<<15 )
+#define s_RGA_MODE_CTRL_SRC_TR_B_EN(x)            (  (x&0x1)<<16 )
+#define s_RGA_MODE_CTRL_SRC_TR_A_EN(x)            (  (x&0x1)<<17 )
+#define s_RGA_MODE_CTRL_ROTATE_MODE(x)            (  (x&0x3)<<18 )
+#define s_RGA_MODE_CTRL_SCALE_MODE(x)             (  (x&0x3)<<20 )
+#define s_RGA_MODE_CTRL_PAT_SEL(x)                (  (x&0x1)<<22 )
+#define s_RGA_MODE_CTRL_DST_FORMAT(x)             (  (x&0x3)<<23 )
+#define s_RGA_MODE_CTRL_DST_RGB_PACK(x)           (  (x&0x1)<<25 )
+#define s_RGA_MODE_CTRL_DST_RB_SWAP(x)            (  (x&0x1)<<26 )
+#define s_RGA_MODE_CTRL_DST_ALPHA_SWAP(x)         (  (x&0x1)<<27 )
+#define s_RGA_MODE_CTRL_LUT_ENDIAN_MODE(x)        (  (x&0x1)<<28 )
+#define s_RGA_MODE_CTRL_CMD_INT_ENABLE(x)         (  (x&0x1)<<29 )
+#define s_RGA_MODE_CTRL_ZERO_MODE_ENABLE(x)       (  (x&0x1)<<30 )
+#define s_RGA_MODE_CTRL_DST_ALPHA_ENABLE(x)       (  (x&0x1)<<31 )
+
+
+
+/* RGA_LINE_DRAW */
+#define m_RGA_LINE_DRAW_MAJOR_WIDTH            ( 0x7ff<<0 )
+#define m_RGA_LINE_DRAW_LINE_DIRECTION         (   0x1<<11)
+#define m_RGA_LINE_DRAW_LINE_WIDTH             (   0xf<<12)
+#define m_RGA_LINE_DRAW_INCR_VALUE             ( 0xfff<<16)
+#define m_RGA_LINE_DRAW_DIR_MAJOR              (   0x1<<28)
+#define m_RGA_LINE_DRAW_DIR_SEMI_MAJOR         (   0x1<<29)
+#define m_RGA_LINE_DRAW_LAST_POINT             (   0x1<<30)
+#define m_RGA_LINE_DRAW_ANTI_ALISING           (   0x1<<31)
+
+#define s_RGA_LINE_DRAW_MAJOR_WIDTH(x)            (((x)&0x7ff)<<0 )
+#define s_RGA_LINE_DRAW_LINE_DIRECTION(x)         (  ((x)&0x1)<<11)
+#define s_RGA_LINE_DRAW_LINE_WIDTH(x)             (  ((x)&0xf)<<12)
+#define s_RGA_LINE_DRAW_INCR_VALUE(x)             (((x)&0xfff)<<16)
+#define s_RGA_LINE_DRAW_DIR_MAJOR(x)              (  ((x)&0x1)<<28)
+#define s_RGA_LINE_DRAW_DIR_SEMI_MAJOR(x)         (  ((x)&0x1)<<29)
+#define s_RGA_LINE_DRAW_LAST_POINT(x)             (  ((x)&0x1)<<30)
+#define s_RGA_LINE_DRAW_ANTI_ALISING(x)           (  ((x)&0x1)<<31)
+
+
+/* RGA_ALPHA_CON */
+#define m_RGA_ALPHA_CON_ENABLE                  ( 0x1<<0 )
+#define m_RGA_ALPHA_CON_A_OR_R_SEL              ( 0x1<<1 )
+#define m_RGA_ALPHA_CON_ALPHA_MODE              ( 0x3<<2 )
+#define m_RGA_ALPHA_CON_PD_MODE                 ( 0xf<<4 )
+#define m_RGA_ALPHA_CON_SET_CONSTANT_VALUE      (0xff<<8 )
+#define m_RGA_ALPHA_CON_PD_M_SEL                ( 0x1<<16)
+#define m_RGA_ALPHA_CON_FADING_ENABLE           ( 0x1<<17)
+#define m_RGA_ALPHA_CON_ROP_MODE_SEL            ( 0x3<<18)
+#define m_RGA_ALPHA_CON_CAL_MODE_SEL            ( 0x1<<28)
+#define m_RGA_ALPHA_CON_DITHER_ENABLE           ( 0x1<<29)
+#define m_RGA_ALPHA_CON_GRADIENT_CAL_MODE       ( 0x1<<30)
+#define m_RGA_ALPHA_CON_AA_SEL                  ( 0x1<<31)
+
+#define s_RGA_ALPHA_CON_ENABLE(x)                  ( (x&0x1)<<0 )
+#define s_RGA_ALPHA_CON_A_OR_R_SEL(x)              ( (x&0x1)<<1 )
+#define s_RGA_ALPHA_CON_ALPHA_MODE(x)              ( (x&0x3)<<2 )
+#define s_RGA_ALPHA_CON_PD_MODE(x)                 ( (x&0xf)<<4 )
+#define s_RGA_ALPHA_CON_SET_CONSTANT_VALUE(x)      ((x&0xff)<<8 )
+#define s_RGA_ALPHA_CON_PD_M_SEL(x)                ( (x&0x1)<<16)
+#define s_RGA_ALPHA_CON_FADING_ENABLE(x)           ( (x&0x1)<<17)
+#define s_RGA_ALPHA_CON_ROP_MODE_SEL(x)            ( (x&0x3)<<18)
+#define s_RGA_ALPHA_CON_CAL_MODE_SEL(x)            ( (x&0x1)<<28)
+#define s_RGA_ALPHA_CON_DITHER_ENABLE(x)           ( (x&0x1)<<29)
+#define s_RGA_ALPHA_CON_GRADIENT_CAL_MODE(x)       ( (x&0x1)<<30)
+#define s_RGA_ALPHA_CON_AA_SEL(x)                  ( (x&0x1)<<31)
+
+
+/* blur sharp mode */
+#define m_RGA_BLUR_SHARP_FILTER_MODE                    (  0x1<<25 )
+#define m_RGA_BLUR_SHARP_FILTER_TYPE                    (  0x3<<26 )
+
+#define s_RGA_BLUR_SHARP_FILTER_MODE(x)                 ( (x&0x1)<<25 )
+#define s_RGA_BLUR_SHARP_FILTER_TYPE(x)                 ( (x&0x3)<<26 )
+
+
+/* pre scale mode */
+#define m_RGA_PRE_SCALE_HOR_RATIO                       ( 0x3 <<20 )
+#define m_RGA_PRE_SCALE_VER_RATIO                       ( 0x3 <<22 )
+#define m_RGA_PRE_SCALE_OUTPUT_FORMAT                   ( 0x1 <<24 )
+
+#define s_RGA_PRE_SCALE_HOR_RATIO(x)                    ( (x&0x3) <<20 )
+#define s_RGA_PRE_SCALE_VER_RATIO(x)                    ( (x&0x3) <<22 )
+#define s_RGA_PRE_SCALE_OUTPUT_FORMAT(x)                ( (x&0x1) <<24 )
+
+
+
+/* RGA_MMU_CTRL*/
+#define m_RGA_MMU_CTRL_TLB_ADDR                         ( 0xffffffff<<0)
+#define m_RGA_MMU_CTRL_PAGE_TABLE_SIZE                  ( 0x3<<4 )
+#define m_RGA_MMU_CTRL_MMU_ENABLE                       ( 0x1<<0 )
+#define m_RGA_MMU_CTRL_SRC_FLUSH                        ( 0x1<<1 )
+#define m_RGA_MMU_CTRL_DST_FLUSH                        ( 0x1<<2 )
+#define m_RGA_MMU_CTRL_CMD_CHAN_FLUSH                   ( 0x1<<3 )
+
+#define s_RGA_MMU_CTRL_TLB_ADDR(x)                      ((x&0xffffffff))
+#define s_RGA_MMU_CTRL_PAGE_TABLE_SIZE(x)               ((x&0x3)<<4)
+#define s_RGA_MMU_CTRL_MMU_ENABLE(x)                    ((x&0x1)<<0)
+#define s_RGA_MMU_CTRL_SRC_FLUSH(x)                     ((x&0x1)<<1)
+#define s_RGA_MMU_CTRL_DST_FLUSH(x)                     ((x&0x1)<<2)
+#define s_RGA_MMU_CTRL_CMD_CHAN_FLUSH(x)                ((x&0x1)<<3)
+
+#endif
+
+/*
+#define RGA_MODE_CTRL_OFFSET            0x0
+#define RGA_SRC_Y_MST_OFFSET            0x4
+#define RGA_SRC_CB_MST_OFFSET           0x8
+#define RGA_SRC_CR_MST_OFFSET           0xc
+#define RGA_SRC_VIR_INFO_OFFSET         0x10
+#define RGA_SRC_ACT_INFO_OFFSET         0x14
+#define RGA_SRC_X_PARA_OFFSET           0x18
+#define RGA_SRC_Y_PARA_OFFSET           0x1c
+#define RGA_SRC_TILE_XINFO_OFFSET       0x20
+#define RGA_SRC_TILE_YINFO_OFFSET       0x24
+#define RGA_SRC_TILE_H_INCR_OFFSET      0x28
+#define RGA_SRC_TILE_V_INCR_OFFSET      0x2c
+#define RGA_SRC_TILE_OFFSETX_OFFSET     0x30
+#define RGA_SRC_TILE_OFFSETY_OFFSET     0x34
+#define RGA_SRC_BG_COLOR_OFFSET         0x38
+
+#define RGA_SRC_FG_COLOR_OFFSET         0x3c
+#define RGA_LINE_DRAWING_COLOR_OFFSET   0x3c
+
+#define RGA_SRC_TR_COLOR0_OFFSET        0x40
+#define RGA_CP_GR_A_OFFSET              0x40  //repeat
+
+#define RGA_SRC_TR_COLOR1_OFFSET        0x44
+#define RGA_CP_GR_B_OFFSET              0x44  //repeat
+
+#define RGA_LINE_DRAW_OFFSET            0x48
+#define RGA_PAT_START_POINT_OFFSET      0x48  //repeat
+
+#define RGA_DST_MST_OFFSET              0x4c
+#define RGA_LUT_MST_OFFSET              0x4c  //repeat
+#define RGA_PAT_MST_OFFSET              0x4c  //repeat
+#define RGA_LINE_DRAWING_MST_OFFSET     0x4c  //repeat
+
+#define RGA_DST_VIR_INFO_OFFSET         0x50
+
+#define RGA_DST_CTR_INFO_OFFSET         0x54
+#define RGA_LINE_DRAW_XY_INFO_OFFSET    0x54  //repeat
+
+#define RGA_ALPHA_CON_OFFSET            0x58
+#define RGA_FADING_CON_OFFSET           0x5c
+
+#define RGA_PAT_CON_OFFSET              0x60
+#define RGA_LINE_DRAWING_WIDTH_OFFSET   0x60  //repeat
+
+#define RGA_ROP_CON0_OFFSET             0x64
+#define RGA_CP_GR_G_OFFSET              0x64  //repeat
+#define RGA_PRESCL_CB_MST_OFFSET        0x64  //repeat
+
+#define RGA_ROP_CON1_OFFSET             0x68
+#define RGA_CP_GR_R_OFFSET              0x68  //repeat
+#define RGA_PRESCL_CR_MST_OFFSET        0x68  //repeat
+
+#define RGA_MMU_CTRL_OFFSET             0x6c
+
+
+#define RGA_SYS_CTRL_OFFSET             0x000
+#define RGA_CMD_CTRL_OFFSET             0x004
+#define RGA_CMD_ADDR_OFFSET             0x008
+#define RGA_STATUS_OFFSET               0x00c
+#define RGA_INT_OFFSET                  0x010
+#define RGA_AXI_ID_OFFSET               0x014
+#define RGA_MMU_STA_CTRL_OFFSET         0x018
+#define RGA_MMU_STA_OFFSET              0x01c
+*/
+//hxx
+
+#define RGA_SYS_CTRL_OFFSET             (RGA_SYS_CTRL-0x100)
+#define RGA_CMD_CTRL_OFFSET             (RGA_CMD_CTRL-0x100)
+#define RGA_CMD_ADDR_OFFSET             (RGA_CMD_ADDR-0x100)
+#define RGA_STATUS_OFFSET               (RGA_STATUS-0x100)
+#define RGA_INT_OFFSET                  (RGA_INT-0x100)
+#define RGA_AXI_ID_OFFSET               (RGA_AXI_ID-0x100)
+#define RGA_MMU_STA_CTRL_OFFSET         (RGA_MMU_STA_CTRL-0x100)
+#define RGA_MMU_STA_OFFSET              (RGA_MMU_STA-0x100)
+
+#define RGA_MODE_CTRL_OFFSET            (RGA_MODE_CTRL-0x100)
+#define RGA_SRC_Y_MST_OFFSET            (RGA_SRC_Y_MST-0x100)
+#define RGA_SRC_CB_MST_OFFSET           (RGA_SRC_CB_MST-0x100)
+#define RGA_SRC_CR_MST_OFFSET           (RGA_SRC_CR_MST-0x100)
+#define RGA_SRC_VIR_INFO_OFFSET         (RGA_SRC_VIR_INFO-0x100)
+#define RGA_SRC_ACT_INFO_OFFSET         (RGA_SRC_ACT_INFO-0x100)
+#define RGA_SRC_X_PARA_OFFSET           (RGA_SRC_X_PARA-0x100)
+#define RGA_SRC_Y_PARA_OFFSET           (RGA_SRC_Y_PARA-0x100)
+#define RGA_SRC_TILE_XINFO_OFFSET       (RGA_SRC_TILE_XINFO-0x100)
+#define RGA_SRC_TILE_YINFO_OFFSET       (RGA_SRC_TILE_YINFO-0x100)
+#define RGA_SRC_TILE_H_INCR_OFFSET      (RGA_SRC_TILE_H_INCR-0x100)
+#define RGA_SRC_TILE_V_INCR_OFFSET      (RGA_SRC_TILE_V_INCR-0x100)
+#define RGA_SRC_TILE_OFFSETX_OFFSET     (RGA_SRC_TILE_OFFSETX-0x100)
+#define RGA_SRC_TILE_OFFSETY_OFFSET     (RGA_SRC_TILE_OFFSETY-0x100)
+#define RGA_SRC_BG_COLOR_OFFSET         (RGA_SRC_BG_COLOR-0x100)
+
+#define RGA_SRC_FG_COLOR_OFFSET         (RGA_SRC_FG_COLOR-0x100)
+#define RGA_LINE_DRAWING_COLOR_OFFSET   (RGA_LINE_DRAWING_COLOR-0x100)
+
+#define RGA_SRC_TR_COLOR0_OFFSET        (RGA_SRC_TR_COLOR0-0x100)
+#define RGA_CP_GR_A_OFFSET              (RGA_CP_GR_A-0x100)  //repeat
+
+#define RGA_SRC_TR_COLOR1_OFFSET        (RGA_SRC_TR_COLOR1-0x100)
+#define RGA_CP_GR_B_OFFSET              (RGA_CP_GR_B-0x100)  //repeat
+
+#define RGA_LINE_DRAW_OFFSET            (RGA_LINE_DRAW-0x100)
+#define RGA_PAT_START_POINT_OFFSET      (RGA_PAT_START_POINT-0x100)  //repeat
+
+#define RGA_DST_MST_OFFSET              (RGA_DST_MST-0x100)
+#define RGA_LUT_MST_OFFSET              (RGA_LUT_MST-0x100)  //repeat
+#define RGA_PAT_MST_OFFSET              (RGA_PAT_MST-0x100)  //repeat
+#define RGA_LINE_DRAWING_MST_OFFSET     (RGA_LINE_DRAWING_MST-0x100)  //repeat
+
+#define RGA_DST_VIR_INFO_OFFSET         (RGA_DST_VIR_INFO-0x100)
+
+#define RGA_DST_CTR_INFO_OFFSET         (RGA_DST_CTR_INFO-0x100)
+#define RGA_LINE_DRAW_XY_INFO_OFFSET    (RGA_LINE_DRAW_XY_INFO-0x100)  //repeat
+
+#define RGA_ALPHA_CON_OFFSET            (RGA_ALPHA_CON-0x100)
+
+#define RGA_PAT_CON_OFFSET              (RGA_PAT_CON-0x100)
+#define RGA_LINE_DRAWING_WIDTH_OFFSET   (RGA_DST_VIR_WIDTH_PIX-0x100)  //repeat
+
+#define RGA_ROP_CON0_OFFSET             (RGA_ROP_CON0-0x100)
+#define RGA_CP_GR_G_OFFSET              (RGA_CP_GR_G-0x100)  //repeat
+#define RGA_PRESCL_CB_MST_OFFSET        (RGA_PRESCL_CB_MST-0x100)  //repeat
+
+#define RGA_ROP_CON1_OFFSET             (RGA_ROP_CON1-0x100)
+#define RGA_CP_GR_R_OFFSET              (RGA_CP_GR_R-0x100)  //repeat
+#define RGA_PRESCL_CR_MST_OFFSET        (RGA_PRESCL_CR_MST-0x100)  //repeat
+
+#define RGA_FADING_CON_OFFSET           (RGA_FADING_CON-0x100)
+#define RGA_MMU_TLB_OFFSET              (RGA_MMU_TBL-0x100)
+
+#define RGA_YUV_OUT_CFG_OFFSET         (RGA_YUV_OUT_CFG-0x100)
+#define RGA_DST_UV_MST_OFFSET          (RGA_DST_UV_MST-0x100)
+
+
+
+void matrix_cal(const struct rga_req *msg, TILE_INFO *tile);
+
+
+int RGA_gen_reg_info(const struct rga_req *msg, unsigned char *base);
+uint8_t   RGA_pixel_width_init(uint32_t format);
+
diff --git a/drivers/video/rockchip/rga/rga_rop.h b/drivers/video/rockchip/rga/rga_rop.h
new file mode 100644
index 0000000000000..c38f05a13dfe0
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_rop.h
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_ROP_H__
+#define __RGA_ROP_H__
+
+unsigned int ROP3_code[256] =
+{
+    0x00000007, 0x00000451, 0x00006051, 0x00800051, 0x00007041, 0x00800041, 0x00804830, 0x000004f0,//0
+    0x00800765, 0x000004b0, 0x00000065, 0x000004f4, 0x00000075, 0x000004e6, 0x00804850, 0x00800005,
+
+    0x00006850, 0x00800050, 0x00805028, 0x00000568, 0x00804031, 0x00000471, 0x002b6071, 0x018037aa,//1
+    0x008007aa, 0x00036071, 0x00002c6a, 0x00803631, 0x00002d68, 0x00802721, 0x008002d0, 0x000006d0,
+
+    0x0080066e, 0x00000528, 0x00000066, 0x0000056c, 0x018007aa, 0x0002e06a, 0x00003471, 0x00834031,//2
+    0x00800631, 0x0002b471, 0x00006071, 0x008037aa, 0x000036d0, 0x008002d4, 0x00002d28, 0x000006d4,
+
+    0x0000006e, 0x00000565, 0x00003451, 0x00800006, 0x000034f0, 0x00834830, 0x00800348, 0x00000748,//3
+    0x00002f48, 0x0080034c, 0x000034b0, 0x0000074c, 0x00000031, 0x00834850, 0x000034e6, 0x00800071,
+
+    0x008006f4, 0x00000431, 0x018007a1, 0x00b6e870, 0x00000074, 0x0000046e, 0x00002561, 0x00802f28,//4
+    0x00800728, 0x0002a561, 0x000026c2, 0x008002c6, 0x00007068, 0x018035aa, 0x00002c2a, 0x000006c6,
+
+    0x0000006c, 0x00000475, 0x000024e2, 0x008036b0, 0x00804051, 0x00800004, 0x00800251, 0x00000651,
+    0x00002e4a, 0x0080024e, 0x00000028, 0x00824842, 0x000024a2, 0x0000064e, 0x000024f4, 0x00800068,//5
+
+    0x008006b0, 0x000234f0, 0x00002741, 0x00800345, 0x00003651, 0x00800255, 0x00000030, 0x00834051,
+    0x00a34842, 0x000002b0, 0x00800271, 0x0002b651, 0x00800368, 0x0002a741, 0x0000364e, 0x00806830,//6
+
+    0x00006870, 0x008037a2, 0x00003431, 0x00000745, 0x00002521, 0x00000655, 0x0000346e, 0x00800062,
+    0x008002f0, 0x000236d0, 0x000026d4, 0x00807028, 0x000036c6, 0x00806031, 0x008005aa, 0x00000671,//7
+
+    0x00800671, 0x000005aa, 0x00006031, 0x008036c6, 0x00007028, 0x00802e55, 0x008236d0, 0x000002f0,
+    0x00000070, 0x0080346e, 0x00800655, 0x00802521, 0x00800745, 0x00803431, 0x000037a2, 0x00806870,//8
+
+    0x00006830, 0x0080364e, 0x00822f48, 0x00000361, 0x0082b651, 0x00000271, 0x00800231, 0x002b4051,
+    0x00034051, 0x00800030, 0x0080026e, 0x00803651, 0x0080036c, 0x00802741, 0x008234f0, 0x000006b0,//9
+
+    0x00000068, 0x00802c75, 0x0080064e, 0x008024a2, 0x0002c04a, 0x00800021, 0x00800275, 0x00802e51,
+    0x00800651, 0x00000251, 0x00800000, 0x00004051, 0x000036b0, 0x008024e2, 0x00800475, 0x00000045,//a
+
+    0x008006c6, 0x00802c2a, 0x000035aa, 0x00807068, 0x008002f4, 0x008026c2, 0x00822d68, 0x00000728,
+    0x00002f28, 0x00802561, 0x0080046e, 0x00000046, 0x00836870, 0x000007a2, 0x00800431, 0x00004071,//b
+
+    0x00000071, 0x008034e6, 0x00034850, 0x00800031, 0x0080074c, 0x008034b0, 0x00800365, 0x00802f48,
+    0x00800748, 0x00000341, 0x000026a2, 0x008034f0, 0x00800002, 0x00005048, 0x00800565, 0x00000055,//c
+
+    0x008006d4, 0x00802d28, 0x008002e6, 0x008036d0, 0x000037aa, 0x00806071, 0x0082b471, 0x00000631,
+    0x00002e2a, 0x00803471, 0x00826862, 0x010007aa, 0x0080056c, 0x00000054, 0x00800528, 0x00005068,//d
+
+    0x008006d0, 0x000002d0, 0x00002721, 0x00802d68, 0x00003631, 0x00802c6a, 0x00836071, 0x000007aa,
+    0x010037aa, 0x00a36870, 0x00800471, 0x00004031, 0x00800568, 0x00005028, 0x00000050, 0x00800545,//e
+
+    0x00800001, 0x00004850, 0x008004e6, 0x0000004e, 0x008004f4, 0x0000004c, 0x008004b0, 0x00004870,
+    0x008004f0, 0x00004830, 0x00000048, 0x0080044e, 0x00000051, 0x008004d4, 0x00800451, 0x00800007,//f
+};
+
+#endif
diff --git a/drivers/video/rockchip/rga/rga_type.h b/drivers/video/rockchip/rga/rga_type.h
new file mode 100644
index 0000000000000..30f5df2f38e58
--- /dev/null
+++ b/drivers/video/rockchip/rga/rga_type.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_TYPE_H__
+#define __RGA_TYPE_H__
+
+
+#ifdef __cplusplus
+#if __cplusplus
+}
+#endif
+#endif /* __cplusplus */
+
+typedef  unsigned int     UWORD32;
+typedef  unsigned int     uint32;
+typedef  unsigned int     RK_U32;
+
+typedef  unsigned short   UWORD16;
+typedef  unsigned short   RK_U16;
+
+typedef  unsigned char    UBYTE;
+typedef  unsigned char    RK_U8;
+
+typedef  int              WORD32;
+typedef  int              RK_S32;
+
+typedef  short            WORD16;
+typedef  short            RK_S16;
+
+typedef  char             BYTE;
+typedef  char             RK_S8;
+
+
+#ifndef NULL
+#define NULL              0L
+#endif
+
+#ifndef TRUE
+#define TRUE              1L
+#endif
+
+
+#ifdef __cplusplus
+#if __cplusplus
+}
+#endif
+#endif /* __cplusplus */
+
+
+#endif /* __RGA_TYPR_H__ */
+
diff --git a/drivers/video/rockchip/rga2/Kconfig b/drivers/video/rockchip/rga2/Kconfig
new file mode 100644
index 0000000000000..49a0f62b99a05
--- /dev/null
+++ b/drivers/video/rockchip/rga2/Kconfig
@@ -0,0 +1,30 @@
+# SPDX-License-Identifier: GPL-2.0
+menuconfig ROCKCHIP_RGA2
+	tristate "RGA2"
+	depends on ARCH_ROCKCHIP && !ROCKCHIP_MULTI_RGA
+	help
+	  rk32 rga2 module.
+
+if ROCKCHIP_RGA2
+
+config ROCKCHIP_RGA2_PROC_FS
+	bool "Enable RGA2 procfs"
+	select ROCKCHIP_RGA2_DEBUGGER
+	depends on PROC_FS
+	help
+	  Enable procfs to debug RGA driver.
+
+config ROCKCHIP_RGA2_DEBUG_FS
+	bool "Enable RGA2 debugfs"
+	select ROCKCHIP_RGA2_DEBUGGER
+	depends on DEBUG_FS
+	default y
+	help
+	  Enable debugfs to debug RGA driver.
+
+config ROCKCHIP_RGA2_DEBUGGER
+	bool
+	help
+	  Enabling the debugger of RGA2, you can use procfs and debugfs for debugging.
+
+endif
diff --git a/drivers/video/rockchip/rga2/Makefile b/drivers/video/rockchip/rga2/Makefile
new file mode 100644
index 0000000000000..60181fffa4c88
--- /dev/null
+++ b/drivers/video/rockchip/rga2/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0
+rga2-y	:= rga2_drv.o rga2_mmu_info.o rga2_reg_info.o RGA2_API.o
+rga2-$(CONFIG_ROCKCHIP_RGA2_DEBUGGER) += rga2_debugger.o
+
+obj-$(CONFIG_ROCKCHIP_RGA2)	+= rga2.o
diff --git a/drivers/video/rockchip/rga2/RGA2_API.c b/drivers/video/rockchip/rga2/RGA2_API.c
new file mode 100644
index 0000000000000..0be2c1ae64b6a
--- /dev/null
+++ b/drivers/video/rockchip/rga2/RGA2_API.c
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#include <linux/memory.h>
+#include "RGA2_API.h"
+#include "rga2.h"
+//#include "rga_angle.h"
+
+#define IS_YUV_420(format) \
+     ((format == RK_FORMAT_YCbCr_420_P) | (format == RK_FORMAT_YCbCr_420_SP) | \
+      (format == RK_FORMAT_YCrCb_420_P) | (format == RK_FORMAT_YCrCb_420_SP))
+
+#define IS_YUV_422(format) \
+     ((format == RK_FORMAT_YCbCr_422_P) | (format == RK_FORMAT_YCbCr_422_SP) | \
+      (format == RK_FORMAT_YCrCb_422_P) | (format == RK_FORMAT_YCrCb_422_SP))
+
+#define IS_YUV(format) \
+     ((format == RK_FORMAT_YCbCr_420_P) | (format == RK_FORMAT_YCbCr_420_SP) | \
+      (format == RK_FORMAT_YCrCb_420_P) | (format == RK_FORMAT_YCrCb_420_SP) | \
+      (format == RK_FORMAT_YCbCr_422_P) | (format == RK_FORMAT_YCbCr_422_SP) | \
+      (format == RK_FORMAT_YCrCb_422_P) | (format == RK_FORMAT_YCrCb_422_SP))
+
+
+
diff --git a/drivers/video/rockchip/rga2/RGA2_API.h b/drivers/video/rockchip/rga2/RGA2_API.h
new file mode 100644
index 0000000000000..88b68a38f735c
--- /dev/null
+++ b/drivers/video/rockchip/rga2/RGA2_API.h
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_API_H__
+#define __RGA_API_H__
+
+#include <linux/miscdevice.h>
+#include <linux/wakelock.h>
+
+#include "rga2_reg_info.h"
+#include "rga2_debugger.h"
+#include "rga2.h"
+
+/* Driver information */
+#define DRIVER_DESC			"RGA2 Device Driver"
+#define DRIVER_NAME			"rga2"
+#define DRIVER_VERSION		"2.1.1"
+
+/* Logging */
+#define RGA_DEBUG 1
+#if RGA_DEBUG
+#define DBG(format, args...) printk(KERN_DEBUG "%s: " format, DRIVER_NAME, ## args)
+#define ERR(format, args...) printk(KERN_ERR "%s: " format, DRIVER_NAME, ## args)
+#define WARNING(format, args...) printk(KERN_WARN "%s: " format, DRIVER_NAME, ## args)
+#define INFO(format, args...) printk(KERN_INFO "%s: " format, DRIVER_NAME, ## args)
+#else
+#define DBG(format, args...)
+#define ERR(format, args...)
+#define WARNING(format, args...)
+#define INFO(format, args...)
+#endif
+
+struct rga2_drvdata_t {
+	struct miscdevice miscdev;
+	struct device *dev;
+	void *rga_base;
+	int irq;
+
+	struct delayed_work power_off_work;
+	struct wake_lock wake_lock;
+	void (*rga_irq_callback)(int rga_retval);
+
+	struct clk *aclk_rga2;
+	struct clk *hclk_rga2;
+	struct clk *pd_rga2;
+	struct clk *clk_rga2;
+
+	struct ion_client *ion_client;
+	char version[16];
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	struct rga_debugger *debugger;
+#endif
+};
+
+#define ENABLE      1
+#define DISABLE     0
+
+
+
+#endif
diff --git a/drivers/video/rockchip/rga2/rga2.h b/drivers/video/rockchip/rga2/rga2.h
new file mode 100644
index 0000000000000..335970c44ed6b
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2.h
@@ -0,0 +1,792 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _RGA_DRIVER_H_
+#define _RGA_DRIVER_H_
+
+#include <linux/mutex.h>
+#include <linux/scatterlist.h>
+#include <linux/dma-buf.h>
+
+
+#define RGA_BLIT_SYNC	0x5017
+#define RGA_BLIT_ASYNC  0x5018
+#define RGA_FLUSH       0x5019
+#define RGA_GET_RESULT  0x501a
+#define RGA_GET_VERSION 0x501b
+#define RGA_CACHE_FLUSH 0x501c
+
+#define RGA2_BLIT_SYNC	 0x6017
+#define RGA2_BLIT_ASYNC  0x6018
+#define RGA2_FLUSH       0x6019
+#define RGA2_GET_RESULT  0x601a
+#define RGA2_GET_VERSION 0x601b
+
+
+#define RGA2_REG_CTRL_LEN    0x8    /* 8  */
+#define RGA2_REG_CMD_LEN     0x20   /* 32 */
+#define RGA2_CMD_BUF_SIZE    0x700  /* 16*28*4 */
+
+#define RGA2_OUT_OF_RESOURCES    -10
+#define RGA2_MALLOC_ERROR        -11
+
+#define SCALE_DOWN_LARGE 1
+
+#define rgaIS_ERROR(status)			(status < 0)
+#define rgaNO_ERROR(status)			(status >= 0)
+#define rgaIS_SUCCESS(status)		(status == 0)
+
+#define RGA_BUF_GEM_TYPE_MASK      0xC0
+#define RGA_BUF_GEM_TYPE_DMA       0x80
+#define RGA2_MAJOR_VERSION_MASK     (0xFF000000)
+#define RGA2_MINOR_VERSION_MASK     (0x00F00000)
+#define RGA2_SVN_VERSION_MASK       (0x000FFFFF)
+
+/* RGA2 process mode enum */
+enum
+{
+    bitblt_mode               = 0x0,
+    color_palette_mode        = 0x1,
+    color_fill_mode           = 0x2,
+    update_palette_table_mode = 0x3,
+    update_patten_buff_mode   = 0x4,
+};  /*render mode*/
+
+enum
+{
+    A_B_B =0x0,
+    A_B_C =0x1,
+};  //bitblt_mode select
+
+enum
+{
+    rop_enable_mask          = 0x2,
+    dither_enable_mask       = 0x8,
+    fading_enable_mask       = 0x10,
+    PD_enbale_mask           = 0x20,
+};
+
+
+
+/*
+//          Alpha    Red     Green   Blue
+{  4, 32, {{32,24,   8, 0,  16, 8,  24,16 }}, GGL_RGBA },   // RK_FORMAT_RGBA_8888
+{  4, 24, {{ 0, 0,   8, 0,  16, 8,  24,16 }}, GGL_RGB  },   // RK_FORMAT_RGBX_8888
+{  3, 24, {{ 0, 0,   8, 0,  16, 8,  24,16 }}, GGL_RGB  },   // RK_FORMAT_RGB_888
+{  4, 32, {{32,24,  24,16,  16, 8,   8, 0 }}, GGL_BGRA },   // RK_FORMAT_BGRA_8888
+{  2, 16, {{ 0, 0,  16,11,  11, 5,   5, 0 }}, GGL_RGB  },   // RK_FORMAT_RGB_565
+{  2, 16, {{ 1, 0,  16,11,  11, 6,   6, 1 }}, GGL_RGBA },   // RK_FORMAT_RGBA_5551
+{  2, 16, {{ 4, 0,  16,12,  12, 8,   8, 4 }}, GGL_RGBA },   // RK_FORMAT_RGBA_4444
+{  2, 16, {{ 0, 0,   5, 0   11, 5,   16,11}}, GGL_BGR  },   // RK_FORMAT_BGR_565
+{  2, 16, {{ 1, 0,   6, 1,  11, 6,   16,11}}, GGL_BGRA },   // RK_FORMAT_BGRA_5551
+{  2, 16, {{ 4, 0,   8, 4,  12, 8,   16,12}}, GGL_BGRA },   // RK_FORMAT_BGRA_4444
+
+*/
+enum
+{
+	RGA2_FORMAT_RGBA_8888    = 0x0,
+    RGA2_FORMAT_RGBX_8888    = 0x1,
+    RGA2_FORMAT_RGB_888      = 0x2,
+    RGA2_FORMAT_BGRA_8888    = 0x3,
+    RGA2_FORMAT_BGRX_8888    = 0x4,
+    RGA2_FORMAT_BGR_888      = 0x5,
+    RGA2_FORMAT_RGB_565      = 0x6,
+    RGA2_FORMAT_RGBA_5551    = 0x7,
+    RGA2_FORMAT_RGBA_4444    = 0x8,
+    RGA2_FORMAT_BGR_565      = 0x9,
+    RGA2_FORMAT_BGRA_5551    = 0xa,
+    RGA2_FORMAT_BGRA_4444    = 0xb,
+
+    RGA2_FORMAT_Y4           = 0xe,
+    RGA2_FORMAT_YCbCr_400    = 0xf,
+
+    RGA2_FORMAT_YCbCr_422_SP = 0x10,
+    RGA2_FORMAT_YCbCr_422_P  = 0x11,
+    RGA2_FORMAT_YCbCr_420_SP = 0x12,
+    RGA2_FORMAT_YCbCr_420_P  = 0x13,
+    RGA2_FORMAT_YCrCb_422_SP = 0x14,
+    RGA2_FORMAT_YCrCb_422_P  = 0x15,
+    RGA2_FORMAT_YCrCb_420_SP = 0x16,
+    RGA2_FORMAT_YCrCb_420_P  = 0x17,
+
+	RGA2_FORMAT_YVYU_422 = 0x18,
+	RGA2_FORMAT_YVYU_420 = 0x19,
+	RGA2_FORMAT_VYUY_422 = 0x1a,
+	RGA2_FORMAT_VYUY_420 = 0x1b,
+	RGA2_FORMAT_YUYV_422 = 0x1c,
+	RGA2_FORMAT_YUYV_420 = 0x1d,
+	RGA2_FORMAT_UYVY_422 = 0x1e,
+	RGA2_FORMAT_UYVY_420 = 0x1f,
+
+    RGA2_FORMAT_YCbCr_420_SP_10B = 0x20,
+    RGA2_FORMAT_YCrCb_420_SP_10B = 0x21,
+    RGA2_FORMAT_YCbCr_422_SP_10B = 0x22,
+    RGA2_FORMAT_YCrCb_422_SP_10B = 0x23,
+
+	RGA2_FORMAT_BPP_1            = 0x24,
+	RGA2_FORMAT_BPP_2            = 0x25,
+	RGA2_FORMAT_BPP_4            = 0x26,
+	RGA2_FORMAT_BPP_8            = 0x27,
+
+	RGA2_FORMAT_ARGB_8888    = 0x28,
+	RGA2_FORMAT_XRGB_8888    = 0x29,
+	RGA2_FORMAT_ARGB_5551    = 0x2a,
+	RGA2_FORMAT_ARGB_4444    = 0x2b,
+	RGA2_FORMAT_ABGR_8888    = 0x2c,
+	RGA2_FORMAT_XBGR_8888    = 0x2d,
+	RGA2_FORMAT_ABGR_5551    = 0x2e,
+	RGA2_FORMAT_ABGR_4444    = 0x2f,
+};
+
+typedef struct mdp_img
+{
+    u16 width;
+    u16 height;
+    u32 format;
+    u32 mem_addr;
+}
+mdp_img;
+
+typedef struct mdp_img_act
+{
+    u16 width;     // width
+    u16 height;    // height
+    s16 x_off;     // x offset for the vir
+    s16 y_off;     // y offset for the vir
+    s16 uv_x_off;
+    s16 uv_y_off;
+}
+mdp_img_act;
+
+typedef struct mdp_img_vir
+{
+    u16 width;
+    u16 height;
+    u32 format;
+    u32 mem_addr;
+    u32 uv_addr;
+    u32 v_addr;
+}
+mdp_img_vir;
+
+
+typedef struct MMU_INFO
+{
+    unsigned long src0_base_addr;
+    unsigned long src1_base_addr;
+    unsigned long dst_base_addr;
+    unsigned long els_base_addr;
+
+    u8 src0_mmu_flag;     /* [0] src0 mmu enable [1] src0_flush [2] src0_prefetch_en [3] src0_prefetch dir */
+    u8 src1_mmu_flag;     /* [0] src1 mmu enable [1] src1_flush [2] src1_prefetch_en [3] src1_prefetch dir */
+    u8 dst_mmu_flag;      /* [0] dst  mmu enable [1] dst_flush  [2] dst_prefetch_en  [3] dst_prefetch dir  */
+    u8 els_mmu_flag;      /* [0] els  mmu enable [1] els_flush  [2] els_prefetch_en  [3] els_prefetch dir  */
+} MMU_INFO;
+
+
+enum
+{
+	MMU_DIS = 0x0,
+	MMU_EN  = 0x1
+};
+enum
+{
+	MMU_FLUSH_DIS = 0x0,
+	MMU_FLUSH_EN  = 0x2
+};
+enum
+{
+	MMU_PRE_DIS = 0x0,
+	MMU_PRE_EN  = 0x4
+};
+enum
+{
+	MMU_PRE_DIR_FORW  = 0x0,
+	MMU_PRE_DIR_BACK  = 0x8
+};
+typedef struct COLOR_FILL
+{
+    s16 gr_x_a;
+    s16 gr_y_a;
+    s16 gr_x_b;
+    s16 gr_y_b;
+    s16 gr_x_g;
+    s16 gr_y_g;
+    s16 gr_x_r;
+    s16 gr_y_r;
+}
+COLOR_FILL;
+
+enum
+{
+	ALPHA_ORIGINAL = 0x0,
+	ALPHA_NO_128   = 0x1
+};
+
+enum
+{
+	R2_BLACK       = 0x00,
+	R2_COPYPEN     = 0xf0,
+	R2_MASKNOTPEN  = 0x0a,
+	R2_MASKPEN     = 0xa0,
+	R2_MASKPENNOT  = 0x50,
+	R2_MERGENOTPEN = 0xaf,
+	R2_MERGEPEN    = 0xfa,
+	R2_MERGEPENNOT = 0xf5,
+	R2_NOP         = 0xaa,
+	R2_NOT         = 0x55,
+	R2_NOTCOPYPEN  = 0x0f,
+	R2_NOTMASKPEN  = 0x5f,
+	R2_NOTMERGEPEN = 0x05,
+	R2_NOTXORPEN   = 0xa5,
+	R2_WHITE       = 0xff,
+	R2_XORPEN      = 0x5a
+};
+
+
+/***************************************/
+/* porting from rga.h for msg convert  */
+/***************************************/
+
+typedef struct FADING
+{
+    uint8_t b;
+    uint8_t g;
+    uint8_t r;
+    uint8_t res;
+}
+FADING;
+
+typedef struct MMU
+{
+    unsigned char mmu_en;
+    unsigned long base_addr;
+    uint32_t mmu_flag;     /* [0] mmu enable [1] src_flush [2] dst_flush [3] CMD_flush [4~5] page size*/
+} MMU;
+
+typedef struct MMU_32
+{
+    unsigned char mmu_en;
+    uint32_t base_addr;
+    uint32_t mmu_flag;     /* [0] mmu enable [1] src_flush [2] dst_flush [3] CMD_flush [4~5] page size*/
+} MMU_32;
+
+typedef struct RECT
+{
+    unsigned short xmin;
+    unsigned short xmax; // width - 1
+    unsigned short ymin;
+    unsigned short ymax; // height - 1
+} RECT;
+
+typedef struct POINT
+{
+    unsigned short x;
+    unsigned short y;
+}
+POINT;
+
+typedef struct line_draw_t
+{
+    POINT start_point;              /* LineDraw_start_point                */
+    POINT end_point;                /* LineDraw_end_point                  */
+    uint32_t   color;               /* LineDraw_color                      */
+    uint32_t   flag;                /* (enum) LineDrawing mode sel         */
+    uint32_t   line_width;          /* range 1~16 */
+}
+line_draw_t;
+
+/* color space convert coefficient. */
+typedef struct csc_coe_t {
+    int16_t r_v;
+    int16_t g_y;
+    int16_t b_u;
+    int32_t off;
+} csc_coe_t;
+
+typedef struct full_csc_t {
+    unsigned char flag;
+    csc_coe_t coe_y;
+    csc_coe_t coe_u;
+    csc_coe_t coe_v;
+} full_csc_t;
+
+typedef struct rga_img_info_t
+{
+    unsigned long yrgb_addr;      /* yrgb    mem addr         */
+    unsigned long uv_addr;        /* cb/cr   mem addr         */
+    unsigned long v_addr;         /* cr      mem addr         */
+    unsigned int format;         //definition by RK_FORMAT
+
+    unsigned short act_w;
+    unsigned short act_h;
+    unsigned short x_offset;
+    unsigned short y_offset;
+
+    unsigned short vir_w;
+    unsigned short vir_h;
+
+    unsigned short endian_mode; //for BPP
+    unsigned short alpha_swap;    /* not use */
+}
+rga_img_info_t;
+
+typedef struct rga_img_info_32_t
+{
+    uint32_t yrgb_addr;      /* yrgb    mem addr         */
+    uint32_t uv_addr;        /* cb/cr   mem addr         */
+    uint32_t v_addr;         /* cr      mem addr         */
+    unsigned int format;         //definition by RK_FORMAT
+    unsigned short act_w;
+    unsigned short act_h;
+    unsigned short x_offset;
+    unsigned short y_offset;
+    unsigned short vir_w;
+    unsigned short vir_h;
+    unsigned short endian_mode; //for BPP
+    unsigned short alpha_swap;
+}
+rga_img_info_32_t;
+
+struct rga_dma_buffer_t {
+	/* DMABUF information */
+	struct dma_buf *dma_buf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+
+	dma_addr_t iova;
+	unsigned long size;
+	void *vaddr;
+	enum dma_data_direction dir;
+};
+
+struct rga_req {
+    uint8_t render_mode;            /* (enum) process mode sel */
+
+    rga_img_info_t src;             /* src image info */
+    rga_img_info_t dst;             /* dst image info */
+    rga_img_info_t pat;             /* patten image info */
+
+    unsigned long rop_mask_addr;         /* rop4 mask addr */
+    unsigned long LUT_addr;              /* LUT addr */
+
+    RECT clip;                      /* dst clip window default value is dst_vir */
+                                    /* value from [0, w-1] / [0, h-1]*/
+
+    int32_t sina;                   /* dst angle  default value 0  16.16 scan from table */
+    int32_t cosa;                   /* dst angle  default value 0  16.16 scan from table */
+
+    uint16_t alpha_rop_flag;        /* alpha rop process flag           */
+                                    /* ([0] = 1 alpha_rop_enable)       */
+                                    /* ([1] = 1 rop enable)             */
+                                    /* ([2] = 1 fading_enable)          */
+                                    /* ([3] = 1 PD_enable)              */
+                                    /* ([4] = 1 alpha cal_mode_sel)     */
+                                    /* ([5] = 1 dither_enable)          */
+                                    /* ([6] = 1 gradient fill mode sel) */
+                                    /* ([7] = 1 AA_enable)              */
+
+    uint8_t  scale_mode;            /* 0 nearst / 1 bilnear / 2 bicubic */
+
+    uint32_t color_key_max;         /* color key max */
+    uint32_t color_key_min;         /* color key min */
+
+    uint32_t fg_color;              /* foreground color */
+    uint32_t bg_color;              /* background color */
+
+    COLOR_FILL gr_color;            /* color fill use gradient */
+
+    line_draw_t line_draw_info;
+
+    FADING fading;
+
+    uint8_t PD_mode;                /* porter duff alpha mode sel */
+
+    uint8_t alpha_global_value;     /* global alpha value */
+
+    uint16_t rop_code;              /* rop2/3/4 code  scan from rop code table*/
+
+    uint8_t bsfilter_flag;          /* [2] 0 blur 1 sharp / [1:0] filter_type*/
+
+    uint8_t palette_mode;           /* (enum) color palatte  0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+
+    uint8_t yuv2rgb_mode;           /* (enum) BT.601 MPEG / BT.601 JPEG / BT.709  */
+
+    uint8_t endian_mode;            /* 0/big endian 1/little endian*/
+
+    uint8_t rotate_mode;            /* (enum) rotate mode  */
+                                    /* 0x0,     no rotate  */
+                                    /* 0x1,     rotate     */
+                                    /* 0x2,     x_mirror   */
+                                    /* 0x3,     y_mirror   */
+
+    uint8_t color_fill_mode;        /* 0 solid color / 1 patten color */
+
+    MMU mmu_info;                   /* mmu information */
+
+    uint8_t  alpha_rop_mode;        /* ([0~1] alpha mode)            */
+                                    /* ([2~3] rop   mode)            */
+                                    /* ([4]   zero  mode en)         */
+                                    /* ([5]   dst   alpha mode)      */
+                                    /* ([6]   alpha output mode sel) 0 src / 1 dst*/
+
+    uint8_t  src_trans_mode;
+
+    uint8_t dither_mode;
+
+    full_csc_t full_csc;            /* full color space convert */
+};
+struct rga_req_32
+{
+    uint8_t render_mode;            /* (enum) process mode sel */
+    rga_img_info_32_t src;             /* src image info */
+    rga_img_info_32_t dst;             /* dst image info */
+    rga_img_info_32_t pat;             /* patten image info */
+    uint32_t rop_mask_addr;         /* rop4 mask addr */
+    uint32_t LUT_addr;              /* LUT addr */
+    RECT clip;                      /* dst clip window default value is dst_vir */
+                                    /* value from [0, w-1] / [0, h-1]*/
+    int32_t sina;                   /* dst angle  default value 0  16.16 scan from table */
+    int32_t cosa;                   /* dst angle  default value 0  16.16 scan from table */
+    uint16_t alpha_rop_flag;        /* alpha rop process flag           */
+                                    /* ([0] = 1 alpha_rop_enable)       */
+                                    /* ([1] = 1 rop enable)             */
+                                    /* ([2] = 1 fading_enable)          */
+                                    /* ([3] = 1 PD_enable)              */
+                                    /* ([4] = 1 alpha cal_mode_sel)     */
+                                    /* ([5] = 1 dither_enable)          */
+                                    /* ([6] = 1 gradient fill mode sel) */
+                                    /* ([7] = 1 AA_enable)              */
+    uint8_t  scale_mode;            /* 0 nearst / 1 bilnear / 2 bicubic */
+    uint32_t color_key_max;         /* color key max */
+    uint32_t color_key_min;         /* color key min */
+    uint32_t fg_color;              /* foreground color */
+    uint32_t bg_color;              /* background color */
+    COLOR_FILL gr_color;            /* color fill use gradient */
+    line_draw_t line_draw_info;
+    FADING fading;
+    uint8_t PD_mode;                /* porter duff alpha mode sel */
+    uint8_t alpha_global_value;     /* global alpha value */
+    uint16_t rop_code;              /* rop2/3/4 code  scan from rop code table*/
+    uint8_t bsfilter_flag;          /* [2] 0 blur 1 sharp / [1:0] filter_type*/
+    uint8_t palette_mode;           /* (enum) color palatte  0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+    uint8_t yuv2rgb_mode;           /* (enum) BT.601 MPEG / BT.601 JPEG / BT.709  */
+    uint8_t endian_mode;            /* 0/big endian 1/little endian*/
+    uint8_t rotate_mode;            /* (enum) rotate mode  */
+                                    /* 0x0,     no rotate  */
+                                    /* 0x1,     rotate     */
+                                    /* 0x2,     x_mirror   */
+                                    /* 0x3,     y_mirror   */
+    uint8_t color_fill_mode;        /* 0 solid color / 1 patten color */
+    MMU_32 mmu_info;                   /* mmu information */
+    uint8_t  alpha_rop_mode;        /* ([0~1] alpha mode)            */
+                                    /* ([2~3] rop   mode)            */
+                                    /* ([4]   zero  mode en)         */
+                                    /* ([5]   dst   alpha mode)      */
+                                    /* ([6]   alpha output mode sel) 0 src / 1 dst*/
+    uint8_t  src_trans_mode;
+
+    uint8_t dither_mode;
+
+    full_csc_t full_csc;            /* full color space convert */
+};
+
+
+
+struct rga2_req
+{
+    u8 render_mode;          /* (enum) process mode sel */
+
+    rga_img_info_t src;    // src  active window
+    rga_img_info_t src1;   // src1 active window
+    rga_img_info_t dst;    // dst  active window
+    rga_img_info_t pat;    // patten active window
+
+    unsigned long rop_mask_addr;       // rop4 mask addr
+    unsigned long LUT_addr;            // LUT addr
+
+    u32 rop_mask_stride;
+
+    u8 bitblt_mode;          /* 0: SRC + DST  => DST     */
+                             /* 1: SRC + SRC1 => DST     */
+
+    u8 rotate_mode;          /* [1:0]                           */
+                             /* 0   degree 0x0                  */
+                             /* 90  degree 0x1                  */
+                             /* 180 degree 0x2                  */
+                             /* 270 degree 0x3                  */
+                             /* [5:4]                           */
+                             /* none                0x0         */
+                             /* x_mirror            0x1         */
+                             /* y_mirror            0x2         */
+                             /* x_mirror + y_mirror 0x3         */
+
+    u16 alpha_rop_flag;         /* alpha rop process flag           */
+                                /* ([0] = 1 alpha_rop_enable)       */
+                                /* ([1] = 1 rop enable)             */
+                                /* ([2] = 1 fading_enable)          */
+                                /* ([3] = 1 alpha cal_mode_sel)     */
+                                /* ([4] = 1 src_dither_up_enable)   */
+                                /* ([5] = 1 dst_dither_up_enable)   */
+                                /* ([6] = 1 dither_down_enable)     */
+                                /* ([7] = 1 gradient fill mode sel) */
+
+
+    u16 alpha_mode_0;           /* [0]     SrcAlphaMode0          */
+                                /* [2:1]   SrcGlobalAlphaMode0    */
+                                /* [3]     SrcAlphaSelectMode0    */
+                                /* [6:4]   SrcFactorMode0         */
+                                /* [7]     SrcColorMode           */
+
+                                /* [8]     DstAlphaMode0          */
+                                /* [10:9]  DstGlobalAlphaMode0    */
+                                /* [11]    DstAlphaSelectMode0    */
+                                /* [14:12] DstFactorMode0         */
+                                /* [15]    DstColorMode0          */
+
+    u16 alpha_mode_1;           /* [0]     SrcAlphaMode1          */
+                                /* [2:1]   SrcGlobalAlphaMode1    */
+                                /* [3]     SrcAlphaSelectMode1    */
+                                /* [6:4]   SrcFactorMode1         */
+
+                                /* [8]     DstAlphaMode1          */
+                                /* [10:9]  DstGlobalAlphaMode1    */
+                                /* [11]    DstAlphaSelectMode1    */
+                                /* [14:12] DstFactorMode1         */
+
+    u8  scale_bicu_mode;    /* 0   1   2  3 */
+
+    u32 color_key_max;      /* color key max */
+    u32 color_key_min;      /* color key min */
+
+    u32 fg_color;           /* foreground color */
+    u32 bg_color;           /* background color */
+
+    u8 color_fill_mode;
+    COLOR_FILL gr_color;    /* color fill use gradient */
+
+    u8 fading_alpha_value;  /* Fading value */
+    u8 fading_r_value;
+    u8 fading_g_value;
+    u8 fading_b_value;
+
+    u8 src_a_global_val;    /* src global alpha value        */
+    u8 dst_a_global_val;    /* dst global alpha value        */
+
+    u8  rop_mode;	    /* rop mode select 0 : rop2 1 : rop3 2 : rop4 */
+    u16 rop_code;           /* rop2/3/4 code */
+
+    u8 palette_mode;        /* (enum) color palatte  0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+
+    u8 yuv2rgb_mode;        /* (enum) BT.601 MPEG / BT.601 JPEG / BT.709  */
+                            /* [1:0]   src0 csc mode        */
+                            /* [3:2]   dst csc mode         */
+                            /* [4]     dst csc clip enable  */
+                            /* [6:5]   src1 csc mdoe        */
+                            /* [7]     src1 csc clip enable */
+    full_csc_t full_csc;    /* full color space convert */
+
+    u8 endian_mode;         /* 0/little endian 1/big endian */
+
+    u8 CMD_fin_int_enable;
+
+    MMU_INFO mmu_info;               /* mmu infomation */
+
+    u8 alpha_zero_key;
+    u8 src_trans_mode;
+
+    u8 alpha_swp;           /* not use */
+    u8 dither_mode;
+
+    u8 rgb2yuv_mode;
+
+	u8 buf_type;
+};
+
+struct rga2_mmu_buf_t {
+    int32_t front;
+    int32_t back;
+    int32_t size;
+    int32_t curr;
+    unsigned int *buf;
+    unsigned int *buf_virtual;
+
+    struct page **pages;
+
+    u8 buf_order;
+    u8 pages_order;
+};
+
+enum
+{
+    BB_ROTATE_OFF   = 0x0,     /* no rotate  */
+    BB_ROTATE_90    = 0x1,     /* rotate 90  */
+    BB_ROTATE_180   = 0x2,     /* rotate 180 */
+    BB_ROTATE_270   = 0x3,     /* rotate 270 */
+};  /*rotate mode*/
+
+enum
+{
+    BB_MIRROR_OFF   = (0x0 << 4),     /* no mirror  */
+    BB_MIRROR_X     = (0x1 << 4),     /* x  mirror  */
+    BB_MIRROR_Y     = (0x2 << 4),     /* y  mirror  */
+    BB_MIRROR_XY    = (0x3 << 4),     /* xy mirror  */
+};  /*mirror mode*/
+
+enum
+{
+    BB_COPY_USE_TILE = (0x1 << 6),    /* bitblt mode copy but use Tile mode */
+};
+
+enum
+{
+	//BYPASS        = 0x0,
+    BT_601_RANGE0   = 0x1,
+    BT_601_RANGE1   = 0x2,
+    BT_709_RANGE0   = 0x3,
+}; /*yuv2rgb_mode*/
+
+enum
+{
+    BPP1        = 0x0,     /* BPP1 */
+    BPP2        = 0x1,     /* BPP2 */
+    BPP4        = 0x2,     /* BPP4 */
+    BPP8        = 0x3      /* BPP8 */
+}; /*palette_mode*/
+
+enum
+{
+	SOLID_COLOR   = 0x0, //color fill mode; ROP4: SOLID_rop4_mask_addr COLOR
+	PATTERN_COLOR = 0x1  //pattern_fill_mode;ROP4:PATTERN_COLOR
+};  /*color fill mode*/
+
+enum
+{
+	COLOR_FILL_CLIP     = 0x0,
+	COLOR_FILL_NOT_CLIP = 0x1
+};
+
+enum
+{
+    CATROM    = 0x0,
+    MITCHELL  = 0x1,
+    HERMITE   = 0x2,
+    B_SPLINE  = 0x3,
+};  /*bicubic coefficient*/
+
+enum
+{
+	ROP2 = 0x0,
+	ROP3 = 0x1,
+	ROP4 = 0x2
+};  /*ROP mode*/
+
+enum
+{
+	BIG_ENDIAN    = 0x0,
+	LITTLE_ENDIAN = 0x1
+};  /*endian mode*/
+
+enum
+{
+	MMU_TABLE_4KB  = 0x0,
+	MMU_TABLE_64KB = 0x1,
+};  /*MMU table size*/
+
+enum
+{
+    RGB_2_666 = 0x0,
+    RGB_2_565 = 0x1,
+    RGB_2_555 = 0x2,
+    RGB_2_444 = 0x3,
+};  /*dither down mode*/
+
+
+
+/**
+ * struct for process session which connect to rga
+ *
+ * @author ZhangShengqin (2012-2-15)
+ */
+typedef struct rga2_session {
+	/* a linked list of data so we can access them for debugging */
+	struct list_head    list_session;
+	/* a linked list of register data waiting for process */
+	struct list_head    waiting;
+	/* a linked list of register data in processing */
+	struct list_head    running;
+	/* all coommand this thread done */
+    atomic_t            done;
+	wait_queue_head_t   wait;
+	pid_t           pid;
+	atomic_t        task_running;
+    atomic_t        num_done;
+} rga2_session;
+
+struct rga2_reg {
+	rga2_session		*session;
+	struct list_head	session_link;
+	struct list_head	status_link;
+	uint32_t  sys_reg[8];
+	uint32_t  csc_reg[12];
+	uint32_t  cmd_reg[32];
+
+	uint32_t *MMU_src0_base;
+	uint32_t *MMU_src1_base;
+	uint32_t *MMU_dst_base;
+	uint32_t MMU_src0_count;
+	uint32_t MMU_src1_count;
+	uint32_t MMU_dst_count;
+
+	uint32_t MMU_len;
+	bool MMU_map;
+
+	struct rga_dma_buffer_t dma_buffer_src0;
+	struct rga_dma_buffer_t dma_buffer_src1;
+	struct rga_dma_buffer_t dma_buffer_dst;
+	struct rga_dma_buffer_t dma_buffer_els;
+};
+
+struct rga2_service_info {
+    struct mutex	lock;
+    struct timer_list	timer;			/* timer for power off */
+    struct list_head	waiting;		/* link to link_reg in struct vpu_reg */
+    struct list_head	running;		/* link to link_reg in struct vpu_reg */
+    struct list_head	done;			/* link to link_reg in struct vpu_reg */
+    struct list_head	session;		/* link to list_session in struct vpu_session */
+    atomic_t		total_running;
+
+    struct rga2_reg        *reg;
+
+    uint32_t            cmd_buff[32*8];/* cmd_buff for rga */
+    uint32_t            *pre_scale_buf;
+    atomic_t            int_disable;     /* 0 int enable 1 int disable  */
+    atomic_t            cmd_num;
+    atomic_t            src_format_swt;
+    int                 last_prc_src_format;
+    atomic_t            rga_working;
+    bool                enable;
+    uint32_t            dev_mode;
+
+    //struct rga_req      req[10];
+
+    struct mutex	mutex;	// mutex
+};
+
+#define RGA2_TEST_CASE 0
+
+//General Registers
+#define RGA2_SYS_CTRL             0x000
+#define RGA2_CMD_CTRL             0x004
+#define RGA2_CMD_BASE             0x008
+#define RGA2_STATUS               0x00c
+#define RGA2_INT                  0x010
+#define RGA2_MMU_CTRL0            0x018
+#define RGA2_MMU_CMD_BASE         0x01c
+
+//Full Csc Coefficient
+#define RGA2_CSC_COE_BASE         0x60
+
+//Command code start
+#define RGA2_MODE_CTRL            0x100
+#define RGA_BLIT_COMPLETE_EVENT 1
+
+#endif /*_RK29_IPP_DRIVER_H_*/
diff --git a/drivers/video/rockchip/rga2/rga2_debugger.c b/drivers/video/rockchip/rga2/rga2_debugger.c
new file mode 100644
index 0000000000000..66ba484df9a98
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_debugger.c
@@ -0,0 +1,395 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Author: Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/syscalls.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include "rga2.h"
+#include "RGA2_API.h"
+#include "rga2_mmu_info.h"
+#include "rga2_debugger.h"
+
+#define RGA_DEBUGGER_ROOT_NAME  "rkrga"
+
+#define STR_ENABLE(en) (en ? "EN" : "DIS")
+
+extern struct rga2_drvdata_t *rga2_drvdata;
+
+void rga2_slt(void);
+
+int RGA2_TEST_REG;
+int RGA2_TEST_MSG;
+int RGA2_TEST_TIME;
+int RGA2_CHECK_MODE;
+int RGA2_NONUSE;
+int RGA2_INT_FLAG;
+
+static int rga_debug_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "REG   [%s]\n"
+		      "MSG   [%s]\n"
+		      "TIME  [%s]\n"
+		      "INT   [%s]\n"
+		      "CHECK [%s]\n"
+		      "STOP  [%s]\n",
+		   STR_ENABLE(RGA2_TEST_REG), STR_ENABLE(RGA2_TEST_MSG),
+		   STR_ENABLE(RGA2_TEST_TIME), STR_ENABLE(RGA2_CHECK_MODE),
+		   STR_ENABLE(RGA2_NONUSE), STR_ENABLE(RGA2_INT_FLAG));
+
+	seq_puts(m, "\nhelp:\n");
+	seq_puts(m, "  'echo reg   > debug' to enable/disable register log printing.\n");
+	seq_puts(m, "  'echo msg   > debug' to enable/disable message log printing.\n");
+	seq_puts(m, "  'echo time  > debug' to enable/disable time log printing.\n");
+	seq_puts(m, "  'echo int   > debug' to enable/disable interruppt log printing.\n");
+	seq_puts(m, "  'echo check > debug' to enable/disable check mode.\n");
+	seq_puts(m, "  'echo stop  > debug' to enable/disable stop using hardware\n");
+
+	return 0;
+}
+
+static ssize_t rga_debug_write(struct file *file, const char __user *ubuf,
+			      size_t len, loff_t *offp)
+{
+	char buf[14];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strncmp(buf, "reg", 4) == 0) {
+		if (RGA2_TEST_REG) {
+			RGA2_TEST_REG = 0;
+			INFO("close rga2 reg!\n");
+		} else {
+			RGA2_TEST_REG = 1;
+			INFO("open rga2 reg!\n");
+		}
+	} else if (strncmp(buf, "msg", 3) == 0) {
+		if (RGA2_TEST_MSG) {
+			RGA2_TEST_MSG = 0;
+			INFO("close rga2 test MSG!\n");
+		} else {
+			RGA2_TEST_MSG = 1;
+			INFO("open rga2 test MSG!\n");
+		}
+	} else if (strncmp(buf, "time", 4) == 0) {
+		if (RGA2_TEST_TIME) {
+			RGA2_TEST_TIME = 0;
+			INFO("close rga2 test time!\n");
+		} else {
+			RGA2_TEST_TIME = 1;
+			INFO("open rga2 test time!\n");
+		}
+	} else if (strncmp(buf, "check", 5) == 0) {
+		if (RGA2_CHECK_MODE) {
+			RGA2_CHECK_MODE = 0;
+			INFO("close rga2 check flag!\n");
+		} else {
+			RGA2_CHECK_MODE = 1;
+			INFO("open rga2 check flag!\n");
+		}
+	} else if (strncmp(buf, "stop", 4) == 0) {
+		if (RGA2_NONUSE) {
+			RGA2_NONUSE = 0;
+			INFO("stop using rga hardware!\n");
+		} else {
+			RGA2_NONUSE = 1;
+			INFO("use rga hardware!\n");
+		}
+	} else if (strncmp(buf, "int", 3) == 0) {
+		if (RGA2_INT_FLAG) {
+			RGA2_INT_FLAG = 0;
+			INFO("close inturrupt MSG!\n");
+		} else {
+			RGA2_INT_FLAG = 1;
+			INFO("open inturrupt MSG!\n");
+		}
+	} else if (strncmp(buf, "slt", 3) == 0) {
+		rga2_slt();
+	}
+
+	return len;
+}
+
+static int rga_version_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "%s: v%s\n", DRIVER_DESC, DRIVER_VERSION);
+
+	return 0;
+}
+
+struct rga_debugger_list rga_root_list[] = {
+	{ "debug", rga_debug_show, rga_debug_write, NULL },
+	{ "driver_version", rga_version_show, NULL, NULL },
+};
+
+static ssize_t rga_debugger_write(struct file *file, const char __user *ubuf,
+				  size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rga_debugger_node *node = priv->private;
+
+	if (node->info_ent->write)
+		return node->info_ent->write(file, ubuf, len, offp);
+	else
+		return len;
+}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS
+static int rga_debugfs_open(struct inode *inode, struct file *file)
+{
+	struct rga_debugger_node *node = inode->i_private;
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct file_operations rga_debugfs_fops = {
+	.owner	 = THIS_MODULE,
+	.open	 = rga_debugfs_open,
+	.read	 = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write	 = rga_debugger_write,
+};
+
+static int rga_debugfs_remove_files(struct rga_debugger *debugger)
+{
+	struct rga_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->debugfs_lock);
+
+	/* Delete debugfs entry list */
+	entry_list = &debugger->debugfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->dent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all debugfs node in this directory */
+	debugfs_remove_recursive(debugger->debugfs_dir);
+	debugger->debugfs_dir = NULL;
+
+	mutex_unlock(&debugger->debugfs_lock);
+
+	return 0;
+}
+
+static int rga_debugfs_create_files(const struct rga_debugger_list *files, int count,
+			     struct dentry *root, struct rga_debugger *debugger)
+{
+	int i;
+	struct dentry *ent;
+	struct rga_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rga_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			ERR("Cannot alloc rga_debugger_node for /sys/kernel/debug/%pd/%s\n",
+			    root, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = debugfs_create_file(files[i].name, S_IFREG | S_IRUGO,
+					  root, tmp, &rga_debugfs_fops);
+		if (!ent) {
+			ERR("Cannot create /sys/kernel/debug/%pd/%s\n", root, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->dent = ent;
+
+		mutex_lock(&debugger->debugfs_lock);
+		list_add_tail(&tmp->list, &debugger->debugfs_entry_list);
+		mutex_unlock(&debugger->debugfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rga_debugfs_remove_files(debugger);
+
+	return -1;
+}
+
+int rga2_debugfs_remove(void)
+{
+	struct rga_debugger *debugger;
+
+	debugger = rga2_drvdata->debugger;
+
+	rga_debugfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rga2_debugfs_init(void)
+{
+	int ret;
+	struct rga_debugger *debugger;
+
+	debugger = rga2_drvdata->debugger;
+
+	debugger->debugfs_dir = debugfs_create_dir(RGA_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->debugfs_dir)) {
+		ERR("failed on mkdir /sys/kernel/debug/%s\n", RGA_DEBUGGER_ROOT_NAME);
+		debugger->debugfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rga_debugfs_create_files(rga_root_list, ARRAY_SIZE(rga_root_list),
+				       debugger->debugfs_dir, debugger);
+	if (ret) {
+		ERR("Could not install rga_root_list debugfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rga2_debugfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS
+static int rga_procfs_open(struct inode *inode, struct file *file)
+{
+	struct rga_debugger_node *node = pde_data(inode);
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct proc_ops rga_procfs_fops = {
+	.proc_open = rga_procfs_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = rga_debugger_write,
+};
+
+static int rga_procfs_remove_files(struct rga_debugger *debugger)
+{
+	struct rga_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->procfs_lock);
+
+	/* Delete procfs entry list */
+	entry_list = &debugger->procfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->pent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all procfs node in this directory */
+	proc_remove(debugger->procfs_dir);
+	debugger->procfs_dir = NULL;
+
+	mutex_unlock(&debugger->procfs_lock);
+
+	return 0;
+}
+
+static int rga_procfs_create_files(const struct rga_debugger_list *files, int count,
+			    struct proc_dir_entry *root, struct rga_debugger *debugger)
+{
+	int i;
+	struct proc_dir_entry *ent;
+	struct rga_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rga_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			ERR("Cannot alloc rga_debugger_node for /proc/%s/%s\n",
+			    RGA_DEBUGGER_ROOT_NAME, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = proc_create_data(files[i].name, S_IFREG | S_IRUGO,
+				       root, &rga_procfs_fops, tmp);
+		if (!ent) {
+			ERR("Cannot create /proc/%s/%s\n", RGA_DEBUGGER_ROOT_NAME, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->pent = ent;
+
+		mutex_lock(&debugger->procfs_lock);
+		list_add_tail(&tmp->list, &debugger->procfs_entry_list);
+		mutex_unlock(&debugger->procfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rga_procfs_remove_files(debugger);
+	return -1;
+}
+
+int rga2_procfs_remove(void)
+{
+	struct rga_debugger *debugger;
+
+	debugger = rga2_drvdata->debugger;
+
+	rga_procfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rga2_procfs_init(void)
+{
+	int ret;
+	struct rga_debugger *debugger;
+
+	debugger = rga2_drvdata->debugger;
+
+	debugger->procfs_dir = proc_mkdir(RGA_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->procfs_dir)) {
+		ERR("failed on mkdir /proc/%s\n", RGA_DEBUGGER_ROOT_NAME);
+		debugger->procfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rga_procfs_create_files(rga_root_list, ARRAY_SIZE(rga_root_list),
+				      debugger->procfs_dir, debugger);
+	if (ret) {
+		ERR("Could not install rga_root_list procfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rga2_procfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS */
diff --git a/drivers/video/rockchip/rga2/rga2_debugger.h b/drivers/video/rockchip/rga2/rga2_debugger.h
new file mode 100644
index 0000000000000..33b2f43e9f8a8
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_debugger.h
@@ -0,0 +1,120 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Author: Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#ifndef _RGA_DEBUGGER_H_
+#define _RGA_DEBUGGER_H_
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+extern int RGA2_TEST_REG;
+extern int RGA2_TEST_MSG;
+extern int RGA2_TEST_TIME;
+extern int RGA2_CHECK_MODE;
+extern int RGA2_NONUSE;
+extern int RGA2_INT_FLAG;
+
+/*
+ * struct rga_debugger - RGA debugger information
+ *
+ * This structure represents a debugger  to be created by the rga driver
+ * or core.
+ */
+struct rga_debugger {
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS
+	/* Directory of debugfs file */
+	struct dentry *debugfs_dir;
+	struct list_head debugfs_entry_list;
+	struct mutex debugfs_lock;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS
+	/* Directory of procfs file */
+	struct proc_dir_entry *procfs_dir;
+	struct list_head procfs_entry_list;
+	struct mutex procfs_lock;
+#endif
+};
+
+/*
+ * struct rga_debugger_list - debugfs/procfs info list entry
+ *
+ * This structure represents a debugfs/procfs file to be created by the rga
+ * driver or core.
+ */
+struct rga_debugger_list {
+	/* File name */
+	const char *name;
+	/*
+	 * Show callback. &seq_file->private will be set to the &struct
+	 * rga_debugger_node corresponding to the instance of this info on a given
+	 * &struct rga_debugger.
+	 */
+	int (*show)(struct seq_file *seq, void *data);
+	/*
+	 * Write callback. &seq_file->private will be set to the &struct
+	 * rga_debugger_node corresponding to the instance of this info on a given
+	 * &struct rga_debugger.
+	 */
+	ssize_t (*write)(struct file *file, const char __user *ubuf, size_t len, loff_t *offp);
+	/* Procfs/Debugfs private data. */
+	void *data;
+};
+
+/*
+ * struct rga_debugger_node - Nodes for debugfs/procfs
+ *
+ * This structure represents each instance of procfs/debugfs created from the
+ * template.
+ */
+struct rga_debugger_node {
+	struct rga_debugger *debugger;
+
+	/* template for this node. */
+	const struct rga_debugger_list *info_ent;
+
+	/* Each Procfs/Debugfs file. */
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS
+	struct dentry *dent;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS
+	struct proc_dir_entry *pent;
+#endif
+
+	struct list_head list;
+};
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS
+int rga2_debugfs_init(void);
+int rga2_debugfs_remove(void);
+#else
+static inline int rga2_debugfs_remove(void)
+{
+	return 0;
+}
+static inline int rga2_debugfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS
+int rga2_procfs_remove(void);
+int rga2_procfs_init(void);
+#else
+static inline int rga2_procfs_remove(void)
+{
+	return 0;
+}
+static inline int rga2_procfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS */
+
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER */
+
+#endif /* #ifndef _RGA_DEBUGGER_H_ */
+
diff --git a/drivers/video/rockchip/rga2/rga2_drv.c b/drivers/video/rockchip/rga2/rga2_drv.c
new file mode 100644
index 0000000000000..87d185257b720
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_drv.c
@@ -0,0 +1,2277 @@
+/*
+ * Copyright (C) 2012 Rockchip Electronics Co., Ltd.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#define pr_fmt(fmt) "rga2: " fmt
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/err.h>
+#include <linux/clk.h>
+#include <asm/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/miscdevice.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/syscalls.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <asm/cacheflush.h>
+#include <linux/slab.h>
+#include <linux/fb.h>
+#include <linux/wakelock.h>
+#include <linux/scatterlist.h>
+#include <linux/version.h>
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+#include <linux/pm_runtime.h>
+#ifdef CONFIG_DMABUF_CACHE
+#include <linux/dma-buf-cache.h>
+#else
+#include <linux/dma-buf.h>
+#endif
+#endif
+
+#include "rga2.h"
+#include "rga2_reg_info.h"
+#include "rga2_mmu_info.h"
+#include "RGA2_API.h"
+#include "rga2_debugger.h"
+
+#if IS_ENABLED(CONFIG_ION_ROCKCHIP) && (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+#include <linux/rockchip_ion.h>
+#endif
+
+#if ((defined(CONFIG_RK_IOMMU) || defined(CONFIG_ROCKCHIP_IOMMU)) && defined(CONFIG_ION_ROCKCHIP))
+#define CONFIG_RGA_IOMMU
+#endif
+
+#define RGA2_TEST_FLUSH_TIME 0
+#define RGA2_INFO_BUS_ERROR 1
+#define RGA2_POWER_OFF_DELAY	4*HZ /* 4s */
+#define RGA2_TIMEOUT_DELAY	(HZ / 2) /* 500ms */
+#define RGA2_MAJOR		255
+#define RGA2_RESET_TIMEOUT	1000
+/*
+ * The maximum input is 8192*8192, the maximum output is 4096*4096
+ * The size of physical pages requested is:
+ * ( ( maximum_input_value * maximum_input_value * format_bpp ) / 4K_page_size ) + 1
+ */
+#define RGA2_PHY_PAGE_SIZE	(((8192 * 8192 * 4) / 4096) + 1)
+
+ktime_t rga2_start;
+int rga2_flag;
+int first_RGA2_proc;
+static int rk3368;
+
+rga2_session rga2_session_global;
+long (*rga2_ioctl_kernel_p)(struct rga_req *);
+
+struct rga2_drvdata_t *rga2_drvdata;
+struct rga2_service_info rga2_service;
+struct rga2_mmu_buf_t rga2_mmu_buf;
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+extern struct ion_client *rockchip_ion_client_create(const char *name);
+#endif
+
+static int rga2_blit_async(rga2_session *session, struct rga2_req *req);
+static void rga2_del_running_list(void);
+static void rga2_del_running_list_timeout(void);
+static void rga2_try_set_reg(void);
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+static const char *rga2_get_cmd_mode_str(u32 cmd)
+{
+	switch (cmd) {
+	/* RGA1 */
+	case RGA_BLIT_SYNC:
+		return "RGA_BLIT_SYNC";
+	case RGA_BLIT_ASYNC:
+		return "RGA_BLIT_ASYNC";
+	case RGA_FLUSH:
+		return "RGA_FLUSH";
+	case RGA_GET_RESULT:
+		return "RGA_GET_RESULT";
+	case RGA_GET_VERSION:
+		return "RGA_GET_VERSION";
+	/* RGA2 */
+	case RGA2_BLIT_SYNC:
+		return "RGA2_BLIT_SYNC";
+	case RGA2_BLIT_ASYNC:
+		return "RGA2_BLIT_ASYNC";
+	case RGA2_FLUSH:
+		return "RGA2_FLUSH";
+	case RGA2_GET_RESULT:
+		return "RGA2_GET_RESULT";
+	case RGA2_GET_VERSION:
+		return "RGA2_GET_VERSION";
+	default:
+		return "UNF";
+	}
+}
+
+static const char *rga2_get_blend_mode_str(u16 alpha_rop_flag, u16 alpha_mode_0,
+					   u16 alpha_mode_1)
+{
+	if (alpha_rop_flag == 0) {
+		return "no blend";
+	} else if (alpha_rop_flag == 0x9) {
+		if (alpha_mode_0 == 0x381A  && alpha_mode_1 == 0x381A)
+			return "105 src + (1-src.a)*dst";
+		else if (alpha_mode_0 == 0x483A  && alpha_mode_1 == 0x483A)
+			return "405 src.a * src + (1-src.a) * dst";
+		else
+			return "check reg for more imformation";
+	} else {
+		return "check reg for more imformation";
+	}
+}
+
+static const char *rga2_get_render_mode_str(u8 mode)
+{
+	switch (mode) {
+	case 0x0:
+		return "bitblt";
+	case 0x1:
+		return "color_palette";
+	case 0x2:
+		return "color_fill";
+	case 0x3:
+		return "update_palette_table";
+	case 0x4:
+		return "update_patten_buff";
+	default:
+		return "UNF";
+	}
+}
+
+static const char *rga2_get_rotate_mode_str(u8 mode)
+{
+	switch (mode) {
+	case 0x0:
+		return "0";
+	case 0x1:
+		return "90 degree";
+	case 0x2:
+		return "180 degree";
+	case 0x3:
+		return "270 degree";
+	case 0x10:
+		return "xmirror";
+	case 0x20:
+		return "ymirror";
+	case 0x30:
+		return "xymirror";
+	default:
+		return "UNF";
+	}
+}
+
+static bool rga2_is_yuv10bit_format(uint32_t format)
+{
+	bool ret  = false;
+
+	switch (format) {
+	case RGA2_FORMAT_YCbCr_420_SP_10B:
+	case RGA2_FORMAT_YCrCb_420_SP_10B:
+	case RGA2_FORMAT_YCbCr_422_SP_10B:
+	case RGA2_FORMAT_YCrCb_422_SP_10B:
+		ret = true;
+		break;
+	}
+	return ret;
+}
+
+static bool rga2_is_yuv8bit_format(uint32_t format)
+{
+	bool ret  = false;
+
+	switch (format) {
+	case RGA2_FORMAT_YCbCr_422_SP:
+	case RGA2_FORMAT_YCbCr_422_P:
+	case RGA2_FORMAT_YCbCr_420_SP:
+	case RGA2_FORMAT_YCbCr_420_P:
+	case RGA2_FORMAT_YCrCb_422_SP:
+	case RGA2_FORMAT_YCrCb_422_P:
+	case RGA2_FORMAT_YCrCb_420_SP:
+	case RGA2_FORMAT_YCrCb_420_P:
+		ret = true;
+		break;
+	}
+	return ret;
+}
+
+static const char *rga2_get_format_name(uint32_t format)
+{
+	switch (format) {
+	case RGA2_FORMAT_RGBA_8888:
+		return "RGBA8888";
+	case RGA2_FORMAT_RGBX_8888:
+		return "RGBX8888";
+	case RGA2_FORMAT_RGB_888:
+		return "RGB888";
+	case RGA2_FORMAT_BGRA_8888:
+		return "BGRA8888";
+	case RGA2_FORMAT_BGRX_8888:
+		return "BGRX8888";
+	case RGA2_FORMAT_BGR_888:
+		return "BGR888";
+	case RGA2_FORMAT_RGB_565:
+		return "RGB565";
+	case RGA2_FORMAT_RGBA_5551:
+		return "RGBA5551";
+	case RGA2_FORMAT_RGBA_4444:
+		return "RGBA4444";
+	case RGA2_FORMAT_BGR_565:
+		return "BGR565";
+	case RGA2_FORMAT_BGRA_5551:
+		return "BGRA5551";
+	case RGA2_FORMAT_BGRA_4444:
+		return "BGRA4444";
+
+	case RGA2_FORMAT_ARGB_8888:
+		return "ARGB8888";
+	case RGA2_FORMAT_XRGB_8888:
+		return "XBGR8888";
+	case RGA2_FORMAT_ARGB_5551:
+		return "ARGB5551";
+	case RGA2_FORMAT_ARGB_4444:
+		return "ARGB4444";
+	case RGA2_FORMAT_ABGR_8888:
+		return "ABGR8888";
+	case RGA2_FORMAT_XBGR_8888:
+		return "XBGR8888";
+	case RGA2_FORMAT_ABGR_5551:
+		return "ABGR5551";
+	case RGA2_FORMAT_ABGR_4444:
+		return "ABGR4444";
+
+	case RGA2_FORMAT_YCbCr_422_SP:
+		return "YCbCr422SP";
+	case RGA2_FORMAT_YCbCr_422_P:
+		return "YCbCr422P";
+	case RGA2_FORMAT_YCbCr_420_SP:
+		return "YCbCr420SP";
+	case RGA2_FORMAT_YCbCr_420_P:
+		return "YCbCr420P";
+	case RGA2_FORMAT_YCrCb_422_SP:
+		return "YCrCb422SP";
+	case RGA2_FORMAT_YCrCb_422_P:
+		return "YCrCb422P";
+	case RGA2_FORMAT_YCrCb_420_SP:
+		return "YCrCb420SP";
+	case RGA2_FORMAT_YCrCb_420_P:
+		return "YCrCb420P";
+
+	case RGA2_FORMAT_YVYU_422:
+		return "YVYU422";
+	case RGA2_FORMAT_YVYU_420:
+		return "YVYU420";
+	case RGA2_FORMAT_VYUY_422:
+		return "VYUY422";
+	case RGA2_FORMAT_VYUY_420:
+		return "VYUY420";
+	case RGA2_FORMAT_YUYV_422:
+		return "YUYV422";
+	case RGA2_FORMAT_YUYV_420:
+		return "YUYV420";
+	case RGA2_FORMAT_UYVY_422:
+		return "UYVY422";
+	case RGA2_FORMAT_UYVY_420:
+		return "UYVY420";
+
+	case RGA2_FORMAT_YCbCr_420_SP_10B:
+		return "YCrCb420SP10B";
+	case RGA2_FORMAT_YCrCb_420_SP_10B:
+		return "YCbCr420SP10B";
+	case RGA2_FORMAT_YCbCr_422_SP_10B:
+		return "YCbCr422SP10B";
+	case RGA2_FORMAT_YCrCb_422_SP_10B:
+		return "YCrCb422SP10B";
+	case RGA2_FORMAT_BPP_1:
+		return "BPP1";
+	case RGA2_FORMAT_BPP_2:
+		return "BPP2";
+	case RGA2_FORMAT_BPP_4:
+		return "BPP4";
+	case RGA2_FORMAT_BPP_8:
+		return "BPP8";
+	case RGA2_FORMAT_YCbCr_400:
+		return "YCbCr400";
+	case RGA2_FORMAT_Y4:
+		return "y4";
+	default:
+		return "UNF";
+	}
+}
+
+static void print_debug_info(struct rga2_req *req)
+{
+	INFO("render_mode:%s,bitblit_mode=%d,rotate_mode:%s\n",
+	     rga2_get_render_mode_str(req->render_mode), req->bitblt_mode,
+	     rga2_get_rotate_mode_str(req->rotate_mode));
+	INFO("src : y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d xoff=%d yoff=%d format=%s\n",
+	     req->src.yrgb_addr, req->src.uv_addr, req->src.v_addr,
+	     req->src.act_w, req->src.act_h, req->src.vir_w, req->src.vir_h,
+	     req->src.x_offset, req->src.y_offset,
+	     rga2_get_format_name(req->src.format));
+	if (req->src1.yrgb_addr != 0 ||
+	    req->src1.uv_addr != 0 ||
+	    req->src1.v_addr != 0) {
+		INFO("src1 : y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d xoff=%d yoff=%d format=%s\n",
+		     req->src1.yrgb_addr, req->src1.uv_addr, req->src1.v_addr,
+		     req->src1.act_w, req->src1.act_h, req->src1.vir_w, req->src1.vir_h,
+		     req->src1.x_offset, req->src1.y_offset,
+		     rga2_get_format_name(req->src1.format));
+	}
+	INFO("dst : y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d xoff=%d yoff=%d format=%s\n",
+	     req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+	     req->dst.act_w, req->dst.act_h, req->dst.vir_w, req->dst.vir_h,
+	     req->dst.x_offset, req->dst.y_offset,
+	     rga2_get_format_name(req->dst.format));
+	INFO("mmu : src=%.2x src1=%.2x dst=%.2x els=%.2x\n",
+	     req->mmu_info.src0_mmu_flag, req->mmu_info.src1_mmu_flag,
+	     req->mmu_info.dst_mmu_flag, req->mmu_info.els_mmu_flag);
+	INFO("alpha : flag %x mode0=%x mode1=%x\n",
+	     req->alpha_rop_flag, req->alpha_mode_0, req->alpha_mode_1);
+	INFO("blend mode is %s\n",
+	     rga2_get_blend_mode_str(req->alpha_rop_flag,
+	     req->alpha_mode_0, req->alpha_mode_1));
+	INFO("yuv2rgb mode is %x\n", req->yuv2rgb_mode);
+}
+
+static int rga2_align_check(struct rga2_req *req)
+{
+	if (rga2_is_yuv10bit_format(req->src.format))
+		if ((req->src.vir_w % 16) || (req->src.x_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_h % 2) || (req->src.vir_h % 2))
+			INFO("err src wstride is not align to 16 or yuv not align to 2");
+	if (rga2_is_yuv10bit_format(req->dst.format))
+		if ((req->dst.vir_w % 16) || (req->dst.x_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_h % 2) || (req->dst.vir_h % 2))
+			INFO("err dst wstride is not align to 16 or yuv not align to 2");
+	if (rga2_is_yuv8bit_format(req->src.format))
+		if ((req->src.vir_w % 8) || (req->src.x_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_h % 2) || (req->src.vir_h % 2))
+			INFO("err src wstride is not align to 8 or yuv not align to 2");
+	if (rga2_is_yuv8bit_format(req->dst.format))
+		if ((req->dst.vir_w % 8) || (req->dst.x_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_h % 2) || (req->dst.vir_h % 2))
+			INFO("err dst wstride is not align to 8 or yuv not align to 2");
+	INFO("rga align check over!\n");
+	return 0;
+}
+
+int rga2_scale_check(struct rga2_req *req)
+{
+	u32 saw, sah, daw, dah;
+	struct rga2_drvdata_t *data = rga2_drvdata;
+
+	saw = req->src.act_w;
+	sah = req->src.act_h;
+	daw = req->dst.act_w;
+	dah = req->dst.act_h;
+
+	if (strncmp(data->version, "2.20", 4) == 0) {
+		if (((saw >> 4) >= daw) || ((sah >> 4) >= dah))
+			INFO("unsupported to scaling less than 1/16 times.\n");
+		if (((daw >> 4) >= saw) || ((dah >> 4) >= sah))
+			INFO("unsupported to scaling more than 16 times.\n");
+	} else {
+		if (((saw >> 3) >= daw) || ((sah >> 3) >= dah))
+			INFO("unsupported to scaling less than 1/8 tiems.\n");
+		if (((daw >> 3) >= saw) || ((dah >> 3) >= sah))
+			INFO("unsupported to scaling more than 8 times.\n");
+	}
+	INFO("rga2 scale check over.\n");
+	return 0;
+}
+#endif
+
+static void rga2_printf_cmd_buf(u32 *cmd_buf)
+{
+	u32 reg_p[32];
+	u32 i = 0;
+	u32 src_stride, dst_stride, src_format, dst_format;
+	u32 src_aw, src_ah, dst_aw, dst_ah;
+
+	for (i = 0; i < 32; i++)
+		reg_p[i] = *(cmd_buf + i);
+
+	src_stride = reg_p[6];
+	dst_stride = reg_p[18];
+
+	src_format = reg_p[1] & (~0xfffffff0);
+	dst_format = reg_p[14] & (~0xfffffff0);
+
+	src_aw = (reg_p[7] & (~0xffff0000)) + 1;
+	src_ah = ((reg_p[7] & (~0x0000ffff)) >> 16) + 1;
+
+	dst_aw = (reg_p[19] & (~0xffff0000)) + 1;
+	dst_ah = ((reg_p[19] & (~0x0000ffff)) >> 16) + 1;
+
+	DBG("src : aw = %d ah = %d stride = %d format is %x\n",
+	     src_aw, src_ah, src_stride, src_format);
+	DBG("dst : aw = %d ah = %d stride = %d format is %x\n",
+	     dst_aw, dst_ah, dst_stride, dst_format);
+}
+
+static inline void rga2_write(u32 b, u32 r)
+{
+	*((volatile unsigned int *)(rga2_drvdata->rga_base + r)) = b;
+}
+
+static inline u32 rga2_read(u32 r)
+{
+	return *((volatile unsigned int *)(rga2_drvdata->rga_base + r));
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 1, 0))
+static inline int rga2_init_version(void)
+{
+	struct rga2_drvdata_t *rga = rga2_drvdata;
+	u32 major_version, minor_version, svn_version;
+	u32 reg_version;
+
+	if (!rga) {
+		pr_err("rga2_drvdata is null\n");
+		return -EINVAL;
+	}
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_get_sync(rga2_drvdata->dev);
+#endif
+
+	clk_prepare_enable(rga2_drvdata->aclk_rga2);
+	clk_prepare_enable(rga2_drvdata->hclk_rga2);
+
+	reg_version = rga2_read(0x028);
+
+	clk_disable_unprepare(rga2_drvdata->aclk_rga2);
+	clk_disable_unprepare(rga2_drvdata->hclk_rga2);
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_put(rga2_drvdata->dev);
+#endif
+
+	major_version = (reg_version & RGA2_MAJOR_VERSION_MASK) >> 24;
+	minor_version = (reg_version & RGA2_MINOR_VERSION_MASK) >> 20;
+	svn_version = (reg_version & RGA2_SVN_VERSION_MASK);
+
+	/*
+	 * some old rga ip has no rga version register, so force set to 2.00
+	 */
+	if (!major_version && !minor_version)
+		major_version = 2;
+	snprintf(rga->version, 10, "%x.%01x.%05x", major_version, minor_version, svn_version);
+
+	return 0;
+}
+#endif
+static void rga2_soft_reset(void)
+{
+	u32 i;
+	u32 reg;
+
+	rga2_write((1 << 3) | (1 << 4) | (1 << 6), RGA2_SYS_CTRL);
+
+	for(i = 0; i < RGA2_RESET_TIMEOUT; i++)
+	{
+		reg = rga2_read(RGA2_SYS_CTRL) & 1; //RGA_SYS_CTRL
+
+		if(reg == 0)
+			break;
+
+		udelay(1);
+	}
+
+	if(i == RGA2_RESET_TIMEOUT)
+		ERR("soft reset timeout.\n");
+}
+
+static void rga2_dump(void)
+{
+	int running;
+	struct rga2_reg *reg, *reg_tmp;
+	rga2_session *session, *session_tmp;
+
+	running = atomic_read(&rga2_service.total_running);
+	printk("rga total_running %d\n", running);
+	list_for_each_entry_safe(session, session_tmp, &rga2_service.session,
+		list_session)
+	{
+		printk("session pid %d:\n", session->pid);
+		running = atomic_read(&session->task_running);
+		printk("task_running %d\n", running);
+		list_for_each_entry_safe(reg, reg_tmp, &session->waiting, session_link)
+		{
+			printk("waiting register set 0x %.lu\n", (unsigned long)reg);
+		}
+		list_for_each_entry_safe(reg, reg_tmp, &session->running, session_link)
+		{
+			printk("running register set 0x %.lu\n", (unsigned long)reg);
+		}
+	}
+}
+
+static inline void rga2_queue_power_off_work(void)
+{
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	queue_delayed_work(system_wq, &rga2_drvdata->power_off_work,
+		RGA2_POWER_OFF_DELAY);
+#else
+	queue_delayed_work(system_nrt_wq, &rga2_drvdata->power_off_work,
+		RGA2_POWER_OFF_DELAY);
+#endif
+}
+
+/* Caller must hold rga_service.lock */
+static void rga2_power_on(void)
+{
+	static ktime_t last;
+	ktime_t now = ktime_get();
+
+	if (ktime_to_ns(ktime_sub(now, last)) > NSEC_PER_SEC) {
+		cancel_delayed_work_sync(&rga2_drvdata->power_off_work);
+		rga2_queue_power_off_work();
+		last = now;
+	}
+
+	if (rga2_service.enable)
+		return;
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_get_sync(rga2_drvdata->dev);
+#else
+	clk_prepare_enable(rga2_drvdata->pd_rga2);
+#endif
+	clk_prepare_enable(rga2_drvdata->clk_rga2);
+	clk_prepare_enable(rga2_drvdata->aclk_rga2);
+	clk_prepare_enable(rga2_drvdata->hclk_rga2);
+	wake_lock(&rga2_drvdata->wake_lock);
+	rga2_service.enable = true;
+}
+
+/* Caller must hold rga_service.lock */
+static void rga2_power_off(void)
+{
+	int total_running;
+
+	if (!rga2_service.enable) {
+		return;
+	}
+
+	total_running = atomic_read(&rga2_service.total_running);
+	if (total_running) {
+		pr_err("power off when %d task running!!\n", total_running);
+		mdelay(50);
+		pr_err("delay 50 ms for running task\n");
+		rga2_dump();
+	}
+
+	clk_disable_unprepare(rga2_drvdata->clk_rga2);
+	clk_disable_unprepare(rga2_drvdata->aclk_rga2);
+	clk_disable_unprepare(rga2_drvdata->hclk_rga2);
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_put(rga2_drvdata->dev);
+#else
+	clk_disable_unprepare(rga2_drvdata->pd_rga2);
+#endif
+
+	wake_unlock(&rga2_drvdata->wake_lock);
+    first_RGA2_proc = 0;
+	rga2_service.enable = false;
+}
+
+static void rga2_power_off_work(struct work_struct *work)
+{
+	if (mutex_trylock(&rga2_service.lock)) {
+		rga2_power_off();
+		mutex_unlock(&rga2_service.lock);
+	} else {
+		/* Come back later if the device is busy... */
+		rga2_queue_power_off_work();
+	}
+}
+
+static int rga2_flush(rga2_session *session, unsigned long arg)
+{
+	int ret = 0;
+	int ret_timeout;
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	ktime_t start = ktime_set(0, 0);
+	ktime_t end = ktime_set(0, 0);
+
+	if (RGA2_TEST_TIME)
+		start = ktime_get();
+#endif
+	ret_timeout = wait_event_timeout(session->wait, atomic_read(&session->done), RGA2_TIMEOUT_DELAY);
+
+	if (unlikely(ret_timeout < 0)) {
+		u32 i;
+		u32 *p;
+
+		p = rga2_service.cmd_buff;
+		pr_err("flush pid %d wait task ret %d\n", session->pid, ret);
+		pr_err("interrupt = %x status = %x\n", rga2_read(RGA2_INT),
+		       rga2_read(RGA2_STATUS));
+		rga2_printf_cmd_buf(p);
+		DBG("rga2 CMD\n");
+		for (i = 0; i < 7; i++)
+			DBG("%.8x %.8x %.8x %.8x\n",
+			     p[0 + i * 4], p[1 + i * 4],
+			     p[2 + i * 4], p[3 + i * 4]);
+		mutex_lock(&rga2_service.lock);
+		rga2_del_running_list();
+		mutex_unlock(&rga2_service.lock);
+		ret = ret_timeout;
+	} else if (0 == ret_timeout) {
+		u32 i;
+		u32 *p;
+
+		p = rga2_service.cmd_buff;
+		pr_err("flush pid %d wait %d task done timeout\n",
+		       session->pid, atomic_read(&session->task_running));
+		pr_err("interrupt = %x status = %x\n",
+		       rga2_read(RGA2_INT), rga2_read(RGA2_STATUS));
+		rga2_printf_cmd_buf(p);
+		DBG("rga2 CMD\n");
+		for (i = 0; i < 7; i++)
+			DBG("%.8x %.8x %.8x %.8x\n",
+			     p[0 + i * 4], p[1 + i * 4],
+			     p[2 + i * 4], p[3 + i * 4]);
+		mutex_lock(&rga2_service.lock);
+		rga2_del_running_list_timeout();
+		rga2_try_set_reg();
+		mutex_unlock(&rga2_service.lock);
+		ret = -ETIMEDOUT;
+	}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_TIME) {
+		end = ktime_get();
+		end = ktime_sub(end, start);
+		DBG("one flush wait time %d\n", (int)ktime_to_us(end));
+	}
+#endif
+	return ret;
+}
+
+
+static int rga2_get_result(rga2_session *session, unsigned long arg)
+{
+	int ret = 0;
+	int num_done;
+
+	num_done = atomic_read(&session->num_done);
+	if (unlikely(copy_to_user((void __user *)arg, &num_done, sizeof(int)))) {
+	    printk("copy_to_user failed\n");
+	    ret =  -EFAULT;
+	}
+	return ret;
+}
+
+
+static int rga2_check_param(const struct rga2_req *req)
+{
+	if(!((req->render_mode == color_fill_mode)))
+	{
+	    if (unlikely((req->src.act_w <= 0) || (req->src.act_w > 8192) || (req->src.act_h <= 0) || (req->src.act_h > 8192)))
+	    {
+		printk("invalid source resolution act_w = %d, act_h = %d\n", req->src.act_w, req->src.act_h);
+		return -EINVAL;
+	    }
+	}
+
+	if(!((req->render_mode == color_fill_mode)))
+	{
+	    if (unlikely((req->src.vir_w <= 0) || (req->src.vir_w > 8192) || (req->src.vir_h <= 0) || (req->src.vir_h > 8192)))
+	    {
+		printk("invalid source resolution vir_w = %d, vir_h = %d\n", req->src.vir_w, req->src.vir_h);
+		return -EINVAL;
+	    }
+	}
+
+	//check dst width and height
+	if (unlikely((req->dst.act_w <= 0) || (req->dst.act_w > 4096) || (req->dst.act_h <= 0) || (req->dst.act_h > 4096)))
+	{
+	    printk("invalid destination resolution act_w = %d, act_h = %d\n", req->dst.act_w, req->dst.act_h);
+	    return -EINVAL;
+	}
+
+	if (unlikely((req->dst.vir_w <= 0) || (req->dst.vir_w > 4096) || (req->dst.vir_h <= 0) || (req->dst.vir_h > 4096)))
+	{
+	    printk("invalid destination resolution vir_w = %d, vir_h = %d\n", req->dst.vir_w, req->dst.vir_h);
+	    return -EINVAL;
+	}
+
+	//check src_vir_w
+	if(unlikely(req->src.vir_w < req->src.act_w)){
+	    printk("invalid src_vir_w act_w = %d, vir_w = %d\n", req->src.act_w, req->src.vir_w);
+	    return -EINVAL;
+	}
+
+	//check dst_vir_w
+	if(unlikely(req->dst.vir_w < req->dst.act_w)){
+	    if(req->rotate_mode != 1)
+	    {
+		printk("invalid dst_vir_w act_h = %d, vir_h = %d\n", req->dst.act_w, req->dst.vir_w);
+		return -EINVAL;
+	    }
+	}
+
+	return 0;
+}
+
+static void rga2_copy_reg(struct rga2_reg *reg, uint32_t offset)
+{
+    uint32_t i;
+    uint32_t *cmd_buf;
+    uint32_t *reg_p;
+
+    if(atomic_read(&reg->session->task_running) != 0)
+        printk(KERN_ERR "task_running is no zero\n");
+
+    atomic_add(1, &rga2_service.cmd_num);
+	atomic_add(1, &reg->session->task_running);
+
+    cmd_buf = (uint32_t *)rga2_service.cmd_buff + offset*32;
+    reg_p = (uint32_t *)reg->cmd_reg;
+
+    for(i=0; i<32; i++)
+        cmd_buf[i] = reg_p[i];
+}
+
+
+static struct rga2_reg * rga2_reg_init(rga2_session *session, struct rga2_req *req)
+{
+    int32_t ret;
+
+	/* Alloc 4k size for rga2_reg use. */
+	struct rga2_reg *reg = (struct rga2_reg *)get_zeroed_page(GFP_KERNEL | GFP_DMA32);
+
+	if (NULL == reg) {
+		pr_err("get_zeroed_page fail in rga_reg_init\n");
+		return NULL;
+	}
+
+    reg->session = session;
+	INIT_LIST_HEAD(&reg->session_link);
+	INIT_LIST_HEAD(&reg->status_link);
+
+    ret = rga2_get_dma_info(reg, req);
+    if (ret < 0) {
+        pr_err("fail to get dma buffer info!\n");
+        free_page((unsigned long)reg);
+
+        return NULL;
+    }
+
+    if ((req->mmu_info.src0_mmu_flag & 1) || (req->mmu_info.src1_mmu_flag & 1)
+        || (req->mmu_info.dst_mmu_flag & 1) || (req->mmu_info.els_mmu_flag & 1))
+    {
+        ret = rga2_set_mmu_info(reg, req);
+        if(ret < 0) {
+            printk("%s, [%d] set mmu info error \n", __FUNCTION__, __LINE__);
+            free_page((unsigned long)reg);
+
+            return NULL;
+        }
+    }
+
+    if (RGA2_gen_reg_info((uint8_t *)reg->cmd_reg, (uint8_t *)reg->csc_reg, req) == -1) {
+        printk("gen reg info error\n");
+        free_page((unsigned long)reg);
+
+        return NULL;
+    }
+
+    mutex_lock(&rga2_service.lock);
+	list_add_tail(&reg->status_link, &rga2_service.waiting);
+	list_add_tail(&reg->session_link, &session->waiting);
+	mutex_unlock(&rga2_service.lock);
+
+    return reg;
+}
+
+
+/* Caller must hold rga_service.lock */
+static void rga2_reg_deinit(struct rga2_reg *reg)
+{
+	list_del_init(&reg->session_link);
+	list_del_init(&reg->status_link);
+	free_page((unsigned long)reg);
+}
+
+/* Caller must hold rga_service.lock */
+static void rga2_reg_from_wait_to_run(struct rga2_reg *reg)
+{
+	list_del_init(&reg->status_link);
+	list_add_tail(&reg->status_link, &rga2_service.running);
+
+	list_del_init(&reg->session_link);
+	list_add_tail(&reg->session_link, &reg->session->running);
+}
+
+/* Caller must hold rga_service.lock */
+static void rga2_service_session_clear(rga2_session *session)
+{
+	struct rga2_reg *reg, *n;
+
+	list_for_each_entry_safe(reg, n, &session->waiting, session_link)
+	{
+		rga2_reg_deinit(reg);
+	}
+
+	list_for_each_entry_safe(reg, n, &session->running, session_link)
+	{
+		rga2_reg_deinit(reg);
+	}
+}
+
+/* Caller must hold rga_service.lock */
+static void rga2_try_set_reg(void)
+{
+	int i;
+	struct rga2_reg *reg ;
+
+	if (list_empty(&rga2_service.running))
+	{
+		if (!list_empty(&rga2_service.waiting))
+		{
+			/* RGA is idle */
+			reg = list_entry(rga2_service.waiting.next, struct rga2_reg, status_link);
+
+			rga2_power_on();
+			udelay(1);
+
+			rga2_copy_reg(reg, 0);
+			rga2_reg_from_wait_to_run(reg);
+
+			rga2_dma_flush_range(&reg->cmd_reg[0], &reg->cmd_reg[32]);
+
+			//rga2_soft_reset();
+
+			rga2_write(0x0, RGA2_SYS_CTRL);
+
+			/* CMD buff */
+			rga2_write(virt_to_phys(reg->cmd_reg), RGA2_CMD_BASE);
+
+			/* full csc reg */
+			for (i = 0; i < 12; i++) {
+				rga2_write(reg->csc_reg[i], RGA2_CSC_COE_BASE + i * 4);
+			}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_TEST_REG) {
+				if (rga2_flag) {
+					int32_t *p;
+
+					p = rga2_service.cmd_buff;
+					INFO("CMD_REG\n");
+					for (i=0; i<8; i++)
+						INFO("%.8x %.8x %.8x %.8x\n",
+						     p[0 + i * 4], p[1 + i * 4],
+						     p[2 + i * 4], p[3 + i * 4]);
+
+					p = reg->csc_reg;
+					INFO("CSC_REG\n");
+					for (i = 0; i < 3; i++)
+						INFO("%.8x %.8x %.8x %.8x\n",
+						     p[0 + i * 4], p[1 + i * 4],
+						     p[2 + i * 4], p[3 + i * 4]);
+				}
+			}
+#endif
+
+			/* master mode */
+			rga2_write((0x1<<1)|(0x1<<2)|(0x1<<5)|(0x1<<6), RGA2_SYS_CTRL);
+
+			/* All CMD finish int */
+			rga2_write(rga2_read(RGA2_INT)|(0x1<<10)|(0x1<<9)|(0x1<<8), RGA2_INT);
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_TEST_TIME)
+				rga2_start = ktime_get();
+#endif
+
+			/* Start proc */
+			atomic_set(&reg->session->done, 0);
+			rga2_write(0x1, RGA2_CMD_CTRL);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_TEST_REG) {
+				if (rga2_flag) {
+					INFO("CMD_READ_BACK_REG\n");
+					for (i=0; i<8; i++)
+						INFO("%.8x %.8x %.8x %.8x\n",
+						     rga2_read(0x100 + i * 16 + 0),
+						     rga2_read(0x100 + i * 16 + 4),
+						     rga2_read(0x100 + i * 16 + 8),
+						     rga2_read(0x100 + i * 16 + 12));
+
+					INFO("CSC_READ_BACK_REG\n");
+					for (i = 0; i < 3; i++)
+						INFO("%.8x %.8x %.8x %.8x\n",
+						     rga2_read(RGA2_CSC_COE_BASE + i * 16 + 0),
+						     rga2_read(RGA2_CSC_COE_BASE + i * 16 + 4),
+						     rga2_read(RGA2_CSC_COE_BASE + i * 16 + 8),
+						     rga2_read(RGA2_CSC_COE_BASE + i * 16 + 12));
+				}
+
+			}
+#endif
+		}
+	}
+}
+
+static void rga2_del_running_list(void)
+{
+	struct rga2_mmu_buf_t *tbuf = &rga2_mmu_buf;
+	struct rga2_reg *reg;
+
+	while (!list_empty(&rga2_service.running)) {
+		reg = list_entry(rga2_service.running.next, struct rga2_reg,
+				 status_link);
+		if (reg->MMU_len && tbuf) {
+			if (tbuf->back + reg->MMU_len > 2 * tbuf->size)
+				tbuf->back = reg->MMU_len + tbuf->size;
+			else
+				tbuf->back += reg->MMU_len;
+		}
+		rga2_put_dma_info(reg);
+		atomic_sub(1, &reg->session->task_running);
+		atomic_sub(1, &rga2_service.total_running);
+
+		if(list_empty(&reg->session->waiting))
+		{
+			atomic_set(&reg->session->done, 1);
+			wake_up(&reg->session->wait);
+		}
+
+		rga2_reg_deinit(reg);
+	}
+}
+
+static void rga2_del_running_list_timeout(void)
+{
+	struct rga2_mmu_buf_t *tbuf = &rga2_mmu_buf;
+	struct rga2_reg *reg;
+
+	while (!list_empty(&rga2_service.running)) {
+		reg = list_entry(rga2_service.running.next, struct rga2_reg,
+				 status_link);
+#if 0
+		kfree(reg->MMU_base);
+#endif
+		if (reg->MMU_len && tbuf) {
+			if (tbuf->back + reg->MMU_len > 2 * tbuf->size)
+				tbuf->back = reg->MMU_len + tbuf->size;
+			else
+				tbuf->back += reg->MMU_len;
+		}
+		rga2_put_dma_info(reg);
+		atomic_sub(1, &reg->session->task_running);
+		atomic_sub(1, &rga2_service.total_running);
+		rga2_soft_reset();
+		if (list_empty(&reg->session->waiting)) {
+			atomic_set(&reg->session->done, 1);
+			wake_up(&reg->session->wait);
+		}
+		rga2_reg_deinit(reg);
+	}
+	return;
+}
+
+static int rga2_blit_flush_cache(rga2_session *session, struct rga2_req *req)
+{
+	int ret = 0;
+	/* Alloc 4k size for rga2_reg use. */
+	struct rga2_reg *reg = (struct rga2_reg *)get_zeroed_page(GFP_KERNEL | GFP_DMA32);
+	struct rga2_mmu_buf_t *tbuf = &rga2_mmu_buf;
+
+	if (!reg) {
+		pr_err("%s, [%d] kzalloc error\n", __func__, __LINE__);
+		ret = -ENOMEM;
+		goto err_free_reg;
+	}
+
+	ret = rga2_get_dma_info(reg, req);
+	if (ret < 0) {
+		pr_err("fail to get dma buffer info!\n");
+		goto err_free_reg;
+	}
+
+	if ((req->mmu_info.src0_mmu_flag & 1) || (req->mmu_info.src1_mmu_flag & 1) ||
+	    (req->mmu_info.dst_mmu_flag & 1) || (req->mmu_info.els_mmu_flag & 1)) {
+		reg->MMU_map = true;
+		ret = rga2_set_mmu_info(reg, req);
+		if (ret < 0) {
+			pr_err("%s, [%d] set mmu info error\n", __func__, __LINE__);
+			ret = -EFAULT;
+			goto err_free_reg;
+		}
+	}
+	if (reg->MMU_len && tbuf) {
+		if (tbuf->back + reg->MMU_len > 2 * tbuf->size)
+			tbuf->back = reg->MMU_len + tbuf->size;
+		else
+			tbuf->back += reg->MMU_len;
+	}
+err_free_reg:
+	free_page((unsigned long)reg);
+
+	return ret;
+}
+
+static int rga2_blit(rga2_session *session, struct rga2_req *req)
+{
+	int ret = -1;
+	int num = 0;
+	struct rga2_reg *reg;
+
+	/* check value if legal */
+	ret = rga2_check_param(req);
+	if (ret == -EINVAL) {
+		pr_err("req argument is inval\n");
+		return ret;
+	}
+
+	reg = rga2_reg_init(session, req);
+	if (reg == NULL) {
+		pr_err("init reg fail\n");
+		return -EFAULT;
+	}
+
+	num = 1;
+	mutex_lock(&rga2_service.lock);
+	atomic_add(num, &rga2_service.total_running);
+	rga2_try_set_reg();
+	mutex_unlock(&rga2_service.lock);
+
+	return 0;
+}
+
+static int rga2_blit_async(rga2_session *session, struct rga2_req *req)
+{
+	int ret = -1;
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_MSG) {
+		if (1) {
+			print_debug_info(req);
+			rga2_flag = 1;
+			INFO("*** rga_blit_async proc ***\n");
+		} else {
+			rga2_flag = 0;
+		}
+	}
+#endif
+	atomic_set(&session->done, 0);
+	ret = rga2_blit(session, req);
+
+	return ret;
+	}
+
+static int rga2_blit_sync(rga2_session *session, struct rga2_req *req)
+{
+	struct rga2_req req_bak;
+	int restore = 0;
+	int try = 10;
+	int ret = -1;
+	int ret_timeout = 0;
+
+	memcpy(&req_bak, req, sizeof(req_bak));
+retry:
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_MSG) {
+		if (1) {
+			print_debug_info(req);
+			rga2_flag = 1;
+			INFO("*** rga2_blit_sync proc ***\n");
+		} else {
+			rga2_flag = 0;
+		}
+	}
+	if (RGA2_CHECK_MODE) {
+		rga2_align_check(req);
+		/*rga2_scale_check(req);*/
+	}
+#endif
+
+	atomic_set(&session->done, 0);
+
+	ret = rga2_blit(session, req);
+	if(ret < 0)
+		return ret;
+
+	if (rk3368)
+		ret_timeout = wait_event_timeout(session->wait,
+						 atomic_read(&session->done),
+						 RGA2_TIMEOUT_DELAY / 4);
+	else
+		ret_timeout = wait_event_timeout(session->wait,
+						 atomic_read(&session->done),
+						 RGA2_TIMEOUT_DELAY);
+
+	if (unlikely(ret_timeout < 0)) {
+		u32 i;
+		u32 *p;
+
+		p = rga2_service.cmd_buff;
+		pr_err("Rga sync pid %d wait task ret %d\n", session->pid,
+			ret_timeout);
+		pr_err("interrupt = %x status = %x\n",
+		       rga2_read(RGA2_INT), rga2_read(RGA2_STATUS));
+		rga2_printf_cmd_buf(p);
+		DBG("rga2 CMD\n");
+		for (i = 0; i < 7; i++)
+			DBG("%.8x %.8x %.8x %.8x\n",
+			     p[0 + i * 4], p[1 + i * 4],
+			     p[2 + i * 4], p[3 + i * 4]);
+		mutex_lock(&rga2_service.lock);
+		rga2_del_running_list();
+		mutex_unlock(&rga2_service.lock);
+		ret = ret_timeout;
+	} else if (ret_timeout == 0) {
+		u32 i;
+		u32 *p;
+
+		p = rga2_service.cmd_buff;
+		pr_err("Rga sync pid %d wait %d task done timeout\n",
+			session->pid, atomic_read(&session->task_running));
+		pr_err("interrupt = %x status = %x\n",
+		       rga2_read(RGA2_INT), rga2_read(RGA2_STATUS));
+		rga2_printf_cmd_buf(p);
+		DBG("rga2 CMD\n");
+		for (i = 0; i < 7; i++)
+			DBG("%.8x %.8x %.8x %.8x\n",
+			     p[0 + i * 4], p[1 + i * 4],
+			     p[2 + i * 4], p[3 + i * 4]);
+		mutex_lock(&rga2_service.lock);
+		rga2_del_running_list_timeout();
+		rga2_try_set_reg();
+		mutex_unlock(&rga2_service.lock);
+		ret = -ETIMEDOUT;
+	}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_TIME) {
+		ktime_t rga2_cmd_end;
+
+		rga2_cmd_end = ktime_get();
+		rga2_cmd_end = ktime_sub(rga2_cmd_end, rga2_start);
+		DBG("sync one cmd end time %d us\n", (int)ktime_to_us(rga2_cmd_end));
+	}
+#endif
+	if (ret == -ETIMEDOUT && try--) {
+		memcpy(req, &req_bak, sizeof(req_bak));
+		/*
+		 * if rga work timeout with scaling, need do a non-scale work
+		 * first, restore hardware status, then do actually work.
+		 */
+		if (req->src.act_w != req->dst.act_w ||
+		    req->src.act_h != req->dst.act_h) {
+			req->src.act_w = MIN(320, MIN(req->src.act_w,
+						      req->dst.act_w));
+			req->src.act_h = MIN(240, MIN(req->src.act_h,
+						      req->dst.act_h));
+			req->dst.act_w = req->src.act_w;
+			req->dst.act_h = req->src.act_h;
+			restore = 1;
+		}
+		goto retry;
+	}
+	if (!ret && restore) {
+		memcpy(req, &req_bak, sizeof(req_bak));
+		restore = 0;
+		goto retry;
+	}
+
+	return ret;
+}
+
+static long rga_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	struct rga2_drvdata_t *rga = rga2_drvdata;
+	struct rga2_req req, req_first;
+	struct rga_req req_rga;
+	int ret = 0;
+	int major_version = 0, minor_version = 0;
+	char version[16] = {0};
+	rga2_session *session;
+
+	if (!rga) {
+		pr_err("rga2_drvdata is null, rga2 is not init\n");
+		return -ENODEV;
+	}
+	memset(&req, 0x0, sizeof(req));
+
+	mutex_lock(&rga2_service.mutex);
+
+	session = (rga2_session *)file->private_data;
+
+	if (NULL == session)
+	{
+		printk("%s [%d] rga thread session is null\n",__FUNCTION__,__LINE__);
+		mutex_unlock(&rga2_service.mutex);
+		return -EINVAL;
+	}
+
+	memset(&req, 0x0, sizeof(req));
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_MSG)
+		INFO("cmd is %s\n", rga2_get_cmd_mode_str(cmd));
+	if (RGA2_NONUSE) {
+		mutex_unlock(&rga2_service.mutex);
+		return 0;
+	}
+#endif
+	switch (cmd)
+	{
+		case RGA_BLIT_SYNC:
+			if (unlikely(copy_from_user(&req_rga, (struct rga_req*)arg, sizeof(struct rga_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+			RGA_MSG_2_RGA2_MSG(&req_rga, &req);
+
+			if (first_RGA2_proc == 0 && req.render_mode == bitblt_mode && rga2_service.dev_mode == 1) {
+				memcpy(&req_first, &req, sizeof(struct rga2_req));
+				if ((req_first.src.act_w != req_first.dst.act_w)
+						|| (req_first.src.act_h != req_first.dst.act_h)) {
+					req_first.src.act_w = MIN(320, MIN(req_first.src.act_w, req_first.dst.act_w));
+					req_first.src.act_h = MIN(240, MIN(req_first.src.act_h, req_first.dst.act_h));
+					req_first.dst.act_w = req_first.src.act_w;
+					req_first.dst.act_h = req_first.src.act_h;
+					ret = rga2_blit_async(session, &req_first);
+				}
+				ret = rga2_blit_sync(session, &req);
+				first_RGA2_proc = 1;
+			}
+			else {
+				ret = rga2_blit_sync(session, &req);
+			}
+			break;
+		case RGA_BLIT_ASYNC:
+			if (unlikely(copy_from_user(&req_rga, (struct rga_req*)arg, sizeof(struct rga_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+
+			RGA_MSG_2_RGA2_MSG(&req_rga, &req);
+			if (first_RGA2_proc == 0 && req.render_mode == bitblt_mode && rga2_service.dev_mode == 1) {
+				memcpy(&req_first, &req, sizeof(struct rga2_req));
+				if ((req_first.src.act_w != req_first.dst.act_w)
+						|| (req_first.src.act_h != req_first.dst.act_h)
+						|| rk3368) {
+					req_first.src.act_w = MIN(320, MIN(req_first.src.act_w, req_first.dst.act_w));
+					req_first.src.act_h = MIN(240, MIN(req_first.src.act_h, req_first.dst.act_h));
+					req_first.dst.act_w = req_first.src.act_w;
+					req_first.dst.act_h = req_first.src.act_h;
+					if (rk3368)
+						ret = rga2_blit_sync(session, &req_first);
+					else
+						ret = rga2_blit_async(session, &req_first);
+				}
+				ret = rga2_blit_async(session, &req);
+				first_RGA2_proc = 1;
+			}
+			else {
+				if (rk3368)
+				{
+					memcpy(&req_first, &req, sizeof(struct rga2_req));
+
+					/*
+					 * workround for gts
+					 * run gts --skip-all-system-status-check --ignore-business-logic-failure -m GtsMediaTestCases -t com.google.android.media.gts.WidevineYouTubePerformanceTests#testClear1080P30
+					 */
+					if ((req_first.src.act_w == 1920) && (req_first.src.act_h == 1008) && (req_first.src.act_h == req_first.dst.act_w)) {
+						printk("src : aw=%d ah=%d vw=%d vh=%d  \n",
+							req_first.src.act_w, req_first.src.act_h, req_first.src.vir_w, req_first.src.vir_h);
+						printk("dst : aw=%d ah=%d vw=%d vh=%d  \n",
+							req_first.dst.act_w, req_first.dst.act_h, req_first.dst.vir_w, req_first.dst.vir_h);
+					} else {
+							req_first.src.act_w = MIN(320, MIN(req_first.src.act_w, req_first.dst.act_w));
+							req_first.src.act_h = MIN(240, MIN(req_first.src.act_h, req_first.dst.act_h));
+							req_first.dst.act_w = req_first.src.act_w;
+							req_first.dst.act_h = req_first.src.act_h;
+							ret = rga2_blit_sync(session, &req_first);
+					}
+				}
+				ret = rga2_blit_async(session, &req);
+			}
+			break;
+		case RGA_CACHE_FLUSH:
+			if (unlikely(copy_from_user(&req_rga, (struct rga_req*)arg, sizeof(struct rga_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+			RGA_MSG_2_RGA2_MSG(&req_rga, &req);
+			ret = rga2_blit_flush_cache(session, &req);
+			break;
+		case RGA2_BLIT_SYNC:
+			if (unlikely(copy_from_user(&req, (struct rga2_req*)arg, sizeof(struct rga2_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+			ret = rga2_blit_sync(session, &req);
+			break;
+		case RGA2_BLIT_ASYNC:
+			if (unlikely(copy_from_user(&req, (struct rga2_req*)arg, sizeof(struct rga2_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+
+			if((atomic_read(&rga2_service.total_running) > 16))
+			{
+				ret = rga2_blit_sync(session, &req);
+			}
+			else
+			{
+				ret = rga2_blit_async(session, &req);
+			}
+			break;
+		case RGA_FLUSH:
+		case RGA2_FLUSH:
+			ret = rga2_flush(session, arg);
+			break;
+		case RGA_GET_RESULT:
+		case RGA2_GET_RESULT:
+			ret = rga2_get_result(session, arg);
+			break;
+		case RGA_GET_VERSION:
+			sscanf(rga->version, "%x.%x.%*x", &major_version, &minor_version);
+			snprintf(version, 5, "%x.%02x", major_version, minor_version);
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+			ret = copy_to_user((void *)arg, version, sizeof(rga->version));
+#else
+			ret = copy_to_user((void *)arg, RGA2_VERSION, sizeof(RGA2_VERSION));
+#endif
+			if (ret != 0)
+				ret = -EFAULT;
+			break;
+		case RGA2_GET_VERSION:
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+			ret = copy_to_user((void *)arg, rga->version, sizeof(rga->version));
+#else
+			ret = copy_to_user((void *)arg, RGA2_VERSION, sizeof(RGA2_VERSION));
+#endif
+			if (ret != 0)
+				ret = -EFAULT;
+			break;
+		default:
+			ERR("unknown ioctl cmd!\n");
+			ret = -EINVAL;
+			break;
+	}
+
+	mutex_unlock(&rga2_service.mutex);
+
+	return ret;
+}
+
+#ifdef CONFIG_COMPAT
+static long compat_rga_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	struct rga2_drvdata_t *rga = rga2_drvdata;
+	struct rga2_req req, req_first;
+	struct rga_req_32 req_rga;
+	int ret = 0;
+	rga2_session *session;
+
+	if (!rga) {
+		pr_err("rga2_drvdata is null, rga2 is not init\n");
+		return -ENODEV;
+	}
+	memset(&req, 0x0, sizeof(req));
+
+	mutex_lock(&rga2_service.mutex);
+
+	session = (rga2_session *)file->private_data;
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_TEST_MSG)
+		INFO("using %s\n", __func__);
+#endif
+
+	if (NULL == session) {
+		ERR("%s [%d] rga thread session is null\n", __func__, __LINE__);
+		mutex_unlock(&rga2_service.mutex);
+		return -EINVAL;
+	}
+
+	memset(&req, 0x0, sizeof(req));
+
+	switch (cmd) {
+		case RGA_BLIT_SYNC:
+			if (unlikely(copy_from_user(&req_rga, compat_ptr((compat_uptr_t)arg), sizeof(struct rga_req_32))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+
+			RGA_MSG_2_RGA2_MSG_32(&req_rga, &req);
+
+			if (first_RGA2_proc == 0 && req.render_mode == bitblt_mode && rga2_service.dev_mode == 1) {
+				memcpy(&req_first, &req, sizeof(struct rga2_req));
+				if ((req_first.src.act_w != req_first.dst.act_w)
+						|| (req_first.src.act_h != req_first.dst.act_h)) {
+					req_first.src.act_w = MIN(320, MIN(req_first.src.act_w, req_first.dst.act_w));
+					req_first.src.act_h = MIN(240, MIN(req_first.src.act_h, req_first.dst.act_h));
+					req_first.dst.act_w = req_first.src.act_w;
+					req_first.dst.act_h = req_first.src.act_h;
+					ret = rga2_blit_async(session, &req_first);
+				}
+				ret = rga2_blit_sync(session, &req);
+				first_RGA2_proc = 1;
+			}
+			else {
+				ret = rga2_blit_sync(session, &req);
+			}
+			break;
+		case RGA_BLIT_ASYNC:
+			if (unlikely(copy_from_user(&req_rga, compat_ptr((compat_uptr_t)arg), sizeof(struct rga_req_32))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+			RGA_MSG_2_RGA2_MSG_32(&req_rga, &req);
+
+			if (first_RGA2_proc == 0 && req.render_mode == bitblt_mode && rga2_service.dev_mode == 1) {
+				memcpy(&req_first, &req, sizeof(struct rga2_req));
+				if ((req_first.src.act_w != req_first.dst.act_w)
+						|| (req_first.src.act_h != req_first.dst.act_h)) {
+					req_first.src.act_w = MIN(320, MIN(req_first.src.act_w, req_first.dst.act_w));
+					req_first.src.act_h = MIN(240, MIN(req_first.src.act_h, req_first.dst.act_h));
+					req_first.dst.act_w = req_first.src.act_w;
+					req_first.dst.act_h = req_first.src.act_h;
+					ret = rga2_blit_async(session, &req_first);
+				}
+				ret = rga2_blit_sync(session, &req);
+				first_RGA2_proc = 1;
+			}
+			else {
+				ret = rga2_blit_sync(session, &req);
+			}
+
+			//if((atomic_read(&rga2_service.total_running) > 8))
+			//    ret = rga2_blit_sync(session, &req);
+			//else
+			//    ret = rga2_blit_async(session, &req);
+
+			break;
+		case RGA2_BLIT_SYNC:
+			if (unlikely(copy_from_user(&req, compat_ptr((compat_uptr_t)arg), sizeof(struct rga2_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+			ret = rga2_blit_sync(session, &req);
+			break;
+		case RGA2_BLIT_ASYNC:
+			if (unlikely(copy_from_user(&req, compat_ptr((compat_uptr_t)arg), sizeof(struct rga2_req))))
+			{
+				ERR("copy_from_user failed\n");
+				ret = -EFAULT;
+				break;
+			}
+
+			if((atomic_read(&rga2_service.total_running) > 16))
+				ret = rga2_blit_sync(session, &req);
+			else
+				ret = rga2_blit_async(session, &req);
+
+			break;
+		case RGA_FLUSH:
+		case RGA2_FLUSH:
+			ret = rga2_flush(session, arg);
+			break;
+		case RGA_GET_RESULT:
+		case RGA2_GET_RESULT:
+			ret = rga2_get_result(session, arg);
+			break;
+		case RGA_GET_VERSION:
+		case RGA2_GET_VERSION:
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+			ret = copy_to_user((void *)arg, rga->version, 16);
+#else
+			ret = copy_to_user((void *)arg, RGA2_VERSION, sizeof(RGA2_VERSION));
+#endif
+			if (ret != 0)
+				ret = -EFAULT;
+			break;
+		default:
+			ERR("unknown ioctl cmd!\n");
+			ret = -EINVAL;
+			break;
+	}
+
+	mutex_unlock(&rga2_service.mutex);
+
+	return ret;
+}
+#endif
+
+
+static long rga2_ioctl_kernel(struct rga_req *req_rga)
+{
+	int ret = 0;
+	rga2_session *session;
+	struct rga2_req req;
+
+	memset(&req, 0x0, sizeof(req));
+	mutex_lock(&rga2_service.mutex);
+	session = &rga2_session_global;
+	if (NULL == session)
+	{
+		ERR("%s [%d] rga thread session is null\n", __func__, __LINE__);
+		mutex_unlock(&rga2_service.mutex);
+		return -EINVAL;
+	}
+
+	RGA_MSG_2_RGA2_MSG(req_rga, &req);
+	ret = rga2_blit_sync(session, &req);
+	mutex_unlock(&rga2_service.mutex);
+
+	return ret;
+}
+
+
+static int rga2_open(struct inode *inode, struct file *file)
+{
+	rga2_session *session = kzalloc(sizeof(rga2_session), GFP_KERNEL);
+
+	if (NULL == session) {
+		pr_err("unable to allocate memory for rga_session.");
+		return -ENOMEM;
+	}
+
+	session->pid = current->pid;
+	INIT_LIST_HEAD(&session->waiting);
+	INIT_LIST_HEAD(&session->running);
+	INIT_LIST_HEAD(&session->list_session);
+	init_waitqueue_head(&session->wait);
+	mutex_lock(&rga2_service.lock);
+	list_add_tail(&session->list_session, &rga2_service.session);
+	mutex_unlock(&rga2_service.lock);
+	atomic_set(&session->task_running, 0);
+	atomic_set(&session->num_done, 0);
+	file->private_data = (void *)session;
+
+	return nonseekable_open(inode, file);
+}
+
+static int rga2_release(struct inode *inode, struct file *file)
+{
+	int task_running;
+	rga2_session *session = (rga2_session *)file->private_data;
+
+	if (NULL == session)
+		return -EINVAL;
+
+	task_running = atomic_read(&session->task_running);
+	if (task_running)
+	{
+		pr_err("rga2_service session %d still has %d task running when closing\n", session->pid, task_running);
+		msleep(100);
+	}
+
+	wake_up(&session->wait);
+	mutex_lock(&rga2_service.lock);
+	list_del(&session->list_session);
+	rga2_service_session_clear(session);
+	kfree(session);
+	mutex_unlock(&rga2_service.lock);
+
+	return 0;
+}
+
+static void RGA2_flush_page(void)
+{
+	struct rga2_reg *reg;
+	int i;
+
+	reg = list_entry(rga2_service.running.prev,
+			 struct rga2_reg, status_link);
+
+	if (reg == NULL)
+		return;
+
+	if (reg->MMU_src0_base != NULL) {
+		for (i = 0; i < reg->MMU_src0_count; i++)
+			rga2_dma_flush_page(phys_to_page(reg->MMU_src0_base[i]),
+					    MMU_UNMAP_CLEAN);
+	}
+
+	if (reg->MMU_src1_base != NULL) {
+		for (i = 0; i < reg->MMU_src1_count; i++)
+			rga2_dma_flush_page(phys_to_page(reg->MMU_src1_base[i]),
+					    MMU_UNMAP_CLEAN);
+	}
+
+	if (reg->MMU_dst_base != NULL) {
+		for (i = 0; i < reg->MMU_dst_count; i++)
+			rga2_dma_flush_page(phys_to_page(reg->MMU_dst_base[i]),
+					    MMU_UNMAP_INVALID);
+	}
+}
+
+static irqreturn_t rga2_irq_thread(int irq, void *dev_id)
+{
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_INT_FLAG)
+		INFO("irqthread INT[%x],STATS[%x]\n", rga2_read(RGA2_INT),
+		     rga2_read(RGA2_STATUS));
+
+	if (RGA2_TEST_TIME) {
+		ktime_t rga2_hw_end;
+
+		rga2_hw_end = ktime_get();
+		rga2_hw_end = ktime_sub(rga2_hw_end, rga2_start);
+		DBG("RGA hardware cost time %d us\n", (int)ktime_to_us(rga2_hw_end));
+	}
+#endif
+	RGA2_flush_page();
+	mutex_lock(&rga2_service.lock);
+	if (rga2_service.enable) {
+		rga2_del_running_list();
+		rga2_try_set_reg();
+	}
+	mutex_unlock(&rga2_service.lock);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t rga2_irq(int irq,  void *dev_id)
+{
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_INT_FLAG)
+		INFO("irq INT[%x], STATS[%x]\n", rga2_read(RGA2_INT),
+		     rga2_read(RGA2_STATUS));
+#endif
+	/*if error interrupt then soft reset hardware*/
+	if (rga2_read(RGA2_INT) & 0x01) {
+		pr_err("Rga err irq! INT[%x],STATS[%x]\n",
+		       rga2_read(RGA2_INT), rga2_read(RGA2_STATUS));
+		rga2_soft_reset();
+	}
+	/*clear INT */
+	rga2_write(rga2_read(RGA2_INT) | (0x1<<4) | (0x1<<5) | (0x1<<6) | (0x1<<7), RGA2_INT);
+
+	return IRQ_WAKE_THREAD;
+}
+
+struct file_operations rga2_fops = {
+	.owner		= THIS_MODULE,
+	.open		= rga2_open,
+	.release	= rga2_release,
+	.unlocked_ioctl		= rga_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl		= compat_rga_ioctl,
+#endif
+};
+
+static struct miscdevice rga2_dev ={
+	.minor = RGA2_MAJOR,
+	.name  = "rga",
+	.fops  = &rga2_fops,
+};
+
+static const struct of_device_id rockchip_rga_dt_ids[] = {
+	{ .compatible = "rockchip,rga2", },
+	{},
+};
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+static int rga2_debugger_init(struct rga_debugger **debugger_p)
+{
+	struct rga_debugger *debugger;
+
+	*debugger_p = kzalloc(sizeof(struct rga_debugger), GFP_KERNEL);
+	if (*debugger_p == NULL) {
+		ERR("can not alloc for rga2 debugger\n");
+		return -ENOMEM;
+	}
+
+	debugger = *debugger_p;
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUG_FS
+	mutex_init(&debugger->debugfs_lock);
+	INIT_LIST_HEAD(&debugger->debugfs_entry_list);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA2_PROC_FS
+	mutex_init(&debugger->procfs_lock);
+	INIT_LIST_HEAD(&debugger->procfs_entry_list);
+#endif
+
+	rga2_debugfs_init();
+	rga2_procfs_init();
+
+	return 0;
+}
+
+static int rga2_debugger_remove(struct rga_debugger **debugger_p)
+{
+	rga2_debugfs_remove();
+	rga2_procfs_remove();
+
+	kfree(*debugger_p);
+	*debugger_p = NULL;
+
+	return 0;
+}
+#endif
+
+static int rga2_drv_probe(struct platform_device *pdev)
+{
+	struct rga2_drvdata_t *data;
+	struct resource *res;
+	int ret = 0;
+	struct device_node *np = pdev->dev.of_node;
+
+	mutex_init(&rga2_service.lock);
+	mutex_init(&rga2_service.mutex);
+	atomic_set(&rga2_service.total_running, 0);
+	atomic_set(&rga2_service.src_format_swt, 0);
+	rga2_service.last_prc_src_format = 1; /* default is yuv first*/
+	rga2_service.enable = false;
+
+	rga2_ioctl_kernel_p = rga2_ioctl_kernel;
+
+	data = devm_kzalloc(&pdev->dev, sizeof(struct rga2_drvdata_t), GFP_KERNEL);
+	if(NULL == data)
+	{
+		ERR("failed to allocate driver data.\n");
+		return -ENOMEM;
+	}
+
+	INIT_DELAYED_WORK(&data->power_off_work, rga2_power_off_work);
+	wake_lock_init(&data->wake_lock, WAKE_LOCK_SUSPEND, "rga");
+
+	data->clk_rga2 = devm_clk_get(&pdev->dev, "clk_rga");
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	pm_runtime_enable(&pdev->dev);
+#else
+	data->pd_rga2 = devm_clk_get(&pdev->dev, "pd_rga");
+#endif
+	data->aclk_rga2 = devm_clk_get(&pdev->dev, "aclk_rga");
+	data->hclk_rga2 = devm_clk_get(&pdev->dev, "hclk_rga");
+
+	/* map the registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	data->rga_base = devm_ioremap_resource(&pdev->dev, res);
+	if (!data->rga_base) {
+		ERR("rga ioremap failed\n");
+		ret = -ENOENT;
+		goto err_ioremap;
+	}
+
+	/* get the IRQ */
+	data->irq = platform_get_irq(pdev, 0);
+	if (data->irq <= 0) {
+		ERR("failed to get rga irq resource (%d).\n", data->irq);
+		ret = data->irq;
+		goto err_irq;
+	}
+
+	/* request the IRQ */
+	ret = devm_request_threaded_irq(&pdev->dev, data->irq, rga2_irq, rga2_irq_thread, 0, "rga", pdev);
+	if (ret)
+	{
+		ERR("rga request_irq failed (%d).\n", ret);
+		goto err_irq;
+	}
+
+	platform_set_drvdata(pdev, data);
+	data->dev = &pdev->dev;
+	rga2_drvdata = data;
+	of_property_read_u32(np, "dev_mode", &rga2_service.dev_mode);
+	if (of_machine_is_compatible("rockchip,rk3368"))
+		rk3368 = 1;
+
+#if defined(CONFIG_ION_ROCKCHIP) && (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+	data->ion_client = rockchip_ion_client_create("rga");
+	if (IS_ERR(data->ion_client)) {
+		dev_err(&pdev->dev, "failed to create ion client for rga");
+		return PTR_ERR(data->ion_client);
+	} else {
+		dev_info(&pdev->dev, "rga ion client create success!\n");
+	}
+#endif
+
+	ret = misc_register(&rga2_dev);
+	if(ret)
+	{
+		ERR("cannot register miscdev (%d)\n", ret);
+		goto err_misc_register;
+	}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	rga2_debugger_init(&rga2_drvdata->debugger);
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 1, 0))
+	rga2_init_version();
+	INFO("Driver loaded successfully ver:%s\n", rga2_drvdata->version);
+#else
+	INFO("Driver loaded successfully\n");
+#endif
+	return 0;
+
+err_misc_register:
+	free_irq(data->irq, pdev);
+err_irq:
+	iounmap(data->rga_base);
+err_ioremap:
+	wake_lock_destroy(&data->wake_lock);
+	//kfree(data);
+
+	return ret;
+}
+
+static int rga2_drv_remove(struct platform_device *pdev)
+{
+	struct rga2_drvdata_t *data = platform_get_drvdata(pdev);
+	DBG("%s [%d]\n",__FUNCTION__,__LINE__);
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	rga2_debugger_remove(&data->debugger);
+#endif
+
+	wake_lock_destroy(&data->wake_lock);
+	misc_deregister(&(data->miscdev));
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	free_irq(data->irq, &data->miscdev);
+	iounmap((void __iomem *)(data->rga_base));
+
+	devm_clk_put(&pdev->dev, data->clk_rga2);
+	devm_clk_put(&pdev->dev, data->aclk_rga2);
+	devm_clk_put(&pdev->dev, data->hclk_rga2);
+	pm_runtime_disable(&pdev->dev);
+#endif
+
+	//kfree(data);
+	return 0;
+}
+
+static struct platform_driver rga2_driver = {
+	.probe		= rga2_drv_probe,
+	.remove		= rga2_drv_remove,
+	.driver		= {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+		.owner  = THIS_MODULE,
+#endif
+		.name	= "rga2",
+		.of_match_table = of_match_ptr(rockchip_rga_dt_ids),
+	},
+};
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+void rga2_slt(void)
+{
+	int i;
+	int src_size, dst_size, src_order, dst_order;
+	int err_count = 0, right_count = 0;
+	int task_running;
+	unsigned int srcW, srcH, dstW, dstH;
+	unsigned int *pstd, *pnow;
+	unsigned long *src_vir, *dst_vir;
+	struct rga2_req req;
+	rga2_session session;
+
+	srcW = 400;
+	srcH = 200;
+	dstW = 400;
+	dstH = 200;
+
+	src_size = srcW * srcH * 4;
+	dst_size = dstW * dstH * 4;
+
+	src_order = get_order(src_size);
+	src_vir = (unsigned long *)__get_free_pages(GFP_KERNEL | GFP_DMA32, src_order);
+	if (src_vir == NULL) {
+		ERR("%s[%d], can not alloc pages for src, order = %d\n",
+		    __func__, __LINE__, src_order);
+		return;
+	}
+
+	dst_order = get_order(dst_size);
+	dst_vir = (unsigned long *)__get_free_pages(GFP_KERNEL | GFP_DMA32, dst_order);
+	if (dst_vir == NULL) {
+		ERR("%s[%d], can not alloc pages for dst, order = %d\n",
+		    __func__, __LINE__, dst_order);
+		return;
+	}
+
+	/* Init session */
+	session.pid = current->pid;
+
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	mutex_lock(&rga2_service.lock);
+	list_add_tail(&session.list_session, &rga2_service.session);
+	mutex_unlock(&rga2_service.lock);
+	atomic_set(&session.task_running, 0);
+	atomic_set(&session.num_done, 0);
+
+	INFO("**********************************\n");
+	INFO("************ RGA_TEST ************\n");
+	INFO("**********************************\n");
+
+	memset(src_vir, 0x50, src_size);
+	memset(dst_vir, 0x50, dst_size);
+
+	rga2_dma_flush_range(src_vir, src_vir + src_size);
+	rga2_dma_flush_range(dst_vir, dst_vir + dst_size);
+
+	memset(&req, 0, sizeof(struct rga2_req));
+	req.src.x_offset = 0;
+	req.src.y_offset = 0;
+	req.src.act_w = srcW;
+	req.src.act_h = srcH;
+	req.src.vir_w = srcW;
+	req.src.vir_h = srcW;
+	req.src.format = RGA2_FORMAT_RGBA_8888;
+
+	req.src.yrgb_addr = 0;
+	req.src.uv_addr = (unsigned long)virt_to_phys(src_vir);
+	req.src.v_addr = req.src.uv_addr + srcH * srcW;
+
+	req.dst.x_offset = 0;
+	req.dst.y_offset = 0;
+	req.dst.act_w = dstW;
+	req.dst.act_h = dstH;
+	req.dst.vir_w = dstW;
+	req.dst.vir_h = dstH;
+	req.dst.format = RGA2_FORMAT_RGBA_8888;
+
+	req.dst.yrgb_addr = 0;
+	req.dst.uv_addr = (unsigned long)virt_to_phys(dst_vir);
+	req.dst.v_addr = req.dst.uv_addr + dstH * dstW;
+
+	rga2_blit_sync(&session, &req);
+
+	/* Check buffer */
+	pstd = (unsigned int *)src_vir;
+	pnow = (unsigned int *)dst_vir;
+
+	INFO("[  num   : srcInfo    dstInfo ]\n");
+	for (i = 0; i < dst_size / 4; i++) {
+		if (*pstd != *pnow) {
+			INFO("[X%.8d : 0x%x 0x%x]", i, *pstd, *pnow);
+			if (i % 4 == 0)
+				INFO("\n");
+			err_count++;
+		} else {
+			if (i % (640 * 1024) == 0)
+				INFO("[Y%.8d : 0x%.8x 0x%.8x]\n",
+				     i, *pstd, *pnow);
+			right_count++;
+		}
+		pstd++;
+		pnow++;
+		if (err_count > 64)
+			break;
+	}
+
+	INFO("err_count=%d, right_count=%d\n", err_count, right_count);
+	if (err_count != 0)
+		INFO("rga slt err !!\n");
+	else
+		INFO("rga slt success !!\n");
+
+	/* Deinit session */
+	task_running = atomic_read(&session.task_running);
+	if (task_running) {
+		pr_err("%s[%d], session %d still has %d task running when closing\n",
+		       __func__, __LINE__, session.pid, task_running);
+		msleep(100);
+	}
+	wake_up(&session.wait);
+	mutex_lock(&rga2_service.lock);
+	list_del(&session.list_session);
+	rga2_service_session_clear(&session);
+	mutex_unlock(&rga2_service.lock);
+
+	free_pages((unsigned long)src_vir, src_order);
+	free_pages((unsigned long)dst_vir, dst_order);
+}
+#endif
+
+void rga2_test_0(void);
+
+static int __init rga2_init(void)
+{
+	int ret;
+	int order = 0;
+	uint32_t *buf_p;
+	uint32_t *buf;
+
+	/*
+	 * malloc pre scale mid buf mmu table:
+	 * RGA2_PHY_PAGE_SIZE * channel_num * address_size
+	 */
+	order = get_order(RGA2_PHY_PAGE_SIZE * 3 * sizeof(buf_p));
+	buf_p = (uint32_t *)__get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+	if (buf_p == NULL) {
+		ERR("Can not alloc pages for mmu_page_table\n");
+	}
+
+	rga2_mmu_buf.buf_virtual = buf_p;
+	rga2_mmu_buf.buf_order = order;
+#if (defined(CONFIG_ARM) && defined(CONFIG_ARM_LPAE))
+	buf = (uint32_t *)(uint32_t)virt_to_phys((void *)((unsigned long)buf_p));
+#else
+	buf = (uint32_t *)virt_to_phys((void *)((unsigned long)buf_p));
+#endif
+	rga2_mmu_buf.buf = buf;
+	rga2_mmu_buf.front = 0;
+	rga2_mmu_buf.back = RGA2_PHY_PAGE_SIZE * 3;
+	rga2_mmu_buf.size = RGA2_PHY_PAGE_SIZE * 3;
+
+	order = get_order(RGA2_PHY_PAGE_SIZE * sizeof(struct page *));
+	rga2_mmu_buf.pages = (struct page **)__get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+	if (rga2_mmu_buf.pages == NULL) {
+		ERR("Can not alloc pages for rga2_mmu_buf.pages\n");
+	}
+	rga2_mmu_buf.pages_order = order;
+
+	ret = platform_driver_register(&rga2_driver);
+	if (ret != 0) {
+		printk(KERN_ERR "Platform device register failed (%d).\n", ret);
+		return ret;
+	}
+
+	rga2_session_global.pid = 0x0000ffff;
+	INIT_LIST_HEAD(&rga2_session_global.waiting);
+	INIT_LIST_HEAD(&rga2_session_global.running);
+	INIT_LIST_HEAD(&rga2_session_global.list_session);
+
+	INIT_LIST_HEAD(&rga2_service.waiting);
+	INIT_LIST_HEAD(&rga2_service.running);
+	INIT_LIST_HEAD(&rga2_service.done);
+	INIT_LIST_HEAD(&rga2_service.session);
+	init_waitqueue_head(&rga2_session_global.wait);
+	//mutex_lock(&rga_service.lock);
+	list_add_tail(&rga2_session_global.list_session, &rga2_service.session);
+	//mutex_unlock(&rga_service.lock);
+	atomic_set(&rga2_session_global.task_running, 0);
+	atomic_set(&rga2_session_global.num_done, 0);
+
+#if RGA2_TEST_CASE
+	rga2_test_0();
+#endif
+	INFO("Module initialized.\n");
+
+	return 0;
+}
+
+static void __exit rga2_exit(void)
+{
+	rga2_power_off();
+
+	free_pages((unsigned long)rga2_mmu_buf.buf_virtual, rga2_mmu_buf.buf_order);
+	free_pages((unsigned long)rga2_mmu_buf.pages, rga2_mmu_buf.pages_order);
+
+	platform_driver_unregister(&rga2_driver);
+}
+
+
+#if RGA2_TEST_CASE
+
+void rga2_test_0(void)
+{
+	struct rga2_req req;
+	rga2_session session;
+	unsigned int *src, *dst;
+
+	session.pid	= current->pid;
+	INIT_LIST_HEAD(&session.waiting);
+	INIT_LIST_HEAD(&session.running);
+	INIT_LIST_HEAD(&session.list_session);
+	init_waitqueue_head(&session.wait);
+	/* no need to protect */
+	list_add_tail(&session.list_session, &rga2_service.session);
+	atomic_set(&session.task_running, 0);
+	atomic_set(&session.num_done, 0);
+
+	memset(&req, 0, sizeof(struct rga2_req));
+	src = kmalloc(800*480*4, GFP_KERNEL);
+	dst = kmalloc(800*480*4, GFP_KERNEL);
+
+	printk("\n********************************\n");
+	printk("************ RGA2_TEST ************\n");
+	printk("********************************\n\n");
+
+#if 1
+	memset(src, 0x80, 800 * 480 * 4);
+	memset(dst, 0xcc, 800 * 480 * 4);
+#endif
+#if 0
+	dmac_flush_range(src, &src[800 * 480]);
+	outer_flush_range(virt_to_phys(src), virt_to_phys(&src[800 * 480]));
+
+	dmac_flush_range(dst, &dst[800 * 480]);
+	outer_flush_range(virt_to_phys(dst), virt_to_phys(&dst[800 * 480]));
+#endif
+
+#if 0
+	req.pat.act_w = 16;
+	req.pat.act_h = 16;
+	req.pat.vir_w = 16;
+	req.pat.vir_h = 16;
+	req.pat.yrgb_addr = virt_to_phys(src);
+	req.render_mode = 0;
+	rga2_blit_sync(&session, &req);
+#endif
+	{
+		uint32_t i, j;
+		uint8_t *sp;
+
+		sp = (uint8_t *)src;
+		for (j = 0; j < 240; j++) {
+			sp = (uint8_t *)src + j * 320 * 10 / 8;
+			for (i = 0; i < 320; i++) {
+				if ((i & 3) == 0) {
+					sp[i * 5 / 4] = 0;
+					sp[i * 5 / 4+1] = 0x1;
+				} else if ((i & 3) == 1) {
+					sp[i * 5 / 4+1] = 0x4;
+				} else if ((i & 3) == 2) {
+					sp[i * 5 / 4+1] = 0x10;
+				} else if ((i & 3) == 3) {
+					sp[i * 5 / 4+1] = 0x40;
+			    }
+			}
+		}
+		sp = (uint8_t *)src;
+		for (j = 0; j < 100; j++)
+			printk("src %.2x\n", sp[j]);
+	}
+	req.src.act_w = 320;
+	req.src.act_h = 240;
+
+	req.src.vir_w = 320;
+	req.src.vir_h = 240;
+	req.src.yrgb_addr = 0;//(uint32_t)virt_to_phys(src);
+	req.src.uv_addr = (unsigned long)virt_to_phys(src);
+	req.src.v_addr = 0;
+	req.src.format = RGA2_FORMAT_YCbCr_420_SP_10B;
+
+	req.dst.act_w  = 320;
+	req.dst.act_h = 240;
+	req.dst.x_offset = 0;
+	req.dst.y_offset = 0;
+
+	req.dst.vir_w = 320;
+	req.dst.vir_h = 240;
+
+	req.dst.yrgb_addr = 0;//((uint32_t)virt_to_phys(dst));
+	req.dst.uv_addr = (unsigned long)virt_to_phys(dst);
+	req.dst.format = RGA2_FORMAT_YCbCr_420_SP;
+
+	//dst = dst0;
+
+	//req.render_mode = color_fill_mode;
+	//req.fg_color = 0x80ffffff;
+
+	req.rotate_mode = 0;
+	req.scale_bicu_mode = 2;
+
+#if 0
+	//req.alpha_rop_flag = 0;
+	//req.alpha_rop_mode = 0x19;
+	//req.PD_mode = 3;
+
+	//req.mmu_info.mmu_flag = 0x21;
+	//req.mmu_info.mmu_en = 1;
+
+	//printk("src = %.8x\n", req.src.yrgb_addr);
+	//printk("src = %.8x\n", req.src.uv_addr);
+	//printk("dst = %.8x\n", req.dst.yrgb_addr);
+#endif
+
+	rga2_blit_sync(&session, &req);
+
+#if 0
+	uint32_t j;
+	for (j = 0; j < 320 * 240 * 10 / 8; j++) {
+        if (src[j] != dst[j])
+		printk("error value dst not equal src j %d, s %.2x d %.2x\n",
+			j, src[j], dst[j]);
+	}
+#endif
+
+#if 1
+	{
+		uint32_t j;
+		uint8_t *dp = (uint8_t *)dst;
+
+		for (j = 0; j < 100; j++)
+			printk("%d %.2x\n", j, dp[j]);
+	}
+#endif
+
+	kfree(src);
+	kfree(dst);
+}
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+#ifdef CONFIG_ROCKCHIP_THUNDER_BOOT
+module_init(rga2_init);
+#else
+late_initcall(rga2_init);
+#endif
+#else
+fs_initcall(rga2_init);
+#endif
+module_exit(rga2_exit);
+
+/* Module information */
+MODULE_AUTHOR("zsq@rock-chips.com");
+MODULE_DESCRIPTION("Driver for rga device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/video/rockchip/rga2/rga2_mmu_info.c b/drivers/video/rockchip/rga2/rga2_mmu_info.c
new file mode 100644
index 0000000000000..a01ba08e6ff89
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_mmu_info.c
@@ -0,0 +1,1843 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#define pr_fmt(fmt) "rga2_mmu: " fmt
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/pagemap.h>
+#include <linux/seq_file.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/memory.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <asm/memory.h>
+#include <asm/atomic.h>
+#include <asm/cacheflush.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 4, 0))
+#include <linux/rockchip_ion.h>
+#endif
+#include "rga2_mmu_info.h"
+#include "rga2_debugger.h"
+
+extern struct rga2_service_info rga2_service;
+extern struct rga2_mmu_buf_t rga2_mmu_buf;
+extern struct rga2_drvdata_t *rga2_drvdata;
+
+//extern int mmu_buff_temp[1024];
+
+#define KERNEL_SPACE_VALID    0xc0000000
+
+#define V7_VATOPA_SUCESS_MASK	(0x1)
+#define V7_VATOPA_GET_PADDR(X)	(X & 0xFFFFF000)
+#define V7_VATOPA_GET_INER(X)		((X>>4) & 7)
+#define V7_VATOPA_GET_OUTER(X)		((X>>2) & 3)
+#define V7_VATOPA_GET_SH(X)		((X>>7) & 1)
+#define V7_VATOPA_GET_NS(X)		((X>>9) & 1)
+#define V7_VATOPA_GET_SS(X)		((X>>1) & 1)
+
+void rga2_dma_flush_range(void *pstart, void *pend)
+{
+	dma_sync_single_for_device(rga2_drvdata->dev, virt_to_phys(pstart), pend - pstart, DMA_TO_DEVICE);
+}
+
+dma_addr_t rga2_dma_flush_page(struct page *page, int map)
+{
+	dma_addr_t paddr;
+
+	/*
+	 * Through dma_map_page to ensure that the physical address
+	 * will not exceed the addressing range of dma.
+	 */
+	if (map & MMU_MAP_MASK) {
+		switch (map) {
+		case MMU_MAP_CLEAN:
+			paddr = dma_map_page(rga2_drvdata->dev, page, 0,
+					     PAGE_SIZE, DMA_TO_DEVICE);
+			break;
+		case MMU_MAP_INVALID:
+			paddr = dma_map_page(rga2_drvdata->dev, page, 0,
+					     PAGE_SIZE, DMA_FROM_DEVICE);
+			break;
+		case MMU_MAP_CLEAN | MMU_MAP_INVALID:
+			paddr = dma_map_page(rga2_drvdata->dev, page, 0,
+					     PAGE_SIZE, DMA_BIDIRECTIONAL);
+			break;
+		default:
+			paddr = 0;
+			pr_err("unknown map cmd 0x%x\n", map);
+			break;
+		}
+
+		return paddr;
+	} else if (map & MMU_UNMAP_MASK) {
+		paddr = page_to_phys(page);
+
+		switch (map) {
+		case MMU_UNMAP_CLEAN:
+			dma_unmap_page(rga2_drvdata->dev, paddr,
+				       PAGE_SIZE, DMA_TO_DEVICE);
+			break;
+		case MMU_UNMAP_INVALID:
+			dma_unmap_page(rga2_drvdata->dev, paddr,
+				       PAGE_SIZE, DMA_FROM_DEVICE);
+			break;
+		case MMU_UNMAP_CLEAN | MMU_UNMAP_INVALID:
+			dma_unmap_page(rga2_drvdata->dev, paddr,
+				       PAGE_SIZE, DMA_BIDIRECTIONAL);
+			break;
+		default:
+			pr_err("unknown map cmd 0x%x\n", map);
+			break;
+		}
+
+		return paddr;
+	}
+
+	pr_err("RGA2 failed to flush page, map= %x\n", map);
+	return 0;
+}
+
+#if 0
+static unsigned int armv7_va_to_pa(unsigned int v_addr)
+{
+	unsigned int p_addr;
+	__asm__ volatile (	"mcr p15, 0, %1, c7, c8, 0\n"
+						"isb\n"
+						"dsb\n"
+						"mrc p15, 0, %0, c7, c4, 0\n"
+						: "=r" (p_addr)
+						: "r" (v_addr)
+						: "cc");
+
+	if (p_addr & V7_VATOPA_SUCESS_MASK)
+		return 0xFFFFFFFF;
+	else
+		return (V7_VATOPA_GET_SS(p_addr) ? 0xFFFFFFFF : V7_VATOPA_GET_PADDR(p_addr));
+}
+#endif
+
+static bool rga2_is_yuv422p_format(u32 format)
+{
+	bool ret = false;
+
+	switch (format) {
+	case RGA2_FORMAT_YCbCr_422_P:
+	case RGA2_FORMAT_YCrCb_422_P:
+		ret = true;
+		break;
+	}
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+static int rga2_get_format_bits(u32 format)
+{
+	int bits = 0;
+
+	switch (format) {
+	case RGA2_FORMAT_RGBA_8888:
+	case RGA2_FORMAT_RGBX_8888:
+	case RGA2_FORMAT_BGRA_8888:
+	case RGA2_FORMAT_BGRX_8888:
+	case RGA2_FORMAT_ARGB_8888:
+	case RGA2_FORMAT_XRGB_8888:
+	case RGA2_FORMAT_ABGR_8888:
+	case RGA2_FORMAT_XBGR_8888:
+		bits = 32;
+		break;
+	case RGA2_FORMAT_RGB_888:
+	case RGA2_FORMAT_BGR_888:
+		bits = 24;
+		break;
+	case RGA2_FORMAT_RGB_565:
+	case RGA2_FORMAT_RGBA_5551:
+	case RGA2_FORMAT_RGBA_4444:
+	case RGA2_FORMAT_BGR_565:
+	case RGA2_FORMAT_YCbCr_422_SP:
+	case RGA2_FORMAT_YCbCr_422_P:
+	case RGA2_FORMAT_YCrCb_422_SP:
+	case RGA2_FORMAT_YCrCb_422_P:
+	case RGA2_FORMAT_BGRA_5551:
+	case RGA2_FORMAT_BGRA_4444:
+	case RGA2_FORMAT_ARGB_5551:
+	case RGA2_FORMAT_ARGB_4444:
+	case RGA2_FORMAT_ABGR_5551:
+	case RGA2_FORMAT_ABGR_4444:
+		bits = 16;
+		break;
+	case RGA2_FORMAT_YCbCr_420_SP:
+	case RGA2_FORMAT_YCbCr_420_P:
+	case RGA2_FORMAT_YCrCb_420_SP:
+	case RGA2_FORMAT_YCrCb_420_P:
+		bits = 12;
+		break;
+	case RGA2_FORMAT_YCbCr_420_SP_10B:
+	case RGA2_FORMAT_YCrCb_420_SP_10B:
+	case RGA2_FORMAT_YCbCr_422_SP_10B:
+	case RGA2_FORMAT_YCrCb_422_SP_10B:
+		bits = 15;
+		break;
+	default:
+		pr_err("unknown format [%d]\n", format);
+		return -1;
+	}
+
+	return bits;
+}
+static int rga2_user_memory_check(struct page **pages, u32 w, u32 h, u32 format, int flag)
+{
+	int bits;
+	void *vaddr = NULL;
+	int taipage_num;
+	int taidata_num;
+	int *tai_vaddr = NULL;
+
+	bits = rga2_get_format_bits(format);
+	if (bits < 0)
+		return -1;
+
+	taipage_num = w * h * bits / 8 / (1024 * 4);
+	taidata_num = w * h * bits / 8 % (1024 * 4);
+	if (taidata_num == 0) {
+		vaddr = kmap(pages[taipage_num - 1]);
+		tai_vaddr = (int *)vaddr + 1023;
+	} else {
+		vaddr = kmap(pages[taipage_num]);
+		tai_vaddr = (int *)vaddr + taidata_num / 4 - 1;
+	}
+
+	if (flag == 1) {
+		pr_info("src user memory check\n");
+		pr_info("tai data is %d\n", *tai_vaddr);
+	} else {
+		pr_info("dst user memory check\n");
+		pr_info("tai data is %d\n", *tai_vaddr);
+	}
+
+	if (taidata_num == 0)
+		kunmap(pages[taipage_num - 1]);
+	else
+		kunmap(pages[taipage_num]);
+
+	return 0;
+}
+
+static int rga2_virtual_memory_check(void *vaddr, u32 w, u32 h, u32 format, int fd)
+{
+	int bits = 32;
+	int temp_data = 0;
+	void *one_line = NULL;
+
+	bits = rga2_get_format_bits(format);
+	if (bits < 0)
+		return -1;
+
+	one_line = kzalloc(w * 4, GFP_KERNEL);
+	if (!one_line) {
+		ERR("kzalloc fail %s[%d]\n", __func__, __LINE__);
+		return 0;
+	}
+
+	temp_data = w * (h - 1) * bits >> 3;
+	if (fd > 0) {
+		INFO("vaddr is%p, bits is %d, fd check\n", vaddr, bits);
+		memcpy(one_line, (char *)vaddr + temp_data, w * bits >> 3);
+		INFO("fd check ok\n");
+	} else {
+		INFO("vir addr memory check.\n");
+		memcpy((void *)((char *)vaddr + temp_data), one_line,
+		       w * bits >> 3);
+		INFO("vir addr check ok.\n");
+	}
+
+	kfree(one_line);
+	return 0;
+}
+
+static int rga2_dma_memory_check(struct rga_dma_buffer_t *buffer,
+				 struct rga_img_info_t *img)
+{
+	int ret = 0;
+	void *vaddr;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	struct iosys_map map;
+#endif
+	struct dma_buf *dma_buffer;
+
+	dma_buffer = buffer->dma_buf;
+
+	if (!IS_ERR_OR_NULL(dma_buffer)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		ret = dma_buf_vmap(dma_buffer, &map);
+		vaddr = ret ? NULL : map.vaddr;
+#else
+		vaddr = dma_buf_vmap(dma_buffer);
+#endif
+		if (vaddr) {
+			ret = rga2_virtual_memory_check(vaddr, img->vir_w, img->vir_h,
+							img->format, img->yrgb_addr);
+		} else {
+			pr_err("can't vmap the dma buffer!\n");
+			return -EINVAL;
+		}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		dma_buf_vunmap(dma_buffer, &map);
+#else
+		dma_buf_vunmap(dma_buffer, vaddr);
+#endif
+	}
+
+	return ret;
+}
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static int rga2_map_dma_buffer(int fd,
+			       struct rga_dma_buffer_t *rga_dma_buffer,
+			       enum dma_data_direction dir)
+{
+	struct device *rga_dev = NULL;
+	struct dma_buf *dma_buf = NULL;
+	struct dma_buf_attachment *attach = NULL;
+	struct sg_table *sgt = NULL;
+	int ret = 0;
+
+	rga_dev = rga2_drvdata->dev;
+
+	dma_buf = dma_buf_get(fd);
+	if (IS_ERR(dma_buf)) {
+		ret = -EINVAL;
+		pr_err("dma_buf_get fail fd[%d]\n", fd);
+		return ret;
+	}
+
+	attach = dma_buf_attach(dma_buf, rga_dev);
+	if (IS_ERR(attach)) {
+		ret = -EINVAL;
+		pr_err("Failed to attach dma_buf\n");
+		goto err_get_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, dir);
+	if (IS_ERR(sgt)) {
+		ret = -EINVAL;
+		pr_err("Failed to map src attachment\n");
+		goto err_get_sgt;
+	}
+
+	rga_dma_buffer->dma_buf = dma_buf;
+	rga_dma_buffer->attach = attach;
+	rga_dma_buffer->sgt = sgt;
+	rga_dma_buffer->size = sg_dma_len(sgt->sgl);
+	rga_dma_buffer->dir = dir;
+
+	return ret;
+
+err_get_sgt:
+	if (attach)
+		dma_buf_detach(dma_buf, attach);
+err_get_attach:
+	if (dma_buf)
+		dma_buf_put(dma_buf);
+
+	return ret;
+}
+
+static void rga2_unmap_dma_buffer(struct rga_dma_buffer_t *rga_dma_buffer)
+{
+	if (rga_dma_buffer->attach && rga_dma_buffer->sgt)
+		dma_buf_unmap_attachment(rga_dma_buffer->attach,
+					 rga_dma_buffer->sgt,
+					 rga_dma_buffer->dir);
+	if (rga_dma_buffer->attach) {
+		dma_buf_detach(rga_dma_buffer->dma_buf, rga_dma_buffer->attach);
+		dma_buf_put(rga_dma_buffer->dma_buf);
+	}
+}
+
+static void rga2_convert_addr(struct rga_img_info_t *img)
+{
+	/*
+	 * If it is not using dma fd, the virtual/phyical address is assigned
+	 * to the address of the corresponding channel.
+	 */
+	img->yrgb_addr = img->uv_addr;
+	img->uv_addr = img->yrgb_addr + (img->vir_w * img->vir_h);
+	if (rga2_is_yuv422p_format(img->format))
+		img->v_addr = img->uv_addr + (img->vir_w * img->vir_h) / 2;
+	else
+		img->v_addr = img->uv_addr + (img->vir_w * img->vir_h) / 4;
+}
+
+int rga2_get_dma_info(struct rga2_reg *reg, struct rga2_req *req)
+{
+	uint32_t mmu_flag;
+	int ret;
+
+	struct rga_dma_buffer_t *buffer_src0, *buffer_src1, *buffer_dst, *buffer_els;
+	struct rga_img_info_t *src0, *src1, *dst, *els;
+
+	/*
+	 * Since the life cycle of rga2_req cannot satisfy the release of
+	 * dmabuffer after the task is over, the mapped dmabuffer is saved
+	 * in rga2_reg.
+	 */
+	buffer_src0 = &reg->dma_buffer_src0;
+	buffer_src1 = &reg->dma_buffer_src1;
+	buffer_dst = &reg->dma_buffer_dst;
+	buffer_els = &reg->dma_buffer_els;
+
+	src0 = &req->src;
+	src1 = &req->src1;
+	dst = &req->dst;
+	els = &req->pat;
+
+	/* src0 chanel */
+	mmu_flag = req->mmu_info.src0_mmu_flag;
+	if (unlikely(!mmu_flag && src0->yrgb_addr)) {
+		pr_err("Fix it please enable src0 mmu\n");
+		return -EINVAL;
+	} else if (mmu_flag && src0->yrgb_addr) {
+		ret = rga2_map_dma_buffer(src0->yrgb_addr, buffer_src0, DMA_BIDIRECTIONAL);
+		if (ret < 0) {
+			pr_err("src0: can't map dma-buf\n");
+			return ret;
+		}
+	}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		ret = rga2_dma_memory_check(buffer_src0, src0);
+		if (ret < 0) {
+			pr_err("src0 channel check memory error!\n");
+			return ret;
+		}
+	}
+#endif
+	rga2_convert_addr(src0);
+
+	/* src1 chanel */
+	mmu_flag = req->mmu_info.src1_mmu_flag;
+	if (unlikely(!mmu_flag && src1->yrgb_addr)) {
+		pr_err("Fix it please enable src1 mmu\n");
+		ret = -EINVAL;
+		goto err_src1_channel;
+	} else if (mmu_flag && src1->yrgb_addr) {
+		ret = rga2_map_dma_buffer(src1->yrgb_addr, buffer_src1, DMA_BIDIRECTIONAL);
+		if (ret < 0) {
+			pr_err("src1: can't map dma-buf\n");
+			goto err_src1_channel;
+		}
+	}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		ret = rga2_dma_memory_check(buffer_src1, src1);
+		if (ret < 0) {
+			pr_err("src1 channel check memory error!\n");
+			goto err_src1_channel;
+		}
+	}
+#endif
+	rga2_convert_addr(src1);
+
+	/* dst chanel */
+	mmu_flag = req->mmu_info.dst_mmu_flag;
+	if (unlikely(!mmu_flag && dst->yrgb_addr)) {
+		pr_err("Fix it please enable dst mmu\n");
+		ret = -EINVAL;
+		goto err_dst_channel;
+	} else if (mmu_flag && dst->yrgb_addr) {
+		ret = rga2_map_dma_buffer(dst->yrgb_addr, buffer_dst, DMA_BIDIRECTIONAL);
+		if (ret < 0) {
+			pr_err("dst: can't map dma-buf\n");
+			goto err_dst_channel;
+		}
+	}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		ret = rga2_dma_memory_check(buffer_dst, dst);
+		if (ret < 0) {
+			pr_err("dst channel check memory error!\n");
+			goto err_dst_channel;
+		}
+	}
+#endif
+	rga2_convert_addr(dst);
+
+	/* els chanel */
+	mmu_flag = req->mmu_info.els_mmu_flag;
+	if (unlikely(!mmu_flag && els->yrgb_addr)) {
+		pr_err("Fix it please enable els mmu\n");
+		ret = -EINVAL;
+		goto err_els_channel;
+	} else if (mmu_flag && els->yrgb_addr) {
+		ret = rga2_map_dma_buffer(els->yrgb_addr, buffer_els, DMA_BIDIRECTIONAL);
+		if (ret < 0) {
+			pr_err("els: can't map dma-buf\n");
+			goto err_els_channel;
+		}
+	}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		ret = rga2_dma_memory_check(buffer_els, els);
+		if (ret < 0) {
+			pr_err("els channel check memory error!\n");
+			goto err_els_channel;
+		}
+	}
+#endif
+	rga2_convert_addr(els);
+
+	return 0;
+
+err_els_channel:
+	rga2_unmap_dma_buffer(buffer_dst);
+err_dst_channel:
+	rga2_unmap_dma_buffer(buffer_src1);
+err_src1_channel:
+	rga2_unmap_dma_buffer(buffer_src0);
+
+	return ret;
+}
+
+void rga2_put_dma_info(struct rga2_reg *reg)
+{
+	rga2_unmap_dma_buffer(&reg->dma_buffer_src0);
+	rga2_unmap_dma_buffer(&reg->dma_buffer_src1);
+	rga2_unmap_dma_buffer(&reg->dma_buffer_dst);
+	rga2_unmap_dma_buffer(&reg->dma_buffer_els);
+}
+#else
+static int rga2_get_dma_info(struct rga2_reg *reg, struct rga2_req *req)
+{
+	struct ion_handle *hdl;
+	ion_phys_addr_t phy_addr;
+	size_t len;
+	int ret;
+	u32 src_vir_w, dst_vir_w;
+	void *vaddr = NULL;
+	struct rga_dma_buffer_t *buffer_src0, *buffer_src1, *buffer_dst, *buffer_els;
+
+	src_vir_w = req->src.vir_w;
+	dst_vir_w = req->dst.vir_w;
+
+	buffer_src0 = &reg->dma_buffer_src0;
+	buffer_src1 = &reg->dma_buffer_src1;
+	buffer_dst = &reg->dma_buffer_dst;
+	buffer_els = &reg->dma_buffer_els;
+
+	if ((int)req->src.yrgb_addr > 0) {
+		hdl = ion_import_dma_buf(rga2_drvdata->ion_client,
+					 req->src.yrgb_addr);
+		if (IS_ERR(hdl)) {
+			ret = PTR_ERR(hdl);
+			pr_err("RGA2 SRC ERROR ion buf handle\n");
+			return ret;
+		}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		vaddr = ion_map_kernel(rga2_drvdata->ion_client, hdl);
+		if (vaddr)
+			rga2_memory_check(vaddr, req->src.vir_w, req->src.vir_h,
+					  req->src.format, req->src.yrgb_addr);
+		ion_unmap_kernel(rga2_drvdata->ion_client, hdl);
+	}
+#endif
+		if (req->mmu_info.src0_mmu_flag) {
+			buffer_src0.sgt =
+				ion_sg_table(rga2_drvdata->ion_client, hdl);
+			req->src.yrgb_addr = req->src.uv_addr;
+			req->src.uv_addr =
+				req->src.yrgb_addr + (src_vir_w * req->src.vir_h);
+			req->src.v_addr =
+				req->src.uv_addr + (src_vir_w * req->src.vir_h) / 4;
+		} else {
+			ion_phys(rga2_drvdata->ion_client, hdl, &phy_addr, &len);
+			req->src.yrgb_addr = phy_addr;
+			req->src.uv_addr =
+				req->src.yrgb_addr + (src_vir_w * req->src.vir_h);
+			req->src.v_addr =
+				req->src.uv_addr + (src_vir_w * req->src.vir_h) / 4;
+		}
+		ion_free(rga2_drvdata->ion_client, hdl);
+	} else {
+		req->src.yrgb_addr = req->src.uv_addr;
+		req->src.uv_addr =
+			req->src.yrgb_addr + (src_vir_w * req->src.vir_h);
+		req->src.v_addr =
+			req->src.uv_addr + (src_vir_w * req->src.vir_h) / 4;
+	}
+
+	if ((int)req->dst.yrgb_addr > 0) {
+		hdl = ion_import_dma_buf(rga2_drvdata->ion_client,
+					 req->dst.yrgb_addr);
+		if (IS_ERR(hdl)) {
+			ret = PTR_ERR(hdl);
+			pr_err("RGA2 DST ERROR ion buf handle\n");
+			return ret;
+		}
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+	if (RGA2_CHECK_MODE) {
+		vaddr = ion_map_kernel(rga2_drvdata->ion_client, hdl);
+		if (vaddr)
+			rga2_memory_check(vaddr, req->dst.vir_w, req->dst.vir_h,
+					  req->dst.format, req->dst.yrgb_addr);
+		ion_unmap_kernel(rga2_drvdata->ion_client, hdl);
+	}
+#endif
+		if (req->mmu_info.dst_mmu_flag) {
+			buffer_dst.sgt =
+				ion_sg_table(rga2_drvdata->ion_client, hdl);
+			req->dst.yrgb_addr = req->dst.uv_addr;
+			req->dst.uv_addr =
+				req->dst.yrgb_addr + (dst_vir_w * req->dst.vir_h);
+			req->dst.v_addr =
+				req->dst.uv_addr + (dst_vir_w * req->dst.vir_h) / 4;
+		} else {
+			ion_phys(rga2_drvdata->ion_client, hdl, &phy_addr, &len);
+			req->dst.yrgb_addr = phy_addr;
+			req->dst.uv_addr =
+				req->dst.yrgb_addr + (dst_vir_w * req->dst.vir_h);
+			req->dst.v_addr =
+				req->dst.uv_addr + (dst_vir_w * req->dst.vir_h) / 4;
+		}
+		ion_free(rga2_drvdata->ion_client, hdl);
+	} else {
+		req->dst.yrgb_addr = req->dst.uv_addr;
+		req->dst.uv_addr =
+			req->dst.yrgb_addr + (dst_vir_w * req->dst.vir_h);
+		req->dst.v_addr =
+			req->dst.uv_addr + (dst_vir_w * req->dst.vir_h) / 4;
+	}
+
+	if ((int)req->src1.yrgb_addr > 0) {
+		hdl = ion_import_dma_buf(rga2_drvdata->ion_client,
+					 req->src1.yrgb_addr);
+		if (IS_ERR(hdl)) {
+			ret = PTR_ERR(hdl);
+			pr_err("RGA2 ERROR ion buf handle\n");
+			return ret;
+		}
+		if (req->mmu_info.dst_mmu_flag) {
+			buffer_src1.sgt =
+				ion_sg_table(rga2_drvdata->ion_client, hdl);
+			req->src1.yrgb_addr = req->src1.uv_addr;
+			req->src1.uv_addr =
+				req->src1.yrgb_addr + (req->src1.vir_w * req->src1.vir_h);
+			req->src1.v_addr =
+				req->src1.uv_addr + (req->src1.vir_w * req->src1.vir_h) / 4;
+		} else {
+			ion_phys(rga2_drvdata->ion_client, hdl, &phy_addr, &len);
+			req->src1.yrgb_addr = phy_addr;
+			req->src1.uv_addr =
+				req->src1.yrgb_addr + (req->src1.vir_w * req->src1.vir_h);
+			req->src1.v_addr =
+				req->src1.uv_addr + (req->src1.vir_w * req->src1.vir_h) / 4;
+		}
+		ion_free(rga2_drvdata->ion_client, hdl);
+	} else {
+		req->src1.yrgb_addr = req->src1.uv_addr;
+		req->src1.uv_addr =
+			req->src1.yrgb_addr + (req->src1.vir_w * req->src1.vir_h);
+		req->src1.v_addr =
+			req->src1.uv_addr + (req->src1.vir_w * req->src1.vir_h) / 4;
+	}
+	if (rga2_is_yuv422p_format(req->src.format))
+		req->src.v_addr = req->src.uv_addr + (req->src.vir_w * req->src.vir_h) / 2;
+	if (rga2_is_yuv422p_format(req->dst.format))
+		req->dst.v_addr = req->dst.uv_addr + (req->dst.vir_w * req->dst.vir_h) / 2;
+	if (rga2_is_yuv422p_format(req->src1.format))
+		req->src1.v_addr = req->src1.uv_addr + (req->src1.vir_w * req->dst.vir_h) / 2;
+
+	return 0;
+}
+
+/* When the kernel version is lower than 4.4, no put buffer operation is required. */
+void rga2_put_dma_info(struct rga2_reg *reg) {}
+#endif
+
+static int rga2_mmu_buf_get(struct rga2_mmu_buf_t *t, uint32_t size)
+{
+    mutex_lock(&rga2_service.lock);
+    t->front += size;
+    mutex_unlock(&rga2_service.lock);
+
+    return 0;
+}
+
+static int rga2_mmu_buf_get_try(struct rga2_mmu_buf_t *t, uint32_t size)
+{
+	int ret = 0;
+
+	mutex_lock(&rga2_service.lock);
+	if ((t->back - t->front) > t->size) {
+		if (t->front + size > t->back - t->size) {
+			pr_info("front %d, back %d dsize %d size %d",
+				t->front, t->back, t->size, size);
+			ret = -ENOMEM;
+			goto out;
+		}
+	} else {
+		if ((t->front + size) > t->back) {
+			pr_info("front %d, back %d dsize %d size %d",
+				t->front, t->back, t->size, size);
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		if (t->front + size > t->size) {
+			if (size > (t->back - t->size)) {
+				pr_info("front %d, back %d dsize %d size %d",
+					t->front, t->back, t->size, size);
+				ret = -ENOMEM;
+				goto out;
+			}
+			t->front = 0;
+		}
+	}
+out:
+	mutex_unlock(&rga2_service.lock);
+	return ret;
+}
+
+static int rga2_mem_size_cal(unsigned long Mem, uint32_t MemSize, unsigned long *StartAddr)
+{
+    unsigned long start, end;
+    uint32_t pageCount;
+
+    end = (Mem + (MemSize + PAGE_SIZE - 1)) >> PAGE_SHIFT;
+    start = Mem >> PAGE_SHIFT;
+    pageCount = end - start;
+    *StartAddr = start;
+    return pageCount;
+}
+
+static int rga2_buf_size_cal(unsigned long yrgb_addr, unsigned long uv_addr, unsigned long v_addr,
+                                        int format, uint32_t w, uint32_t h, unsigned long *StartAddr )
+{
+    uint32_t size_yrgb = 0;
+    uint32_t size_uv = 0;
+    uint32_t size_v = 0;
+    uint32_t stride = 0;
+    unsigned long start, end;
+    uint32_t pageCount;
+
+    switch(format)
+    {
+        case RGA2_FORMAT_RGBA_8888 :
+        case RGA2_FORMAT_RGBX_8888 :
+        case RGA2_FORMAT_BGRA_8888 :
+        case RGA2_FORMAT_BGRX_8888 :
+        case RGA2_FORMAT_ARGB_8888 :
+        case RGA2_FORMAT_XRGB_8888 :
+        case RGA2_FORMAT_ABGR_8888 :
+        case RGA2_FORMAT_XBGR_8888 :
+            stride = (w * 4 + 3) & (~3);
+            size_yrgb = stride*h;
+            start = yrgb_addr >> PAGE_SHIFT;
+	    end = yrgb_addr + size_yrgb;
+	    end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	    pageCount = end - start;
+            break;
+        case RGA2_FORMAT_RGB_888 :
+        case RGA2_FORMAT_BGR_888 :
+            stride = (w * 3 + 3) & (~3);
+            size_yrgb = stride*h;
+            start = yrgb_addr >> PAGE_SHIFT;
+	    end = yrgb_addr + size_yrgb;
+	    end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	    pageCount = end - start;
+            break;
+        case RGA2_FORMAT_RGB_565 :
+        case RGA2_FORMAT_RGBA_5551 :
+        case RGA2_FORMAT_RGBA_4444 :
+        case RGA2_FORMAT_BGR_565 :
+        case RGA2_FORMAT_BGRA_5551 :
+        case RGA2_FORMAT_BGRA_4444 :
+        case RGA2_FORMAT_ARGB_5551 :
+        case RGA2_FORMAT_ARGB_4444 :
+        case RGA2_FORMAT_ABGR_5551 :
+        case RGA2_FORMAT_ABGR_4444 :
+            stride = (w*2 + 3) & (~3);
+            size_yrgb = stride * h;
+            start = yrgb_addr >> PAGE_SHIFT;
+	    end = yrgb_addr + size_yrgb;
+	    end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	    pageCount = end - start;
+            break;
+
+        /* YUV FORMAT */
+        case RGA2_FORMAT_YCbCr_422_SP :
+        case RGA2_FORMAT_YCrCb_422_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = stride * h;
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RGA2_FORMAT_YCbCr_422_P :
+        case RGA2_FORMAT_YCrCb_422_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * h);
+            size_v = ((stride >> 1) * h);
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start = start >> PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RGA2_FORMAT_YCbCr_420_SP :
+        case RGA2_FORMAT_YCrCb_420_SP :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = (stride * (h >> 1));
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        case RGA2_FORMAT_YCbCr_420_P :
+        case RGA2_FORMAT_YCrCb_420_P :
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = ((stride >> 1) * (h >> 1));
+            size_v = ((stride >> 1) * (h >> 1));
+            start = MIN(MIN(yrgb_addr, uv_addr), v_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX(MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv)), (v_addr + size_v));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+		case RGA2_FORMAT_YCbCr_400:
+			stride = (w + 3) & (~3);
+			size_yrgb = stride * h;
+			size_uv = 0;
+			size_v = 0;
+			start = yrgb_addr >> PAGE_SHIFT;
+			end = yrgb_addr + size_yrgb;
+			end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+			pageCount = end - start;
+			break;
+		case RGA2_FORMAT_Y4:
+			stride = ((w + 3) & (~3) ) >> 1;
+			size_yrgb = stride * h;
+			size_uv = 0;
+			size_v = 0;
+			start = yrgb_addr >> PAGE_SHIFT;
+			end = yrgb_addr + size_yrgb;
+			end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+			pageCount = end - start;
+			break;
+		case RGA2_FORMAT_YVYU_422:
+		case RGA2_FORMAT_VYUY_422:
+		case RGA2_FORMAT_YUYV_422:
+		case RGA2_FORMAT_UYVY_422:
+			stride = (w + 3) & (~3);
+			size_yrgb = stride * h;
+			size_uv = stride * h;
+			start = MIN(yrgb_addr, uv_addr);
+			start >>= PAGE_SHIFT;
+			end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+			end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+			pageCount = end - start;
+			break;
+		case RGA2_FORMAT_YVYU_420:
+		case RGA2_FORMAT_VYUY_420:
+		case RGA2_FORMAT_YUYV_420:
+		case RGA2_FORMAT_UYVY_420:
+			stride = (w + 3) & (~3);
+			size_yrgb = stride * h;
+			size_uv = (stride * (h >> 1));
+			start = MIN(yrgb_addr, uv_addr);
+			start >>= PAGE_SHIFT;
+			end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+			end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+			pageCount = end - start;
+			break;
+#if 0
+        case RK_FORMAT_BPP1 :
+            break;
+        case RK_FORMAT_BPP2 :
+            break;
+        case RK_FORMAT_BPP4 :
+            break;
+        case RK_FORMAT_BPP8 :
+            break;
+#endif
+        case RGA2_FORMAT_YCbCr_420_SP_10B:
+        case RGA2_FORMAT_YCrCb_420_SP_10B:
+            stride = (w + 3) & (~3);
+            size_yrgb = stride * h;
+            size_uv = (stride * (h >> 1));
+            start = MIN(yrgb_addr, uv_addr);
+            start >>= PAGE_SHIFT;
+            end = MAX((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+            end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+            pageCount = end - start;
+            break;
+        default :
+            pageCount = 0;
+            start = 0;
+            break;
+    }
+
+    *StartAddr = start;
+    return pageCount;
+}
+
+static int rga2_MapUserMemory(struct page **pages, uint32_t *pageTable,
+			      unsigned long Memory, uint32_t pageCount,
+			      int writeFlag, int map)
+{
+	struct vm_area_struct *vma;
+	int32_t result;
+	uint32_t i;
+	uint32_t status;
+	unsigned long Address;
+	unsigned long pfn;
+	struct page __maybe_unused *page;
+	spinlock_t * ptl;
+	pte_t * pte;
+	pgd_t * pgd;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	p4d_t * p4d;
+#endif
+	pud_t * pud;
+	pmd_t * pmd;
+
+	status = 0;
+	Address = 0;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_lock(current->mm);
+#else
+	down_read(&current->mm->mmap_sem);
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 168) && LINUX_VERSION_CODE < KERNEL_VERSION(4, 5, 0)
+	result = get_user_pages(current, current->mm, Memory << PAGE_SHIFT,
+				pageCount, writeFlag ? FOLL_WRITE : 0,
+				pages, NULL);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
+	result = get_user_pages(current, current->mm, Memory << PAGE_SHIFT,
+				pageCount, writeFlag, 0, pages, NULL);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(5, 10, 0)
+	result = get_user_pages_remote(current, current->mm,
+				       Memory << PAGE_SHIFT,
+				       pageCount, writeFlag, pages, NULL, NULL);
+#else
+	result = get_user_pages_remote(current->mm, Memory << PAGE_SHIFT,
+				       pageCount, writeFlag, pages, NULL, NULL);
+#endif
+
+	if (result > 0 && result >= pageCount) {
+		/* Fill the page table. */
+		for (i = 0; i < pageCount; i++) {
+			/* Get the physical address from page struct. */
+			pageTable[i] = rga2_dma_flush_page(pages[i], map);
+		}
+
+		for (i = 0; i < result; i++)
+			put_page(pages[i]);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+		mmap_read_unlock(current->mm);
+#else
+		up_read(&current->mm->mmap_sem);
+#endif
+		return 0;
+	}
+	if (result > 0) {
+		for (i = 0; i < result; i++)
+			put_page(pages[i]);
+	}
+	for (i = 0; i < pageCount; i++) {
+		vma = find_vma(current->mm, (Memory + i) << PAGE_SHIFT);
+		if (!vma) {
+			pr_err("RGA2 failed to get vma, result = %d, pageCount = %d\n",
+			       result, pageCount);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+		pgd = pgd_offset(current->mm, (Memory + i) << PAGE_SHIFT);
+		if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd))) {
+			pr_err("RGA2 failed to get pgd, result = %d, pageCount = %d\n",
+			       result, pageCount);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+		/* In the four-level page table, it will do nothing and return pgd. */
+		p4d = p4d_offset(pgd, (Memory + i) << PAGE_SHIFT);
+		if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d))) {
+			pr_err("RGA2 failed to get p4d, result = %d, pageCount = %d\n",
+			       result, pageCount);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+
+		pud = pud_offset(p4d, (Memory + i) << PAGE_SHIFT);
+#else
+		pud = pud_offset(pgd, (Memory + i) << PAGE_SHIFT);
+#endif
+		if (pud_none(*pud) || unlikely(pud_bad(*pud))) {
+			pr_err("RGA2 failed to get pud, result = %d, pageCount = %d\n",
+			       result, pageCount);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+		pmd = pmd_offset(pud, (Memory + i) << PAGE_SHIFT);
+		if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd))) {
+			pr_err("RGA2 failed to get pmd, result = %d, pageCount = %d\n",
+			       result, pageCount);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+		pte = pte_offset_map_lock(current->mm, pmd,
+					  (Memory + i) << PAGE_SHIFT,
+					  &ptl);
+		if (pte_none(*pte)) {
+			pr_err("RGA2 failed to get pte, result = %d, pageCount = %d\n",
+				result, pageCount);
+			pte_unmap_unlock(pte, ptl);
+			status = RGA2_OUT_OF_RESOURCES;
+			break;
+		}
+		pfn = pte_pfn(*pte);
+		Address = ((pfn << PAGE_SHIFT) |
+			  (((unsigned long)((Memory + i) << PAGE_SHIFT)) & ~PAGE_MASK));
+
+		pageTable[i] = rga2_dma_flush_page(phys_to_page(Address), map);
+
+		pte_unmap_unlock(pte, ptl);
+	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_unlock(current->mm);
+#else
+	up_read(&current->mm->mmap_sem);
+#endif
+	return status;
+}
+
+static int rga2_MapION(struct sg_table *sg,
+                               uint32_t *Memory,
+                               int32_t  pageCount)
+{
+    uint32_t i;
+    uint32_t status;
+    unsigned long Address;
+    uint32_t mapped_size = 0;
+    uint32_t len;
+    struct scatterlist *sgl = sg->sgl;
+    uint32_t sg_num = 0;
+    uint32_t break_flag = 0;
+
+    status = 0;
+    Address = 0;
+    do {
+        len = sg_dma_len(sgl) >> PAGE_SHIFT;
+	/*
+	 * The fd passed by user space gets sg through dma_buf_map_attachment,
+	 * so dma_address can be use here.
+	 */
+        Address = sg_dma_address(sgl);
+
+        for(i=0; i<len; i++) {
+            if (mapped_size + i >= pageCount) {
+                break_flag = 1;
+                break;
+            }
+            Memory[mapped_size + i] = (uint32_t)(Address + (i << PAGE_SHIFT));
+        }
+        if (break_flag)
+            break;
+        mapped_size += len;
+        sg_num += 1;
+    }
+    while((sgl = sg_next(sgl)) && (mapped_size < pageCount) && (sg_num < sg->nents));
+
+    return 0;
+}
+
+static int rga2_mmu_flush_cache(struct rga2_reg *reg, struct rga2_req *req)
+{
+	int DstMemSize;
+	unsigned long DstStart, DstPageCount;
+	uint32_t *MMU_Base, *MMU_Base_phys;
+	int ret;
+	int status;
+	struct page **pages = NULL;
+	struct rga_dma_buffer_t *dma_buffer = NULL;
+
+	MMU_Base = NULL;
+	DstMemSize  = 0;
+	DstPageCount = 0;
+	DstStart = 0;
+
+	if (reg->MMU_map != true) {
+		status = -EINVAL;
+		goto out;
+	}
+
+	/* cal dst buf mmu info */
+	if (req->mmu_info.dst_mmu_flag & 1) {
+		DstPageCount = rga2_buf_size_cal(req->dst.yrgb_addr,
+						 req->dst.uv_addr,
+						 req->dst.v_addr,
+						 req->dst.format,
+						 req->dst.vir_w,
+						 req->dst.vir_h,
+						 &DstStart);
+		if (DstPageCount == 0)
+			return -EINVAL;
+	}
+	/* Cal out the needed mem size */
+	DstMemSize  = (DstPageCount + 15) & (~15);
+
+	if (rga2_mmu_buf_get_try(&rga2_mmu_buf, DstMemSize)) {
+		pr_err("RGA2 Get MMU mem failed\n");
+		status = RGA2_MALLOC_ERROR;
+		goto out;
+	}
+	pages = rga2_mmu_buf.pages;
+	mutex_lock(&rga2_service.lock);
+	MMU_Base = rga2_mmu_buf.buf_virtual +
+		   (rga2_mmu_buf.front & (rga2_mmu_buf.size - 1));
+	MMU_Base_phys = rga2_mmu_buf.buf +
+			(rga2_mmu_buf.front & (rga2_mmu_buf.size - 1));
+
+	mutex_unlock(&rga2_service.lock);
+	if (DstMemSize) {
+		dma_buffer = &reg->dma_buffer_dst;
+		if (dma_buffer->sgt) {
+			status = -EINVAL;
+			goto out;
+		} else {
+			ret = rga2_MapUserMemory(&pages[0],
+						 MMU_Base,
+						 DstStart, DstPageCount, 1,
+						 MMU_MAP_CLEAN | MMU_MAP_INVALID);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_CHECK_MODE)
+				rga2_user_memory_check(&pages[0],
+						       req->dst.vir_w,
+						       req->dst.vir_h,
+						       req->dst.format,
+						       2);
+#endif
+		}
+		if (ret < 0) {
+			pr_err("rga2 unmap dst memory failed\n");
+			status = ret;
+			goto out;
+		}
+	}
+	rga2_mmu_buf_get(&rga2_mmu_buf, DstMemSize);
+	reg->MMU_len = DstMemSize;
+	status = 0;
+out:
+	return status;
+}
+
+static int rga2_mmu_info_BitBlt_mode(struct rga2_reg *reg, struct rga2_req *req)
+{
+	int Src0MemSize, DstMemSize, Src1MemSize;
+	unsigned long Src0Start, Src1Start, DstStart;
+	unsigned long Src0PageCount, Src1PageCount, DstPageCount;
+	uint32_t AllSize;
+	uint32_t *MMU_Base, *MMU_Base_phys;
+	int ret;
+	int status;
+	uint32_t uv_size, v_size;
+	struct page **pages = NULL;
+	struct rga_dma_buffer_t *dma_buffer = NULL;
+
+	MMU_Base = NULL;
+	Src0MemSize = 0;
+	Src1MemSize = 0;
+	DstMemSize  = 0;
+	Src0PageCount = 0;
+	Src1PageCount = 0;
+	DstPageCount = 0;
+	Src0Start = 0;
+	Src1Start = 0;
+	DstStart = 0;
+
+	/* cal src0 buf mmu info */
+	if (req->mmu_info.src0_mmu_flag & 1) {
+		Src0PageCount = rga2_buf_size_cal(req->src.yrgb_addr,
+						  req->src.uv_addr,
+						  req->src.v_addr,
+						  req->src.format,
+						  req->src.vir_w,
+						  (req->src.vir_h),
+						  &Src0Start);
+		if (Src0PageCount == 0)
+			return -EINVAL;
+	}
+	/* cal src1 buf mmu info */
+	if (req->mmu_info.src1_mmu_flag & 1) {
+		Src1PageCount = rga2_buf_size_cal(req->src1.yrgb_addr,
+						  req->src1.uv_addr,
+						  req->src1.v_addr,
+						  req->src1.format,
+						  req->src1.vir_w,
+						  (req->src1.vir_h),
+						  &Src1Start);
+		if (Src1PageCount == 0)
+			return -EINVAL;
+	}
+	/* cal dst buf mmu info */
+	if (req->mmu_info.dst_mmu_flag & 1) {
+		DstPageCount = rga2_buf_size_cal(req->dst.yrgb_addr,
+						 req->dst.uv_addr,
+						 req->dst.v_addr,
+						 req->dst.format,
+						 req->dst.vir_w,
+						 req->dst.vir_h,
+						 &DstStart);
+		if (DstPageCount == 0)
+			return -EINVAL;
+	}
+	/* Cal out the needed mem size */
+	Src0MemSize = (Src0PageCount + 15) & (~15);
+	Src1MemSize = (Src1PageCount + 15) & (~15);
+	DstMemSize  = (DstPageCount + 15) & (~15);
+	AllSize = Src0MemSize + Src1MemSize + DstMemSize;
+
+	if (rga2_mmu_buf_get_try(&rga2_mmu_buf, AllSize)) {
+		pr_err("RGA2 Get MMU mem failed\n");
+		status = RGA2_MALLOC_ERROR;
+		goto out;
+	}
+
+	pages = rga2_mmu_buf.pages;
+	if(pages == NULL) {
+		pr_err("RGA MMU malloc pages mem failed\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&rga2_service.lock);
+	MMU_Base = rga2_mmu_buf.buf_virtual + rga2_mmu_buf.front;
+	MMU_Base_phys = rga2_mmu_buf.buf + rga2_mmu_buf.front;
+	mutex_unlock(&rga2_service.lock);
+
+        if (Src0MemSize) {
+		dma_buffer = &reg->dma_buffer_src0;
+
+		if (dma_buffer->sgt) {
+			ret = rga2_MapION(dma_buffer->sgt,
+					  &MMU_Base[0], Src0MemSize);
+		} else {
+			ret = rga2_MapUserMemory(&pages[0], &MMU_Base[0],
+						 Src0Start, Src0PageCount,
+						 0, MMU_MAP_CLEAN);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_CHECK_MODE)
+				rga2_user_memory_check(&pages[0],
+						       req->src.vir_w,
+						       req->src.vir_h,
+						       req->src.format,
+						       1);
+#endif
+
+			/* Save pagetable to unmap. */
+			reg->MMU_src0_base = MMU_Base;
+			reg->MMU_src0_count = Src0PageCount;
+		}
+
+		if (ret < 0) {
+			pr_err("rga2 map src0 memory failed\n");
+			status = ret;
+			goto out;
+		}
+		/* change the buf address in req struct */
+		req->mmu_info.src0_base_addr = (((unsigned long)MMU_Base_phys));
+		uv_size = (req->src.uv_addr
+			   - (Src0Start << PAGE_SHIFT)) >> PAGE_SHIFT;
+		v_size = (req->src.v_addr
+			  - (Src0Start << PAGE_SHIFT)) >> PAGE_SHIFT;
+
+		req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK));
+		req->src.uv_addr = (req->src.uv_addr & (~PAGE_MASK)) |
+							(uv_size << PAGE_SHIFT);
+		req->src.v_addr = (req->src.v_addr & (~PAGE_MASK)) |
+							(v_size << PAGE_SHIFT);
+	}
+
+        if (Src1MemSize) {
+		dma_buffer = &reg->dma_buffer_src1;
+
+		if (dma_buffer->sgt) {
+			ret = rga2_MapION(dma_buffer->sgt,
+					MMU_Base + Src0MemSize, Src1MemSize);
+		} else {
+			ret = rga2_MapUserMemory(&pages[0],
+						 MMU_Base + Src0MemSize,
+						 Src1Start, Src1PageCount,
+						 0, MMU_MAP_CLEAN);
+
+			/* Save pagetable to unmap. */
+			reg->MMU_src1_base = MMU_Base + Src0MemSize;
+			reg->MMU_src1_count = Src1PageCount;
+		}
+		if (ret < 0) {
+			pr_err("rga2 map src1 memory failed\n");
+			status = ret;
+			goto out;
+		}
+		/* change the buf address in req struct */
+		req->mmu_info.src1_base_addr = ((unsigned long)(MMU_Base_phys
+						+ Src0MemSize));
+		req->src1.yrgb_addr = (req->src1.yrgb_addr & (~PAGE_MASK));
+	}
+        if (DstMemSize) {
+		dma_buffer = &reg->dma_buffer_dst;
+
+		if (dma_buffer->sgt) {
+			ret = rga2_MapION(dma_buffer->sgt, MMU_Base + Src0MemSize
+					  + Src1MemSize, DstMemSize);
+		} else if (req->alpha_mode_0 != 0 && req->bitblt_mode == 0) {
+			/* The blend mode of src + dst => dst requires clean and invalidate */
+			ret = rga2_MapUserMemory(&pages[0], MMU_Base
+						 + Src0MemSize + Src1MemSize,
+						 DstStart, DstPageCount, 1,
+						 MMU_MAP_CLEAN | MMU_MAP_INVALID);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_CHECK_MODE)
+				rga2_user_memory_check(&pages[0],
+						       req->dst.vir_w,
+						       req->dst.vir_h,
+						       req->dst.format,
+						       2);
+#endif
+
+			/* Save pagetable to invalid cache and unmap. */
+			reg->MMU_dst_base = MMU_Base + Src0MemSize + Src1MemSize;
+			reg->MMU_dst_count = DstPageCount;
+		} else {
+			ret = rga2_MapUserMemory(&pages[0], MMU_Base
+						 + Src0MemSize + Src1MemSize,
+						 DstStart, DstPageCount,
+						 1, MMU_MAP_INVALID);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+			if (RGA2_CHECK_MODE)
+				rga2_user_memory_check(&pages[0],
+						       req->dst.vir_w,
+						       req->dst.vir_h,
+						       req->dst.format,
+						       2);
+#endif
+
+			/* Save pagetable to invalid cache and unmap. */
+			reg->MMU_dst_base = MMU_Base + Src0MemSize + Src1MemSize;
+			reg->MMU_dst_count = DstPageCount;
+		}
+
+		if (ret < 0) {
+			pr_err("rga2 map dst memory failed\n");
+			status = ret;
+			goto out;
+		}
+		/* change the buf address in req struct */
+		req->mmu_info.dst_base_addr  = ((unsigned long)(MMU_Base_phys
+					+ Src0MemSize + Src1MemSize));
+		req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK));
+		uv_size = (req->dst.uv_addr
+			   - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+		v_size = (req->dst.v_addr
+			  - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+		req->dst.uv_addr = (req->dst.uv_addr & (~PAGE_MASK)) |
+						   ((uv_size) << PAGE_SHIFT);
+		req->dst.v_addr = (req->dst.v_addr & (~PAGE_MASK)) |
+			((v_size) << PAGE_SHIFT);
+
+		if (((req->alpha_rop_flag & 1) == 1) && (req->bitblt_mode == 0)) {
+			req->mmu_info.src1_base_addr = req->mmu_info.dst_base_addr;
+			req->mmu_info.src1_mmu_flag  = req->mmu_info.dst_mmu_flag;
+		}
+	}
+
+	/* flush data to DDR */
+	rga2_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+	rga2_mmu_buf_get(&rga2_mmu_buf, AllSize);
+	reg->MMU_len = AllSize;
+	status = 0;
+out:
+	return status;
+}
+
+static int rga2_mmu_info_color_palette_mode(struct rga2_reg *reg, struct rga2_req *req)
+{
+    int SrcMemSize, DstMemSize;
+    unsigned long SrcStart, DstStart;
+    unsigned long SrcPageCount, DstPageCount;
+    struct page **pages = NULL;
+    uint32_t uv_size, v_size;
+    uint32_t AllSize;
+    uint32_t *MMU_Base = NULL, *MMU_Base_phys;
+    int ret, status;
+    uint32_t stride;
+
+    uint8_t shift;
+    uint32_t sw, byte_num;
+    struct rga_dma_buffer_t *dma_buffer = NULL;
+
+    shift = 3 - (req->palette_mode & 3);
+    sw = req->src.vir_w*req->src.vir_h;
+    byte_num = sw >> shift;
+    stride = (byte_num + 3) & (~3);
+
+    SrcStart = 0;
+    DstStart = 0;
+    SrcPageCount = 0;
+    DstPageCount = 0;
+    SrcMemSize = 0;
+    DstMemSize = 0;
+
+    do {
+        if (req->mmu_info.src0_mmu_flag) {
+            if (req->mmu_info.els_mmu_flag & 1) {
+                req->mmu_info.src0_mmu_flag = 0;
+                req->mmu_info.src1_mmu_flag = 0;
+            } else {
+                req->mmu_info.els_mmu_flag = req->mmu_info.src0_mmu_flag;
+                req->mmu_info.src0_mmu_flag = 0;
+            }
+
+            SrcPageCount = rga2_mem_size_cal(req->src.yrgb_addr, stride, &SrcStart);
+            if(SrcPageCount == 0) {
+                return -EINVAL;
+            }
+        }
+
+        if (req->mmu_info.dst_mmu_flag) {
+            DstPageCount = rga2_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                            req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                            &DstStart);
+            if(DstPageCount == 0) {
+                return -EINVAL;
+            }
+        }
+
+        SrcMemSize = (SrcPageCount + 15) & (~15);
+        DstMemSize = (DstPageCount + 15) & (~15);
+
+        AllSize = SrcMemSize + DstMemSize;
+
+        if (rga2_mmu_buf_get_try(&rga2_mmu_buf, AllSize)) {
+            pr_err("RGA2 Get MMU mem failed\n");
+            status = RGA2_MALLOC_ERROR;
+            break;
+        }
+
+        pages = rga2_mmu_buf.pages;
+        if(pages == NULL) {
+            pr_err("RGA MMU malloc pages mem failed\n");
+            return -EINVAL;
+        }
+
+        mutex_lock(&rga2_service.lock);
+        MMU_Base = rga2_mmu_buf.buf_virtual + rga2_mmu_buf.front;
+        MMU_Base_phys = rga2_mmu_buf.buf + rga2_mmu_buf.front;
+        mutex_unlock(&rga2_service.lock);
+
+        if(SrcMemSize) {
+            dma_buffer = &reg->dma_buffer_src0;
+
+            if (dma_buffer->sgt) {
+                ret = rga2_MapION(dma_buffer->sgt,
+                &MMU_Base[0], SrcMemSize);
+            } else {
+                ret = rga2_MapUserMemory(&pages[0], &MMU_Base[0],
+                SrcStart, SrcPageCount, 0, MMU_MAP_CLEAN);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+                if (RGA2_CHECK_MODE)
+                rga2_user_memory_check(&pages[0], req->src.vir_w,
+                req->src.vir_h, req->src.format,
+                1);
+#endif
+            }
+            if (ret < 0) {
+                pr_err("rga2 map src0 memory failed\n");
+                status = ret;
+                break;
+            }
+
+            /* change the buf address in req struct */
+            req->mmu_info.els_base_addr = (((unsigned long)MMU_Base_phys));
+	    /*
+	     *The color palette mode will not have YUV format as input,
+	     *so UV component address is not needed
+	     */
+            req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK));
+        }
+
+        if(DstMemSize) {
+            dma_buffer = &reg->dma_buffer_dst;
+
+	    if (dma_buffer->sgt) {
+                ret = rga2_MapION(dma_buffer->sgt,
+                MMU_Base + SrcMemSize, DstMemSize);
+            } else {
+                ret = rga2_MapUserMemory(&pages[0], MMU_Base + SrcMemSize,
+                DstStart, DstPageCount, 1, MMU_MAP_INVALID);
+#ifdef CONFIG_ROCKCHIP_RGA2_DEBUGGER
+                if (RGA2_CHECK_MODE)
+                rga2_user_memory_check(&pages[0], req->dst.vir_w,
+                req->dst.vir_h, req->dst.format,
+                1);
+#endif
+            }
+            if (ret < 0) {
+                pr_err("rga2 map dst memory failed\n");
+                status = ret;
+                break;
+            }
+            /* change the buf address in req struct */
+            req->mmu_info.dst_base_addr  = ((unsigned long)(MMU_Base_phys + SrcMemSize));
+            req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK));
+
+	    uv_size = (req->dst.uv_addr
+                       - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            v_size = (req->dst.v_addr
+                      - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            req->dst.uv_addr = (req->dst.uv_addr & (~PAGE_MASK)) |
+                                ((uv_size) << PAGE_SHIFT);
+            req->dst.v_addr = (req->dst.v_addr & (~PAGE_MASK)) |
+                               ((v_size) << PAGE_SHIFT);
+        }
+
+        /* flush data to DDR */
+        rga2_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+        rga2_mmu_buf_get(&rga2_mmu_buf, AllSize);
+        reg->MMU_len = AllSize;
+
+        return 0;
+    }
+    while(0);
+
+    return 0;
+}
+
+static int rga2_mmu_info_color_fill_mode(struct rga2_reg *reg, struct rga2_req *req)
+{
+    int DstMemSize;
+    unsigned long DstStart;
+    unsigned long DstPageCount;
+    struct page **pages = NULL;
+    uint32_t uv_size, v_size;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_Base_phys;
+    int ret;
+    int status;
+    struct rga_dma_buffer_t *dma_buffer = NULL;
+
+    DstMemSize = 0;
+    DstPageCount = 0;
+    DstStart = 0;
+    MMU_Base = NULL;
+
+    do {
+        if(req->mmu_info.dst_mmu_flag & 1) {
+            DstPageCount = rga2_buf_size_cal(req->dst.yrgb_addr, req->dst.uv_addr, req->dst.v_addr,
+                                        req->dst.format, req->dst.vir_w, req->dst.vir_h,
+                                        &DstStart);
+            if(DstPageCount == 0) {
+                return -EINVAL;
+            }
+        }
+
+        DstMemSize = (DstPageCount + 15) & (~15);
+	AllSize = DstMemSize;
+
+        if(rga2_mmu_buf_get_try(&rga2_mmu_buf, AllSize)) {
+           pr_err("RGA2 Get MMU mem failed\n");
+           status = RGA2_MALLOC_ERROR;
+           break;
+        }
+
+        pages = rga2_mmu_buf.pages;
+        if(pages == NULL) {
+            pr_err("RGA MMU malloc pages mem failed\n");
+            return -EINVAL;
+        }
+
+        mutex_lock(&rga2_service.lock);
+        MMU_Base_phys = rga2_mmu_buf.buf + rga2_mmu_buf.front;
+        MMU_Base = rga2_mmu_buf.buf_virtual + rga2_mmu_buf.front;
+        mutex_unlock(&rga2_service.lock);
+
+        if (DstMemSize) {
+            dma_buffer = &reg->dma_buffer_dst;
+
+            if (dma_buffer->sgt) {
+                ret = rga2_MapION(dma_buffer->sgt, &MMU_Base[0], DstMemSize);
+            }
+            else {
+		    ret = rga2_MapUserMemory(&pages[0], &MMU_Base[0],
+					     DstStart, DstPageCount,
+					     1, MMU_MAP_INVALID);
+            }
+            if (ret < 0) {
+                pr_err("rga2 map dst memory failed\n");
+                status = ret;
+                break;
+            }
+
+            /* change the buf address in req struct */
+            req->mmu_info.dst_base_addr = ((unsigned long)MMU_Base_phys);
+            req->dst.yrgb_addr = (req->dst.yrgb_addr & (~PAGE_MASK));
+
+            uv_size = (req->dst.uv_addr
+                       - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            v_size = (req->dst.v_addr
+                      - (DstStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            req->dst.uv_addr = (req->dst.uv_addr & (~PAGE_MASK)) |
+                                ((uv_size) << PAGE_SHIFT);
+            req->dst.v_addr = (req->dst.v_addr & (~PAGE_MASK)) |
+                               ((v_size) << PAGE_SHIFT);
+        }
+
+        /* flush data to DDR */
+        rga2_dma_flush_range(MMU_Base, (MMU_Base + AllSize + 1));
+        rga2_mmu_buf_get(&rga2_mmu_buf, AllSize);
+	reg->MMU_len = AllSize;
+
+        return 0;
+    }
+    while(0);
+
+    return status;
+}
+
+
+static int rga2_mmu_info_update_palette_table_mode(struct rga2_reg *reg, struct rga2_req *req)
+{
+    int LutMemSize;
+    unsigned long LutStart;
+    unsigned long LutPageCount;
+    struct page **pages = NULL;
+    uint32_t uv_size, v_size;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_Base_phys;
+    int ret, status;
+    struct rga_dma_buffer_t *dma_buffer = NULL;
+
+    MMU_Base = NULL;
+    LutPageCount = 0;
+    LutMemSize = 0;
+    LutStart = 0;
+
+    do {
+        /* cal lut buf mmu info */
+        if (req->mmu_info.els_mmu_flag & 1) {
+            req->mmu_info.src0_mmu_flag = req->mmu_info.src0_mmu_flag == 1 ? 0 : req->mmu_info.src0_mmu_flag;
+            req->mmu_info.src1_mmu_flag = req->mmu_info.src1_mmu_flag == 1 ? 0 : req->mmu_info.src1_mmu_flag;
+            req->mmu_info.dst_mmu_flag = req->mmu_info.dst_mmu_flag == 1 ? 0 : req->mmu_info.dst_mmu_flag;
+
+            LutPageCount = rga2_buf_size_cal(req->pat.yrgb_addr, req->pat.uv_addr, req->pat.v_addr,
+                                            req->pat.format, req->pat.vir_w, req->pat.vir_h,
+                                            &LutStart);
+            if(LutPageCount == 0) {
+                return -EINVAL;
+            }
+        }
+
+        LutMemSize = (LutPageCount + 15) & (~15);
+        AllSize = LutMemSize;
+
+        if (rga2_mmu_buf_get_try(&rga2_mmu_buf, AllSize)) {
+            pr_err("RGA2 Get MMU mem failed\n");
+            status = RGA2_MALLOC_ERROR;
+            break;
+        }
+
+        pages = rga2_mmu_buf.pages;
+        if (pages == NULL) {
+            pr_err("RGA MMU malloc pages mem failed\n");
+            return -EINVAL;
+        }
+
+        mutex_lock(&rga2_service.lock);
+        MMU_Base = rga2_mmu_buf.buf_virtual + rga2_mmu_buf.front;
+        MMU_Base_phys = rga2_mmu_buf.buf + rga2_mmu_buf.front;
+        mutex_unlock(&rga2_service.lock);
+
+        if (LutMemSize) {
+            dma_buffer = &reg->dma_buffer_els;
+
+            if (dma_buffer->sgt) {
+                ret = rga2_MapION(dma_buffer->sgt,
+                &MMU_Base[0], LutMemSize);
+            } else {
+                ret = rga2_MapUserMemory(&pages[0], &MMU_Base[0],
+                LutStart, LutPageCount, 0, MMU_MAP_CLEAN);
+            }
+            if (ret < 0) {
+                pr_err("rga2 map palette memory failed\n");
+                status = ret;
+                break;
+            }
+
+            /* change the buf address in req struct */
+            req->mmu_info.els_base_addr = (((unsigned long)MMU_Base_phys));
+
+            req->pat.yrgb_addr = (req->pat.yrgb_addr & (~PAGE_MASK));
+
+            uv_size = (req->pat.uv_addr
+                       - (LutStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            v_size = (req->pat.v_addr
+                      - (LutStart << PAGE_SHIFT)) >> PAGE_SHIFT;
+            req->pat.uv_addr = (req->pat.uv_addr & (~PAGE_MASK)) |
+                                ((uv_size) << PAGE_SHIFT);
+            req->pat.v_addr = (req->pat.v_addr & (~PAGE_MASK)) |
+                               ((v_size) << PAGE_SHIFT);
+        }
+
+        /* flush data to DDR */
+        rga2_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+        rga2_mmu_buf_get(&rga2_mmu_buf, AllSize);
+        reg->MMU_len = AllSize;
+
+        return 0;
+    }
+    while(0);
+
+    return status;
+}
+
+/*
+ * yqw:
+ * This function is currently not sure whether rga2 is used,
+ * because invalidate/clean cache occupies the parameter
+ * reg->MMU_base, so block this function first, and re-implement
+ * this function if necessary.
+ */
+#if 0
+static int rga2_mmu_info_update_patten_buff_mode(struct rga2_reg *reg, struct rga2_req *req)
+{
+    int SrcMemSize, CMDMemSize;
+    unsigned long SrcStart, CMDStart;
+    struct page **pages = NULL;
+    uint32_t i;
+    uint32_t AllSize;
+    uint32_t *MMU_Base, *MMU_p;
+    int ret, status;
+
+    MMU_Base = MMU_p = 0;
+
+    do {
+        /* cal src buf mmu info */
+        SrcMemSize = rga2_mem_size_cal(req->pat.yrgb_addr, req->pat.act_w * req->pat.act_h * 4, &SrcStart);
+        if(SrcMemSize == 0) {
+            return -EINVAL;
+        }
+
+        /* cal cmd buf mmu info */
+        CMDMemSize = rga2_mem_size_cal((unsigned long)rga2_service.cmd_buff, RGA2_CMD_BUF_SIZE, &CMDStart);
+        if(CMDMemSize == 0) {
+            return -EINVAL;
+        }
+
+        AllSize = SrcMemSize + CMDMemSize;
+
+        pages = rga2_mmu_buf.pages;
+
+        MMU_Base = kzalloc(AllSize * sizeof(uint32_t), GFP_KERNEL);
+	if (MMU_Base == NULL)
+		return -EINVAL;
+
+        for(i=0; i<CMDMemSize; i++) {
+            MMU_Base[i] = virt_to_phys((uint32_t *)((CMDStart + i) << PAGE_SHIFT));
+        }
+
+        if (req->src.yrgb_addr < KERNEL_SPACE_VALID)
+        {
+		ret = rga2_MapUserMemory(&pages[CMDMemSize],
+					 &MMU_Base[CMDMemSize],
+					 SrcStart, SrcMemSize,
+					 1, MMU_MAP_CLEAN);
+            if (ret < 0) {
+                pr_err("rga map src memory failed\n");
+                status = ret;
+                break;
+            }
+        }
+        else
+        {
+            MMU_p = MMU_Base + CMDMemSize;
+
+            for(i=0; i<SrcMemSize; i++)
+            {
+                MMU_p[i] = (uint32_t)virt_to_phys((uint32_t *)((SrcStart + i) << PAGE_SHIFT));
+            }
+        }
+
+        /* zsq
+         * change the buf address in req struct
+         * for the reason of lie to MMU
+         */
+        req->mmu_info.src0_base_addr = (virt_to_phys(MMU_Base) >> 2);
+
+        req->src.yrgb_addr = (req->src.yrgb_addr & (~PAGE_MASK)) | (CMDMemSize << PAGE_SHIFT);
+
+        /*record the malloc buf for the cmd end to release*/
+        reg->MMU_base = MMU_Base;
+
+        /* flush data to DDR */
+        rga2_dma_flush_range(MMU_Base, (MMU_Base + AllSize));
+        return 0;
+
+    }
+    while(0);
+
+    return status;
+}
+#endif
+
+int rga2_set_mmu_info(struct rga2_reg *reg, struct rga2_req *req)
+{
+    int ret;
+
+    if (reg->MMU_map == true) {
+        ret = rga2_mmu_flush_cache(reg, req);
+        return ret;
+    }
+
+    switch (req->render_mode) {
+        case bitblt_mode :
+            ret = rga2_mmu_info_BitBlt_mode(reg, req);
+            break;
+        case color_palette_mode :
+            ret = rga2_mmu_info_color_palette_mode(reg, req);
+            break;
+        case color_fill_mode :
+            ret = rga2_mmu_info_color_fill_mode(reg, req);
+            break;
+        case update_palette_table_mode :
+            ret = rga2_mmu_info_update_palette_table_mode(reg, req);
+            break;
+#if 0
+        case update_patten_buff_mode :
+            ret = rga2_mmu_info_update_patten_buff_mode(reg, req);
+            break;
+#endif
+        default :
+            ret = -1;
+            break;
+    }
+
+    return ret;
+}
+
diff --git a/drivers/video/rockchip/rga2/rga2_mmu_info.h b/drivers/video/rockchip/rga2/rga2_mmu_info.h
new file mode 100644
index 0000000000000..a823f2fe7c3a6
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_mmu_info.h
@@ -0,0 +1,35 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_MMU_INFO_H__
+#define __RGA_MMU_INFO_H__
+
+#include "rga2.h"
+#include "RGA2_API.h"
+
+#ifndef MIN
+#define MIN(X, Y)           ((X)<(Y)?(X):(Y))
+#endif
+
+#ifndef MAX
+#define MAX(X, Y)           ((X)>(Y)?(X):(Y))
+#endif
+
+extern struct rga2_drvdata_t *rga2_drvdata;
+
+enum {
+	MMU_MAP_CLEAN		= 1 << 0,
+	MMU_MAP_INVALID		= 1 << 1,
+	MMU_MAP_MASK		= 0x03,
+	MMU_UNMAP_CLEAN		= 1 << 2,
+	MMU_UNMAP_INVALID	= 1 << 3,
+	MMU_UNMAP_MASK		= 0x0c,
+};
+
+int rga2_set_mmu_info(struct rga2_reg *reg, struct rga2_req *req);
+void rga2_dma_flush_range(void *pstart, void *pend);
+dma_addr_t rga2_dma_flush_page(struct page *page, int map);
+
+int rga2_get_dma_info(struct rga2_reg *reg, struct rga2_req *req);
+void rga2_put_dma_info(struct rga2_reg *reg);
+
+#endif
+
diff --git a/drivers/video/rockchip/rga2/rga2_reg_info.c b/drivers/video/rockchip/rga2/rga2_reg_info.c
new file mode 100644
index 0000000000000..1d5071d483809
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_reg_info.c
@@ -0,0 +1,1699 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+//#include <linux/kernel.h>
+#include <linux/memory.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/err.h>
+#include <linux/clk.h>
+#include <asm/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/miscdevice.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/syscalls.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <asm/cacheflush.h>
+#include <linux/slab.h>
+#include <linux/fb.h>
+#include <linux/wakelock.h>
+#include <linux/version.h>
+
+#include "rga2_reg_info.h"
+#include "rga2_type.h"
+#include "rga2_rop.h"
+#include "rga2.h"
+
+static void RGA2_reg_get_param(unsigned char *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_SRC_INFO;
+    RK_U32 *bRGA_SRC_X_FACTOR;
+    RK_U32 *bRGA_SRC_Y_FACTOR;
+    RK_U32 sw, sh;
+    RK_U32 dw, dh;
+    RK_U32 param_x, param_y;
+    RK_U8 x_flag, y_flag;
+
+    RK_U32 reg;
+
+    bRGA_SRC_INFO = (RK_U32 *)(base + RGA2_SRC_INFO_OFFSET);
+    reg = *bRGA_SRC_INFO;
+
+    bRGA_SRC_X_FACTOR = (RK_U32 *)(base + RGA2_SRC_X_FACTOR_OFFSET);
+    bRGA_SRC_Y_FACTOR = (RK_U32 *)(base + RGA2_SRC_Y_FACTOR_OFFSET);
+
+    x_flag = y_flag = 0;
+
+    if(((msg->rotate_mode & 0x3) == 1) || ((msg->rotate_mode & 0x3) == 3))
+    {
+        dw = msg->dst.act_h;
+        dh = msg->dst.act_w;
+    }
+    else
+    {
+        dw = msg->dst.act_w;
+        dh = msg->dst.act_h;
+    }
+
+    sw = msg->src.act_w;
+    sh = msg->src.act_h;
+
+    if (sw > dw)
+    {
+        x_flag = 1;
+        #if SCALE_DOWN_LARGE
+        param_x = ((dw) << 16) / (sw) + 1;
+		#else
+        param_x = ((dw) << 16) / (sw);
+        #endif
+        *bRGA_SRC_X_FACTOR |= ((param_x & 0xffff) << 0 );
+    }
+    else if (sw < dw)
+    {
+        x_flag = 2;
+        #if 1//SCALE_MINUS1
+        param_x = ((sw - 1) << 16) / (dw - 1);
+        #else
+        param_x = ((sw) << 16) / (dw);
+		#endif
+        *bRGA_SRC_X_FACTOR |= ((param_x & 0xffff) << 16);
+    }
+    else
+    {
+        *bRGA_SRC_X_FACTOR = 0;//((1 << 14) << 16) | (1 << 14);
+    }
+
+    if (sh > dh)
+    {
+        y_flag = 1;
+        #if SCALE_DOWN_LARGE
+        param_y = ((dh) << 16) / (sh) + 1;
+		#else
+        param_y = ((dh) << 16) / (sh);
+        #endif
+        *bRGA_SRC_Y_FACTOR |= ((param_y & 0xffff) << 0 );
+    }
+    else if (sh < dh)
+    {
+        y_flag = 2;
+        #if 1//SCALE_MINUS1
+        param_y = ((sh - 1) << 16) / (dh - 1);
+        #else
+        param_y = ((sh) << 16) / (dh);
+		#endif
+        *bRGA_SRC_Y_FACTOR |= ((param_y & 0xffff) << 16);
+    }
+    else
+    {
+        *bRGA_SRC_Y_FACTOR = 0;//((1 << 14) << 16) | (1 << 14);
+    }
+
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE(x_flag)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE(y_flag)));
+}
+
+static void RGA2_set_mode_ctrl(u8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_MODE_CTL;
+    RK_U32 reg = 0;
+    RK_U32 render_mode = msg->render_mode;
+
+    bRGA_MODE_CTL = (u32 *)(base + RGA2_MODE_CTRL_OFFSET);
+
+    if(msg->render_mode == 4)
+    {
+        render_mode = 3;
+    }
+
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_RENDER_MODE)) | (s_RGA2_MODE_CTRL_SW_RENDER_MODE(render_mode)));
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_BITBLT_MODE)) | (s_RGA2_MODE_CTRL_SW_BITBLT_MODE(msg->bitblt_mode)));
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_CF_ROP4_PAT)) | (s_RGA2_MODE_CTRL_SW_CF_ROP4_PAT(msg->color_fill_mode)));
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET)) | (s_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET(msg->alpha_zero_key)));
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_GRADIENT_SAT)) | (s_RGA2_MODE_CTRL_SW_GRADIENT_SAT(msg->alpha_rop_flag >> 7)));
+    reg = ((reg & (~m_RGA2_MODE_CTRL_SW_INTR_CF_E)) | (s_RGA2_MODE_CTRL_SW_INTR_CF_E(msg->CMD_fin_int_enable)));
+
+    *bRGA_MODE_CTL = reg;
+}
+
+static void RGA2_set_reg_src_info(RK_U8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_SRC_INFO;
+    RK_U32 *bRGA_SRC_BASE0, *bRGA_SRC_BASE1, *bRGA_SRC_BASE2;
+    RK_U32 *bRGA_SRC_VIR_INFO;
+    RK_U32 *bRGA_SRC_ACT_INFO;
+    RK_U32 *bRGA_MASK_ADDR;
+	RK_U32 *bRGA_SRC_TR_COLOR0, *bRGA_SRC_TR_COLOR1;
+	RK_U8 src_fmt_yuv400_en = 0;
+
+    RK_U32 reg = 0;
+    RK_U8 src0_format = 0;
+
+    RK_U8 src0_rb_swp = 0;
+    RK_U8 src0_rgb_pack = 0;
+    RK_U8 src0_alpha_swp = 0;
+
+    RK_U8 src0_cbcr_swp = 0;
+    RK_U8 pixel_width = 1;
+    RK_U32 stride = 0;
+    RK_U32 uv_stride = 0;
+    RK_U32 mask_stride = 0;
+    RK_U32 ydiv = 1, xdiv = 2;
+    RK_U8  yuv10 = 0;
+
+    RK_U32 sw, sh;
+    RK_U32 dw, dh;
+    RK_U8 rotate_mode;
+    RK_U8 vsp_scale_mode = 0;
+    RK_U8 scale_w_flag, scale_h_flag;
+
+    bRGA_SRC_INFO = (RK_U32 *)(base + RGA2_SRC_INFO_OFFSET);
+
+    bRGA_SRC_BASE0 = (RK_U32 *)(base + RGA2_SRC_BASE0_OFFSET);
+    bRGA_SRC_BASE1 = (RK_U32 *)(base + RGA2_SRC_BASE1_OFFSET);
+    bRGA_SRC_BASE2 = (RK_U32 *)(base + RGA2_SRC_BASE2_OFFSET);
+
+    bRGA_SRC_VIR_INFO = (RK_U32 *)(base + RGA2_SRC_VIR_INFO_OFFSET);
+    bRGA_SRC_ACT_INFO = (RK_U32 *)(base + RGA2_SRC_ACT_INFO_OFFSET);
+
+    bRGA_MASK_ADDR = (RK_U32 *)(base + RGA2_MASK_BASE_OFFSET);
+
+    bRGA_SRC_TR_COLOR0 = (RK_U32 *)(base + RGA2_SRC_TR_COLOR0_OFFSET);
+    bRGA_SRC_TR_COLOR1 = (RK_U32 *)(base + RGA2_SRC_TR_COLOR1_OFFSET);
+
+    if (msg->src.format == RGA2_FORMAT_YCbCr_420_SP_10B ||
+        msg->src.format == RGA2_FORMAT_YCrCb_420_SP_10B) {
+       if ((msg->src.act_w == msg->dst.act_w) &&
+           (msg->src.act_h == msg->dst.act_h) &&
+           (msg->rotate_mode == 0))
+           msg->rotate_mode = 1 << 6;
+    }
+
+    {
+        rotate_mode = msg->rotate_mode & 0x3;
+
+        sw = msg->src.act_w;
+        sh = msg->src.act_h;
+
+        if((rotate_mode == 1) | (rotate_mode == 3))
+        {
+            dw = msg->dst.act_h;
+            dh = msg->dst.act_w;
+        }
+        else
+        {
+            dw = msg->dst.act_w;
+            dh = msg->dst.act_h;
+        }
+
+        if(sw > dw)
+            scale_w_flag = 1;
+        else if (sw < dw)
+            scale_w_flag = 2;
+        else {
+            scale_w_flag = 0;
+            if(msg->rotate_mode >> 6)
+                scale_w_flag = 3;
+        }
+
+        if(sh > dh)
+            scale_h_flag = 1;
+        else if (sh < dh)
+            scale_h_flag = 2;
+        else {
+            scale_h_flag = 0;
+            if(msg->rotate_mode >> 6)
+                scale_h_flag = 3;
+        }
+    }
+
+    /* VSP scale mode select, HSD > VSD > VSP > HSP */
+    if (scale_h_flag == 0x2) {
+        /* After HSD, VSP needs to check dst_width */
+        if ((scale_w_flag == 0x1) && (dw < RGA2_VSP_BICUBIC_LIMIT))
+            vsp_scale_mode = 0x0;
+        else if (sw < RGA2_VSP_BICUBIC_LIMIT)
+            vsp_scale_mode = 0x0;
+        else
+            /* default select bilinear */
+            vsp_scale_mode = 0x1;
+    }
+
+    switch (msg->src.format)
+    {
+        case RGA2_FORMAT_RGBA_8888    : src0_format = 0x0; pixel_width = 4; break;
+        case RGA2_FORMAT_BGRA_8888    : src0_format = 0x0; src0_rb_swp = 0x1; pixel_width = 4; break;
+        case RGA2_FORMAT_RGBX_8888    : src0_format = 0x1; pixel_width = 4; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_BGRX_8888    : src0_format = 0x1; src0_rb_swp = 0x1; pixel_width = 4; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_RGB_888      : src0_format = 0x2; src0_rgb_pack = 1; pixel_width = 3; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_BGR_888      : src0_format = 0x2; src0_rgb_pack = 1; src0_rb_swp = 1; pixel_width = 3; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_RGB_565      : src0_format = 0x4; pixel_width = 2; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_RGBA_5551    : src0_format = 0x5; pixel_width = 2; src0_rb_swp = 0x1; break;
+        case RGA2_FORMAT_RGBA_4444    : src0_format = 0x6; pixel_width = 2; src0_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGR_565      : src0_format = 0x4; pixel_width = 2; msg->src_trans_mode &= 0x07; src0_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGRA_5551    : src0_format = 0x5; pixel_width = 2; break;
+        case RGA2_FORMAT_BGRA_4444    : src0_format = 0x6; pixel_width = 2; break;
+
+        /* ARGB */
+        /* In colorkey mode, xrgb/xbgr does not need to enable the alpha channel */
+        case RGA2_FORMAT_ARGB_8888    : src0_format = 0x0; pixel_width = 4; src0_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_8888    : src0_format = 0x0; pixel_width = 4; src0_alpha_swp = 1; src0_rb_swp = 0x1; break;
+        case RGA2_FORMAT_XRGB_8888    : src0_format = 0x1; pixel_width = 4; src0_alpha_swp = 1; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_XBGR_8888    : src0_format = 0x1; pixel_width = 4; src0_alpha_swp = 1; src0_rb_swp = 0x1; msg->src_trans_mode &= 0x07; break;
+        case RGA2_FORMAT_ARGB_5551    : src0_format = 0x5; pixel_width = 2; src0_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_5551    : src0_format = 0x5; pixel_width = 2; src0_alpha_swp = 1; src0_rb_swp = 0x1; break;
+        case RGA2_FORMAT_ARGB_4444    : src0_format = 0x6; pixel_width = 2; src0_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_4444    : src0_format = 0x6; pixel_width = 2; src0_alpha_swp = 1; src0_rb_swp = 0x1; break;
+
+		case RGA2_FORMAT_YVYU_422     : src0_format = 0x7; pixel_width = 2; src0_cbcr_swp = 1; src0_rb_swp = 0x1; break;//rbswap=ycswap
+		case RGA2_FORMAT_VYUY_422     : src0_format = 0x7; pixel_width = 2; src0_cbcr_swp = 1; src0_rb_swp = 0x0; break;
+		case RGA2_FORMAT_YUYV_422     : src0_format = 0x7; pixel_width = 2; src0_cbcr_swp = 0; src0_rb_swp = 0x1; break;
+		case RGA2_FORMAT_UYVY_422     : src0_format = 0x7; pixel_width = 2; src0_cbcr_swp = 0; src0_rb_swp = 0x0; break;
+
+        case RGA2_FORMAT_YCbCr_422_SP : src0_format = 0x8; xdiv = 1; ydiv = 1; break;
+        case RGA2_FORMAT_YCbCr_422_P  : src0_format = 0x9; xdiv = 2; ydiv = 1; break;
+        case RGA2_FORMAT_YCbCr_420_SP : src0_format = 0xa; xdiv = 1; ydiv = 2; break;
+        case RGA2_FORMAT_YCbCr_420_P  : src0_format = 0xb; xdiv = 2; ydiv = 2; break;
+        case RGA2_FORMAT_YCrCb_422_SP : src0_format = 0x8; xdiv = 1; ydiv = 1; src0_cbcr_swp = 1; break;
+        case RGA2_FORMAT_YCrCb_422_P  : src0_format = 0x9; xdiv = 2; ydiv = 1; src0_cbcr_swp = 1; break;
+        case RGA2_FORMAT_YCrCb_420_SP : src0_format = 0xa; xdiv = 1; ydiv = 2; src0_cbcr_swp = 1; break;
+        case RGA2_FORMAT_YCrCb_420_P  : src0_format = 0xb; xdiv = 2; ydiv = 2; src0_cbcr_swp = 1; break;
+
+        case RGA2_FORMAT_YCbCr_420_SP_10B : src0_format = 0xa; xdiv = 1; ydiv = 2; yuv10 = 1; break;
+        case RGA2_FORMAT_YCrCb_420_SP_10B : src0_format = 0xa; xdiv = 1; ydiv = 2; src0_cbcr_swp = 1; yuv10 = 1; break;
+		case RGA2_FORMAT_YCbCr_422_SP_10B : src0_format = 0x8; xdiv = 1; ydiv = 1; yuv10 = 1; break;
+		case RGA2_FORMAT_YCrCb_422_SP_10B : src0_format = 0x8; xdiv = 1; ydiv = 1; src0_cbcr_swp = 1; yuv10 = 1; break;
+
+		case RGA2_FORMAT_YCbCr_400 : src0_format = 0x8; src_fmt_yuv400_en = 1; xdiv = 1; ydiv = 1; break;
+    };
+
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SRC_FMT)) | (s_RGA2_SRC_INFO_SW_SRC_FMT(src0_format)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP)) | (s_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP(src0_rb_swp)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP)) | (s_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP(src0_alpha_swp)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP)) | (s_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP(src0_cbcr_swp)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE(msg->yuv2rgb_mode)));
+
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE(msg->rotate_mode & 0x3)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE((msg->rotate_mode >> 4) & 0x3)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE((scale_w_flag))));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE((scale_h_flag))));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER)) | (s_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER((msg->scale_bicu_mode))));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE)) | (s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE(msg->src_trans_mode)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E)) | (s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E(msg->src_trans_mode >> 1)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E)) | (s_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E((msg->alpha_rop_flag >> 4) & 0x1)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL)) | (s_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL(vsp_scale_mode)));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_YUV10_E)) | (s_RGA2_SRC_INFO_SW_SW_YUV10_E((yuv10))));
+#if 1
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E)) | (s_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E((yuv10))));
+#else
+	reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E)) | (s_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E(((msg->yuv2rgb_mode >> 6)&1))));
+#endif
+    RGA2_reg_get_param(base, msg);
+
+    stride = (((msg->src.vir_w * pixel_width) + 3) & ~3) >> 2;
+    uv_stride = ((msg->src.vir_w / xdiv + 3) & ~3);
+
+	/* 10bit code */
+#if 0
+	switch (msg->src.format)
+	{
+		case RGA2_FORMAT_YCbCr_422_SP_10B:
+		case RGA2_FORMAT_YCbCr_420_SP_10B:
+		case RGA2_FORMAT_YCrCb_422_SP_10B:
+		case RGA2_FORMAT_YCrCb_420_SP_10B:
+			stride = (((msg->src.vir_w * 10 + 31) & (~31)) >> 3) >> 2;
+			uv_stride = stride;
+			break;
+	}
+#endif
+
+    if (src_fmt_yuv400_en == 1) {
+        /*
+         * When Y400 as the input format, because the current RGA does not support closing
+         * the access of the UV channel, the address of the UV channel access is equal to
+         * the address of the Y channel access to ensure that the UV channel can access,
+         * preventing the RGA hardware from reporting errors.
+         */
+        *bRGA_SRC_BASE0 = (RK_U32)(msg->src.yrgb_addr + msg->src.y_offset * (stride<<2) + msg->src.x_offset * pixel_width);
+        *bRGA_SRC_BASE1 = *bRGA_SRC_BASE0;
+        *bRGA_SRC_BASE2 = *bRGA_SRC_BASE0;
+    } else {
+        *bRGA_SRC_BASE0 = (RK_U32)(msg->src.yrgb_addr + msg->src.y_offset * (stride<<2) + msg->src.x_offset * pixel_width);
+        *bRGA_SRC_BASE1 = (RK_U32)(msg->src.uv_addr + (msg->src.y_offset / ydiv) * uv_stride + (msg->src.x_offset / xdiv));
+        *bRGA_SRC_BASE2 = (RK_U32)(msg->src.v_addr + (msg->src.y_offset / ydiv) * uv_stride + (msg->src.x_offset / xdiv));
+    }
+
+    //mask_stride = ((msg->src0_act.width + 31) & ~31) >> 5;
+    mask_stride = msg->rop_mask_stride;
+
+    *bRGA_SRC_VIR_INFO = stride | (mask_stride << 16);
+
+    *bRGA_SRC_ACT_INFO = (msg->src.act_w - 1) | ((msg->src.act_h - 1) << 16);
+
+    *bRGA_MASK_ADDR = (RK_U32)msg->rop_mask_addr;
+
+    *bRGA_SRC_INFO = reg;
+
+	*bRGA_SRC_TR_COLOR0 = msg->color_key_min;
+    *bRGA_SRC_TR_COLOR1 = msg->color_key_max;
+}
+
+static void RGA2_set_reg_dst_info(u8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_DST_INFO;
+    RK_U32 *bRGA_DST_BASE0, *bRGA_DST_BASE1, *bRGA_DST_BASE2, *bRGA_SRC_BASE3;
+    RK_U32 *bRGA_DST_VIR_INFO;
+    RK_U32 *bRGA_DST_ACT_INFO;
+
+	RK_U32 *RGA_DST_Y4MAP_LUT0;//Y4 LUT0
+	RK_U32 *RGA_DST_Y4MAP_LUT1;//Y4 LUT1
+	RK_U32 *RGA_DST_NN_QUANTIZE_SCALE;
+	RK_U32 *RGA_DST_NN_QUANTIZE_OFFSET;
+
+	RK_U32 line_width_real;
+
+	RK_U8 ydither_en = 0;
+
+    RK_U8 src1_format = 0;
+    RK_U8 src1_rb_swp = 0;
+    RK_U8 src1_rgb_pack = 0;
+    RK_U8 src1_alpha_swp = 0;
+    RK_U8 dst_format = 0;
+    RK_U8 dst_rb_swp = 0;
+    RK_U8 dst_rgb_pack = 0;
+    RK_U8 dst_cbcr_swp = 0;
+    RK_U8 dst_alpha_swp = 0;
+
+	RK_U8 dst_fmt_yuv400_en = 0;
+	RK_U8 dst_fmt_y4_en   = 0;
+	RK_U8 dst_nn_quantize_en   = 0;
+
+    RK_U32 reg = 0;
+    RK_U8 spw, dpw;
+    RK_U32 s_stride, d_stride;
+    RK_U32 x_mirr, y_mirr, rot_90_flag;
+    RK_U32 yrgb_addr, u_addr, v_addr, s_yrgb_addr;
+    RK_U32 d_uv_stride, x_div, y_div;
+    RK_U32 y_lt_addr, y_ld_addr, y_rt_addr, y_rd_addr;
+    RK_U32 u_lt_addr, u_ld_addr, u_rt_addr, u_rd_addr;
+    RK_U32 v_lt_addr, v_ld_addr, v_rt_addr, v_rd_addr;
+
+    dpw = 1;
+    x_div = y_div = 1;
+
+	dst_nn_quantize_en = (msg->alpha_rop_flag >> 8)&0x1;
+
+    bRGA_DST_INFO = (RK_U32 *)(base + RGA2_DST_INFO_OFFSET);
+    bRGA_DST_BASE0 = (RK_U32 *)(base + RGA2_DST_BASE0_OFFSET);
+    bRGA_DST_BASE1 = (RK_U32 *)(base + RGA2_DST_BASE1_OFFSET);
+    bRGA_DST_BASE2 = (RK_U32 *)(base + RGA2_DST_BASE2_OFFSET);
+
+    bRGA_SRC_BASE3 = (RK_U32 *)(base + RGA2_SRC_BASE3_OFFSET);
+
+    bRGA_DST_VIR_INFO = (RK_U32 *)(base + RGA2_DST_VIR_INFO_OFFSET);
+    bRGA_DST_ACT_INFO = (RK_U32 *)(base + RGA2_DST_ACT_INFO_OFFSET);
+
+	RGA_DST_Y4MAP_LUT0 = (RK_U32 *)(base + RGA2_DST_Y4MAP_LUT0_OFFSET);
+	RGA_DST_Y4MAP_LUT1 = (RK_U32 *)(base + RGA2_DST_Y4MAP_LUT1_OFFSET);
+	RGA_DST_NN_QUANTIZE_SCALE = (RK_U32 *)(base + RGA2_DST_QUANTIZE_SCALE_OFFSET);
+	RGA_DST_NN_QUANTIZE_OFFSET = (RK_U32 *)(base + RGA2_DST_QUANTIZE_OFFSET_OFFSET);
+
+    switch (msg->src1.format)
+    {
+        case RGA2_FORMAT_RGBA_8888    : src1_format = 0x0; spw = 4; break;
+        case RGA2_FORMAT_BGRA_8888    : src1_format = 0x0; src1_rb_swp = 0x1; spw = 4; break;
+        case RGA2_FORMAT_RGBX_8888    : src1_format = 0x1; spw = 4; break;
+        case RGA2_FORMAT_BGRX_8888    : src1_format = 0x1; src1_rb_swp = 0x1; spw = 4; break;
+        case RGA2_FORMAT_RGB_888      : src1_format = 0x2; src1_rgb_pack = 1; spw = 3; break;
+        case RGA2_FORMAT_BGR_888      : src1_format = 0x2; src1_rgb_pack = 1; src1_rb_swp = 1; spw = 3; break;
+        case RGA2_FORMAT_RGB_565      : src1_format = 0x4; spw = 2; break;
+        case RGA2_FORMAT_RGBA_5551    : src1_format = 0x5; spw = 2; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_RGBA_4444    : src1_format = 0x6; spw = 2; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGR_565      : src1_format = 0x4; spw = 2; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGRA_5551    : src1_format = 0x5; spw = 2; break;
+        case RGA2_FORMAT_BGRA_4444    : src1_format = 0x6; spw = 2; break;
+
+        /* ARGB */
+        case RGA2_FORMAT_ARGB_8888    : src1_format = 0x0; spw = 4; src1_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_8888    : src1_format = 0x0; spw = 4; src1_alpha_swp = 1; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_XRGB_8888    : src1_format = 0x1; spw = 4; src1_alpha_swp = 1; break;
+        case RGA2_FORMAT_XBGR_8888    : src1_format = 0x1; spw = 4; src1_alpha_swp = 1; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_ARGB_5551    : src1_format = 0x5; spw = 2; src1_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_5551    : src1_format = 0x5; spw = 2; src1_alpha_swp = 1; src1_rb_swp = 0x1; break;
+        case RGA2_FORMAT_ARGB_4444    : src1_format = 0x6; spw = 2; src1_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_4444    : src1_format = 0x6; spw = 2; src1_alpha_swp = 1; src1_rb_swp = 0x1; break;
+        default                       : spw = 4; break;
+    };
+
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_FMT)) | (s_RGA2_DST_INFO_SW_SRC1_FMT(src1_format)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_RB_SWP)) | (s_RGA2_DST_INFO_SW_SRC1_RB_SWP(src1_rb_swp)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP)) | (s_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP(src1_alpha_swp)));
+
+
+    switch (msg->dst.format)
+    {
+        case RGA2_FORMAT_RGBA_8888    : dst_format = 0x0; dpw = 4; break;
+        case RGA2_FORMAT_BGRA_8888    : dst_format = 0x0; dst_rb_swp = 0x1; dpw = 4; break;
+        case RGA2_FORMAT_RGBX_8888    : dst_format = 0x1; dpw = 4; break;
+        case RGA2_FORMAT_BGRX_8888    : dst_format = 0x1; dst_rb_swp = 0x1; dpw = 4; break;
+        case RGA2_FORMAT_RGB_888      : dst_format = 0x2; dst_rgb_pack = 1; dpw = 3; break;
+        case RGA2_FORMAT_BGR_888      : dst_format = 0x2; dst_rgb_pack = 1; dst_rb_swp = 1; dpw = 3; break;
+        case RGA2_FORMAT_RGB_565      : dst_format = 0x4; dpw = 2; break;
+        case RGA2_FORMAT_RGBA_5551    : dst_format = 0x5; dpw = 2; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_RGBA_4444    : dst_format = 0x6; dpw = 2; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGR_565      : dst_format = 0x4; dpw = 2; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_BGRA_5551    : dst_format = 0x5; dpw = 2; break;
+        case RGA2_FORMAT_BGRA_4444    : dst_format = 0x6; dpw = 2; break;
+
+        /* ARGB */
+        case RGA2_FORMAT_ARGB_8888    : dst_format = 0x0; dpw = 4; dst_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_8888    : dst_format = 0x0; dpw = 4; dst_alpha_swp = 1; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_XRGB_8888    : dst_format = 0x1; dpw = 4; dst_alpha_swp = 1; break;
+        case RGA2_FORMAT_XBGR_8888    : dst_format = 0x1; dpw = 4; dst_alpha_swp = 1; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_ARGB_5551    : dst_format = 0x5; dpw = 2; dst_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_5551    : dst_format = 0x5; dpw = 2; dst_alpha_swp = 1; dst_rb_swp = 0x1; break;
+        case RGA2_FORMAT_ARGB_4444    : dst_format = 0x6; dpw = 2; dst_alpha_swp = 1; break;
+        case RGA2_FORMAT_ABGR_4444    : dst_format = 0x6; dpw = 2; dst_alpha_swp = 1; dst_rb_swp = 0x1; break;
+
+        case RGA2_FORMAT_YCbCr_422_SP : dst_format = 0x8; x_div = 1; y_div = 1; break;
+        case RGA2_FORMAT_YCbCr_422_P  : dst_format = 0x9; x_div = 2; y_div = 1; break;
+        case RGA2_FORMAT_YCbCr_420_SP : dst_format = 0xa; x_div = 1; y_div = 2; break;
+        case RGA2_FORMAT_YCbCr_420_P  : dst_format = 0xb; dst_cbcr_swp = 1; x_div = 2; y_div = 2; break;
+        case RGA2_FORMAT_YCrCb_422_SP : dst_format = 0x8; dst_cbcr_swp = 1; x_div = 1; y_div = 1; break;
+        case RGA2_FORMAT_YCrCb_422_P  : dst_format = 0x9; dst_cbcr_swp = 1; x_div = 2; y_div = 1; break;
+        case RGA2_FORMAT_YCrCb_420_SP : dst_format = 0xa; dst_cbcr_swp = 1; x_div = 1; y_div = 2; break;
+        case RGA2_FORMAT_YCrCb_420_P  : dst_format = 0xb; x_div = 2; y_div = 2; break;
+
+		case RGA2_FORMAT_YCbCr_400    : dst_format = 0x8; dst_fmt_yuv400_en = 1; x_div = 1; y_div = 1; break;
+		case RGA2_FORMAT_Y4           : dst_format = 0x8; dst_fmt_y4_en = 1; dst_fmt_yuv400_en = 1; x_div = 1; y_div = 1; break;
+
+		case RGA2_FORMAT_YUYV_422     : dst_format = 0xe; dpw = 2; dst_cbcr_swp = 1; break;
+		case RGA2_FORMAT_YVYU_422     : dst_format = 0xe; dpw = 2; break;
+		case RGA2_FORMAT_YUYV_420     : dst_format = 0xf; dpw = 2; dst_cbcr_swp = 1; break;
+		case RGA2_FORMAT_YVYU_420     : dst_format = 0xf; dpw = 2; break;
+		case RGA2_FORMAT_UYVY_422     : dst_format = 0xc; dpw = 2; dst_cbcr_swp = 1; break;
+		case RGA2_FORMAT_VYUY_422     : dst_format = 0xc; dpw = 2; break;
+		case RGA2_FORMAT_UYVY_420     : dst_format = 0xd; dpw = 2; dst_cbcr_swp = 1; break;
+		case RGA2_FORMAT_VYUY_420     : dst_format = 0xd; dpw = 2; break;
+    };
+
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_FMT)) | (s_RGA2_DST_INFO_SW_DST_FMT(dst_format)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_RB_SWAP)) | (s_RGA2_DST_INFO_SW_DST_RB_SWAP(dst_rb_swp)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_ALPHA_SWAP)) | (s_RGA2_DST_INFO_SW_ALPHA_SWAP(dst_alpha_swp)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_UV_SWAP)) | (s_RGA2_DST_INFO_SW_DST_UV_SWAP(dst_cbcr_swp)));
+
+	reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN)) | (s_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN(dst_fmt_yuv400_en)));
+	reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_FMT_Y4_EN)) | (s_RGA2_DST_INFO_SW_DST_FMT_Y4_EN(dst_fmt_y4_en)));
+	reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN)) | (s_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN(dst_nn_quantize_en)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DITHER_UP_E)) | (s_RGA2_DST_INFO_SW_DITHER_UP_E(msg->alpha_rop_flag >> 5)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DITHER_DOWN_E)) | (s_RGA2_DST_INFO_SW_DITHER_DOWN_E(msg->alpha_rop_flag >> 6)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DITHER_MODE)) | (s_RGA2_DST_INFO_SW_DITHER_MODE(msg->dither_mode)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_CSC_MODE)) | (s_RGA2_DST_INFO_SW_DST_CSC_MODE(msg->yuv2rgb_mode >> 2)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_CSC_CLIP_MODE)) | (s_RGA2_DST_INFO_SW_CSC_CLIP_MODE(msg->yuv2rgb_mode >> 4)));
+    /* full csc enable */
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_CSC_MODE_2)) | (s_RGA2_DST_INFO_SW_DST_CSC_MODE_2(msg->full_csc.flag)));
+    /* Some older chips do not support src1 csc mode, they do not have these two registers. */
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_CSC_MODE)) | (s_RGA2_DST_INFO_SW_SRC1_CSC_MODE(msg->yuv2rgb_mode >> 5)));
+    reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE)) | (s_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE(msg->yuv2rgb_mode >> 7)));
+
+	ydither_en = (msg->dst.format == RGA2_FORMAT_Y4) && ((msg->alpha_rop_flag >> 6)&0x1);
+
+    *bRGA_DST_INFO = reg;
+
+    s_stride = ((msg->src1.vir_w * spw + 3) & ~3) >> 2;
+    d_stride = ((msg->dst.vir_w * dpw + 3) & ~3) >> 2;
+
+	if (dst_fmt_y4_en) {
+		/* Y4 output will HALF */
+		d_stride = ((d_stride+1)&~1) >> 1;
+	}
+
+    d_uv_stride = (d_stride << 2) / x_div;
+
+    *bRGA_DST_VIR_INFO = d_stride | (s_stride << 16);
+	if ((msg->dst.vir_w % 2 != 0) &&
+		(msg->dst.act_w == msg->src.act_w) && (msg->dst.act_h == msg->src.act_h) &&
+		(msg->dst.format == RGA2_FORMAT_BGR_888 || msg->dst.format == RGA2_FORMAT_RGB_888))
+		*bRGA_DST_ACT_INFO = (msg->dst.act_w) | ((msg->dst.act_h - 1) << 16);
+	else
+		*bRGA_DST_ACT_INFO = (msg->dst.act_w - 1) | ((msg->dst.act_h - 1) << 16);
+    s_stride <<= 2;
+	d_stride <<= 2;
+
+    if(((msg->rotate_mode & 0xf) == 0) || ((msg->rotate_mode & 0xf) == 1))
+    {
+        x_mirr = 0;
+        y_mirr = 0;
+    }
+    else
+    {
+        x_mirr = 1;
+        y_mirr = 1;
+    }
+
+    rot_90_flag = msg->rotate_mode & 1;
+    x_mirr = (x_mirr + ((msg->rotate_mode >> 4) & 1)) & 1;
+    y_mirr = (y_mirr + ((msg->rotate_mode >> 5) & 1)) & 1;
+
+	if (ydither_en) {
+		if (x_mirr && y_mirr) {
+			printk(KERN_ERR "rga: [ERROR] YDITHER MODE DO NOT SUPPORT ROTATION !!x_mirr=%d,y_mirr=%d \n", x_mirr, y_mirr);
+		}
+		if (msg->dst.act_w != msg->src.act_w) {
+			printk(KERN_ERR "rga: [ERROR] YDITHER MODE DO NOT SUPPORT SCL !!src0.act_w=%d,dst.act_w=%d \n", msg->src.act_w, msg->dst.act_w);
+		}
+		if (msg->dst.act_h != msg->src.act_h) {
+			printk(KERN_ERR "rga: [ERROR] YDITHER MODE DO NOT SUPPORT SCL !!src0.act_h=%d,dst.act_h=%d \n", msg->src.act_h, msg->dst.act_h);
+		}
+	}
+
+	if (dst_fmt_y4_en) {
+		*RGA_DST_Y4MAP_LUT0 = (msg->gr_color.gr_x_r & 0xffff) | (msg->gr_color.gr_x_g << 16);
+		*RGA_DST_Y4MAP_LUT1 = (msg->gr_color.gr_y_r & 0xffff) | (msg->gr_color.gr_y_g << 16);
+	}
+
+	if (dst_nn_quantize_en) {
+		*RGA_DST_NN_QUANTIZE_SCALE = (msg->gr_color.gr_x_r & 0xffff) | (msg->gr_color.gr_x_g << 10) | (msg->gr_color.gr_x_b << 20);
+		*RGA_DST_NN_QUANTIZE_OFFSET = (msg->gr_color.gr_y_r & 0xffff) | (msg->gr_color.gr_y_g << 10) | (msg->gr_color.gr_y_b << 20);
+	}
+
+    s_yrgb_addr = (RK_U32)msg->src1.yrgb_addr + (msg->src1.y_offset * s_stride) + (msg->src1.x_offset * spw);
+
+    *bRGA_SRC_BASE3 = s_yrgb_addr;
+
+	if (dst_fmt_y4_en) {
+		yrgb_addr = (RK_U32)msg->dst.yrgb_addr + (msg->dst.y_offset * d_stride) + ((msg->dst.x_offset * dpw)>>1);
+	} else {
+		yrgb_addr = (RK_U32)msg->dst.yrgb_addr + (msg->dst.y_offset * d_stride) + (msg->dst.x_offset * dpw);
+	}
+    u_addr = (RK_U32)msg->dst.uv_addr + (msg->dst.y_offset / y_div) * d_uv_stride + msg->dst.x_offset / x_div;
+    v_addr = (RK_U32)msg->dst.v_addr + (msg->dst.y_offset / y_div) * d_uv_stride + msg->dst.x_offset / x_div;
+
+    y_lt_addr = yrgb_addr;
+    u_lt_addr = u_addr;
+    v_lt_addr = v_addr;
+
+	/* Warning */
+	line_width_real = dst_fmt_y4_en ? ((msg->dst.act_w) >>1) : msg->dst.act_w;
+
+	if (msg->dst.format < 0x18 ||
+	    (msg->dst.format >= RGA2_FORMAT_ARGB_8888 &&
+	     msg->dst.format <= RGA2_FORMAT_ABGR_4444)) {
+		/* 270 degree & Mirror V*/
+		y_ld_addr = yrgb_addr + (msg->dst.act_h - 1) * (d_stride);
+		/* 90 degree & Mirror H  */
+		y_rt_addr = yrgb_addr + (line_width_real - 1) * dpw;
+		/* 180 degree */
+		y_rd_addr = y_ld_addr + (line_width_real - 1) * dpw;
+	} else {
+		if (msg->dst.format == RGA2_FORMAT_YUYV_422 ||
+		    msg->dst.format == RGA2_FORMAT_YVYU_422 ||
+		    msg->dst.format == RGA2_FORMAT_UYVY_422 ||
+		    msg->dst.format == RGA2_FORMAT_VYUY_422) {
+			y_ld_addr = yrgb_addr + (msg->dst.act_h - 1) * (d_stride);
+			y_rt_addr = yrgb_addr + (msg->dst.act_w * 2 - 1);
+			y_rd_addr = y_ld_addr + (msg->dst.act_w * 2 - 1);
+		} else {
+			y_ld_addr = (RK_U32)msg->dst.yrgb_addr +
+			((msg->dst.y_offset + (msg->dst.act_h -1)) * d_stride) +
+			msg->dst.x_offset;
+			y_rt_addr = yrgb_addr + (msg->dst.act_w * 2 - 1);
+			y_rd_addr = y_ld_addr + (msg->dst.act_w - 1);
+		}
+	}
+
+	u_ld_addr = u_addr + ((msg->dst.act_h / y_div) - 1) * (d_uv_stride);
+	v_ld_addr = v_addr + ((msg->dst.act_h / y_div) - 1) * (d_uv_stride);
+
+	u_rt_addr = u_addr + (msg->dst.act_w / x_div) - 1;
+	v_rt_addr = v_addr + (msg->dst.act_w / x_div) - 1;
+
+	u_rd_addr = u_ld_addr + (msg->dst.act_w / x_div) - 1;
+	v_rd_addr = v_ld_addr + (msg->dst.act_w / x_div) - 1;
+
+    if(rot_90_flag == 0)
+    {
+        if(y_mirr == 1)
+        {
+            if(x_mirr == 1)
+            {
+                yrgb_addr = y_rd_addr;
+                u_addr = u_rd_addr;
+                v_addr = v_rd_addr;
+            }
+            else
+            {
+                yrgb_addr = y_ld_addr;
+                u_addr = u_ld_addr;
+                v_addr = v_ld_addr;
+            }
+        }
+        else
+        {
+            if(x_mirr == 1)
+            {
+                yrgb_addr = y_rt_addr;
+                u_addr = u_rt_addr;
+                v_addr = v_rt_addr;
+            }
+            else
+            {
+                yrgb_addr = y_lt_addr;
+                u_addr = u_lt_addr;
+                v_addr = v_lt_addr;
+            }
+        }
+    }
+    else
+    {
+        if(y_mirr == 1)
+        {
+            if(x_mirr == 1)
+            {
+                yrgb_addr = y_ld_addr;
+                u_addr = u_ld_addr;
+                v_addr = v_ld_addr;
+            }
+            else
+            {
+                yrgb_addr = y_rd_addr;
+                u_addr = u_rd_addr;
+                v_addr = v_rd_addr;
+            }
+        }
+        else
+        {
+            if(x_mirr == 1)
+            {
+                yrgb_addr = y_lt_addr;
+                u_addr = u_lt_addr;
+                v_addr = v_lt_addr;
+            }
+            else
+            {
+                yrgb_addr = y_rt_addr;
+                u_addr = u_rt_addr;
+                v_addr = v_rt_addr;
+            }
+        }
+    }
+
+    *bRGA_DST_BASE0 = (RK_U32)yrgb_addr;
+
+    if((msg->dst.format == RGA2_FORMAT_YCbCr_420_P) || (msg->dst.format == RGA2_FORMAT_YCrCb_420_P))
+    {
+        if(dst_cbcr_swp == 0) {
+            *bRGA_DST_BASE1 = (RK_U32)v_addr;
+            *bRGA_DST_BASE2 = (RK_U32)u_addr;
+        }
+        else {
+            *bRGA_DST_BASE1 = (RK_U32)u_addr;
+            *bRGA_DST_BASE2 = (RK_U32)v_addr;
+        }
+    }
+    else {
+        *bRGA_DST_BASE1 = (RK_U32)u_addr;
+        *bRGA_DST_BASE2 = (RK_U32)v_addr;
+    }
+
+	//if (msg->dst.format >= 0x18) {
+	//	*bRGA_DST_BASE1 = msg->dst.x_offset;
+	//}
+}
+
+static void RGA2_set_reg_alpha_info(u8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_ALPHA_CTRL0;
+    RK_U32 *bRGA_ALPHA_CTRL1;
+    RK_U32 *bRGA_FADING_CTRL;
+    RK_U32 reg0 = 0;
+    RK_U32 reg1 = 0;
+
+    bRGA_ALPHA_CTRL0 = (RK_U32 *)(base + RGA2_ALPHA_CTRL0_OFFSET);
+    bRGA_ALPHA_CTRL1 = (RK_U32 *)(base + RGA2_ALPHA_CTRL1_OFFSET);
+    bRGA_FADING_CTRL = (RK_U32 *)(base + RGA2_FADING_CTRL_OFFSET);
+
+    reg0 = ((reg0 & (~m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0)) | (s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0(msg->alpha_rop_flag)));
+    reg0 = ((reg0 & (~m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL)) | (s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL(msg->alpha_rop_flag >> 1)));
+    reg0 = ((reg0 & (~m_RGA2_ALPHA_CTRL0_SW_ROP_MODE)) | (s_RGA2_ALPHA_CTRL0_SW_ROP_MODE(msg->rop_mode)));
+    reg0 = ((reg0 & (~m_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA)) | (s_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA(msg->src_a_global_val)));
+    reg0 = ((reg0 & (~m_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA)) | (s_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA(msg->dst_a_global_val)));
+
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0)) | (s_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0(msg->alpha_mode_0 >> 15)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0(msg->alpha_mode_0 >> 7)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0)) | (s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0(msg->alpha_mode_0 >> 12)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0(msg->alpha_mode_0 >> 4)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0)) | (s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0(msg->alpha_mode_0 >> 11)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0(msg->alpha_mode_0 >> 3)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0)) | (s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0(msg->alpha_mode_0 >> 9)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0(msg->alpha_mode_0 >> 1)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0)) | (s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0(msg->alpha_mode_0 >> 8)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0(msg->alpha_mode_0 >> 0)));
+
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1)) | (s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1(msg->alpha_mode_1 >> 12)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1(msg->alpha_mode_1 >> 4)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1)) | (s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1(msg->alpha_mode_1 >> 11)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1(msg->alpha_mode_1 >> 3)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1)) | (s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1(msg->alpha_mode_1 >> 9)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1(msg->alpha_mode_1 >> 1)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1)) | (s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1(msg->alpha_mode_1 >> 8)));
+    reg1 = ((reg1 & (~m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1)) | (s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1(msg->alpha_mode_1 >> 0)));
+
+    *bRGA_ALPHA_CTRL0 = reg0;
+    *bRGA_ALPHA_CTRL1 = reg1;
+
+    if((msg->alpha_rop_flag>>2)&1)
+    {
+        *bRGA_FADING_CTRL = (1<<24) | (msg->fading_b_value<<16) | (msg->fading_g_value<<8) | (msg->fading_r_value);
+    }
+}
+
+static void RGA2_set_reg_rop_info(u8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_ALPHA_CTRL0;
+    RK_U32 *bRGA_ROP_CTRL0;
+    RK_U32 *bRGA_ROP_CTRL1;
+    RK_U32 *bRGA_MASK_ADDR;
+    RK_U32 *bRGA_FG_COLOR;
+    RK_U32 *bRGA_PAT_CON;
+
+    RK_U32 rop_code0 = 0;
+    RK_U32 rop_code1 = 0;
+
+    bRGA_ALPHA_CTRL0 = (RK_U32 *)(base + RGA2_ALPHA_CTRL0_OFFSET);
+    bRGA_ROP_CTRL0 = (RK_U32 *)(base + RGA2_ROP_CTRL0_OFFSET);
+    bRGA_ROP_CTRL1 = (RK_U32 *)(base + RGA2_ROP_CTRL1_OFFSET);
+	bRGA_MASK_ADDR = (RK_U32 *)(base + RGA2_MASK_BASE_OFFSET);
+    bRGA_FG_COLOR  = (RK_U32 *)(base + RGA2_SRC_FG_COLOR_OFFSET);
+    bRGA_PAT_CON   = (RK_U32 *)(base + RGA2_PAT_CON_OFFSET);
+
+    if(msg->rop_mode == 0) {
+	rop_code0 = RGA2_ROP3_code[(msg->rop_code & 0xff)];
+    }
+    else if(msg->rop_mode == 1) {
+	rop_code0 = RGA2_ROP3_code[(msg->rop_code & 0xff)];
+    }
+    else if(msg->rop_mode == 2) {
+	rop_code0 = RGA2_ROP3_code[(msg->rop_code & 0xff)];
+	rop_code1 = RGA2_ROP3_code[(msg->rop_code & 0xff00)>>8];
+    }
+
+    *bRGA_ROP_CTRL0 = rop_code0;
+    *bRGA_ROP_CTRL1 = rop_code1;
+    *bRGA_FG_COLOR = msg->fg_color;
+    *bRGA_MASK_ADDR = (RK_U32)msg->rop_mask_addr;
+    *bRGA_PAT_CON = (msg->pat.act_w-1) | ((msg->pat.act_h-1) << 8)
+                     | (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+    *bRGA_ALPHA_CTRL0 = *bRGA_ALPHA_CTRL0 | (((msg->endian_mode >> 1) & 1) << 20);
+
+}
+
+static void RGA2_set_reg_full_csc(u8 *base, struct rga2_req *msg)
+{
+	RK_U32 *bRGA2_DST_CSC_00;
+	RK_U32 *bRGA2_DST_CSC_01;
+	RK_U32 *bRGA2_DST_CSC_02;
+	RK_U32 *bRGA2_DST_CSC_OFF0;
+
+	RK_U32 *bRGA2_DST_CSC_10;
+	RK_U32 *bRGA2_DST_CSC_11;
+	RK_U32 *bRGA2_DST_CSC_12;
+	RK_U32 *bRGA2_DST_CSC_OFF1;
+
+	RK_U32 *bRGA2_DST_CSC_20;
+	RK_U32 *bRGA2_DST_CSC_21;
+	RK_U32 *bRGA2_DST_CSC_22;
+	RK_U32 *bRGA2_DST_CSC_OFF2;
+
+	bRGA2_DST_CSC_00 = (RK_U32 *)(base + RGA2_DST_CSC_00_OFFSET);
+	bRGA2_DST_CSC_01 = (RK_U32 *)(base + RGA2_DST_CSC_01_OFFSET);
+	bRGA2_DST_CSC_02 = (RK_U32 *)(base + RGA2_DST_CSC_02_OFFSET);
+	bRGA2_DST_CSC_OFF0 = (RK_U32 *)(base + RGA2_DST_CSC_OFF0_OFFSET);
+
+	bRGA2_DST_CSC_10 = (RK_U32 *)(base + RGA2_DST_CSC_10_OFFSET);
+	bRGA2_DST_CSC_11 = (RK_U32 *)(base + RGA2_DST_CSC_11_OFFSET);
+	bRGA2_DST_CSC_12 = (RK_U32 *)(base + RGA2_DST_CSC_12_OFFSET);
+	bRGA2_DST_CSC_OFF1 = (RK_U32 *)(base + RGA2_DST_CSC_OFF1_OFFSET);
+
+	bRGA2_DST_CSC_20 = (RK_U32 *)(base + RGA2_DST_CSC_20_OFFSET);
+	bRGA2_DST_CSC_21 = (RK_U32 *)(base + RGA2_DST_CSC_21_OFFSET);
+	bRGA2_DST_CSC_22 = (RK_U32 *)(base + RGA2_DST_CSC_22_OFFSET);
+	bRGA2_DST_CSC_OFF2 = (RK_U32 *)(base + RGA2_DST_CSC_OFF2_OFFSET);
+
+	/* full csc coefficient */
+	/* Y coefficient */
+	*bRGA2_DST_CSC_00 = msg->full_csc.coe_y.r_v;
+	*bRGA2_DST_CSC_01 = msg->full_csc.coe_y.g_y;
+	*bRGA2_DST_CSC_02 = msg->full_csc.coe_y.b_u;
+	*bRGA2_DST_CSC_OFF0 = msg->full_csc.coe_y.off;
+	/* U coefficient */
+	*bRGA2_DST_CSC_10 = msg->full_csc.coe_u.r_v;
+	*bRGA2_DST_CSC_11 = msg->full_csc.coe_u.g_y;
+	*bRGA2_DST_CSC_12 = msg->full_csc.coe_u.b_u;
+	*bRGA2_DST_CSC_OFF1 = msg->full_csc.coe_u.off;
+	/* V coefficient */
+	*bRGA2_DST_CSC_20 = msg->full_csc.coe_v.r_v;
+	*bRGA2_DST_CSC_21 = msg->full_csc.coe_v.g_y;
+	*bRGA2_DST_CSC_22 = msg->full_csc.coe_v.b_u;
+	*bRGA2_DST_CSC_OFF2 = msg->full_csc.coe_v.off;
+}
+
+static void RGA2_set_reg_color_palette(RK_U8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_SRC_BASE0, *bRGA_SRC_INFO, *bRGA_SRC_VIR_INFO, *bRGA_SRC_ACT_INFO, *bRGA_SRC_FG_COLOR, *bRGA_SRC_BG_COLOR;
+    RK_U32  *p;
+    RK_S16  x_off, y_off;
+    RK_U16  src_stride;
+    RK_U8   shift;
+    RK_U32  sw;
+    RK_U32  byte_num;
+    RK_U32 reg;
+
+    bRGA_SRC_BASE0 = (RK_U32 *)(base + RGA2_SRC_BASE0_OFFSET);
+	bRGA_SRC_INFO = (RK_U32 *)(base + RGA2_SRC_INFO_OFFSET);
+    bRGA_SRC_VIR_INFO = (RK_U32 *)(base + RGA2_SRC_VIR_INFO_OFFSET);
+    bRGA_SRC_ACT_INFO = (RK_U32 *)(base + RGA2_SRC_ACT_INFO_OFFSET);
+    bRGA_SRC_FG_COLOR = (RK_U32 *)(base + RGA2_SRC_FG_COLOR_OFFSET);
+    bRGA_SRC_BG_COLOR = (RK_U32 *)(base + RGA2_SRC_BG_COLOR_OFFSET);
+
+    reg = 0;
+
+    shift = 3 - msg->palette_mode;
+
+    x_off = msg->src.x_offset;
+    y_off = msg->src.y_offset;
+
+    sw = msg->src.vir_w;
+    byte_num = sw >> shift;
+
+    src_stride = (byte_num + 3) & (~3);
+
+    p = (RK_U32 *)((unsigned long)msg->src.yrgb_addr);
+
+    #if 0
+    if(endian_mode)
+    {
+        p = p + (x_off>>shift) + y_off*src_stride;
+    }
+    else
+    {
+        p = p + (((x_off>>shift)>>2)<<2) + (3 - ((x_off>>shift) & 3)) + y_off*src_stride;
+    }
+    #endif
+
+    p = p + (x_off>>shift) + y_off*src_stride;
+
+
+    *bRGA_SRC_BASE0 = (unsigned long)p;
+
+	reg = ((reg & (~m_RGA2_SRC_INFO_SW_SRC_FMT)) | (s_RGA2_SRC_INFO_SW_SRC_FMT((msg->palette_mode | 0xc))));
+    reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_CP_ENDAIN)) | (s_RGA2_SRC_INFO_SW_SW_CP_ENDAIN(msg->endian_mode & 1)));
+    *bRGA_SRC_VIR_INFO = src_stride >> 2;
+    *bRGA_SRC_ACT_INFO = (msg->src.act_w - 1) | ((msg->src.act_h - 1) << 16);
+    *bRGA_SRC_INFO = reg;
+
+    *bRGA_SRC_FG_COLOR = msg->fg_color;
+    *bRGA_SRC_BG_COLOR = msg->bg_color;
+
+}
+
+static void RGA2_set_reg_color_fill(u8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_CF_GR_A;
+    RK_U32 *bRGA_CF_GR_B;
+    RK_U32 *bRGA_CF_GR_G;
+    RK_U32 *bRGA_CF_GR_R;
+    RK_U32 *bRGA_SRC_FG_COLOR;
+    RK_U32 *bRGA_MASK_ADDR;
+    RK_U32 *bRGA_PAT_CON;
+
+    RK_U32 mask_stride;
+    RK_U32 *bRGA_SRC_VIR_INFO;
+
+    bRGA_SRC_FG_COLOR = (RK_U32 *)(base + RGA2_SRC_FG_COLOR_OFFSET);
+
+    bRGA_CF_GR_A = (RK_U32 *)(base + RGA2_CF_GR_A_OFFSET);
+    bRGA_CF_GR_B = (RK_U32 *)(base + RGA2_CF_GR_B_OFFSET);
+    bRGA_CF_GR_G = (RK_U32 *)(base + RGA2_CF_GR_G_OFFSET);
+    bRGA_CF_GR_R = (RK_U32 *)(base + RGA2_CF_GR_R_OFFSET);
+
+    bRGA_MASK_ADDR = (RK_U32 *)(base + RGA2_MASK_BASE_OFFSET);
+    bRGA_PAT_CON = (RK_U32 *)(base + RGA2_PAT_CON_OFFSET);
+
+    bRGA_SRC_VIR_INFO = (RK_U32 *)(base + RGA2_SRC_VIR_INFO_OFFSET);
+
+    mask_stride = msg->rop_mask_stride;
+
+    if(msg->color_fill_mode == 0)
+    {
+        /* solid color */
+        *bRGA_CF_GR_A = (msg->gr_color.gr_x_a & 0xffff) | (msg->gr_color.gr_y_a << 16);
+        *bRGA_CF_GR_B = (msg->gr_color.gr_x_b & 0xffff) | (msg->gr_color.gr_y_b << 16);
+        *bRGA_CF_GR_G = (msg->gr_color.gr_x_g & 0xffff) | (msg->gr_color.gr_y_g << 16);
+        *bRGA_CF_GR_R = (msg->gr_color.gr_x_r & 0xffff) | (msg->gr_color.gr_y_r << 16);
+
+        *bRGA_SRC_FG_COLOR = msg->fg_color;
+    }
+    else
+    {
+        /* patten color */
+        *bRGA_MASK_ADDR = (RK_U32)msg->pat.yrgb_addr;
+        *bRGA_PAT_CON = (msg->pat.act_w - 1) | ((msg->pat.act_h - 1) << 8)
+                       | (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+    }
+	*bRGA_SRC_VIR_INFO = mask_stride << 16;
+}
+
+static void RGA2_set_reg_update_palette_table(RK_U8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_MASK_BASE;
+    RK_U32 *bRGA_FADING_CTRL;
+
+    bRGA_MASK_BASE  = (RK_U32 *)(base + RGA2_MASK_BASE_OFFSET);
+    bRGA_FADING_CTRL = (RK_U32 *)(base + RGA2_FADING_CTRL_OFFSET);
+
+    *bRGA_FADING_CTRL = msg->fading_g_value << 8;
+    *bRGA_MASK_BASE = (RK_U32)msg->pat.yrgb_addr;
+}
+
+
+static void RGA2_set_reg_update_patten_buff(RK_U8 *base, struct rga2_req *msg)
+{
+    u32 *bRGA_PAT_MST;
+    u32 *bRGA_PAT_CON;
+    u32 *bRGA_PAT_START_POINT;
+    RK_U32 *bRGA_FADING_CTRL;
+    u32 reg = 0;
+    rga_img_info_t *pat;
+
+    RK_U32 num, offset;
+
+    pat = &msg->pat;
+
+    num = (pat->act_w * pat->act_h) - 1;
+
+    offset = pat->act_w * pat->y_offset + pat->x_offset;
+
+    bRGA_PAT_START_POINT = (RK_U32 *)(base + RGA2_FADING_CTRL_OFFSET);
+    bRGA_PAT_MST = (RK_U32 *)(base + RGA2_MASK_BASE_OFFSET);
+    bRGA_PAT_CON = (RK_U32 *)(base + RGA2_PAT_CON_OFFSET);
+    bRGA_FADING_CTRL = (RK_U32 *)(base + RGA2_FADING_CTRL_OFFSET);
+
+    *bRGA_PAT_MST = (RK_U32)msg->pat.yrgb_addr;
+    *bRGA_PAT_START_POINT = (pat->act_w * pat->y_offset) + pat->x_offset;
+
+    reg = (pat->act_w-1) | ((pat->act_h-1) << 8) | (pat->x_offset << 16) | (pat->y_offset << 24);
+    *bRGA_PAT_CON = reg;
+
+    *bRGA_FADING_CTRL = (num << 8) | offset;
+}
+
+static void RGA2_set_pat_info(RK_U8 *base, struct rga2_req *msg)
+{
+    u32 *bRGA_PAT_CON;
+    u32 *bRGA_FADING_CTRL;
+    u32 reg = 0;
+    rga_img_info_t *pat;
+
+    RK_U32 num, offset;
+
+    pat = &msg->pat;
+
+    num = ((pat->act_w * pat->act_h) - 1) & 0xff;
+
+    offset = (pat->act_w * pat->y_offset) + pat->x_offset;
+
+    bRGA_PAT_CON     = (RK_U32 *)(base + RGA2_PAT_CON_OFFSET);
+    bRGA_FADING_CTRL = (RK_U32 *)(base + RGA2_FADING_CTRL_OFFSET);
+
+    reg = (pat->act_w-1) | ((pat->act_h-1) << 8) | (pat->x_offset << 16) | (pat->y_offset << 24);
+    *bRGA_PAT_CON = reg;
+    *bRGA_FADING_CTRL = (num << 8) | offset;
+}
+
+static void RGA2_set_mmu_info(RK_U8 *base, struct rga2_req *msg)
+{
+    RK_U32 *bRGA_MMU_CTRL1;
+    RK_U32 *bRGA_MMU_SRC_BASE;
+    RK_U32 *bRGA_MMU_SRC1_BASE;
+    RK_U32 *bRGA_MMU_DST_BASE;
+    RK_U32 *bRGA_MMU_ELS_BASE;
+
+    RK_U32 reg;
+
+    bRGA_MMU_CTRL1 = (RK_U32 *)(base + RGA2_MMU_CTRL1_OFFSET);
+    bRGA_MMU_SRC_BASE = (RK_U32 *)(base + RGA2_MMU_SRC_BASE_OFFSET);
+    bRGA_MMU_SRC1_BASE = (RK_U32 *)(base + RGA2_MMU_SRC1_BASE_OFFSET);
+    bRGA_MMU_DST_BASE = (RK_U32 *)(base + RGA2_MMU_DST_BASE_OFFSET);
+    bRGA_MMU_ELS_BASE = (RK_U32 *)(base + RGA2_MMU_ELS_BASE_OFFSET);
+
+    reg = (msg->mmu_info.src0_mmu_flag & 0xf) | ((msg->mmu_info.src1_mmu_flag & 0xf) << 4)
+         | ((msg->mmu_info.dst_mmu_flag & 0xf) << 8) | ((msg->mmu_info.els_mmu_flag & 0x3) << 12);
+
+    *bRGA_MMU_CTRL1 = reg;
+    *bRGA_MMU_SRC_BASE  = (RK_U32)(msg->mmu_info.src0_base_addr) >> 4;
+    *bRGA_MMU_SRC1_BASE = (RK_U32)(msg->mmu_info.src1_base_addr) >> 4;
+    *bRGA_MMU_DST_BASE  = (RK_U32)(msg->mmu_info.dst_base_addr)  >> 4;
+    *bRGA_MMU_ELS_BASE  = (RK_U32)(msg->mmu_info.els_base_addr)  >> 4;
+}
+
+int
+RGA2_gen_reg_info(RK_U8 *base, RK_U8 *csc_base, struct rga2_req *msg)
+{
+	RK_U8 dst_nn_quantize_en = 0;
+
+    RGA2_set_mode_ctrl(base, msg);
+
+    RGA2_set_pat_info(base, msg);
+
+    switch(msg->render_mode)
+    {
+        case bitblt_mode:
+            RGA2_set_reg_src_info(base, msg);
+            RGA2_set_reg_dst_info(base, msg);
+			dst_nn_quantize_en = (msg->alpha_rop_flag >> 8)&0x1 ;
+			if (dst_nn_quantize_en != 1) {
+				if ((msg->dst.format != RGA2_FORMAT_Y4)) {
+					RGA2_set_reg_alpha_info(base, msg);
+					RGA2_set_reg_rop_info(base, msg);
+				}
+			}
+
+			if (msg->full_csc.flag) {
+				RGA2_set_reg_full_csc(csc_base, msg);
+			}
+            break;
+        case color_fill_mode :
+            RGA2_set_reg_color_fill(base, msg);
+            RGA2_set_reg_dst_info(base, msg);
+            RGA2_set_reg_alpha_info(base, msg);
+            break;
+        case color_palette_mode :
+            RGA2_set_reg_color_palette(base, msg);
+            RGA2_set_reg_dst_info(base, msg);
+            break;
+        case update_palette_table_mode :
+            RGA2_set_reg_update_palette_table(base, msg);
+            break;
+        case update_patten_buff_mode :
+            RGA2_set_reg_update_patten_buff(base, msg);
+            break;
+        default :
+            printk("RGA2 ERROR msg render mode %d \n", msg->render_mode);
+            break;
+
+    }
+
+    RGA2_set_mmu_info(base, msg);
+
+    return 0;
+
+}
+
+static void format_name_convert(uint32_t *df, uint32_t sf)
+{
+    switch(sf)
+    {
+        case 0x0: *df = RGA2_FORMAT_RGBA_8888; break;
+        case 0x1: *df = RGA2_FORMAT_RGBX_8888; break;
+        case 0x2: *df = RGA2_FORMAT_RGB_888; break;
+        case 0x3: *df = RGA2_FORMAT_BGRA_8888; break;
+        case 0x4: *df = RGA2_FORMAT_RGB_565; break;
+        case 0x5: *df = RGA2_FORMAT_RGBA_5551; break;
+        case 0x6: *df = RGA2_FORMAT_RGBA_4444; break;
+        case 0x7: *df = RGA2_FORMAT_BGR_888; break;
+        case 0x16: *df = RGA2_FORMAT_BGRX_8888; break;
+        case 0x8: *df = RGA2_FORMAT_YCbCr_422_SP; break;
+        case 0x9: *df = RGA2_FORMAT_YCbCr_422_P; break;
+        case 0xa: *df = RGA2_FORMAT_YCbCr_420_SP; break;
+        case 0xb: *df = RGA2_FORMAT_YCbCr_420_P; break;
+        case 0xc: *df = RGA2_FORMAT_YCrCb_422_SP; break;
+        case 0xd: *df = RGA2_FORMAT_YCrCb_422_P; break;
+        case 0xe: *df = RGA2_FORMAT_YCrCb_420_SP; break;
+        case 0xf: *df = RGA2_FORMAT_YCrCb_420_P; break;
+
+        case 0x10: *df = RGA2_FORMAT_BPP_1; break;
+        case 0x11: *df = RGA2_FORMAT_BPP_2; break;
+        case 0x12: *df = RGA2_FORMAT_BPP_4; break;
+        case 0x13: *df = RGA2_FORMAT_BPP_8; break;
+
+        case 0x14: *df = RGA2_FORMAT_Y4; break;
+        case 0x15: *df = RGA2_FORMAT_YCbCr_400; break;
+
+        case 0x18: *df = RGA2_FORMAT_YVYU_422; break;
+        case 0x19: *df = RGA2_FORMAT_YVYU_420; break;
+        case 0x1a: *df = RGA2_FORMAT_VYUY_422; break;
+        case 0x1b: *df = RGA2_FORMAT_VYUY_420; break;
+        case 0x1c: *df = RGA2_FORMAT_YUYV_422; break;
+        case 0x1d: *df = RGA2_FORMAT_YUYV_420; break;
+        case 0x1e: *df = RGA2_FORMAT_UYVY_422; break;
+        case 0x1f: *df = RGA2_FORMAT_UYVY_420; break;
+
+        case 0x20:*df = RGA2_FORMAT_YCbCr_420_SP_10B; break;
+        case 0x21:*df = RGA2_FORMAT_YCrCb_420_SP_10B; break;
+        case 0x22:*df = RGA2_FORMAT_YCbCr_422_SP_10B; break;
+        case 0x23:*df = RGA2_FORMAT_YCrCb_422_SP_10B; break;
+
+	case 0x24:*df = RGA2_FORMAT_BGR_565; break;
+	case 0x25:*df = RGA2_FORMAT_BGRA_5551; break;
+	case 0x26:*df = RGA2_FORMAT_BGRA_4444; break;
+
+
+	case 0x28 : *df = RGA2_FORMAT_ARGB_8888; break;
+	case 0x29 : *df = RGA2_FORMAT_XRGB_8888; break;
+	case 0x2a : *df = RGA2_FORMAT_ARGB_5551; break;
+	case 0x2b : *df = RGA2_FORMAT_ARGB_4444; break;
+	case 0x2c : *df = RGA2_FORMAT_ABGR_8888; break;
+	case 0x2d : *df = RGA2_FORMAT_XBGR_8888; break;
+	case 0x2e : *df = RGA2_FORMAT_ABGR_5551; break;
+	case 0x2f : *df = RGA2_FORMAT_ABGR_4444; break;
+    }
+}
+
+void RGA_MSG_2_RGA2_MSG(struct rga_req *req_rga, struct rga2_req *req)
+{
+	u16 alpha_mode_0, alpha_mode_1;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	if (req_rga->render_mode & RGA_BUF_GEM_TYPE_MASK)
+		req->buf_type = RGA_BUF_GEM_TYPE_MASK & RGA_BUF_GEM_TYPE_DMA;
+
+	req_rga->render_mode &= (~RGA_BUF_GEM_TYPE_MASK);
+#endif
+
+    if (req_rga->render_mode == 6)
+        req->render_mode = update_palette_table_mode;
+    else if (req_rga->render_mode == 7)
+        req->render_mode = update_patten_buff_mode;
+    else if (req_rga->render_mode == 5)
+        req->render_mode = bitblt_mode;
+    else
+        req->render_mode = req_rga->render_mode;
+
+    memcpy(&req->src, &req_rga->src, sizeof(req_rga->src));
+    memcpy(&req->dst, &req_rga->dst, sizeof(req_rga->dst));
+    /* The application will only import pat or src1. */
+    if (req->render_mode == update_palette_table_mode) {
+        memcpy(&req->pat, &req_rga->pat, sizeof(req_rga->pat));
+    } else {
+        memcpy(&req->src1, &req_rga->pat, sizeof(req_rga->pat));
+    }
+
+    format_name_convert(&req->src.format, req_rga->src.format);
+    format_name_convert(&req->dst.format, req_rga->dst.format);
+    format_name_convert(&req->src1.format, req_rga->pat.format);
+
+    switch (req_rga->rotate_mode & 0x0F) {
+    case 1:
+        if(req_rga->sina == 0 && req_rga->cosa == 65536) {
+            /* rotate 0 */
+            req->rotate_mode = 0;
+        } else if (req_rga->sina == 65536 && req_rga->cosa == 0) {
+            /* rotate 90 */
+            req->rotate_mode = 1;
+            req->dst.x_offset = req_rga->dst.x_offset - req_rga->dst.act_h + 1;
+            req->dst.act_w = req_rga->dst.act_h;
+            req->dst.act_h = req_rga->dst.act_w;
+        } else if (req_rga->sina == 0 && req_rga->cosa == -65536) {
+            /* rotate 180 */
+            req->rotate_mode = 2;
+            req->dst.x_offset = req_rga->dst.x_offset - req_rga->dst.act_w + 1;
+            req->dst.y_offset = req_rga->dst.y_offset - req_rga->dst.act_h + 1;
+        } else if (req_rga->sina == -65536 && req_rga->cosa == 0) {
+            /* totate 270 */
+            req->rotate_mode = 3;
+            req->dst.y_offset = req_rga->dst.y_offset - req_rga->dst.act_w + 1;
+            req->dst.act_w = req_rga->dst.act_h;
+            req->dst.act_h = req_rga->dst.act_w;
+        }
+        break;
+    case 2:
+        //x_mirror
+        req->rotate_mode |= (1 << 4);
+        break;
+    case 3:
+        //y_mirror
+        req->rotate_mode |= (2 << 4);
+        break;
+    case 4:
+        //x_mirror+y_mirror
+        req->rotate_mode |= (3 << 4);
+        break;
+    default:
+        req->rotate_mode = 0;
+        break;
+    }
+
+    switch ((req_rga->rotate_mode & 0xF0) >> 4) {
+    case 2:
+        //x_mirror
+        req->rotate_mode |= (1 << 4);
+        break;
+    case 3:
+        //y_mirror
+        req->rotate_mode |= (2 << 4);
+        break;
+    case 4:
+        //x_mirror+y_mirror
+        req->rotate_mode |= (3 << 4);
+        break;
+    }
+
+    req->LUT_addr = req_rga->LUT_addr;
+    req->rop_mask_addr = req_rga->rop_mask_addr;
+
+    req->bitblt_mode = req_rga->bsfilter_flag;
+
+    req->src_a_global_val = req_rga->alpha_global_value;
+    req->dst_a_global_val = req_rga->alpha_global_value;
+    req->rop_code = req_rga->rop_code;
+    req->rop_mode = req_rga->alpha_rop_mode;
+
+    req->color_fill_mode = req_rga->color_fill_mode;
+    req->alpha_zero_key = req_rga->alpha_rop_mode >> 4;
+    req->src_trans_mode = req_rga->src_trans_mode;
+    req->color_key_min   = req_rga->color_key_min;
+    req->color_key_max   = req_rga->color_key_max;
+
+    req->fg_color = req_rga->fg_color;
+    req->bg_color = req_rga->bg_color;
+    memcpy(&req->gr_color, &req_rga->gr_color, sizeof(req_rga->gr_color));
+    memcpy(&req->full_csc, &req_rga->full_csc, sizeof(req_rga->full_csc));
+
+    req->palette_mode = req_rga->palette_mode;
+    req->yuv2rgb_mode = req_rga->yuv2rgb_mode;
+    req->endian_mode = req_rga->endian_mode;
+    req->rgb2yuv_mode = 0;
+
+    req->fading_alpha_value = 0;
+    req->fading_r_value = req_rga->fading.r;
+    req->fading_g_value = req_rga->fading.g;
+    req->fading_b_value = req_rga->fading.b;
+
+    /* alpha mode set */
+    req->alpha_rop_flag = 0;
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag & 1)));           // alpha_rop_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 1) & 1) << 1); // rop_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 2) & 1) << 2); // fading_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 4) & 1) << 3); // alpha_cal_mode_sel
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 5) & 1) << 6); // dst_dither_down
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 6) & 1) << 7); // gradient fill mode sel
+
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 8) & 1) << 8); // nn_quantize
+	req->dither_mode = req_rga->dither_mode;
+
+    if (((req_rga->alpha_rop_flag) & 1)) {
+        if ((req_rga->alpha_rop_flag >> 3) & 1) {
+            /* porter duff alpha enable */
+            switch (req_rga->PD_mode)
+            {
+                case 0: //dst = 0
+                    break;
+                case 1: //dst = src
+                    req->alpha_mode_0 = 0x0212;
+                    req->alpha_mode_1 = 0x0212;
+                    break;
+                case 2: //dst = dst
+                    req->alpha_mode_0 = 0x1202;
+                    req->alpha_mode_1 = 0x1202;
+                    break;
+                case 3: //dst = (256*sc + (256 - sa)*dc) >> 8
+                    if((req_rga->alpha_rop_mode & 3) == 0) {
+                        /* both use globalAlpha. */
+                        alpha_mode_0 = 0x3010;
+                        alpha_mode_1 = 0x3010;
+                    }
+                    else if ((req_rga->alpha_rop_mode & 3) == 1) {
+                        /* Do not use globalAlpha. */
+                        alpha_mode_0 = 0x3212;
+                        alpha_mode_1 = 0x3212;
+                    }
+                    else if ((req_rga->alpha_rop_mode & 3) == 2) {
+                        /* dst use globalAlpha, and dst has pixelAlpha. */
+                        alpha_mode_0 = 0x3014;
+                        alpha_mode_1 = 0x3014;
+                    }
+                    else {
+                        /* dst use globalAlpha, and dst does not have pixelAlpha. */
+                        alpha_mode_0 = 0x3012;
+                        alpha_mode_1 = 0x3012;
+                    }
+                    req->alpha_mode_0 = alpha_mode_0;
+                    req->alpha_mode_1 = alpha_mode_1;
+                    break;
+                case 4: //dst = (sc*(256-da) + 256*dc) >> 8
+                    /* Do not use globalAlpha. */
+                    req->alpha_mode_0 = 0x1232;
+                    req->alpha_mode_1 = 0x1232;
+                    break;
+                case 5: //dst = (da*sc) >> 8
+                    break;
+                case 6: //dst = (sa*dc) >> 8
+                    break;
+                case 7: //dst = ((256-da)*sc) >> 8
+                    break;
+                case 8: //dst = ((256-sa)*dc) >> 8
+                    break;
+                case 9: //dst = (da*sc + (256-sa)*dc) >> 8
+                    req->alpha_mode_0 = 0x3040;
+                    req->alpha_mode_1 = 0x3040;
+                    break;
+                case 10://dst = ((256-da)*sc + (sa*dc)) >> 8
+                    break;
+                case 11://dst = ((256-da)*sc + (256-sa)*dc) >> 8;
+                    break;
+		case 12:
+		    req->alpha_mode_0 = 0x0010;
+		    req->alpha_mode_1 = 0x0820;
+		    break;
+                default:
+                    break;
+            }
+            /* Real color mode */
+            if ((req_rga->alpha_rop_flag >> 9) & 1) {
+                if (req->alpha_mode_0 & (0x01 << 1))
+                    req->alpha_mode_0 |= (1 << 7);
+                if (req->alpha_mode_0 & (0x01 << 9))
+                    req->alpha_mode_0 |= (1 << 15);
+            }
+        }
+        else {
+            if((req_rga->alpha_rop_mode & 3) == 0) {
+                req->alpha_mode_0 = 0x3040;
+                req->alpha_mode_1 = 0x3040;
+            }
+            else if ((req_rga->alpha_rop_mode & 3) == 1) {
+		req->alpha_mode_0 = 0x3042;
+		req->alpha_mode_1 = 0x3242;
+            }
+            else if ((req_rga->alpha_rop_mode & 3) == 2) {
+                req->alpha_mode_0 = 0x3044;
+                req->alpha_mode_1 = 0x3044;
+            }
+        }
+    }
+
+    if (req_rga->mmu_info.mmu_en && (req_rga->mmu_info.mmu_flag & 1) == 1) {
+        req->mmu_info.src0_mmu_flag = 1;
+        req->mmu_info.dst_mmu_flag = 1;
+
+        if (req_rga->mmu_info.mmu_flag >> 31) {
+            req->mmu_info.src0_mmu_flag = ((req_rga->mmu_info.mmu_flag >> 8)  & 1);
+            req->mmu_info.src1_mmu_flag = ((req_rga->mmu_info.mmu_flag >> 9)  & 1);
+            req->mmu_info.dst_mmu_flag  = ((req_rga->mmu_info.mmu_flag >> 10) & 1);
+            req->mmu_info.els_mmu_flag  = ((req_rga->mmu_info.mmu_flag >> 11) & 1);
+        }
+        else {
+            if (req_rga->src.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.src0_mmu_flag = 0;
+               req->src.yrgb_addr = req_rga->src.yrgb_addr - 0x60000000;
+               req->src.uv_addr   = req_rga->src.uv_addr - 0x60000000;
+               req->src.v_addr    = req_rga->src.v_addr - 0x60000000;
+            }
+
+            if (req_rga->dst.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.dst_mmu_flag = 0;
+               req->dst.yrgb_addr = req_rga->dst.yrgb_addr - 0x60000000;
+            }
+
+	    if (req_rga->pat.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.src1_mmu_flag = 0;
+               req->src1.yrgb_addr = req_rga->pat.yrgb_addr - 0x60000000;
+            }
+        }
+    }
+}
+
+static void memcpy_img_info(struct rga_img_info_t *dst, struct rga_img_info_32_t *src)
+{
+    dst->yrgb_addr = src->yrgb_addr;      /* yrgb    mem addr         */
+    dst->uv_addr = src->uv_addr;        /* cb/cr   mem addr         */
+    dst->v_addr = src->v_addr;         /* cr      mem addr         */
+    dst->format = src->format;         //definition by RK_FORMAT
+
+    dst->act_w = src->act_w;
+    dst->act_h = src->act_h;
+    dst->x_offset = src->x_offset;
+    dst->y_offset = src->y_offset;
+
+    dst->vir_w = src->vir_w;
+    dst->vir_h = src->vir_h;
+    dst->endian_mode = src->endian_mode; //for BPP
+    dst->alpha_swap = src->alpha_swap;
+}
+
+void RGA_MSG_2_RGA2_MSG_32(struct rga_req_32 *req_rga, struct rga2_req *req)
+{
+	u16 alpha_mode_0, alpha_mode_1;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	if (req_rga->render_mode & RGA_BUF_GEM_TYPE_MASK)
+		req->buf_type = RGA_BUF_GEM_TYPE_MASK & RGA_BUF_GEM_TYPE_DMA;
+
+	req_rga->render_mode &= (~RGA_BUF_GEM_TYPE_MASK);
+#endif
+    if (req_rga->render_mode == 6)
+        req->render_mode = update_palette_table_mode;
+    else if (req_rga->render_mode == 7)
+        req->render_mode = update_patten_buff_mode;
+    else if (req_rga->render_mode == 5)
+        req->render_mode = bitblt_mode;
+    else
+        req->render_mode = req_rga->render_mode;
+    memcpy_img_info(&req->src, &req_rga->src);
+    memcpy_img_info(&req->dst, &req_rga->dst);
+    /* The application will only import pat or src1. */
+    if (req->render_mode == update_palette_table_mode) {
+        memcpy_img_info(&req->pat, &req_rga->pat);
+    } else {
+        memcpy_img_info(&req->src1,&req_rga->pat);
+    }
+    format_name_convert(&req->src.format, req_rga->src.format);
+    format_name_convert(&req->dst.format, req_rga->dst.format);
+    format_name_convert(&req->src1.format, req_rga->pat.format);
+
+    switch (req_rga->rotate_mode & 0x0F) {
+    case 1:
+        if(req_rga->sina == 0 && req_rga->cosa == 65536) {
+            /* rotate 0 */
+            req->rotate_mode = 0;
+        } else if (req_rga->sina == 65536 && req_rga->cosa == 0) {
+            /* rotate 90 */
+            req->rotate_mode = 1;
+            req->dst.x_offset = req_rga->dst.x_offset - req_rga->dst.act_h + 1;
+            req->dst.act_w = req_rga->dst.act_h;
+            req->dst.act_h = req_rga->dst.act_w;
+        } else if (req_rga->sina == 0 && req_rga->cosa == -65536) {
+            /* rotate 180 */
+            req->rotate_mode = 2;
+            req->dst.x_offset = req_rga->dst.x_offset - req_rga->dst.act_w + 1;
+            req->dst.y_offset = req_rga->dst.y_offset - req_rga->dst.act_h + 1;
+        } else if (req_rga->sina == -65536 && req_rga->cosa == 0) {
+            /* totate 270 */
+            req->rotate_mode = 3;
+            req->dst.y_offset = req_rga->dst.y_offset - req_rga->dst.act_w + 1;
+            req->dst.act_w = req_rga->dst.act_h;
+            req->dst.act_h = req_rga->dst.act_w;
+        }
+        break;
+    case 2:
+        //x_mirror
+        req->rotate_mode |= (1 << 4);
+        break;
+    case 3:
+        //y_mirror
+        req->rotate_mode |= (2 << 4);
+        break;
+    case 4:
+        //x_mirror+y_mirror
+        req->rotate_mode |= (3 << 4);
+        break;
+    default:
+        req->rotate_mode = 0;
+        break;
+    }
+
+    switch ((req_rga->rotate_mode & 0xF0) >> 4) {
+    case 2:
+        //x_mirror
+        req->rotate_mode |= (1 << 4);
+        break;
+    case 3:
+        //y_mirror
+        req->rotate_mode |= (2 << 4);
+        break;
+    case 4:
+        //x_mirror+y_mirror
+        req->rotate_mode |= (3 << 4);
+        break;
+    }
+
+    if((req->dst.act_w > 2048) && (req->src.act_h < req->dst.act_h))
+        req->scale_bicu_mode |= (1<<4);
+    req->LUT_addr = req_rga->LUT_addr;
+    req->rop_mask_addr = req_rga->rop_mask_addr;
+    req->bitblt_mode = req_rga->bsfilter_flag;
+    req->src_a_global_val = req_rga->alpha_global_value;
+    req->dst_a_global_val = req_rga->alpha_global_value;
+    req->rop_code = req_rga->rop_code;
+    req->rop_mode = req_rga->alpha_rop_mode;
+    req->color_fill_mode = req_rga->color_fill_mode;
+    req->alpha_zero_key = req_rga->alpha_rop_mode >> 4;
+    req->src_trans_mode = req_rga->src_trans_mode;
+    req->color_key_min   = req_rga->color_key_min;
+    req->color_key_max   = req_rga->color_key_max;
+    req->fg_color = req_rga->fg_color;
+    req->bg_color = req_rga->bg_color;
+    memcpy(&req->gr_color, &req_rga->gr_color, sizeof(req_rga->gr_color));
+    memcpy(&req->full_csc, &req_rga->full_csc, sizeof(req_rga->full_csc));
+
+    req->palette_mode = req_rga->palette_mode;
+    req->yuv2rgb_mode = req_rga->yuv2rgb_mode;
+    req->endian_mode = req_rga->endian_mode;
+    req->rgb2yuv_mode = 0;
+    req->fading_alpha_value = 0;
+    req->fading_r_value = req_rga->fading.r;
+    req->fading_g_value = req_rga->fading.g;
+    req->fading_b_value = req_rga->fading.b;
+
+    /* alpha mode set */
+    req->alpha_rop_flag = 0;
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag & 1)));           // alpha_rop_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 1) & 1) << 1); // rop_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 2) & 1) << 2); // fading_enable
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 4) & 1) << 3); // alpha_cal_mode_sel
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 5) & 1) << 6); // dst_dither_down
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 6) & 1) << 7); // gradient fill mode sel
+
+    req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 8) & 1) << 8); // nn_quantize
+    req->dither_mode = req_rga->dither_mode;
+
+    if (((req_rga->alpha_rop_flag) & 1)) {
+        if ((req_rga->alpha_rop_flag >> 3) & 1) {
+            /* porter duff alpha enable */
+            switch (req_rga->PD_mode)
+            {
+                case 0: //dst = 0
+                    break;
+                case 1: //dst = src
+                    req->alpha_mode_0 = 0x0212;
+                    req->alpha_mode_1 = 0x0212;
+                    break;
+                case 2: //dst = dst
+                    req->alpha_mode_0 = 0x1202;
+                    req->alpha_mode_1 = 0x1202;
+                    break;
+                case 3: //dst = (256*sc + (256 - sa)*dc) >> 8
+                    if((req_rga->alpha_rop_mode & 3) == 0) {
+                        /* both use globalAlpha. */
+                        alpha_mode_0 = 0x3010;
+                        alpha_mode_1 = 0x3010;
+                    }
+                    else if ((req_rga->alpha_rop_mode & 3) == 1) {
+                        /* dst use globalAlpha, and dst does not have pixelAlpha. */
+                        alpha_mode_0 = 0x3012;
+                        alpha_mode_1 = 0x3012;
+                    }
+                    else if ((req_rga->alpha_rop_mode & 3) == 2) {
+                        /* dst use globalAlpha, and dst has pixelAlpha. */
+                        alpha_mode_0 = 0x3014;
+                        alpha_mode_1 = 0x3014;
+                    }
+                    else {
+                        /* Do not use globalAlpha. */
+                        alpha_mode_0 = 0x3212;
+                        alpha_mode_1 = 0x3212;
+                    }
+                    req->alpha_mode_0 = alpha_mode_0;
+                    req->alpha_mode_1 = alpha_mode_1;
+                    break;
+                case 4: //dst = (sc*(256-da) + 256*dc) >> 8
+                    /* Do not use globalAlpha. */
+                    req->alpha_mode_0 = 0x1232;
+                    req->alpha_mode_1 = 0x1232;
+                    break;
+                case 5: //dst = (da*sc) >> 8
+                    break;
+                case 6: //dst = (sa*dc) >> 8
+                    break;
+                case 7: //dst = ((256-da)*sc) >> 8
+                    break;
+                case 8: //dst = ((256-sa)*dc) >> 8
+                    break;
+                case 9: //dst = (da*sc + (256-sa)*dc) >> 8
+                    req->alpha_mode_0 = 0x3040;
+                    req->alpha_mode_1 = 0x3040;
+                    break;
+                case 10://dst = ((256-da)*sc + (sa*dc)) >> 8
+                    break;
+                case 11://dst = ((256-da)*sc + (256-sa)*dc) >> 8;
+                    break;
+		case 12:
+		    req->alpha_mode_0 = 0x0010;
+		    req->alpha_mode_1 = 0x0820;
+		    break;
+                default:
+                    break;
+            }
+            /* Real color mode */
+            if ((req_rga->alpha_rop_flag >> 9) & 1) {
+                if (req->alpha_mode_0 & (0x01 << 1))
+                    req->alpha_mode_0 |= (1 << 7);
+                if (req->alpha_mode_0 & (0x01 << 9))
+                    req->alpha_mode_0 |= (1 << 15);
+            }
+        }
+        else {
+            if((req_rga->alpha_rop_mode & 3) == 0) {
+                req->alpha_mode_0 = 0x3040;
+                req->alpha_mode_1 = 0x3040;
+            }
+            else if ((req_rga->alpha_rop_mode & 3) == 1) {
+		req->alpha_mode_0 = 0x3042;
+		req->alpha_mode_1 = 0x3242;
+            }
+            else if ((req_rga->alpha_rop_mode & 3) == 2) {
+                req->alpha_mode_0 = 0x3044;
+                req->alpha_mode_1 = 0x3044;
+            }
+        }
+    }
+
+    if (req_rga->mmu_info.mmu_en && (req_rga->mmu_info.mmu_flag & 1) == 1) {
+        req->mmu_info.src0_mmu_flag = 1;
+        req->mmu_info.dst_mmu_flag = 1;
+        if (req_rga->mmu_info.mmu_flag >> 31) {
+            req->mmu_info.src0_mmu_flag = ((req_rga->mmu_info.mmu_flag >> 8)  & 1);
+            req->mmu_info.src1_mmu_flag = ((req_rga->mmu_info.mmu_flag >> 9)  & 1);
+            req->mmu_info.dst_mmu_flag  = ((req_rga->mmu_info.mmu_flag >> 10) & 1);
+            req->mmu_info.els_mmu_flag  = ((req_rga->mmu_info.mmu_flag >> 11) & 1);
+        }
+        else {
+            if (req_rga->src.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.src0_mmu_flag = 0;
+               req->src.yrgb_addr = req_rga->src.yrgb_addr - 0x60000000;
+               req->src.uv_addr   = req_rga->src.uv_addr - 0x60000000;
+               req->src.v_addr    = req_rga->src.v_addr - 0x60000000;
+            }
+
+            if (req_rga->dst.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.dst_mmu_flag = 0;
+               req->dst.yrgb_addr = req_rga->dst.yrgb_addr - 0x60000000;
+            }
+
+	    if (req_rga->pat.yrgb_addr >= 0xa0000000) {
+               req->mmu_info.src1_mmu_flag = 0;
+               req->src1.yrgb_addr = req_rga->pat.yrgb_addr - 0x60000000;
+            }
+        }
+    }
+}
diff --git a/drivers/video/rockchip/rga2/rga2_reg_info.h b/drivers/video/rockchip/rga2/rga2_reg_info.h
new file mode 100644
index 0000000000000..ef9689318bf54
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_reg_info.h
@@ -0,0 +1,332 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __REG2_INFO_H__
+#define __REG2_INFO_H__
+
+
+//#include "chip_register.h"
+
+//#include "rga_struct.h"
+#include "rga2.h"
+
+#ifndef MIN
+#define MIN(X, Y)           ((X)<(Y)?(X):(Y))
+#endif
+
+#ifndef MAX
+#define MAX(X, Y)           ((X)>(Y)?(X):(Y))
+#endif
+
+#ifndef ABS
+#define ABS(X)              (((X) < 0) ? (-(X)) : (X))
+#endif
+
+#ifndef CLIP
+#define CLIP(x, a,  b)				((x) < (a)) ? (a) : (((x) > (b)) ? (b) : (x))
+#endif
+
+#define rRGA_SYS_CTRL             (*(volatile u32 *)(RGA2_BASE + RGA2_SYS_CTRL_OFFSET    ))
+#define rRGA_CMD_CTRL             (*(volatile u32 *)(RGA2_BASE + RGA2_CMD_CTRL_OFFSET    ))
+#define rRGA_CMD_BASE             (*(volatile u32 *)(RGA2_BASE + RGA2_CMD_BASE_OFFSET    ))
+#define rRGA_STATUS               (*(volatile u32 *)(RGA2_BASE + RGA2_STATUS_OFFSET      ))
+#define rRGA_INT                  (*(volatile u32 *)(RGA2_BASE + RGA2_INT_OFFSET         ))
+#define rRGA_MMU_CTRL0            (*(volatile u32 *)(RGA2_BASE + RGA2_MMU_CTRL0_OFFSET   ))
+#define rRGA_MMU_CMD_BASE         (*(volatile u32 *)(RGA2_BASE + RGA2_MMU_CMD_BASE_OFFSET))
+#define rRGA_CMD_ADDR             (*(volatile u32 *)(RGA2_BASE + RGA2_CMD_ADDR))
+
+/*RGA_INT*/
+#define m_RGA2_INT_ALL_CMD_DONE_INT_EN             ( 1<<10 )
+#define m_RGA2_INT_MMU_INT_EN                      ( 1<<9  )
+#define m_RGA2_INT_ERROR_INT_EN                    ( 1<<8  )
+#define m_RGA2_INT_NOW_CMD_DONE_INT_CLEAR          ( 1<<7  )
+#define m_RGA2_INT_ALL_CMD_DONE_INT_CLEAR          ( 1<<6  )
+#define m_RGA2_INT_MMU_INT_CLEAR                   ( 1<<5  )
+#define m_RGA2_INT_ERROR_INT_CLEAR                 ( 1<<4  )
+#define m_RGA2_INT_CUR_CMD_DONE_INT_FLAG           ( 1<<3  )
+#define m_RGA2_INT_ALL_CMD_DONE_INT_FLAG           ( 1<<2  )
+#define m_RGA2_INT_MMU_INT_FLAG                    ( 1<<1  )
+#define m_RGA2_INT_ERROR_INT_FLAG                  ( 1<<0  )
+
+#define s_RGA2_INT_ALL_CMD_DONE_INT_EN(x)          ( (x&0x1)<<10 )
+#define s_RGA2_INT_MMU_INT_EN(x)                   ( (x&0x1)<<9  )
+#define s_RGA2_INT_ERROR_INT_EN(x)                 ( (x&0x1)<<8  )
+#define s_RGA2_INT_NOW_CMD_DONE_INT_CLEAR(x)       ( (x&0x1)<<7  )
+#define s_RGA2_INT_ALL_CMD_DONE_INT_CLEAR(x)       ( (x&0x1)<<6  )
+#define s_RGA2_INT_MMU_INT_CLEAR(x)                ( (x&0x1)<<5  )
+#define s_RGA2_INT_ERROR_INT_CLEAR(x)              ( (x&0x1)<<4  )
+
+
+
+/* RGA_MODE_CTRL */
+#define m_RGA2_MODE_CTRL_SW_RENDER_MODE         (  0x7<<0  )
+#define m_RGA2_MODE_CTRL_SW_BITBLT_MODE         (  0x1<<3  )
+#define m_RGA2_MODE_CTRL_SW_CF_ROP4_PAT         (  0x1<<4  )
+#define m_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET      (  0x1<<5  )
+#define m_RGA2_MODE_CTRL_SW_GRADIENT_SAT        (  0x1<<6  )
+#define m_RGA2_MODE_CTRL_SW_INTR_CF_E           (  0x1<<7  )
+
+#define s_RGA2_MODE_CTRL_SW_RENDER_MODE(x)      (  (x&0x7)<<0  )
+#define s_RGA2_MODE_CTRL_SW_BITBLT_MODE(x)      (  (x&0x1)<<3  )
+#define s_RGA2_MODE_CTRL_SW_CF_ROP4_PAT(x)      (  (x&0x1)<<4  )
+#define s_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET(x)   (  (x&0x1)<<5  )
+#define s_RGA2_MODE_CTRL_SW_GRADIENT_SAT(x)     (  (x&0x1)<<6  )
+#define s_RGA2_MODE_CTRL_SW_INTR_CF_E(x)        (  (x&0x1)<<7  )
+
+/* RGA_SRC_INFO */
+#define m_RGA2_SRC_INFO_SW_SRC_FMT                (   0xf<<0   )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP         (   0x1<<4   )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP      (   0x1<<5   )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP         (   0x1<<6   )
+#define m_RGA2_SRC_INFO_SW_SW_CP_ENDAIN           (   0x1<<7   )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE        (   0x3<<8   )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE        (   0x3<<10  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE        (   0x3<<12  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE       (   0x3<<14  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE       (   0x3<<16  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE      (   0x1<<18  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E         (   0xf<<19  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E     (   0x1<<23  )
+#define m_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER      (   0x3<<24  )
+#define m_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL        (   0x1<<26  )
+#define m_RGA2_SRC_INFO_SW_SW_YUV10_E             (   0x1<<27  )
+#define m_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E       (   0x1<<28  )
+
+
+
+
+
+#define s_RGA2_SRC_INFO_SW_SRC_FMT(x)                (   (x&0xf)<<0   )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP(x)         (   (x&0x1)<<4   )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP(x)      (   (x&0x1)<<5   )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP(x)         (   (x&0x1)<<6   )
+#define s_RGA2_SRC_INFO_SW_SW_CP_ENDAIN(x)           (   (x&0x1)<<7   )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE(x)        (   (x&0x3)<<8   )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE(x)        (   (x&0x3)<<10  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE(x)        (   (x&0x3)<<12  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE(x)       (   (x&0x3)<<14  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE(x)       (   (x&0x3)<<16  )
+
+#define s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE(x)      (   (x&0x1)<<18  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E(x)         (   (x&0xf)<<19  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E(x)     (   (x&0x1)<<23  )
+#define s_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER(x)      (   (x&0x3)<<24  )
+#define s_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL(x)        (   (x&0x1)<<26  )
+#define s_RGA2_SRC_INFO_SW_SW_YUV10_E(x)             (   (x&0x1)<<27  )
+#define s_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E(x)       (   (x&0x1)<<28  )
+
+/* RGA_SRC_VIR_INFO */
+#define m_RGA2_SRC_VIR_INFO_SW_SRC_VIR_STRIDE        (  0x7fff<<0  )         //modify
+#define m_RGA2_SRC_VIR_INFO_SW_MASK_VIR_STRIDE       (   0x3ff<<16 )         //modify
+
+#define s_RGA2_SRC_VIR_INFO_SW_SRC_VIR_STRIDE(x)        ( (x&0x7fff)<<0  )   //modify
+#define s_RGA2_SRC_VIR_INFO_SW_MASK_VIR_STRIDE(x)       (   (x&0x3ff)<<16 )  //modify
+
+
+/* RGA_SRC_ACT_INFO */
+#define m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH        (  0x1fff<<0  )
+#define m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT       (  0x1fff<<16  )
+
+#define s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH(x)        (  (x&0x1fff)<<0  )
+#define s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT(x)       (  (x&0x1fff<)<16  )
+
+
+/* RGA_DST_INFO */
+#define m_RGA2_DST_INFO_SW_DST_FMT                   (  0xf<<0 )
+#define m_RGA2_DST_INFO_SW_DST_RB_SWAP               (  0x1<<4 )
+#define m_RGA2_DST_INFO_SW_ALPHA_SWAP                (  0x1<<5 )
+#define m_RGA2_DST_INFO_SW_DST_UV_SWAP               (  0x1<<6 )
+#define m_RGA2_DST_INFO_SW_SRC1_FMT                  (  0x7<<7 )
+#define m_RGA2_DST_INFO_SW_SRC1_RB_SWP               (  0x1<<10)
+#define m_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP            (  0x1<<11)
+#define m_RGA2_DST_INFO_SW_DITHER_UP_E               (  0x1<<12)
+#define m_RGA2_DST_INFO_SW_DITHER_DOWN_E             (  0x1<<13)
+#define m_RGA2_DST_INFO_SW_DITHER_MODE               (  0x3<<14)
+#define m_RGA2_DST_INFO_SW_DST_CSC_MODE              (  0x3<<16)    //add
+#define m_RGA2_DST_INFO_SW_CSC_CLIP_MODE             (  0x1<<18)
+#define m_RGA2_DST_INFO_SW_DST_CSC_MODE_2            (  0x1<<19)    //add
+#define m_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN         (  0x1<<24)
+#define m_RGA2_DST_INFO_SW_DST_FMT_Y4_EN             (  0x1<<25)
+#define m_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN        (  0x1<<26)
+#define m_RGA2_DST_INFO_SW_SRC1_CSC_MODE             (  0x3<<20)    //add
+#define m_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE        (  0x1<<22)
+
+#define s_RGA2_DST_INFO_SW_DST_FMT(x)                   (  (x&0xf)<<0 )
+#define s_RGA2_DST_INFO_SW_DST_RB_SWAP(x)               (  (x&0x1)<<4 )
+#define s_RGA2_DST_INFO_SW_ALPHA_SWAP(x)                (  (x&0x1)<<5 )
+#define s_RGA2_DST_INFO_SW_DST_UV_SWAP(x)               (  (x&0x1)<<6 )
+#define s_RGA2_DST_INFO_SW_SRC1_FMT(x)                  (  (x&0x7)<<7 )
+#define s_RGA2_DST_INFO_SW_SRC1_RB_SWP(x)               (  (x&0x1)<<10)
+#define s_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP(x)            (  (x&0x1)<<11)
+#define s_RGA2_DST_INFO_SW_DITHER_UP_E(x)               (  (x&0x1)<<12)
+#define s_RGA2_DST_INFO_SW_DITHER_DOWN_E(x)             (  (x&0x1)<<13)
+#define s_RGA2_DST_INFO_SW_DITHER_MODE(x)               (  (x&0x3)<<14)
+#define s_RGA2_DST_INFO_SW_DST_CSC_MODE(x)              (  (x&0x3)<<16)    //add
+#define s_RGA2_DST_INFO_SW_CSC_CLIP_MODE(x)             (  (x&0x1)<<18)
+#define s_RGA2_DST_INFO_SW_DST_CSC_MODE_2(x)            (  (x&0x1)<<19)    //add
+#define s_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN(x)         (  (x&0x1)<<24)
+#define s_RGA2_DST_INFO_SW_DST_FMT_Y4_EN(x)             (  (x&0x1)<<25)
+#define s_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN(x)        (  (x&0x1)<<26)
+#define s_RGA2_DST_INFO_SW_SRC1_CSC_MODE(x)             (  (x&0x3)<<20)    //add
+#define s_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE(x)        (  (x&0x1)<<22)
+
+
+/* RGA_ALPHA_CTRL0 */
+#define m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0             (  0x1<<0  )
+#define m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL           (  0x1<<1  )
+#define m_RGA2_ALPHA_CTRL0_SW_ROP_MODE                (  0x3<<2  )
+#define m_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA        ( 0xff<<4  )
+#define m_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA        ( 0xff<<12 )
+#define m_RGA2_ALPHA_CTRLO_SW_MASK_ENDIAN             (  0x1<<20 )         //add
+
+#define s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0(x)             (  (x&0x1)<<0  )
+#define s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL(x)           (  (x&0x1)<<1  )
+#define s_RGA2_ALPHA_CTRL0_SW_ROP_MODE(x)                (  (x&0x3)<<2  )
+#define s_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA(x)        ( (x&0xff)<<4  )
+#define s_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA(x)        ( (x&0xff)<<12 )
+#define s_RGA2_ALPHA_CTRLO_SW_MASK_ENDIAN(x)             (  (x&0x1)<<20 )  //add
+
+
+
+/* RGA_ALPHA_CTRL1 */
+#define m_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0            ( 0x1<<0 )
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0            ( 0x1<<1 )
+#define m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0           ( 0x7<<2 )
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0           ( 0x7<<5 )
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0        ( 0x1<<8 )
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0        ( 0x1<<9 )
+#define m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0            ( 0x3<<10)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0            ( 0x3<<12)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0            ( 0x1<<14)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0            ( 0x1<<15)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1           ( 0x7<<16)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1           ( 0x7<<19)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1        ( 0x1<<22)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1        ( 0x1<<23)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1            ( 0x3<<24)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1            ( 0x3<<26)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1            ( 0x1<<28)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1            ( 0x1<<29)
+
+#define s_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0(x)            ( (x&0x1)<<0 )
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0(x)            ( (x&0x1)<<1 )
+#define s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0(x)           ( (x&0x7)<<2 )
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0(x)           ( (x&0x7)<<5 )
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0(x)        ( (x&0x1)<<8 )
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0(x)        ( (x&0x1)<<9 )
+#define s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0(x)            ( (x&0x3)<<10)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0(x)            ( (x&0x3)<<12)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0(x)            ( (x&0x1)<<14)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0(x)            ( (x&0x1)<<15)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1(x)           ( (x&0x7)<<16)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1(x)           ( (x&0x7)<<19)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1(x)        ( (x&0x1)<<22)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1(x)        ( (x&0x1)<<23)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1(x)            ( (x&0x3)<<24)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1(x)            ( (x&0x3)<<26)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1(x)            ( (x&0x1)<<28)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1(x)            ( (x&0x1)<<29)
+
+
+
+/* RGA_MMU_CTRL1 */
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_EN                  (  0x1<<0 )
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_FLUSH               (  0x1<<1 )
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_EN         (  0x1<<2 )
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_DIR        (  0x1<<3 )
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_EN                 (  0x1<<4 )
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_FLUSH              (  0x1<<5 )
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_EN        (  0x1<<6 )
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_DIR       (  0x1<<7 )
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_EN                  (  0x1<<8 )
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_FLUSH               (  0x1<<9 )
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_EN         (  0x1<<10 )
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_DIR        (  0x1<<11 )
+#define m_RGA2_MMU_CTRL1_SW_ELS_MMU_EN                  (  0x1<<12 )
+#define m_RGA2_MMU_CTRL1_SW_ELS_MMU_FLUSH               (  0x1<<13 )
+
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_EN(x)                  (  (x&0x1)<<0 )
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_FLUSH(x)               (  (x&0x1)<<1 )
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_EN(x)         (  (x&0x1)<<2 )
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_DIR(x)        (  (x&0x1)<<3 )
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_EN(x)                 (  (x&0x1)<<4 )
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_FLUSH(x)              (  (x&0x1)<<5 )
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_EN(x)        (  (x&0x1)<<6 )
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_DIR(x)       (  (x&0x1)<<7 )
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_EN(x)                  (  (x&0x1)<<8 )
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_FLUSH(x)               (  (x&0x1)<<9 )
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_EN(x)         (  (x&0x1)<<10 )
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_DIR(x)        (  (x&0x1)<<11 )
+#define s_RGA2_MMU_CTRL1_SW_ELS_MMU_EN(x)                  (  (x&0x1)<<12 )
+#define s_RGA2_MMU_CTRL1_SW_ELS_MMU_FLUSH(x)               (  (x&0x1)<<13 )
+
+#define RGA2_VSP_BICUBIC_LIMIT                         1996
+
+#define RGA2_SYS_CTRL_OFFSET             0x0
+#define RGA2_CMD_CTRL_OFFSET             0x4
+#define RGA2_CMD_BASE_OFFSET             0x8
+#define RGA2_STATUS_OFFSET               0xc
+#define RGA2_INT_OFFSET                  0x10
+#define RGA2_MMU_CTRL0_OFFSET            0x14
+#define RGA2_MMU_CMD_BASE_OFFSET         0x18
+/* dst full csc */
+#define RGA2_DST_CSC_00_OFFSET                  0x0
+#define RGA2_DST_CSC_01_OFFSET                  0x4
+#define RGA2_DST_CSC_02_OFFSET                  0x8
+#define RGA2_DST_CSC_OFF0_OFFSET                0xc
+#define RGA2_DST_CSC_10_OFFSET                  0x10
+#define RGA2_DST_CSC_11_OFFSET                  0x14
+#define RGA2_DST_CSC_12_OFFSET                  0x18
+#define RGA2_DST_CSC_OFF1_OFFSET                0x1c
+#define RGA2_DST_CSC_20_OFFSET                  0x20
+#define RGA2_DST_CSC_21_OFFSET                  0x24
+#define RGA2_DST_CSC_22_OFFSET                  0x28
+#define RGA2_DST_CSC_OFF2_OFFSET                0x2c
+
+#define RGA2_MODE_CTRL_OFFSET                   0x00
+#define RGA2_SRC_INFO_OFFSET                    0x04
+#define RGA2_SRC_BASE0_OFFSET                   0x08
+#define RGA2_SRC_BASE1_OFFSET                   0x0c
+#define RGA2_SRC_BASE2_OFFSET                   0x10
+#define RGA2_SRC_BASE3_OFFSET                   0x14
+#define RGA2_SRC_VIR_INFO_OFFSET                0x18
+#define RGA2_SRC_ACT_INFO_OFFSET                0x1c
+#define RGA2_SRC_X_FACTOR_OFFSET                0x20
+#define RGA2_SRC_Y_FACTOR_OFFSET                0x24
+#define RGA2_SRC_BG_COLOR_OFFSET                0x28
+#define RGA2_SRC_FG_COLOR_OFFSET                0x2c
+#define RGA2_SRC_TR_COLOR0_OFFSET               0x30
+#define RGA2_CF_GR_A_OFFSET                     0x30 // repeat
+#define RGA2_SRC_TR_COLOR1_OFFSET               0x34
+#define RGA2_CF_GR_B_OFFSET                     0x34 // repeat
+#define RGA2_DST_INFO_OFFSET                    0x38
+#define RGA2_DST_BASE0_OFFSET                   0x3c
+#define RGA2_DST_BASE1_OFFSET                   0x40
+#define RGA2_DST_BASE2_OFFSET                   0x44
+#define RGA2_DST_VIR_INFO_OFFSET                0x48
+#define RGA2_DST_ACT_INFO_OFFSET                0x4c
+#define RGA2_ALPHA_CTRL0_OFFSET                 0x50
+#define RGA2_ALPHA_CTRL1_OFFSET                 0x54
+#define RGA2_FADING_CTRL_OFFSET                 0x58
+#define RGA2_PAT_CON_OFFSET                     0x5c
+#define RGA2_ROP_CTRL0_OFFSET                   0x60
+#define RGA2_CF_GR_G_OFFSET                     0x60 // repeat
+#define RGA2_DST_Y4MAP_LUT0_OFFSET             0x60 // repeat
+#define RGA2_DST_QUANTIZE_SCALE_OFFSET         0x60 // repeat
+#define RGA2_ROP_CTRL1_OFFSET                   0x64
+#define RGA2_CF_GR_R_OFFSET                     0x64 // repeat
+#define RGA2_DST_Y4MAP_LUT1_OFFSET              0x64 // repeat
+#define RGA2_DST_QUANTIZE_OFFSET_OFFSET         0x64 // repeat
+#define RGA2_MASK_BASE_OFFSET                   0x68
+#define RGA2_MMU_CTRL1_OFFSET                   0x6c
+#define RGA2_MMU_SRC_BASE_OFFSET                0x70
+#define RGA2_MMU_SRC1_BASE_OFFSET               0x74
+#define RGA2_MMU_DST_BASE_OFFSET                0x78
+#define RGA2_MMU_ELS_BASE_OFFSET                0x7c
+
+int RGA2_gen_reg_info(unsigned char *base, unsigned char *csc_base, struct rga2_req *msg);
+void RGA_MSG_2_RGA2_MSG(struct rga_req *req_rga, struct rga2_req *req);
+void RGA_MSG_2_RGA2_MSG_32(struct rga_req_32 *req_rga, struct rga2_req *req);
+
+
+
+#endif
+
diff --git a/drivers/video/rockchip/rga2/rga2_rop.h b/drivers/video/rockchip/rga2/rga2_rop.h
new file mode 100644
index 0000000000000..dc2a343f4c5cd
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_rop.h
@@ -0,0 +1,56 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_ROP_H__
+#define __RGA_ROP_H__
+
+unsigned int RGA2_ROP3_code[256] =
+{
+    0x00000007, 0x00000451, 0x00006051, 0x00800051, 0x00007041, 0x00800041, 0x00804830, 0x000004f0,//0
+    0x00800765, 0x000004b0, 0x00000065, 0x000004f4, 0x00000075, 0x000004e6, 0x00804850, 0x00800005,
+
+    0x00006850, 0x00800050, 0x00805028, 0x00000568, 0x00804031, 0x00000471, 0x002b6071, 0x018037aa,//1
+    0x008007aa, 0x00036071, 0x00002c6a, 0x00803631, 0x00002d68, 0x00802721, 0x008002d0, 0x000006d0,
+
+    0x0080066e, 0x00000528, 0x00000066, 0x0000056c, 0x018007aa, 0x0002e06a, 0x00003471, 0x00834031,//2
+    0x00800631, 0x0002b471, 0x00006071, 0x008037aa, 0x000036d0, 0x008002d4, 0x00002d28, 0x000006d4,
+
+    0x0000006e, 0x00000565, 0x00003451, 0x00800006, 0x000034f0, 0x00834830, 0x00800348, 0x00000748,//3
+    0x00002f48, 0x0080034c, 0x000034b0, 0x0000074c, 0x00000031, 0x00834850, 0x000034e6, 0x00800071,
+
+    0x008006f4, 0x00000431, 0x018007a1, 0x00b6e870, 0x00000074, 0x0000046e, 0x00002561, 0x00802f28,//4
+    0x00800728, 0x0002a561, 0x000026c2, 0x008002c6, 0x00007068, 0x018035aa, 0x00002c2a, 0x000006c6,
+
+    0x0000006c, 0x00000475, 0x000024e2, 0x008036b0, 0x00804051, 0x00800004, 0x00800251, 0x00000651,
+    0x00002e4a, 0x0080024e, 0x00000028, 0x00824842, 0x000024a2, 0x0000064e, 0x000024f4, 0x00800068,//5
+
+    0x008006b0, 0x000234f0, 0x00002741, 0x00800345, 0x00003651, 0x00800255, 0x00000030, 0x00834051,
+    0x00a34842, 0x000002b0, 0x00800271, 0x0002b651, 0x00800368, 0x0002a741, 0x0000364e, 0x00806830,//6
+
+    0x00006870, 0x008037a2, 0x00003431, 0x00000745, 0x00002521, 0x00000655, 0x0000346e, 0x00800062,
+    0x008002f0, 0x000236d0, 0x000026d4, 0x00807028, 0x000036c6, 0x00806031, 0x008005aa, 0x00000671,//7
+
+    0x00800671, 0x000005aa, 0x00006031, 0x008036c6, 0x00007028, 0x00802e55, 0x008236d0, 0x000002f0,
+    0x00000070, 0x0080346e, 0x00800655, 0x00802521, 0x00800745, 0x00803431, 0x000037a2, 0x00806870,//8
+
+    0x00006830, 0x0080364e, 0x00822f48, 0x00000361, 0x0082b651, 0x00000271, 0x00800231, 0x002b4051,
+    0x00034051, 0x00800030, 0x0080026e, 0x00803651, 0x0080036c, 0x00802741, 0x008234f0, 0x000006b0,//9
+
+    0x00000068, 0x00802c75, 0x0080064e, 0x008024a2, 0x0002c04a, 0x00800021, 0x00800275, 0x00802e51,
+    0x00800651, 0x00000251, 0x00800000, 0x00004051, 0x000036b0, 0x008024e2, 0x00800475, 0x00000045,//a
+
+    0x008006c6, 0x00802c2a, 0x000035aa, 0x00807068, 0x008002f4, 0x008026c2, 0x00822d68, 0x00000728,
+    0x00002f28, 0x00802561, 0x0080046e, 0x00000046, 0x00836870, 0x000007a2, 0x00800431, 0x00004071,//b
+
+    0x00000071, 0x008034e6, 0x00034850, 0x00800031, 0x0080074c, 0x008034b0, 0x00800365, 0x00802f48,
+    0x00800748, 0x00000341, 0x000026a2, 0x008034f0, 0x00800002, 0x00005048, 0x00800565, 0x00000055,//c
+
+    0x008006d4, 0x00802d28, 0x008002e6, 0x008036d0, 0x000037aa, 0x00806071, 0x0082b471, 0x00000631,
+    0x00002e2a, 0x00803471, 0x00826862, 0x010007aa, 0x0080056c, 0x00000054, 0x00800528, 0x00005068,//d
+
+    0x008006d0, 0x000002d0, 0x00002721, 0x00802d68, 0x00003631, 0x00802c6a, 0x00836071, 0x000007aa,
+    0x010037aa, 0x00a36870, 0x00800471, 0x00004031, 0x00800568, 0x00005028, 0x00000050, 0x00800545,//e
+
+    0x00800001, 0x00004850, 0x008004e6, 0x0000004e, 0x008004f4, 0x0000004c, 0x008004b0, 0x00004870,
+    0x008004f0, 0x00004830, 0x00000048, 0x0080044e, 0x00000051, 0x008004d4, 0x00800451, 0x00800007,//f
+};
+
+#endif
diff --git a/drivers/video/rockchip/rga2/rga2_type.h b/drivers/video/rockchip/rga2/rga2_type.h
new file mode 100644
index 0000000000000..30f5df2f38e58
--- /dev/null
+++ b/drivers/video/rockchip/rga2/rga2_type.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_TYPE_H__
+#define __RGA_TYPE_H__
+
+
+#ifdef __cplusplus
+#if __cplusplus
+}
+#endif
+#endif /* __cplusplus */
+
+typedef  unsigned int     UWORD32;
+typedef  unsigned int     uint32;
+typedef  unsigned int     RK_U32;
+
+typedef  unsigned short   UWORD16;
+typedef  unsigned short   RK_U16;
+
+typedef  unsigned char    UBYTE;
+typedef  unsigned char    RK_U8;
+
+typedef  int              WORD32;
+typedef  int              RK_S32;
+
+typedef  short            WORD16;
+typedef  short            RK_S16;
+
+typedef  char             BYTE;
+typedef  char             RK_S8;
+
+
+#ifndef NULL
+#define NULL              0L
+#endif
+
+#ifndef TRUE
+#define TRUE              1L
+#endif
+
+
+#ifdef __cplusplus
+#if __cplusplus
+}
+#endif
+#endif /* __cplusplus */
+
+
+#endif /* __RGA_TYPR_H__ */
+
diff --git a/drivers/video/rockchip/rga3/Kconfig b/drivers/video/rockchip/rga3/Kconfig
new file mode 100644
index 0000000000000..c8c96b2d67ffb
--- /dev/null
+++ b/drivers/video/rockchip/rga3/Kconfig
@@ -0,0 +1,37 @@
+# SPDX-License-Identifier: GPL-2.0
+menuconfig ROCKCHIP_MULTI_RGA
+	tristate "MULTI_RGA"
+	depends on ARCH_ROCKCHIP
+	help
+	  multi_rga module.
+
+if ROCKCHIP_MULTI_RGA
+
+config ROCKCHIP_RGA_ASYNC
+	bool "Enable async mode"
+	depends on SYNC_FILE
+	default y
+	help
+	  Asynchronous calls will be supported.
+
+config ROCKCHIP_RGA_PROC_FS
+	bool "Enable RGA procfs"
+	select ROCKCHIP_RGA_DEBUGGER
+	depends on PROC_FS
+	help
+	  Enable procfs to debug multi RGA driver.
+
+config ROCKCHIP_RGA_DEBUG_FS
+	bool "Enable RGA debugfs"
+	select ROCKCHIP_RGA_DEBUGGER
+	depends on DEBUG_FS
+	default y
+	help
+	  Enable debugfs to debug multi RGA driver.
+
+config ROCKCHIP_RGA_DEBUGGER
+	bool
+	help
+	  Enabling the debugger of multi RGA, you can use procfs and debugfs for debugging.
+
+endif
diff --git a/drivers/video/rockchip/rga3/Makefile b/drivers/video/rockchip/rga3/Makefile
new file mode 100644
index 0000000000000..11f401de2ae0b
--- /dev/null
+++ b/drivers/video/rockchip/rga3/Makefile
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0
+
+ccflags-y += -I$(srctree)/$(src)/include
+
+rga3-y	:= rga_drv.o rga_common.o rga3_reg_info.o rga_iommu.o rga_dma_buf.o rga_job.o rga_hw_config.o rga2_reg_info.o rga_policy.o rga_mm.o
+rga3-$(CONFIG_ROCKCHIP_RGA_ASYNC) += rga_fence.o
+rga3-$(CONFIG_ROCKCHIP_RGA_DEBUGGER) += rga_debugger.o
+
+obj-$(CONFIG_ROCKCHIP_MULTI_RGA)	+= rga3.o
diff --git a/drivers/video/rockchip/rga3/include/rga.h b/drivers/video/rockchip/rga3/include/rga.h
new file mode 100644
index 0000000000000..c54e4a8976950
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga.h
@@ -0,0 +1,1000 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _RGA_DRIVER_H_
+#define _RGA_DRIVER_H_
+
+#include <linux/mutex.h>
+#include <linux/scatterlist.h>
+
+/* Use 'r' as magic number */
+#define RGA_IOC_MAGIC		'r'
+#define RGA_IOW(nr, type)	_IOW(RGA_IOC_MAGIC, nr, type)
+#define RGA_IOR(nr, type)	_IOR(RGA_IOC_MAGIC, nr, type)
+#define RGA_IOWR(nr, type)	_IOWR(RGA_IOC_MAGIC, nr, type)
+
+#define RGA_IOC_GET_DRVIER_VERSION	RGA_IOR(0x1, struct rga_version_t)
+#define RGA_IOC_GET_HW_VERSION		RGA_IOR(0x2, struct rga_hw_versions_t)
+#define RGA_IOC_IMPORT_BUFFER		RGA_IOWR(0x3, struct rga_buffer_pool)
+#define RGA_IOC_RELEASE_BUFFER		RGA_IOW(0x4, struct rga_buffer_pool)
+#define RGA_IOC_REQUEST_CREATE		RGA_IOR(0x5, uint32_t)
+#define RGA_IOC_REQUEST_SUBMIT		RGA_IOWR(0x6, struct rga_user_request)
+#define RGA_IOC_REQUEST_CONFIG		RGA_IOWR(0x7, struct rga_user_request)
+#define RGA_IOC_REQUEST_CANCEL		RGA_IOWR(0x8, uint32_t)
+
+#define RGA_BLIT_SYNC			0x5017
+#define RGA_BLIT_ASYNC			0x5018
+#define RGA_FLUSH			0x5019
+#define RGA_GET_RESULT			0x501a
+#define RGA_GET_VERSION			0x501b
+#define RGA_CACHE_FLUSH			0x501c
+
+#define RGA2_GET_VERSION		0x601b
+#define RGA_IMPORT_DMA			0x601d
+#define RGA_RELEASE_DMA			0x601e
+
+#define RGA_TASK_NUM_MAX		256
+
+#define RGA_OUT_OF_RESOURCES		-10
+#define RGA_MALLOC_ERROR		-11
+
+#define SCALE_DOWN_LARGE		1
+#define SCALE_UP_LARGE			1
+
+#define RGA_BUFFER_POOL_SIZE_MAX 40
+
+#define RGA3_MAJOR_VERSION_MASK	 (0xF0000000)
+#define RGA3_MINOR_VERSION_MASK	 (0x0FF00000)
+#define RGA3_SVN_VERSION_MASK	 (0x000FFFFF)
+
+#define RGA2_MAJOR_VERSION_MASK	 (0xFF000000)
+#define RGA2_MINOR_VERSION_MASK	 (0x00F00000)
+#define RGA2_SVN_VERSION_MASK	 (0x000FFFFF)
+
+#define RGA_MODE_ROTATE_0	 (1<<0)
+#define RGA_MODE_ROTATE_90	 (1<<1)
+#define RGA_MODE_ROTATE_180	 (1<<2)
+#define RGA_MODE_ROTATE_270	 (1<<3)
+#define RGA_MODE_X_MIRROR	 (1<<4)
+#define RGA_MODE_Y_MIRROR	 (1<<5)
+
+#define RGA_MODE_CSC_BT601L	 (1<<0)
+#define RGA_MODE_CSC_BT601F	 (1<<1)
+#define RGA_MODE_CSC_BT709	 (1<<2)
+#define RGA_MODE_CSC_BT2020	 (1<<3)
+
+#define RGA_MODE_ROTATE_MASK (\
+		RGA_MODE_ROTATE_0 | \
+		RGA_MODE_ROTATE_90 | \
+		RGA_MODE_ROTATE_180 | \
+		RGA_MODE_ROTATE_270 | \
+		RGA_MODE_X_MIRROR | \
+		RGA_MODE_Y_MIRROR)
+
+enum rga_memory_type {
+	RGA_DMA_BUFFER = 0,
+	RGA_VIRTUAL_ADDRESS,
+	RGA_PHYSICAL_ADDRESS,
+	RGA_DMA_BUFFER_PTR,
+};
+
+enum rga_scale_up_mode {
+	RGA_SCALE_UP_NONE	= 0x0,
+	RGA_SCALE_UP_BIC	= 0x1,
+};
+
+enum rga_scale_down_mode {
+	RGA_SCALE_DOWN_NONE	= 0x0,
+	RGA_SCALE_DOWN_AVG	= 0x1,
+};
+
+enum RGA_SCHEDULER_CORE {
+	RGA3_SCHEDULER_CORE0	= 1 << 0,
+	RGA3_SCHEDULER_CORE1	= 1 << 1,
+	RGA2_SCHEDULER_CORE0	= 1 << 2,
+	RGA2_SCHEDULER_CORE1	= 1 << 3,
+	RGA_CORE_MASK		= 0xf,
+	RGA_NONE_CORE		= 0x0,
+};
+
+enum rga_scale_interp {
+	RGA_INTERP_DEFAULT   = 0x0,
+	RGA_INTERP_LINEAR    = 0x1,
+	RGA_INTERP_BICUBIC   = 0x2,
+	RGA_INTERP_AVERAGE   = 0x3,
+};
+
+/* RGA process mode enum */
+enum {
+	BITBLT_MODE			= 0x0,
+	COLOR_PALETTE_MODE		= 0x1,
+	COLOR_FILL_MODE			= 0x2,
+	/* used by rga2 */
+	UPDATE_PALETTE_TABLE_MODE	= 0x6,
+	UPDATE_PATTEN_BUF_MODE		= 0x7,
+}; /*render mode*/
+
+/* RGA rd_mode */
+enum {
+	RGA_RASTER_MODE			 = 0x1 << 0,
+	RGA_FBC_MODE			 = 0x1 << 1,
+	RGA_TILE_MODE			 = 0x1 << 2,
+	RGA_TILE4x4_MODE		 = 0x1 << 3,
+	RGA_RKFBC_MODE			 = 0x1 << 4,
+	RGA_AFBC32x8_MODE		 = 0x1 << 5,
+};
+
+enum {
+	RGA_10BIT_COMPACT		= 0x0,
+	RGA_10BIT_INCOMPACT		= 0x1,
+};
+
+enum {
+	RGA_CONTEXT_NONE		= 0x0,
+	RGA_CONTEXT_SRC_FIX_ENABLE	= 0x1 << 0,
+	RGA_CONTEXT_SRC_CACHE_INFO	= 0x1 << 1,
+	RGA_CONTEXT_SRC_MASK		= RGA_CONTEXT_SRC_FIX_ENABLE |
+					  RGA_CONTEXT_SRC_CACHE_INFO,
+	RGA_CONTEXT_PAT_FIX_ENABLE	= 0x1 << 2,
+	RGA_CONTEXT_PAT_CACHE_INFO	= 0x1 << 3,
+	RGA_CONTEXT_PAT_MASK		= RGA_CONTEXT_PAT_FIX_ENABLE |
+					  RGA_CONTEXT_PAT_CACHE_INFO,
+	RGA_CONTEXT_DST_FIX_ENABLE	= 0x1 << 4,
+	RGA_CONTEXT_DST_CACHE_INFO	= 0x1 << 5,
+	RGA_CONTEXT_DST_MASK		= RGA_CONTEXT_DST_FIX_ENABLE |
+					  RGA_CONTEXT_DST_CACHE_INFO,
+};
+
+/* RGA feature */
+enum {
+	RGA_COLOR_FILL			= 0x1 << 0,
+	RGA_COLOR_PALETTE		= 0x1 << 1,
+	RGA_COLOR_KEY			= 0x1 << 2,
+	RGA_ROP_CALCULATE		= 0x1 << 3,
+	RGA_NN_QUANTIZE			= 0x1 << 4,
+	RGA_OSD_BLEND			= 0x1 << 5,
+	RGA_DITHER			= 0x1 << 6,
+	RGA_MOSAIC			= 0x1 << 7,
+	RGA_YIN_YOUT			= 0x1 << 8,
+	RGA_YUV_HDS			= 0x1 << 9,
+	RGA_YUV_VDS			= 0x1 << 10,
+	RGA_OSD				= 0x1 << 11,
+	RGA_PRE_INTR			= 0x1 << 12,
+	RGA_FULL_CSC			= 0x1 << 13,
+	RGA_GAUSS			= 0x1 << 14,
+};
+
+enum rga_surf_format {
+	RGA_FORMAT_RGBA_8888		= 0x0,
+	RGA_FORMAT_RGBX_8888		= 0x1,
+	RGA_FORMAT_RGB_888		= 0x2,
+	RGA_FORMAT_BGRA_8888		= 0x3,
+	RGA_FORMAT_RGB_565		= 0x4,
+	RGA_FORMAT_RGBA_5551		= 0x5,
+	RGA_FORMAT_RGBA_4444		= 0x6,
+	RGA_FORMAT_BGR_888		= 0x7,
+
+	RGA_FORMAT_YCbCr_422_SP		= 0x8,
+	RGA_FORMAT_YCbCr_422_P		= 0x9,
+	RGA_FORMAT_YCbCr_420_SP		= 0xa,
+	RGA_FORMAT_YCbCr_420_P		= 0xb,
+
+	RGA_FORMAT_YCrCb_422_SP		= 0xc,
+	RGA_FORMAT_YCrCb_422_P		= 0xd,
+	RGA_FORMAT_YCrCb_420_SP		= 0xe,
+	RGA_FORMAT_YCrCb_420_P		= 0xf,
+
+	RGA_FORMAT_BPP1			= 0x10,
+	RGA_FORMAT_BPP2			= 0x11,
+	RGA_FORMAT_BPP4			= 0x12,
+	RGA_FORMAT_BPP8			= 0x13,
+
+	RGA_FORMAT_Y4			= 0x14,
+	RGA_FORMAT_YCbCr_400		= 0x15,
+
+	RGA_FORMAT_BGRX_8888		= 0x16,
+
+	RGA_FORMAT_YVYU_422		= 0x18,
+	RGA_FORMAT_YVYU_420		= 0x19,
+	RGA_FORMAT_VYUY_422		= 0x1a,
+	RGA_FORMAT_VYUY_420		= 0x1b,
+	RGA_FORMAT_YUYV_422		= 0x1c,
+	RGA_FORMAT_YUYV_420		= 0x1d,
+	RGA_FORMAT_UYVY_422		= 0x1e,
+	RGA_FORMAT_UYVY_420		= 0x1f,
+
+	RGA_FORMAT_YCbCr_420_SP_10B	= 0x20,
+	RGA_FORMAT_YCrCb_420_SP_10B	= 0x21,
+	RGA_FORMAT_YCbCr_422_SP_10B	= 0x22,
+	RGA_FORMAT_YCrCb_422_SP_10B	= 0x23,
+
+	RGA_FORMAT_BGR_565		= 0x24,
+	RGA_FORMAT_BGRA_5551		= 0x25,
+	RGA_FORMAT_BGRA_4444		= 0x26,
+
+	RGA_FORMAT_ARGB_8888		= 0x28,
+	RGA_FORMAT_XRGB_8888		= 0x29,
+	RGA_FORMAT_ARGB_5551		= 0x2a,
+	RGA_FORMAT_ARGB_4444		= 0x2b,
+	RGA_FORMAT_ABGR_8888		= 0x2c,
+	RGA_FORMAT_XBGR_8888		= 0x2d,
+	RGA_FORMAT_ABGR_5551		= 0x2e,
+	RGA_FORMAT_ABGR_4444		= 0x2f,
+
+	RGA_FORMAT_RGBA_2BPP		= 0x30,
+	RGA_FORMAT_A8			= 0x31,
+
+	RGA_FORMAT_YCbCr_444_SP		= 0x32,
+	RGA_FORMAT_YCrCb_444_SP		= 0x33,
+
+	RGA_FORMAT_Y8			= 0x34,
+
+	RGA_FORMAT_UNKNOWN		= 0x100,
+};
+
+enum rga_alpha_mode {
+	RGA_ALPHA_STRAIGHT		= 0,
+	RGA_ALPHA_INVERSE		= 1,
+};
+
+enum rga_global_blend_mode {
+	RGA_ALPHA_GLOBAL		= 0,
+	RGA_ALPHA_PER_PIXEL		= 1,
+	RGA_ALPHA_PER_PIXEL_GLOBAL	= 2,
+};
+
+enum rga_alpha_cal_mode {
+	RGA_ALPHA_SATURATION		= 0,
+	RGA_ALPHA_NO_SATURATION		= 1,
+};
+
+enum rga_factor_mode {
+	RGA_ALPHA_ZERO			= 0,
+	RGA_ALPHA_ONE			= 1,
+	/*
+	 *   When used as a factor for the SRC channel, it indicates
+	 * the use of the DST channel's alpha value, and vice versa.
+	 */
+	RGA_ALPHA_OPPOSITE		= 2,
+	RGA_ALPHA_OPPOSITE_INVERSE	= 3,
+	RGA_ALPHA_OWN			= 4,
+};
+
+enum rga_color_mode {
+	RGA_ALPHA_PRE_MULTIPLIED	= 0,
+	RGA_ALPHA_NO_PRE_MULTIPLIED	= 1,
+};
+
+enum rga_alpha_blend_mode {
+	RGA_ALPHA_NONE			= 0,
+	RGA_ALPHA_BLEND_SRC,
+	RGA_ALPHA_BLEND_DST,
+	RGA_ALPHA_BLEND_SRC_OVER,
+	RGA_ALPHA_BLEND_DST_OVER,
+	RGA_ALPHA_BLEND_SRC_IN,
+	RGA_ALPHA_BLEND_DST_IN,
+	RGA_ALPHA_BLEND_SRC_OUT,
+	RGA_ALPHA_BLEND_DST_OUT,
+	RGA_ALPHA_BLEND_SRC_ATOP,
+	RGA_ALPHA_BLEND_DST_ATOP,
+	RGA_ALPHA_BLEND_XOR,
+	RGA_ALPHA_BLEND_CLEAR,
+};
+
+#define RGA_SCHED_PRIORITY_DEFAULT 0
+#define RGA_SCHED_PRIORITY_MAX 6
+
+#define RGA_VERSION_SIZE	16
+#define RGA_HW_SIZE		5
+
+struct rga_version_t {
+	uint32_t major;
+	uint32_t minor;
+	uint32_t revision;
+	uint8_t str[RGA_VERSION_SIZE];
+};
+
+struct rga_hw_versions_t {
+	struct rga_version_t version[RGA_HW_SIZE];
+	uint32_t size;
+};
+
+struct rga_memory_parm {
+	uint32_t width;
+	uint32_t height;
+	uint32_t format;
+
+	uint32_t size;
+};
+
+struct rga_external_buffer {
+	uint64_t memory;
+	uint32_t type;
+
+	uint32_t handle;
+	struct rga_memory_parm memory_parm;
+
+	uint8_t reserve[252];
+};
+
+struct rga_buffer_pool {
+	uint64_t buffers_ptr;
+	uint32_t size;
+};
+
+struct rga_mmu_info_t {
+	unsigned long src0_base_addr;
+	unsigned long src1_base_addr;
+	unsigned long dst_base_addr;
+	unsigned long els_base_addr;
+
+	/* [0] mmu enable [1] flush [2] prefetch_en [3] prefetch dir */
+	u8 src0_mmu_flag;
+	u8 src1_mmu_flag;
+	u8 dst_mmu_flag;
+	u8 els_mmu_flag;
+};
+
+struct rga_color_fill_t {
+	int16_t gr_x_a;
+	int16_t gr_y_a;
+	int16_t gr_x_b;
+	int16_t gr_y_b;
+	int16_t gr_x_g;
+	int16_t gr_y_g;
+	int16_t gr_x_r;
+	int16_t gr_y_r;
+};
+
+/***************************************/
+/* porting from rga.h for msg convert */
+/***************************************/
+
+struct rga_fading_t {
+	uint8_t b;
+	uint8_t g;
+	uint8_t r;
+	uint8_t res;
+};
+
+struct rga_mmu_t {
+	uint8_t mmu_en;
+	uint64_t base_addr;
+	/*
+	 * [0] mmu enable [1] src_flush [2] dst_flush
+	 * [3] CMD_flush [4~5] page size
+	 */
+	uint32_t mmu_flag;
+};
+
+struct rga_rect_t {
+	uint16_t xmin;
+	/* width - 1 */
+	uint16_t xmax;
+	uint16_t ymin;
+	/* height - 1 */
+	uint16_t ymax;
+};
+
+struct rga_point_t {
+	uint16_t x;
+	uint16_t y;
+};
+
+struct rga_line_draw_t {
+	/* LineDraw_start_point	*/
+	struct rga_point_t start_point;
+	/* LineDraw_end_point */
+	struct rga_point_t end_point;
+	/* LineDraw_color */
+	uint32_t color;
+	/* (enum) LineDrawing mode sel */
+	uint32_t flag;
+	/* range 1~16 */
+	uint32_t line_width;
+};
+
+/* color space convert coefficient. */
+struct rga_csc_coe {
+	int16_t r_v;
+	int16_t g_y;
+	int16_t b_u;
+	int32_t off;
+};
+
+struct rga_full_csc {
+	uint8_t flag;
+	struct rga_csc_coe coe_y;
+	struct rga_csc_coe coe_u;
+	struct rga_csc_coe coe_v;
+};
+
+struct rga_csc_range {
+	uint16_t max;
+	uint16_t min;
+};
+
+struct rga_csc_clip {
+	struct rga_csc_range y;
+	struct rga_csc_range uv;
+};
+
+struct rga_mosaic_info {
+	uint8_t enable;
+	uint8_t mode;
+};
+
+struct rga_gauss_config {
+	uint32_t size;
+	uint64_t coe_ptr;
+};
+
+/* MAX(min, (max - channel_value)) */
+struct rga_osd_invert_factor {
+	uint8_t alpha_max;
+	uint8_t alpha_min;
+	uint8_t yg_max;
+	uint8_t yg_min;
+	uint8_t crb_max;
+	uint8_t crb_min;
+};
+
+struct rga_color {
+	union {
+		struct {
+			uint8_t red;
+			uint8_t green;
+			uint8_t blue;
+			uint8_t alpha;
+		};
+		uint32_t value;
+	};
+};
+
+struct rga_osd_bpp2 {
+	uint8_t  ac_swap;		// ac swap flag
+					// 0: CA
+					// 1: AC
+	uint8_t  endian_swap;		// rgba2bpp endian swap
+					// 0: Big endian
+					// 1: Little endian
+	struct rga_color color0;
+	struct rga_color color1;
+};
+
+struct rga_osd_mode_ctrl {
+	uint8_t mode;			// OSD cal mode:
+					//   0b'1: statistics mode
+					//   1b'1: auto inversion overlap mode
+	uint8_t direction_mode;		// horizontal or vertical
+					//   0: horizontal
+					//   1: vertical
+	uint8_t width_mode;		// using @fix_width or LUT width
+					//   0: fix width
+					//   1: LUT width
+	uint16_t block_fix_width;	// OSD block fixed width
+					//   real width = (fix_width + 1) * 2
+	uint8_t block_num;		// OSD block num
+	uint16_t flags_index;		// auto invert flags index
+
+	/* invertion config */
+	uint8_t color_mode;		// selete color
+					//   0: src1 color
+					//   1: config data color
+	uint8_t invert_flags_mode;	// invert flag selete
+					//   0: use RAM flag
+					//   1: usr last result
+	uint8_t default_color_sel;	// default color mode
+					//   0: default is bright
+					//   1: default is dark
+	uint8_t invert_enable;		// invert channel enable
+					//   1 << 0: alpha enable
+					//   1 << 1: Y/G disable
+					//   1 << 3: C/RB disable
+	uint8_t invert_mode;		// invert cal mode
+					//   0: normal(max-data)
+					//   1: swap
+	uint8_t invert_thresh;		// if luma > thresh, osd_flag to be 1
+	uint8_t unfix_index;		// OSD width config index
+};
+
+struct rga_osd_info {
+	uint8_t  enable;
+
+	struct rga_osd_mode_ctrl mode_ctrl;
+	struct rga_osd_invert_factor cal_factor;
+	struct rga_osd_bpp2 bpp2_info;
+
+	union {
+		struct {
+			uint32_t last_flags0;
+			uint32_t last_flags1;
+		};
+		uint64_t last_flags;
+	};
+
+	union {
+		struct {
+			uint32_t cur_flags0;
+			uint32_t cur_flags1;
+		};
+		uint64_t cur_flags;
+	};
+};
+
+struct rga_pre_intr_info {
+	uint8_t enable;
+
+	uint8_t read_intr_en;
+	uint8_t write_intr_en;
+	uint8_t read_hold_en;
+	uint32_t read_threshold;
+	uint32_t write_start;
+	uint32_t write_step;
+};
+
+struct rga_win_info_t {
+	/* yrgb	mem addr */
+	unsigned long yrgb_addr;
+	/* cb/cr mem addr */
+	unsigned long uv_addr;
+	/* cr mem addr */
+	unsigned long v_addr;
+	/* definition by RK_FORMAT */
+	unsigned int format;
+
+	unsigned short src_act_w;
+	unsigned short src_act_h;
+
+	unsigned short dst_act_w;
+	unsigned short dst_act_h;
+
+	unsigned short x_offset;
+	unsigned short y_offset;
+
+	unsigned short vir_w;
+	unsigned short vir_h;
+
+	unsigned short y2r_mode;
+	unsigned short r2y_mode;
+
+	unsigned short rotate_mode;
+	/* RASTER or FBCD or TILE */
+	unsigned short rd_mode;
+
+	unsigned short is_10b_compact;
+	unsigned short is_10b_endian;
+
+	unsigned short enable;
+};
+
+struct rga_img_info_t {
+	/* yrgb	mem addr */
+	uint64_t yrgb_addr;
+	/* cb/cr mem addr */
+	uint64_t uv_addr;
+	/* cr mem addr */
+	uint64_t v_addr;
+	/* definition by RK_FORMAT */
+	uint32_t format;
+
+	uint16_t act_w;
+	uint16_t act_h;
+	uint16_t x_offset;
+	uint16_t y_offset;
+
+	uint16_t vir_w;
+	uint16_t vir_h;
+
+	uint16_t endian_mode;
+	/* useless */
+	uint16_t alpha_swap;
+
+	/* used by RGA3 */
+	uint16_t rotate_mode;
+	uint16_t rd_mode;
+
+	uint16_t compact_mode;
+	uint16_t is_10b_endian;
+
+	uint16_t enable;
+};
+
+struct rga_feature {
+	uint32_t global_alpha_en:1;
+	uint32_t full_csc_clip_en:1;
+	uint32_t user_close_fence:1;
+};
+
+struct rga_interp {
+	uint8_t horiz:4;
+	uint8_t verti:4;
+};
+
+struct rga_iommu_prefetch {
+	uint32_t y_threshold;
+	uint32_t uv_threshold;
+};
+
+struct rga_rgba5551_alpha {
+	uint16_t flags;
+	uint8_t alpha0;
+	uint8_t alpha1;
+};
+
+struct rga_req {
+	/* (enum) process mode sel */
+	uint8_t render_mode;
+
+	struct rga_img_info_t src;
+	struct rga_img_info_t dst;
+	struct rga_img_info_t pat;
+
+	/* rop4 mask addr */
+	uint64_t rop_mask_addr;
+	/* LUT addr */
+	uint64_t LUT_addr;
+
+	/* dst clip window default value is dst_vir */
+	/* value from [0, w-1] / [0, h-1]*/
+	struct rga_rect_t clip;
+
+	/* dst angle default value 0 16.16 scan from table */
+	int32_t sina;
+	/* dst angle default value 0 16.16 scan from table */
+	int32_t cosa;
+
+	/* alpha rop process flag		 */
+	/* ([0] = 1 alpha_rop_enable)	 */
+	/* ([1] = 1 rop enable)			 */
+	/* ([2] = 1 fading_enable)		 */
+	/* ([3] = 1 PD_enable)			 */
+	/* ([4] = 1 alpha cal_mode_sel)	 */
+	/* ([5] = 1 dither_enable)		 */
+	/* ([6] = 1 gradient fill mode sel) */
+	/* ([7] = 1 AA_enable)			 */
+	uint16_t alpha_rop_flag;
+
+	union {
+		struct rga_interp interp;
+		/* 0 nearst / 1 bilnear / 2 bicubic */
+		uint8_t scale_mode;
+	};
+
+	/* color key max */
+	uint32_t color_key_max;
+	/* color key min */
+	uint32_t color_key_min;
+
+	/* foreground color */
+	uint32_t fg_color;
+	/* background color */
+	uint32_t bg_color;
+
+	/* color fill use gradient */
+	struct rga_color_fill_t gr_color;
+
+	struct rga_line_draw_t line_draw_info;
+
+	struct rga_fading_t fading;
+
+	/* porter duff alpha mode sel */
+	uint8_t PD_mode;
+
+	/* legacy: global alpha value */
+	uint8_t alpha_global_value;
+
+	/* rop2/3/4 code scan from rop code table*/
+	uint16_t rop_code;
+
+	/* [2] 0 blur 1 sharp / [1:0] filter_type*/
+	uint8_t bsfilter_flag;
+
+	/* (enum) color palette 0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+	uint8_t palette_mode;
+
+	/* (enum) BT.601 MPEG / BT.601 JPEG / BT.709 */
+	uint8_t yuv2rgb_mode;
+
+	/* 0/big endian 1/little endian*/
+	uint8_t endian_mode;
+
+	/* (enum) rotate mode */
+	/* 0x0,	 no rotate */
+	/* 0x1,	 rotate	 */
+	/* 0x2,	 x_mirror */
+	/* 0x3,	 y_mirror */
+	uint8_t rotate_mode;
+
+	/* 0 solid color / 1 pattern color */
+	uint8_t color_fill_mode;
+
+	/* mmu information */
+	struct rga_mmu_t mmu_info;
+
+	/* ([0~1] alpha mode)			*/
+	/* ([2~3] rop mode)			*/
+	/* ([4] zero mode en)		 */
+	/* ([5] dst alpha mode)	 */
+	/* ([6] alpha output mode sel) 0 src / 1 dst*/
+	uint8_t alpha_rop_mode;
+
+	uint8_t src_trans_mode;
+
+	uint8_t dither_mode;
+
+	/* full color space convert */
+	struct rga_full_csc full_csc;
+
+	int32_t in_fence_fd;
+	uint8_t core;
+	uint8_t priority;
+	int32_t out_fence_fd;
+
+	uint8_t handle_flag;
+
+	/* RGA2 1106 add */
+	struct rga_mosaic_info mosaic_info;
+
+	uint8_t uvhds_mode;
+	uint8_t uvvds_mode;
+
+	struct rga_osd_info osd_info;
+
+	struct rga_pre_intr_info pre_intr_info;
+
+	/* global alpha */
+	uint8_t fg_global_alpha;
+	uint8_t bg_global_alpha;
+
+	struct rga_feature feature;
+
+	struct rga_csc_clip full_csc_clip;
+
+	struct rga_rgba5551_alpha rgba5551_alpha;
+
+	struct rga_gauss_config gauss_config;
+
+	uint8_t reservr[24];
+};
+
+struct rga_alpha_config {
+	bool enable;
+	bool fg_pre_multiplied;
+	bool bg_pre_multiplied;
+	bool fg_pixel_alpha_en;
+	bool bg_pixel_alpha_en;
+	bool fg_global_alpha_en;
+	bool bg_global_alpha_en;
+	uint16_t fg_global_alpha_value;
+	uint16_t bg_global_alpha_value;
+	enum rga_alpha_blend_mode mode;
+};
+
+struct rga2_req {
+	/* (enum) process mode sel */
+	u8 render_mode;
+
+	/* active window */
+	struct rga_img_info_t src;
+	struct rga_img_info_t src1;
+	struct rga_img_info_t dst;
+	struct rga_img_info_t pat;
+
+	/* rop4 mask addr */
+	unsigned long rop_mask_addr;
+	/* LUT addr */
+	unsigned long LUT_addr;
+
+	u32 rop_mask_stride;
+
+	/* 0: SRC + DST => DST	 */
+	/* 1: SRC + SRC1 => DST	 */
+	u8 bitblt_mode;
+
+	/* [1:0] */
+	/* 0 degree 0x0				 */
+	/* 90 degree 0x1				 */
+	/* 180 degree 0x2				 */
+	/* 270 degree 0x3				 */
+	/* [5:4]						 */
+	/* none				0x0		 */
+	/* x_mirror			0x1		 */
+	/* y_mirror			0x2		 */
+	/* x_mirror + y_mirror 0x3		 */
+	u8 rotate_mode;
+
+	/* alpha rop process flag		 */
+	/* ([0] = 1 alpha_rop_enable)	 */
+	/* ([1] = 1 rop enable)			 */
+	/* ([2] = 1 fading_enable)		 */
+	/* ([3] = 1 alpha cal_mode_sel)	 */
+	/* ([4] = 1 src_dither_up_enable) */
+	/* ([5] = 1 dst_dither_up_enable) */
+	/* ([6] = 1 dither_down_enable)	 */
+	/* ([7] = 1 gradient fill mode sel) */
+	u16 alpha_rop_flag;
+
+	struct rga_alpha_config alpha_config;
+
+	/* 0 1 2 3 */
+	u8 scale_bicu_mode;
+
+	u32 color_key_max;
+	u32 color_key_min;
+
+	/* foreground color */
+	u32 fg_color;
+	/* background color */
+	u32 bg_color;
+
+	u8 color_fill_mode;
+	/* color fill use gradient */
+	struct rga_color_fill_t gr_color;
+
+	/* Fading value */
+	u8 fading_alpha_value;
+	u8 fading_r_value;
+	u8 fading_g_value;
+	u8 fading_b_value;
+
+	/* src global alpha value */
+	u8 src_a_global_val;
+	/* dst global alpha value */
+	u8 dst_a_global_val;
+
+	/* rop mode select 0 : rop2 1 : rop3 2 : rop4 */
+	u8 rop_mode;
+	/* rop2/3/4 code */
+	u16 rop_code;
+
+	/* (enum) color palette 0/1bpp, 1/2bpp 2/4bpp 3/8bpp*/
+	u8 palette_mode;
+
+	/* (enum) BT.601 MPEG / BT.601 JPEG / BT.709 */
+	u8 yuv2rgb_mode;
+
+	u8 full_csc_en;
+
+	/* 0/little endian 1/big endian */
+	u8 endian_mode;
+
+	u8 CMD_fin_int_enable;
+
+	/* mmu information */
+	struct rga_mmu_info_t mmu_info;
+
+	u8 alpha_zero_key;
+	u8 src_trans_mode;
+
+	/* useless */
+	u8 alpha_swp;
+	u8 dither_mode;
+
+	u8 rgb2yuv_mode;
+
+	/* RGA2 1106 add */
+	struct rga_mosaic_info mosaic_info;
+
+	uint8_t yin_yout_en;
+
+	uint8_t uvhds_mode;
+	uint8_t uvvds_mode;
+
+	struct rga_osd_info osd_info;
+
+	struct rga_interp interp;
+
+	struct rga_iommu_prefetch iommu_prefetch;
+
+	struct rga_rgba5551_alpha rgba5551_alpha;
+
+	struct rga_gauss_config gauss_config;
+};
+
+struct rga3_req {
+	/* (enum) process mode sel */
+	u8 render_mode;
+
+	struct rga_win_info_t win0;
+	struct rga_win_info_t wr;
+	struct rga_win_info_t win1;
+
+	/* rop4 mask addr */
+	unsigned long rop_mask_addr;
+	unsigned long LUT_addr;
+
+	u32 rop_mask_stride;
+
+	u8 bitblt_mode;
+	u8 rotate_mode;
+
+	u16 alpha_rop_flag;
+
+	struct rga_alpha_config alpha_config;
+
+	/* for abb mode presever alpha. */
+	bool abb_alpha_pass;
+
+	u8 scale_bicu_mode;
+
+	u32 color_key_max;
+	u32 color_key_min;
+
+	u32 fg_color;
+	u32 bg_color;
+
+	u8 color_fill_mode;
+	struct rga_color_fill_t gr_color;
+
+	u8 fading_alpha_value;
+	u8 fading_r_value;
+	u8 fading_g_value;
+	u8 fading_b_value;
+
+	/* win0 global alpha value		*/
+	u8 win0_a_global_val;
+	/* win1 global alpha value		*/
+	u8 win1_a_global_val;
+
+	u8 rop_mode;
+	u16 rop_code;
+
+	u8 palette_mode;
+
+	u8 yuv2rgb_mode;
+
+	u8 endian_mode;
+
+	u8 CMD_fin_int_enable;
+
+	struct rga_mmu_info_t mmu_info;
+
+	u8 alpha_zero_key;
+	u8 src_trans_mode;
+
+	u8 alpha_swp;
+	u8 dither_mode;
+
+	u8 rgb2yuv_mode;
+};
+
+struct rga_video_frame_info {
+	uint32_t x_offset;
+	uint32_t y_offset;
+	uint32_t width;
+	uint32_t height;
+	uint32_t format;
+	uint32_t vir_w;
+	uint32_t vir_h;
+	uint32_t rd_mode;
+};
+
+struct rga_mpi_job_t {
+	struct dma_buf *dma_buf_src0;
+	struct dma_buf *dma_buf_src1;
+	struct dma_buf *dma_buf_dst;
+
+	struct rga_video_frame_info *src;
+	struct rga_video_frame_info *pat;
+	struct rga_video_frame_info *dst;
+	struct rga_video_frame_info *output;
+
+	int ctx_id;
+};
+
+struct rga_user_request {
+	uint64_t task_ptr;
+	uint32_t task_num;
+	uint32_t id;
+	uint32_t sync_mode;
+	uint32_t release_fence_fd;
+
+	uint32_t mpi_config_flags;
+
+	uint32_t acquire_fence_fd;
+
+	uint8_t reservr[120];
+};
+
+int rga_mpi_commit(struct rga_mpi_job_t *mpi_job);
+
+#endif /*_RGA_DRIVER_H_*/
diff --git a/drivers/video/rockchip/rga3/include/rga2_reg_info.h b/drivers/video/rockchip/rga3/include/rga2_reg_info.h
new file mode 100644
index 0000000000000..4afac19b0a164
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga2_reg_info.h
@@ -0,0 +1,529 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __REG2_INFO_H__
+#define __REG2_INFO_H__
+
+#include "rga_drv.h"
+
+#define RGA2_SYS_REG_BASE			0x000
+#define RGA2_CSC_REG_BASE			0x060
+#define RGA2_CMD_REG_BASE			0x100
+
+/* sys reg */
+#define RGA2_SYS_CTRL				0x000
+#define RGA2_CMD_CTRL				0x004
+#define RGA2_CMD_BASE				0x008
+#define RGA2_STATUS1				0x00c
+#define RGA2_INT				0x010
+#define RGA2_MMU_CTRL0				0x014
+#define RGA2_MMU_CMD_BASE			0x018
+#define RGA2_STATUS2				0x01c
+#define RGA2_WORK_CNT				0x020
+#define RGA2_VERSION_NUM			0x028
+#define RGA2_READ_LINE_CNT			0x030
+#define RGA2_WRITE_LINE_CNT			0x034
+#define RGA2_LINE_CNT				0x038
+#define RGA2_PERF_CTRL0				0x040
+
+/* full csc reg */
+#define RGA2_DST_CSC_00				0x060
+#define RGA2_DST_CSC_01				0x064
+#define RGA2_DST_CSC_02				0x068
+#define RGA2_DST_CSC_OFF0			0x06c
+#define RGA2_DST_CSC_10				0x070
+#define RGA2_DST_CSC_11				0x074
+#define RGA2_DST_CSC_12				0x078
+#define RGA2_DST_CSC_OFF1			0x07c
+#define RGA2_DST_CSC_20				0x080
+#define RGA2_DST_CSC_21				0x084
+#define RGA2_DST_CSC_22				0x088
+#define RGA2_DST_CSC_OFF2			0x08c
+
+/* osd read-back reg */
+#define RGA2_OSD_CUR_FLAGS0			0x090
+#define RGA2_OSD_CUR_FLAGS1			0x09c
+
+/* mode ctrl */
+#define RGA2_MODE_CTRL_OFFSET			0x000
+#define RGA2_SRC_INFO_OFFSET			0x004
+#define RGA2_SRC_BASE0_OFFSET			0x008
+#define RGA2_FBCIN_HEAD_BASE_OFFSET		0x008 // repeat
+#define RGA2_SRC_BASE1_OFFSET			0x00c
+#define RGA2_FBCIN_PAYL_BASE_OFFSET		0x00c // repeat
+#define RGA2_SRC_BASE2_OFFSET			0x010
+#define RGA2_FBCIN_OFF_OFFSET			0x010 // repeat
+#define RGA2_SRC_BASE3_OFFSET			0x014
+#define RGA2_SRC_VIR_INFO_OFFSET		0x018
+#define RGA2_FBCIN_HEAD_VIR_INFO_OFFSET		0x018 // repeat
+#define RGA2_SRC_ACT_INFO_OFFSET		0x01c
+#define RGA2_SRC_X_FACTOR_OFFSET		0x020
+#define RGA2_OSD_CTRL0_OFFSET			0x020 // repeat
+#define RGA2_SRC_Y_FACTOR_OFFSET		0x024
+#define RGA2_OSD_CTRL1_OFFSET			0x024 // repeat
+#define RGA2_SRC_BG_COLOR_OFFSET		0x028
+#define RGA2_OSD_COLOR0_OFFSET			0x028 // repeat
+#define RGA2_GAUSS_COE_OFFSET			0x028 // repeat
+#define RGA2_SRC_FG_COLOR_OFFSET		0x02c
+#define RGA2_OSD_COLOR1_OFFSET			0x02c // repeat
+#define RGA2_SRC_TR_COLOR0_OFFSET		0x030
+#define RGA2_CF_GR_A_OFFSET			0x030 // repeat
+#define RGA2_OSD_LAST_FLAGS0_OFFSET		0x030 // repeat
+#define RGA2_MOSAIC_MODE_OFFSET			0x030 // repeat
+#define RGA2_SRC_TR_COLOR1_OFFSET		0x034
+#define RGA2_CF_GR_B_OFFSET			0x034 // repeat
+#define RGA2_OSD_LAST_FLAGS1_OFFSET		0x034 // repeat
+#define RGA2_DST_INFO_OFFSET			0x038
+#define RGA2_DST_BASE0_OFFSET			0x03c
+#define RGA2_DST_BASE1_OFFSET			0x040
+#define RGA2_DST_BASE2_OFFSET			0x044
+#define RGA2_TILE4x4_OUT_BASE_OFFSET		0x044 //repeat
+#define RGA2_DST_VIR_INFO_OFFSET		0x048
+#define RGA2_DST_ACT_INFO_OFFSET		0x04c
+#define RGA2_ALPHA_CTRL0_OFFSET			0x050
+#define RGA2_ALPHA_CTRL1_OFFSET			0x054
+#define RGA2_FADING_CTRL_OFFSET			0x058
+#define RGA2_PAT_CON_OFFSET			0x05c
+#define RGA2_ROP_CTRL0_OFFSET			0x060
+#define RGA2_CF_GR_G_OFFSET			0x060 // repeat
+#define RGA2_DST_Y4MAP_LUT0_OFFSET		0x060 // repeat
+#define RGA2_DST_QUANTIZE_SCALE_OFFSET		0x060 // repeat
+#define RGA2_OSD_INVERTSION_CAL0_OFFSET		0x060 // repeat
+#define RGA2_ROP_CTRL1_OFFSET			0x064
+#define RGA2_CF_GR_R_OFFSET			0x064 // repeat
+#define RGA2_DST_Y4MAP_LUT1_OFFSET		0x064 // repeat
+#define RGA2_DST_QUANTIZE_OFFSET_OFFSET		0x064 // repeat
+#define RGA2_OSD_INVERTSION_CAL1_OFFSET		0x064 // repeat
+#define RGA2_MASK_BASE_OFFSET			0x068
+#define RGA2_MMU_CTRL1_OFFSET			0x06c
+#define RGA2_MMU_SRC_BASE_OFFSET		0x070
+#define RGA2_PREFETCH_ADDR_TH_OFFSET		0x070 // repeat
+#define RGA2_MMU_SRC1_BASE_OFFSET		0x074
+#define RGA2_MMU_DST_BASE_OFFSET		0x078
+#define RGA2_MMU_ELS_BASE_OFFSET		0x07c
+
+/*RGA_SYS*/
+#define m_RGA2_SYS_CTRL_SRC0YUV420SP_RD_OPT_DIS		(0x1 << 12)
+#define m_RGA2_SYS_CTRL_DST_WR_OPT_DIS			(0x1 << 11)
+#define m_RGA2_SYS_CTRL_CMD_CONTINUE_P			(0x1 << 10)
+#define m_RGA2_SYS_CTRL_HOLD_MODE_EN			(0x1 << 9)
+#define m_RGA2_SYS_CTRL_RST_HANDSAVE_P			(0x1 << 7)
+#define m_RGA2_SYS_CTRL_RST_PROTECT_P			(0x1 << 6)
+#define m_RGA2_SYS_CTRL_AUTO_RST			(0x1 << 5)
+#define m_RGA2_SYS_CTRL_CCLK_SRESET_P			(0x1 << 4)
+#define m_RGA2_SYS_CTRL_ACLK_SRESET_P			(0x1 << 3)
+#define m_RGA2_SYS_CTRL_AUTO_CKG			(0x1 << 2)
+#define m_RGA2_SYS_CTRL_CMD_MODE			(0x1 << 1)
+#define m_RGA2_SYS_CTRL_CMD_OP_ST_P			(0x1 << 0)
+
+#define s_RGA2_SYS_CTRL_CMD_CONTINUE(x)			((x & 0x1) << 10)
+#define s_RGA2_SYS_CTRL_HOLD_MODE_EN(x)			((x & 0x1) << 9)
+#define s_RGA2_SYS_CTRL_CMD_MODE(x)			((x & 0x1) << 1)
+
+/* RGA_CMD_CTRL */
+#define m_RGA2_CMD_CTRL_INCR_NUM			(0x3ff << 3)
+#define m_RGA2_CMD_CTRL_STOP				(0x1 << 2)
+#define m_RGA2_CMD_CTRL_INCR_VALID_P			(0x1 << 1)
+#define m_RGA2_CMD_CTRL_CMD_LINE_ST_P			(0x1 << 0)
+
+#define s_RGA2_CMD_CTRL_INCR_NUM(x)			((x & 0x3ff) << 3)
+
+/* RGA_STATUS1 */
+#define m_RGA2_STATUS1_SW_CMD_TOTAL_NUM			(0xfff << 8)
+#define m_RGA2_STATUS1_SW_CMD_CUR_NUM			(0xfff << 8)
+#define m_RGA2_STATUS1_SW_RGA_STA			(0x1 << 0)
+
+/*RGA_INT*/
+#define m_RGA2_INT_LINE_WR_CLEAR			(1 << 16)
+#define m_RGA2_INT_LINE_RD_CLEAR			(1 << 15)
+#define m_RGA2_INT_LINE_WR_EN				(1 << 14)
+#define m_RGA2_INT_LINE_RD_EN				(1 << 13)
+#define m_RGA2_INT_WRITE_CNT_FLAG			(1 << 12)
+#define m_RGA2_INT_READ_CNT_FLAG			(1 << 11)
+#define m_RGA2_INT_ALL_CMD_DONE_INT_EN			(1 << 10)
+#define m_RGA2_INT_MMU_INT_EN				(1 << 9)
+#define m_RGA2_INT_ERROR_INT_EN				(1 << 8)
+#define m_RGA2_INT_NOW_CMD_DONE_INT_CLEAR		(1 << 7)
+#define m_RGA2_INT_ALL_CMD_DONE_INT_CLEAR		(1 << 6)
+#define m_RGA2_INT_MMU_INT_CLEAR			(1 << 5)
+#define m_RGA2_INT_ERROR_INT_CLEAR			(1 << 4)
+#define m_RGA2_INT_CUR_CMD_DONE_INT_FLAG		(1 << 3)
+#define m_RGA2_INT_ALL_CMD_DONE_INT_FLAG		(1 << 2)
+#define m_RGA2_INT_MMU_INT_FLAG				(1 << 1)
+#define m_RGA2_INT_ERROR_INT_FLAG			(1 << 0)
+
+#define m_RGA2_INT_ERROR_FLAG_MASK \
+	( \
+		m_RGA2_INT_MMU_INT_FLAG | \
+		m_RGA2_INT_ERROR_INT_FLAG \
+	)
+#define m_RGA2_INT_ERROR_CLEAR_MASK \
+	( \
+	m_RGA2_INT_MMU_INT_CLEAR | \
+	m_RGA2_INT_ERROR_INT_CLEAR \
+)
+#define m_RGA2_INT_ERROR_ENABLE_MASK \
+	( \
+		m_RGA2_INT_MMU_INT_EN | \
+		m_RGA2_INT_ERROR_INT_EN \
+	)
+
+#define s_RGA2_INT_LINE_WR_CLEAR(x)			((x & 0x1) << 16)
+#define s_RGA2_INT_LINE_RD_CLEAR(x)			((x & 0x1) << 15)
+#define s_RGA2_INT_LINE_WR_EN(x)			((x & 0x1) << 14)
+#define s_RGA2_INT_LINE_RD_EN(x)			((x & 0x1) << 13)
+#define s_RGA2_INT_ALL_CMD_DONE_INT_EN(x)		((x & 0x1) << 10)
+#define s_RGA2_INT_MMU_INT_EN(x)			((x & 0x1) << 9)
+#define s_RGA2_INT_ERROR_INT_EN(x)			((x & 0x1) << 8)
+#define s_RGA2_INT_NOW_CMD_DONE_INT_CLEAR(x)		((x & 0x1) << 7)
+#define s_RGA2_INT_ALL_CMD_DONE_INT_CLEAR(x)		((x & 0x1) << 6)
+#define s_RGA2_INT_MMU_INT_CLEAR(x)			((x & 0x1) << 5)
+#define s_RGA2_INT_ERROR_INT_CLEAR(x)			((x & 0x1) << 4)
+
+/* RGA_STATUS2 hardware status */
+#define m_RGA2_STATUS2_RPP_MKRAM_RREADY			(0x2 << 11)
+#define m_RGA2_STATUS2_DSTRPP_OUTBUF_RREADY		(0x1f << 6)
+#define m_RGA2_STATUS2_SRCRPP_OUTBUF_RREADY		(0xf << 2)
+#define m_RGA2_STATUS2_BUS_ERROR			(0x1 << 1)
+#define m_RGA2_STATUS2_RPP_ERROR			(0x1 << 0)
+
+/* RGA_READ_LINE_CNT_TH */
+#define m_RGA2_READ_LINE_SW_INTR_LINE_RD_TH		(0x1fff << 0)
+
+#define s_RGA2_READ_LINE_SW_INTR_LINE_RD_TH(x)		((x & 0x1fff) << 0)
+
+/* RGA_WRITE_LINE_CNT_TN */
+#define m_RGA2_WRITE_LINE_SW_INTR_LINE_WR_START		(0x1fff << 0)
+#define m_RGA2_WRITE_LINE_SW_INTR_LINE_WR_STEP		(0x1fff << 16)
+
+#define s_RGA2_WRITE_LINE_SW_INTR_LINE_WR_START(x)	((x & 0x1fff) << 0)
+#define s_RGA2_WRITE_LINE_SW_INTR_LINE_WR_STEP(x)	((x & 0x1fff) << 16)
+
+/* RGA_MODE_CTRL */
+#define m_RGA2_MODE_CTRL_SW_RENDER_MODE			(0x7 << 0)
+#define m_RGA2_MODE_CTRL_SW_BITBLT_MODE			(0x1 << 3)
+#define m_RGA2_MODE_CTRL_SW_CF_ROP4_PAT			(0x1 << 4)
+#define m_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET		(0x1 << 5)
+#define m_RGA2_MODE_CTRL_SW_GRADIENT_SAT		(0x1 << 6)
+#define m_RGA2_MODE_CTRL_SW_INTR_CF_E			(0x1 << 7)
+#define m_RGA2_MODE_CTRL_SW_OSD_E			(0x1<<8)
+#define m_RGA2_MODE_CTRL_SW_MOSAIC_EN			(0x1<<9)
+#define m_RGA2_MODE_CTRL_SW_YIN_YOUT_EN			(0x1<<10)
+#define m_RGA2_MODE_CTRL_SW_TILE4x4_IN_EN		(0x1 << 12)
+#define m_RGA2_MODE_CTRL_SW_TILE4x4_OUT_EN		(0x1 << 13)
+#define m_RGA2_MODE_CTRL_SW_FBC_IN_EN			(0x1 << 16)
+#define m_RGA2_MODE_CTRL_SW_SRC_GAUSS_EN		(0x1 << 17)
+#define m_RGA2_MODE_CTRL_SW_FBC_BSP_DIS			(0x1 << 18)
+#define m_RGA2_MODE_CTRL_SW_TABLE_PRE_FETCH_DIS		(0x1 << 19)
+#define m_RGA2_MODE_CTRL_SW_AXI_WR128_DIS		(0x1 << 20)
+#define m_RGA2_MODE_CTRL_SW_HSP_LEFT_COPY_DIS		(0x1 << 21)
+
+#define s_RGA2_MODE_CTRL_SW_RENDER_MODE(x)		((x & 0x7) << 0)
+#define s_RGA2_MODE_CTRL_SW_BITBLT_MODE(x)		((x & 0x1) << 3)
+#define s_RGA2_MODE_CTRL_SW_CF_ROP4_PAT(x)		((x & 0x1) << 4)
+#define s_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET(x)		((x & 0x1) << 5)
+#define s_RGA2_MODE_CTRL_SW_GRADIENT_SAT(x)		((x & 0x1) << 6)
+#define s_RGA2_MODE_CTRL_SW_INTR_CF_E(x)		((x & 0x1) << 7)
+#define s_RGA2_MODE_CTRL_SW_OSD_E(x)			((x & 0x1) << 8)
+#define s_RGA2_MODE_CTRL_SW_MOSAIC_EN(x)		((x & 0x1) << 9)
+#define s_RGA2_MODE_CTRL_SW_YIN_YOUT_EN(x)		((x & 0x1) << 10)
+#define s_RGA2_MODE_CTRL_SW_TILE4x4_IN_EN(x)		((x & 0x1) << 12)
+#define s_RGA2_MODE_CTRL_SW_TILE4x4_OUT_EN(x)		((x & 0x1) << 13)
+#define s_RGA2_MODE_CTRL_SW_FBC_IN_EN(x)		((x & 0x1) << 16)
+#define s_RGA2_MODE_CTRL_SW_SRC_GAUSS_EN(x)		((x & 0x1) << 17)
+#define s_RGA2_MODE_CTRL_SW_FBC_BSP_DIS(x)		((x & 0x1) << 18)
+#define s_RGA2_MODE_CTRL_SW_TABLE_PRE_FETCH_DIS(x)	((x & 0x1) << 19)
+#define s_RGA2_MODE_CTRL_SW_AXI_WR128_DIS(x)		((x & 0x1) << 20)
+#define s_RGA2_MODE_CTRL_SW_HSP_LEFT_COPY_DIS(x)	((x & 0x1) << 21)
+
+/* RGA_SRC_INFO */
+#define m_RGA2_SRC_INFO_SW_SRC_FMT			(0xf << 0)
+#define m_RGA2_SRC_INFO_SW_FBCIN_MODE			(0x3 << 0) // repeat
+#define m_RGA2_SRC_INFO_SW_FBCIN_FMT			(0x3 << 2) // repeat
+#define m_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP		(0x1 << 4)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP		(0x1 << 5)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP		(0x1 << 6)
+#define m_RGA2_SRC_INFO_SW_SW_CP_ENDIAN			(0x1 << 7)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE		(0x3 << 8)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE		(0x3 << 10)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE		(0x3 << 12)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE		(0x3 << 14)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE		(0x3 << 16)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE		(0x1 << 18)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E		(0xf << 19)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E		(0x1 << 23)
+#define m_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER		(0x3 << 24)
+#define m_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL		(0x1 << 26)
+#define m_RGA2_SRC_INFO_SW_SW_YUV10_E			(0x1 << 27)
+#define m_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E		(0x1 << 28)
+#define m_RGA2_SRC_INFO_SW_SW_VSD_MODE_SEL		(0x1 << 29)
+#define m_RGA2_SRC_INFO_SW_SW_HSP_MODE_SEL		(0x1 << 30)
+#define m_RGA2_SRC_INFO_SW_SW_HSD_MODE_SEL		(0x1 << 31)
+
+
+#define s_RGA2_SRC_INFO_SW_SRC_FMT(x)			((x & 0xf) << 0)
+#define s_RGA2_SRC_INFO_SW_FBCIN_MODE(x)		((x & 0x3) << 0) // repeat
+#define s_RGA2_SRC_INFO_SW_FBCIN_FMT(x)			((x & 0x3) << 2) // repeat
+#define s_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP(x)		((x & 0x1) << 4)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP(x)		((x & 0x1) << 5)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP(x)		((x & 0x1) << 6)
+#define s_RGA2_SRC_INFO_SW_SW_CP_ENDAIN(x)		((x & 0x1) << 7)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE(x)		((x & 0x3) << 8)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE(x)		((x & 0x3) << 10)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE(x)		((x & 0x3) << 12)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE(x)		((x & 0x3) << 14)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE(x)		((x & 0x3) << 16)
+
+#define s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE(x)		((x & 0x1) << 18)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E(x)		((x & 0xf) << 19)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E(x)	((x & 0x1) << 23)
+#define s_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER(x)		((x & 0x3) << 24)
+#define s_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL(x)		((x & 0x1) << 26)
+#define s_RGA2_SRC_INFO_SW_SW_YUV10_E(x)		((x & 0x1) << 27)
+#define s_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E(x)		((x & 0x1) << 28)
+#define s_RGA2_SRC_INFO_SW_SW_VSD_MODE_SEL(x)		((x & 0x1) << 29)
+#define s_RGA2_SRC_INFO_SW_SW_HSP_MODE_SEL(x)		((x & 0x1) << 30)
+#define s_RGA2_SRC_INFO_SW_SW_HSD_MODE_SEL(x)		((x & 0x1) << 31)
+
+/* RGA_SRC_VIR_INFO */
+#define m_RGA2_SRC_VIR_INFO_SW_SRC_VIR_STRIDE		(0x7fff << 0)
+#define m_RGA2_SRC_VIR_INFO_SW_MASK_VIR_STRIDE		(0x3ff << 16)
+
+#define s_RGA2_SRC_VIR_INFO_SW_SRC_VIR_STRIDE(x)	((x & 0x7fff) << 0)
+#define s_RGA2_SRC_VIR_INFO_SW_MASK_VIR_STRIDE(x)	((x & 0x3ff) << 16)
+
+
+/* RGA_SRC_ACT_INFO */
+#define m_RGA2_SRC_ACT_INFO_SW_TILE4X4_IN_YOFF		(0x3 << 30)
+#define m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT		(0x7ff << 16)
+#define m_RGA2_SRC_ACT_INFO_SW_TILE4X4_IN_XOFF		(0x3 << 14)
+#define m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH		(0x7ff << 0)
+
+#define s_RGA2_SRC_ACT_INFO_SW_TILE4X4_IN_YOFF(x)	((x & 0x3) << 30)
+#define s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT(x)	((x & 0x7ff) << 16)
+#define s_RGA2_SRC_ACT_INFO_SW_TILE4X4_IN_XOFF(x)	((x & 0x3) << 14)
+#define s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH(x)		((x & 0x7ff) << 0)
+
+/* RGA2_OSD_CTRL0 */
+#define m_RGA2_OSD_CTRL0_SW_OSD_MODE			(0x3 << 0)
+#define m_RGA2_OSD_CTRL0_SW_OSD_VER_MODE		(0x1 << 2)
+#define m_RGA2_OSD_CTRL0_SW_OSD_WIDTH_MODE		(0x1 << 3)
+#define m_RGA2_OSD_CTRL0_SW_OSD_BLK_NUM			(0x1f << 4)
+#define m_RGA2_OSD_CTRL0_SW_OSD_FLAGS_INDEX		(0x3f << 10)
+#define m_RGA2_OSD_CTRL0_SW_OSD_FIX_WIDTH		(0x3f << 20)
+#define m_RGA2_OSD_CTRL0_SW_OSD_2BPP_MODE		(0x1 << 30)
+
+#define s_RGA2_OSD_CTRL0_SW_OSD_MODE(x)			((x & 0x3) << 0)
+#define s_RGA2_OSD_CTRL0_SW_OSD_VER_MODE(x)		((x & 0x1) << 2)
+#define s_RGA2_OSD_CTRL0_SW_OSD_WIDTH_MODE(x)		((x & 0x1) << 3)
+#define s_RGA2_OSD_CTRL0_SW_OSD_BLK_NUM(x)		((x & 0x1f) << 4)
+#define s_RGA2_OSD_CTRL0_SW_OSD_FLAGS_INDEX(x)		((x & 0x3ff) << 10)
+#define s_RGA2_OSD_CTRL0_SW_OSD_FIX_WIDTH(x)		((x & 0x3ff) << 20)
+#define s_RGA2_OSD_CTRL0_SW_OSD_2BPP_MODE(x)		((x & 0x1) << 30)
+
+/* RGA2_GAUSS_COE */
+#define m_RGA2_GAUSS_COE_SW_COE0			(0x3f << 0)
+#define m_RGA2_GAUSS_COE_SW_COE1			(0x3f << 8)
+#define m_RGA2_GAUSS_COE_SW_COE2			(0xff << 16)
+
+#define s_RGA2_GAUSS_COE_SW_COE0(x)			((x & 0x3f) << 0)
+#define s_RGA2_GAUSS_COE_SW_COE1(x)			((x & 0x3f) << 8)
+#define s_RGA2_GAUSS_COE_SW_COE2(x)			((x & 0xff) << 16)
+
+/* RGA2_OSD_CTRL1 */
+#define m_RGA2_OSD_CTRL1_SW_OSD_COLOR_SEL		(0x1 << 0)
+#define m_RGA2_OSD_CTRL1_SW_OSD_FLAG_SEL		(0x1 << 1)
+#define m_RGA2_OSD_CTRL1_SW_OSD_DEFAULT_COLOR		(0x1 << 2)
+#define m_RGA2_OSD_CTRL1_SW_OSD_AUTO_INVERST_MODE	(0x1 << 3)
+#define m_RGA2_OSD_CTRL1_SW_OSD_THRESH			(0xff << 4)
+#define m_RGA2_OSD_CTRL1_SW_OSD_INVERT_A_EN		(0x1 << 12)
+#define m_RGA2_OSD_CTRL1_SW_OSD_INVERT_Y_DIS		(0x1 << 13)
+#define m_RGA2_OSD_CTRL1_SW_OSD_INVERT_C_DIS		(0x1 << 14)
+#define m_RGA2_OSD_CTRL1_SW_OSD_UNFIX_INDEX		(0xf << 16)
+
+#define s_RGA2_OSD_CTRL1_SW_OSD_COLOR_SEL(x)		((x & 0x1) << 0)
+#define s_RGA2_OSD_CTRL1_SW_OSD_FLAG_SEL(x)		((x & 0x1) << 1)
+#define s_RGA2_OSD_CTRL1_SW_OSD_DEFAULT_COLOR(x)	((x & 0x1) << 2)
+#define s_RGA2_OSD_CTRL1_SW_OSD_AUTO_INVERST_MODE(x)	((x & 0x1) << 3)
+#define s_RGA2_OSD_CTRL1_SW_OSD_THRESH(x)		((x & 0xff) << 4)
+#define s_RGA2_OSD_CTRL1_SW_OSD_INVERT_A_EN(x)		((x & 0x1) << 12)
+#define s_RGA2_OSD_CTRL1_SW_OSD_INVERT_Y_DIS(x)		((x & 0x1) << 13)
+#define s_RGA2_OSD_CTRL1_SW_OSD_INVERT_C_DIS(x)		((x & 0x1) << 14)
+#define s_RGA2_OSD_CTRL1_SW_OSD_UNFIX_INDEX(x)		((x & 0xf) << 16)
+
+/* RGA_DST_INFO */
+#define m_RGA2_DST_INFO_SW_DST_FMT			(0xf << 0)
+#define m_RGA2_DST_INFO_SW_DST_RB_SWAP			(0x1 << 4)
+#define m_RGA2_DST_INFO_SW_ALPHA_SWAP			(0x1 << 5)
+#define m_RGA2_DST_INFO_SW_DST_UV_SWAP			(0x1 << 6)
+#define m_RGA2_DST_INFO_SW_SRC1_FMT			(0x7 << 7)
+#define m_RGA2_DST_INFO_SW_SRC1_RB_SWP			(0x1 << 10)
+#define m_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP		(0x1 << 11)
+#define m_RGA2_DST_INFO_SW_DITHER_UP_E			(0x1 << 12)
+#define m_RGA2_DST_INFO_SW_DITHER_DOWN_E		(0x1 << 13)
+#define m_RGA2_DST_INFO_SW_DITHER_MODE			(0x3 << 14)
+#define m_RGA2_DST_INFO_SW_DST_CSC_MODE			(0x3 << 16)
+#define m_RGA2_DST_INFO_SW_CSC_CLIP_MODE		(0x1 << 18)
+#define m_RGA2_DST_INFO_SW_DST_CSC_MODE_2		(0x1 << 19)
+#define m_RGA2_DST_INFO_SW_SRC1_CSC_MODE		(0x3 << 20)
+#define m_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE		(0x1 << 22)
+#define m_RGA2_DST_INFO_SW_DST_UVHDS_MODE		(0x1 << 23)
+#define m_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN		(0x1 << 24)
+#define m_RGA2_DST_INFO_SW_DST_FMT_Y4_EN		(0x1 << 25)
+#define m_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN		(0x1 << 26)
+#define m_RGA2_DST_INFO_SW_DST_UVVDS_MODE		(0x1 << 27)
+#define m_RGA2_DST_INFO_SW_SRC1_A1555_ACONFIG_EN	(0x1 << 28)
+
+#define s_RGA2_DST_INFO_SW_DST_FMT(x)			((x & 0xf) << 0)
+#define s_RGA2_DST_INFO_SW_DST_RB_SWAP(x)		((x & 0x1) << 4)
+#define s_RGA2_DST_INFO_SW_ALPHA_SWAP(x)		((x & 0x1) << 5)
+#define s_RGA2_DST_INFO_SW_DST_UV_SWAP(x)		((x & 0x1) << 6)
+#define s_RGA2_DST_INFO_SW_SRC1_FMT(x)			((x & 0x7) << 7)
+#define s_RGA2_DST_INFO_SW_SRC1_RB_SWP(x)		((x & 0x1) << 10)
+#define s_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP(x)		((x & 0x1) << 11)
+#define s_RGA2_DST_INFO_SW_DITHER_UP_E(x)		((x & 0x1) << 12)
+#define s_RGA2_DST_INFO_SW_DITHER_DOWN_E(x)		((x & 0x1) << 13)
+#define s_RGA2_DST_INFO_SW_DITHER_MODE(x)		((x & 0x3) << 14)
+#define s_RGA2_DST_INFO_SW_DST_CSC_MODE(x)		((x & 0x3) << 16)
+#define s_RGA2_DST_INFO_SW_CSC_CLIP_MODE(x)		((x & 0x1) << 18)
+#define s_RGA2_DST_INFO_SW_DST_CSC_MODE_2(x)		((x & 0x1) << 19)
+#define s_RGA2_DST_INFO_SW_SRC1_CSC_MODE(x)		((x & 0x3) << 20)
+#define s_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE(x)	((x & 0x1) << 22)
+#define s_RGA2_DST_INFO_SW_DST_UVHDS_MODE(x)		((x & 0x1) << 23)
+#define s_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN(x)		((x & 0x1) << 24)
+#define s_RGA2_DST_INFO_SW_DST_FMT_Y4_EN(x)		((x & 0x1) << 25)
+#define s_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN(x)	((x & 0x1) << 26)
+#define s_RGA2_DST_INFO_SW_DST_UVVDS_MODE(x)		((x & 0x1) << 27)
+#define s_RGA2_DST_INFO_SW_SRC1_A1555_ACONFIG_EN(x)	((x & 0x1) << 28)
+
+/* RGA_ALPHA_CTRL0 */
+#define m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0		(0x1 << 0)
+#define m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL		(0x1 << 1)
+#define m_RGA2_ALPHA_CTRL0_SW_ROP_MODE			(0x3 << 2)
+#define m_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA		(0xff << 4)
+#define m_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA		(0xff << 12)
+#define m_RGA2_ALPHA_CTRLO_SW_MASK_ENDIAN		(0x1 << 20)
+
+#define s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0(x)		((x & 0x1) << 0)
+#define s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL(x)		((x & 0x1) << 1)
+#define s_RGA2_ALPHA_CTRL0_SW_ROP_MODE(x)		((x & 0x3) << 2)
+#define s_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA(x)	((x & 0xff) << 4)
+#define s_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA(x)	((x & 0xff) << 12)
+#define s_RGA2_ALPHA_CTRLO_SW_MASK_ENDIAN(x)		((x & 0x1) << 20)
+
+
+
+/* RGA_ALPHA_CTRL1 */
+#define m_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0		(0x1 << 0)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0		(0x1 << 1)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0		(0x7 << 2)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0		(0x7 << 5)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0		(0x1 << 8)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0		(0x1 << 9)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0		(0x3 << 10)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0		(0x3 << 12)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0		(0x1 << 14)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0		(0x1 << 15)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1		(0x7 << 16)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1		(0x7 << 19)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1		(0x1 << 22)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1		(0x1 << 23)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1		(0x3 << 24)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1		(0x3 << 26)
+#define m_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1		(0x1 << 28)
+#define m_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1		(0x1 << 29)
+
+#define s_RGA2_ALPHA_CTRL1_SW_DST_COLOR_M0(x)		((x & 0x1) << 0)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_COLOR_M0(x)		((x & 0x1) << 1)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M0(x)		((x & 0x7) << 2)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M0(x)		((x & 0x7) << 5)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M0(x)	((x & 0x1) << 8)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M0(x)	((x & 0x1) << 9)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M0(x)		((x & 0x3) << 10)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M0(x)		((x & 0x3) << 12)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M0(x)		((x & 0x1) << 14)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M0(x)		((x & 0x1) << 15)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_FACTOR_M1(x)		((x & 0x7) << 16)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_FACTOR_M1(x)		((x & 0x7) << 19)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_CAL_M1(x)	((x & 0x1) << 22)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_CAL_M1(x)	((x & 0x1) << 23)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_BLEND_M1(x)		((x & 0x3) << 24)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_BLEND_M1(x)		((x & 0x3) << 26)
+#define s_RGA2_ALPHA_CTRL1_SW_DST_ALPHA_M1(x)		((x & 0x1) << 28)
+#define s_RGA2_ALPHA_CTRL1_SW_SRC_ALPHA_M1(x)		((x & 0x1) << 29)
+
+
+
+/* RGA_MMU_CTRL1 */
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_EN			(0x1 << 0)
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_FLUSH		(0x1 << 1)
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_EN		(0x1 << 2)
+#define m_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_DIR	(0x1 << 3)
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_EN			(0x1 << 4)
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_FLUSH		(0x1 << 5)
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_EN	(0x1 << 6)
+#define m_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_DIR	(0x1 << 7)
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_EN			(0x1 << 8)
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_FLUSH		(0x1 << 9)
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_EN		(0x1 << 10)
+#define m_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_DIR	(0x1 << 11)
+#define m_RGA2_MMU_CTRL1_SW_ELS_MMU_EN			(0x1 << 12)
+#define m_RGA2_MMU_CTRL1_SW_ELS_MMU_FLUSH		(0x1 << 13)
+
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_EN(x)		((x & 0x1) << 0)
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_FLUSH(x)		((x & 0x1) << 1)
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_EN(x)	((x & 0x1) << 2)
+#define s_RGA2_MMU_CTRL1_SW_SRC_MMU_PREFETCH_DIR(x)	((x & 0x1) << 3)
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_EN(x)				((x & 0x1) << 4)
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_FLUSH(x)		((x & 0x1) << 5)
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_EN(x)	((x & 0x1) << 6)
+#define s_RGA2_MMU_CTRL1_SW_SRC1_MMU_PREFETCH_DIR(x)	((x & 0x1) << 7)
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_EN(x)		((x & 0x1) << 8)
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_FLUSH(x)		((x & 0x1) << 9)
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_EN(x)	((x & 0x1) << 10)
+#define s_RGA2_MMU_CTRL1_SW_DST_MMU_PREFETCH_DIR(x)	((x & 0x1) << 11)
+#define s_RGA2_MMU_CTRL1_SW_ELS_MMU_EN(x)		((x & 0x1) << 12)
+#define s_RGA2_MMU_CTRL1_SW_ELS_MMU_FLUSH(x)		((x & 0x1) << 13)
+
+#define RGA2_VSP_BICUBIC_LIMIT				1996
+#define RGA2_BILINEAR_PREC				12
+
+union rga2_color_ctrl {
+	uint32_t value;
+	struct {
+		uint32_t dst_color_mode:1;
+		uint32_t src_color_mode:1;
+
+		uint32_t dst_factor_mode:3;
+		uint32_t src_factor_mode:3;
+
+		uint32_t dst_alpha_cal_mode:1;
+		uint32_t src_alpha_cal_mode:1;
+
+		uint32_t dst_blend_mode:2;
+		uint32_t src_blend_mode:2;
+
+		uint32_t dst_alpha_mode:1;
+		uint32_t src_alpha_mode:1;
+	} bits;
+};
+
+union rga2_alpha_ctrl {
+	uint32_t value;
+	struct {
+		uint32_t dst_factor_mode:3;
+		uint32_t src_factor_mode:3;
+
+		uint32_t dst_alpha_cal_mode:1;
+		uint32_t src_alpha_cal_mode:1;
+
+		uint32_t dst_blend_mode:2;
+		uint32_t src_blend_mode:2;
+
+		uint32_t dst_alpha_mode:1;
+		uint32_t src_alpha_mode:1;
+	} bits;
+};
+
+extern const struct rga_backend_ops rga2_ops;
+
+#endif
+
diff --git a/drivers/video/rockchip/rga3/include/rga3_reg_info.h b/drivers/video/rockchip/rga3/include/rga3_reg_info.h
new file mode 100644
index 0000000000000..4db80cfb09eed
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga3_reg_info.h
@@ -0,0 +1,521 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __REG3_INFO_H__
+#define __REG3_INFO_H__
+
+#include "rga_drv.h"
+
+/* sys reg */
+#define RGA3_SYS_CTRL				0x000
+#define RGA3_CMD_CTRL				0x004
+#define RGA3_CMD_ADDR				0x008
+#define RGA3_MI_GROUP_CTRL			0x00c
+#define RGA3_ARQOS_CTRL				0x010
+#define RGA3_VERSION_NUM			0x018
+#define RGA3_VERSION_TIM			0x01c
+#define RGA3_INT_EN				0x020
+#define RGA3_INT_RAW				0x024
+#define RGA3_INT_MSK				0x028
+#define RGA3_INT_CLR				0x02c
+#define RGA3_RO_SRST				0x030
+#define RGA3_STATUS0				0x034
+#define RGA3_SCAN_CNT				0x038
+#define RGA3_CMD_STATE				0x040
+
+/* cmd reg */
+#define RGA3_WIN0_RD_CTRL_OFFSET		0x000
+#define RGA3_WIN0_Y_BASE_OFFSET			0x010
+#define RGA3_WIN0_U_BASE_OFFSET			0x014
+#define RGA3_WIN0_V_BASE_OFFSET			0x018
+#define RGA3_WIN0_VIR_STRIDE_OFFSET		0x01c
+#define RGA3_WIN0_FBC_OFF_OFFSET		0x020
+#define RGA3_WIN0_SRC_SIZE_OFFSET		0x024
+#define RGA3_WIN0_ACT_OFF_OFFSET		0x028
+#define RGA3_WIN0_ACT_SIZE_OFFSET		0x02c
+#define RGA3_WIN0_DST_SIZE_OFFSET		0x030
+#define RGA3_WIN0_SCL_FAC_OFFSET		0x034
+#define RGA3_WIN0_UV_VIR_STRIDE_OFFSET		0x038
+#define RGA3_WIN1_RD_CTRL_OFFSET		0x040
+#define RGA3_WIN1_Y_BASE_OFFSET			0x050
+#define RGA3_WIN1_U_BASE_OFFSET			0x054
+#define RGA3_WIN1_V_BASE_OFFSET			0x058
+#define RGA3_WIN1_VIR_STRIDE_OFFSET		0x05c
+#define RGA3_WIN1_FBC_OFF_OFFSET		0x060
+#define RGA3_WIN1_SRC_SIZE_OFFSET		0x064
+#define RGA3_WIN1_ACT_OFF_OFFSET		0x068
+#define RGA3_WIN1_ACT_SIZE_OFFSET		0x06c
+#define RGA3_WIN1_DST_SIZE_OFFSET		0x070
+#define RGA3_WIN1_SCL_FAC_OFFSET		0x074
+#define RGA3_WIN1_UV_VIR_STRIDE_OFFSET		0x078
+#define RGA3_OVLP_CTRL_OFFSET			0x080
+#define RGA3_OVLP_OFF_OFFSET			0x084
+#define RGA3_OVLP_TOP_KEY_MIN_OFFSET		0x088
+#define RGA3_OVLP_TOP_KEY_MAX_OFFSET		0x08c
+#define RGA3_OVLP_TOP_CTRL_OFFSET		0x090
+#define RGA3_OVLP_BOT_CTRL_OFFSET		0x094
+#define RGA3_OVLP_TOP_ALPHA_OFFSET		0x098
+#define RGA3_OVLP_BOT_ALPHA_OFFSET		0x09c
+#define RGA3_WR_CTRL_OFFSET			0x0a0
+#define RGA3_WR_FBCE_CTRL_OFFSET		0x0a4
+#define RGA3_WR_VIR_STRIDE_OFFSET		0x0a8
+#define RGA3_WR_PL_VIR_STRIDE_OFFSET		0x0ac
+#define RGA3_WR_Y_BASE_OFFSET			0x0b0
+#define RGA3_WR_U_BASE_OFFSET			0x0b4
+#define RGA3_WR_V_BASE_OFFSET			0x0b8
+
+/* RGA3_SYS_CTRL */
+#define m_RGA3_SYS_CTRL_FRMEND_AUTO_RSTN_EN			(0x1 << 11)
+#define m_RGA3_SYS_CTRL_RGA_BIC_MODE				(0x3 << 9)
+#define m_RGA3_SYS_CTRL_RGA_RAM_CLK_ON				(0x1 << 8)
+#define m_RGA3_SYS_CTRL_CCLK_SRESET				(0x1 << 4)
+#define m_RGA3_SYS_CTRL_ACLK_SRESET				(0x1 << 3)
+#define m_RGA3_SYS_CTRL_RGA_LGC_CLK_ON				(0x1 << 2)
+#define m_RGA3_SYS_CTRL_CMD_MODE				(0x1 << 1)
+#define m_RGA3_SYS_CTRL_RGA_SART				(0x1 << 0)
+
+#define s_RGA3_SYS_CTRL_RGA_BIC_MODE(x)				((x & 0x3) << 9)
+#define s_RGA3_SYS_CTRL_CCLK_SRESET(x)				((x & 0x1) << 4)
+#define s_RGA3_SYS_CTRL_ACLK_SRESET(x)				((x & 0x1) << 3)
+#define s_RGA3_SYS_CTRL_CMD_MODE(x)				((x & 0x1) << 1)
+
+/* TODO: RGA3_INT_EN/RGA3_INT_RAW/RGA3_INT_MSK/RGA3_INT_CLR */
+#define m_RGA3_INT_WIN1_VOR_FIFO_REN_ERR			(0x1 << 29)
+#define m_RGA3_INT_WIN1_VOR_FIFO_WEN_ERR			(0x1 << 28)
+#define m_RGA3_INT_WIN1_HOR_FIFO_REN_ERR			(0x1 << 27)
+#define m_RGA3_INT_WIN1_HOR_FIFO_WEN_ERR			(0x1 << 26)
+#define m_RGA3_INT_WIN1_IN_FIFO_REB_ERR				(0x1 << 25)
+#define m_RGA3_INT_WIN1_IN_FIFO_WEN_ERR				(0x1 << 24)
+#define m_RGA3_INT_WIN0_VOR_FIFO_REN_ERR			(0x1 << 21)
+#define m_RGA3_INT_WIN0_VOR_FIFO_WEN_ERR			(0x1 << 20)
+#define m_RGA3_INT_WIN0_HOR_FIFO_REN_ERR			(0x1 << 19)
+#define m_RGA3_INT_WIN0_HOR_FIFO_WEN_ERR			(0x1 << 18)
+#define m_RGA3_INT_WIN0_IN_FIFO_REB_ERR				(0x1 << 17)
+#define m_RGA3_INT_WIN0_IN_FIFO_WEN_ERR				(0x1 << 16)
+#define m_RGA3_INT_RGA_MI_WR_BUS_ERR				(0x1 << 15)
+#define m_RGA3_INT_RGA_MI_WR_IN_HERR				(0x1 << 14)
+//The signal is invalid, it will be pulled up every time, no need to care.
+// #define m_RGA3_INT_RGA_MI_WR_IN_VERR				(0x1 << 13)
+#define m_RGA3_INT_WIN1_V_ERR					(0x1 << 11)
+#define m_RGA3_INT_WIN1_H_ERR					(0x1 << 10)
+#define m_RGA3_INT_WIN1_FBCD_DEC_ERR				(0x1 << 9)
+#define m_RGA3_INT_WIN1_RD_FRM_END				(0x1 << 8) //not error
+#define m_RGA3_INT_WIN0_V_ERR					(0x1 << 7)
+#define m_RGA3_INT_WIN0_H_ERR					(0x1 << 6)
+#define m_RGA3_INT_WIN0_FBCD_DEC_ERR				(0x1 << 5)
+#define m_RGA3_INT_WIN0_RD_FRM_END				(0x1 << 4) //not error
+#define m_RGA3_INT_CMD_LINE_FINISH				(0x1 << 3) //not error
+#define m_RGA3_INT_RAG_MI_RD_BUS_ERR				(0x1 << 2)
+#define m_RGA3_INT_RGA_MMU_INTR					(0x1 << 1)
+#define m_RGA3_INT_FRM_DONE					(0x1 << 0) //not error
+
+#define m_RGA3_INT_ERROR_MASK \
+	( \
+		m_RGA3_INT_RGA_MMU_INTR | \
+		m_RGA3_INT_RAG_MI_RD_BUS_ERR | \
+		m_RGA3_INT_WIN0_FBCD_DEC_ERR | \
+		m_RGA3_INT_WIN0_H_ERR | \
+		m_RGA3_INT_WIN0_V_ERR | \
+		m_RGA3_INT_WIN1_FBCD_DEC_ERR | \
+		m_RGA3_INT_WIN1_H_ERR | \
+		m_RGA3_INT_WIN1_V_ERR | \
+		m_RGA3_INT_RGA_MI_WR_IN_HERR | \
+		m_RGA3_INT_RGA_MI_WR_BUS_ERR | \
+		m_RGA3_INT_WIN0_IN_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN0_IN_FIFO_REB_ERR | \
+		m_RGA3_INT_WIN0_HOR_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN0_HOR_FIFO_REN_ERR| \
+		m_RGA3_INT_WIN0_VOR_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN0_VOR_FIFO_REN_ERR | \
+		m_RGA3_INT_WIN1_IN_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN1_IN_FIFO_REB_ERR | \
+		m_RGA3_INT_WIN1_HOR_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN1_HOR_FIFO_REN_ERR| \
+		m_RGA3_INT_WIN1_VOR_FIFO_WEN_ERR | \
+		m_RGA3_INT_WIN1_VOR_FIFO_REN_ERR \
+	)
+
+/* RGA3_CMD_CTRL */
+#define m_RGA3_CMD_CTRL_CMD_INCR_NUM				(0x3ff << 3)
+#define m_RGA3_CMD_CTRL_CMD_STOP_MODE				(0x1 << 2)
+#define m_RGA3_CMD_CTRL_CMD_INCR_VALID_P			(0x1 << 1)
+#define m_RGA3_CMD_CTRL_CMD_LINE_ST_P				(0x1 << 0)
+
+/* RGA3_RO_SRST */
+#define m_RGA3_RO_SRST_RO_RST_DONE				(0x3f << 0)
+
+/* RGA3_CMD_STATE */
+#define m_RGA3_CMD_STATE_CMD_CNT_CUR				(0xfff << 16)
+#define m_RGA3_CMD_STATE_CMD_WORKING				(0x1 << 0)
+
+/* RGA3_WIN0_RD_CTRL */
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_ENABLE			(0x1 << 0)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_MODE			(0x3 << 1)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_PIC_FORMAT			(0xf << 4)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_FORMAT			(0x3 << 8)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT		(0x1 << 10)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_ENDIAN_MODE			(0x1 << 11)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_PIX_SWAP			(0x1 << 12)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_YC_SWAP			(0x1 << 13)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_ROT				(0x1 << 16)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_XMIRROR			(0x1 << 17)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_YMIRROR			(0x1 << 18)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_BY			(0x1 << 20)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_UP			(0x1 << 21)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_BY			(0x1 << 22)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_UP			(0x1 << 23)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_Y2R_EN			(0x1 << 24)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_R2Y_EN			(0x1 << 25)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE			(0x3 << 26)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_PERF_OPT_DIS		(0x1 << 29)
+#define m_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_ALIGN_DIS		(0x1 << 30)
+
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_ENABLE(x)			((x & 0x1) << 0)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_MODE(x)			((x & 0x3) << 1)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_PIC_FORMAT(x)		((x & 0xf) << 4)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_FORMAT(x)		((x & 0x3) << 8)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT(x)		((x & 0x1) << 10)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_ENDIAN_MODE(x)		((x & 0x1) << 11)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_PIX_SWAP(x)			((x & 0x1) << 12)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_YC_SWAP(x)			((x & 0x1) << 13)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_ROT(x)			((x & 0x1) << 16)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_XMIRROR(x)			((x & 0x1) << 17)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_YMIRROR(x)			((x & 0x1) << 18)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_BY(x)			((x & 0x1) << 20)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_UP(x)			((x & 0x1) << 21)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_BY(x)			((x & 0x1) << 22)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_UP(x)			((x & 0x1) << 23)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_Y2R_EN(x)			((x & 0x1) << 24)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_R2Y_EN(x)			((x & 0x1) << 25)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE(x)			((x & 0x3) << 26)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_PERF_OPT_DIS(x)		((x & 0x1) << 29)
+#define s_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_ALIGN_DIS(x)		((x & 0x1) << 30)
+
+/* RGA3_WIN0_FBC_OFF */
+#define m_RGA3_WIN0_FBC_OFF_SW_WIN0_FBC_XOFF			(0x1fff << 0)
+#define m_RGA3_WIN0_FBC_OFF_SW_WIN0_FBC_YOFF			(0x1fff << 16)
+
+#define s_RGA3_WIN0_FBC_OFF_SW_WIN0_FBC_XOFF(x)			((x & 0x1fff) << 0)
+#define s_RGA3_WIN0_FBC_OFF_SW_WIN0_FBC_YOFF(x)			((x & 0x1fff) << 16)
+
+/* RGA3_WIN0_SRC_SIZE */
+#define m_RGA3_WIN0_SRC_SIZE_SW_WIN0_SRC_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN0_SRC_SIZE_SW_WIN0_SRC_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN0_SRC_OFF_SW_WIN0_SRC_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN0_SRC_OFF_SW_WIN0_SRC_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN0_ACT_OFF */
+#define m_RGA3_WIN0_ACT_OFF_SW_WIN0_ACT_XOFF			(0x1fff << 0)
+#define m_RGA3_WIN0_ACT_OFF_SW_WIN0_ACT_YOFF			(0x1fff << 16)
+
+#define s_RGA3_WIN0_ACT_OFF_SW_WIN0_ACT_XOFF(x)			((x & 0x1fff) << 0)
+#define s_RGA3_WIN0_ACT_OFF_SW_WIN0_ACT_YOFF(x)			((x & 0x1fff) << 16)
+
+/* RGA3_WIN0_ACT_SIZE */
+#define m_RGA3_WIN0_ACT_SIZE_SW_WIN0_ACT_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN0_ACT_SIZE_SW_WIN0_ACT_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN0_ACT_SIZE_SW_WIN0_ACT_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN0_ACT_SIZE_SW_WIN0_ACT_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN0_DST_SIZE */
+#define m_RGA3_WIN0_DST_SIZE_SW_WIN0_DST_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN0_DST_SIZE_SW_WIN0_DST_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN0_DST_SIZE_SW_WIN0_DST_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN0_DST_SIZE_SW_WIN0_DST_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN0_SCL_FAC */
+#define m_RGA3_WIN0_SCL_FAC_SW_WIN0_VER_FAC			(0xffff << 0)
+#define m_RGA3_WIN0_SCL_FAC_SW_WIN0_HOR_FAC			(0xffff << 16)
+
+#define s_RGA3_WIN0_SCL_FAC_SW_WIN0_VER_FAC(x)			((x & 0xffff) << 0)
+#define s_RGA3_WIN0_SCL_FAC_SW_WIN0_HOR_FAC(x)			((x & 0xffff) << 16)
+
+/* RGA3_WIN1_RD_CTRL */
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_ENABLE			(0x1 << 0)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_MODE			(0x3 << 1)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_PIC_FORMAT			(0xf << 4)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_FORMAT			(0x3 << 8)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT		(0x1 << 10)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_ENDIAN_MODE			(0x1 << 11)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_PIX_SWAP			(0x1 << 12)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_YC_SWAP			(0x1 << 13)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_ROT				(0x1 << 16)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_XMIRROR			(0x1 << 17)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_YMIRROR			(0x1 << 18)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_BY			(0x1 << 20)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_UP			(0x1 << 21)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_BY			(0x1 << 22)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_UP			(0x1 << 23)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_Y2R_EN			(0x1 << 24)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_R2Y_EN			(0x1 << 25)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_CSC_MODE			(0x3 << 26)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_PERF_OPT_DIS		(0x1 << 29)
+#define m_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_ALIGN_DIS		(0x1 << 30)
+
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_ENABLE(x)			((x & 0x1) << 0)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_MODE(x)			((x & 0x3) << 1)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_PIC_FORMAT(x)		((x & 0xf) << 4)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_FORMAT(x)		((x & 0x3) << 8)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT(x)		((x & 0x1) << 10)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_ENDIAN_MODE(x)		((x & 0x1) << 11)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_PIX_SWAP(x)			((x & 0x1) << 12)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_YC_SWAP(x)			((x & 0x1) << 13)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_ROT(x)			((x & 0x1) << 16)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_XMIRROR(x)			((x & 0x1) << 17)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_YMIRROR(x)			((x & 0x1) << 18)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_BY(x)			((x & 0x1) << 20)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_UP(x)			((x & 0x1) << 21)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_BY(x)			((x & 0x1) << 22)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_UP(x)			((x & 0x1) << 23)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_Y2R_EN(x)			((x & 0x1) << 24)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_R2Y_EN(x)			((x & 0x1) << 25)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_CSC_MODE(x)			((x & 0x3) << 26)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_PERF_OPT_DIS(x)		((x & 0x1) << 29)
+#define s_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_ALIGN_DIS(x)		((x & 0x1) << 30)
+
+/* RGA3_WIN1_FBC_OFF */
+#define m_RGA3_WIN1_FBC_OFF_SW_WIN1_FBC_XOFF			(0x1fff << 0)
+#define m_RGA3_WIN1_FBC_OFF_SW_WIN1_FBC_YOFF			(0x1fff << 16)
+
+#define s_RGA3_WIN1_FBC_OFF_SW_WIN1_FBC_XOFF(x)			((x & 0x1fff) << 0)
+#define s_RGA3_WIN1_FBC_OFF_SW_WIN1_FBC_YOFF(x)			((x & 0x1fff) << 16)
+
+/* RGA3_WIN1_SRC_SIZE */
+#define m_RGA3_WIN1_SRC_SIZE_SW_WIN1_SRC_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN1_SRC_SIZE_SW_WIN1_SRC_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN1_SRC_OFF_SW_WIN1_SRC_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN1_SRC_OFF_SW_WIN1_SRC_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN1_ACT_OFF */
+#define m_RGA3_WIN1_ACT_OFF_SW_WIN1_ACT_XOFF			(0x1fff << 0)
+#define m_RGA3_WIN1_ACT_OFF_SW_WIN1_ACT_YOFF			(0x1fff << 16)
+
+#define s_RGA3_WIN1_ACT_OFF_SW_WIN1_ACT_XOFF(x)			((x & 0x1fff) << 0)
+#define s_RGA3_WIN1_ACT_OFF_SW_WIN1_ACT_YOFF(x)			((x & 0x1fff) << 16)
+
+/* RGA3_WIN1_ACT_SIZE */
+#define m_RGA3_WIN1_ACT_SIZE_SW_WIN1_ACT_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN1_ACT_SIZE_SW_WIN1_ACT_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN1_ACT_SIZE_SW_WIN1_ACT_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN1_ACT_SIZE_SW_WIN1_ACT_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN1_DST_SIZE */
+#define m_RGA3_WIN1_DST_SIZE_SW_WIN1_DST_WIDTH			(0x1fff << 0)
+#define m_RGA3_WIN1_DST_SIZE_SW_WIN1_DST_HEIGHT			(0x1fff << 16)
+
+#define s_RGA3_WIN1_DST_SIZE_SW_WIN1_DST_WIDTH(x)		((x & 0x1fff) << 0)
+#define s_RGA3_WIN1_DST_SIZE_SW_WIN1_DST_HEIGHT(x)		((x & 0x1fff) << 16)
+
+/* RGA3_WIN1_SCL_FAC */
+#define m_RGA3_WIN1_SCL_FAC_SW_WIN1_VER_FAC			(0xffff << 0)
+#define m_RGA3_WIN1_SCL_FAC_SW_WIN1_HOR_FAC			(0xffff << 16)
+
+#define s_RGA3_WIN1_SCL_FAC_SW_WIN1_VER_FAC(x)			((x & 0xffff) << 0)
+#define s_RGA3_WIN1_SCL_FAC_SW_WIN1_HOR_FAC(x)			((x & 0xffff) << 16)
+
+/* RGA3_OVLP_CTRL */
+#define m_RGA3_OVLP_CTRL_SW_OVLP_MODE				(0x3 << 0)
+#define m_RGA3_OVLP_CTRL_SW_OVLP_FIELD				(0x1 << 2)
+#define m_RGA3_OVLP_CTRL_SW_TOP_SWAP				(0x1 << 3)
+#define m_RGA3_OVLP_CTRL_SW_TOP_ALPHA_EN			(0x1 << 4)
+#define m_RGA3_OVLP_CTRL_SW_TOP_KEY_EN				(0x7FFF << 5)
+#define m_RGA3_OVLP_CTRL_SW_OVLP_Y2R_EN				(0x1 << 20)
+#define m_RGA3_OVLP_CTRL_SW_OVLP_R2Y_EN				(0x1 << 21)
+#define m_RGA3_OVLP_CTRL_SW_OVLP_CSC_MODE			(0x3 << 22)
+
+#define s_RGA3_OVLP_CTRL_SW_OVLP_MODE(x)			((x & 0x3) << 0)
+#define s_RGA3_OVLP_CTRL_SW_OVLP_FIELD(x)			((x & 0x1) << 2)
+#define s_RGA3_OVLP_CTRL_SW_TOP_SWAP(x)				((x & 0x1) << 3)
+#define s_RGA3_OVLP_CTRL_SW_TOP_ALPHA_EN(x)			((x & 0x1) << 4)
+#define s_RGA3_OVLP_CTRL_SW_TOP_KEY_EN(x)			((x & 0x7FFF) << 5)
+#define s_RGA3_OVLP_CTRL_SW_OVLP_Y2R_EN(x)			((x & 0x1) << 20)
+#define s_RGA3_OVLP_CTRL_SW_OVLP_R2Y_EN(x)			((x & 0x1) << 21)
+#define s_RGA3_OVLP_CTRL_SW_OVLP_CSC_MODE(x)			((x & 0x3) << 22)
+
+/* RGA3_OVLP_OFF */
+#define m_RGA3_OVLP_OFF_SW_OVLP_XOFF				(0x1fff << 0)
+#define m_RGA3_OVLP_OFF_SW_OVLP_YOFF				(0x1fff << 16)
+
+#define s_RGA3_OVLP_OFF_SW_OVLP_XOFF(x)				((x & 0x1fff) << 0)
+#define s_RGA3_OVLP_OFF_SW_OVLP_YOFF(x)				((x & 0x1fff) << 16)
+
+/* RGA3_OVLP_TOP_KEY_MIN */
+#define m_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_YG_MIN		(0x3ff << 0)
+#define m_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_UB_MIN		(0x3ff << 10)
+#define m_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_VR_MIN		(0x3ff << 20)
+
+#define s_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_YG_MIN(x)		((x & 0x3f)f << 0)
+#define s_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_UB_MIN(x)		((x & 0x3ff) << 10)
+#define s_RGA3_OVLP_TOP_KEY_MIN_SW_TOP_KEY_VR_MIN(x)		((x & 0x3ff) << 20)
+
+/* RGA3_OVLP_TOP_KEY_MAX */
+#define m_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_YG_MAX		(0x3ff << 0)
+#define m_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_UB_MAX		(0x3ff << 10)
+#define m_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_VR_MAX		(0x3ff << 20)
+
+#define s_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_YG_MAX(x)		((x & 0x3ff) << 0)
+#define s_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_UB_MAX(x)		((x & 0x3ff) << 10)
+#define s_RGA3_OVLP_TOP_KEY_MAX_SW_TOP_KEY_VR_MAX(x)		((x & 0x3ff) << 20)
+
+/* RGA3_OVLP_TOP_CTRL */
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_COLOR_M0			(0x1 << 0)
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_ALPHA_M0			(0x1 << 1)
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_BLEND_M0			(0x3 << 2)
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_ALPHA_CAL_M0		(0x1 << 4)
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_FACTOR_M0			(0x7 << 5)
+#define m_RGA3_OVLP_TOP_CTRL_SW_TOP_GLOBAL_ALPHA		(0xff << 16)
+
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_COLOR_M0(x)			((x & 0x1) << 0)
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_ALPHA_M0(x)			((x & 0x1) << 1)
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_BLEND_M0(x)			((x & 0x3) << 2)
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_ALPHA_CAL_M0(x)		((x & 0x1) << 4)
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_FACTOR_M0(x)		((x & 0x7) << 5)
+#define s_RGA3_OVLP_TOP_CTRL_SW_TOP_GLOBAL_ALPHA(x)		((x & 0xff) << 16)
+
+/* RGA3_OVLP_BOT_CTRL */
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_COLOR_M0			(0x1 << 0)
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_ALPHA_M0			(0x1 << 1)
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_BLEND_M0			(0x3 << 2)
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_ALPHA_CAL_M0		(0x1 << 4)
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_FACTOR_M0			(0x7 << 5)
+#define m_RGA3_OVLP_BOT_CTRL_SW_BOT_GLOBAL_ALPHA		(0xff << 16)
+
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_COLOR_M0(x)			((x & 0x1) << 0)
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_ALPHA_M0(x)			((x & 0x1) << 1)
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_BLEND_M0(x)			((x & 0x3) << 2)
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_ALPHA_CAL_M0(x)		((x & 0x1) << 4)
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_FACTOR_M0(x)		((x & 0x7) << 5)
+#define s_RGA3_OVLP_BOT_CTRL_SW_BOT_GLOBAL_ALPHA(x)		((x & 0xff) << 16)
+
+/* RGA3_OVLP_TOP_ALPHA */
+#define m_RGA3_OVLP_TOP_ALPHA_SW_TOP_ALPHA_M1			(0x1 << 1)
+#define m_RGA3_OVLP_TOP_ALPHA_SW_TOP_BLEND_M1			(0x3 << 2)
+#define m_RGA3_OVLP_TOP_ALPHA_SW_TOP_ALPHA_CAL_M1		(0x1 << 4)
+#define m_RGA3_OVLP_TOP_ALPHA_SW_TOP_FACTOR_M1			(0x7 << 5)
+
+#define s_RGA3_OVLP_TOP_ALPHA_SW_TOP_ALPHA_M1(x)		((x & 0x1) << 1)
+#define s_RGA3_OVLP_TOP_ALPHA_SW_TOP_BLEND_M1(x)		((x & 0x3) << 2)
+#define s_RGA3_OVLP_TOP_ALPHA_SW_TOP_ALPHA_CAL_M1(x)		((x & 0x1) << 4)
+#define s_RGA3_OVLP_TOP_ALPHA_SW_TOP_FACTOR_M1(x)		((x & 0x7) << 5)
+
+/* RGA3_OVLP_BOT_ALPHA */
+#define m_RGA3_OVLP_BOT_ALPHA_SW_BOT_ALPHA_M1			(0x1 << 1)
+#define m_RGA3_OVLP_BOT_ALPHA_SW_BOT_BLEND_M1			(0x3 << 2)
+#define m_RGA3_OVLP_BOT_ALPHA_SW_BOT_ALPHA_CAL_M1		(0x1 << 4)
+#define m_RGA3_OVLP_BOT_ALPHA_SW_BOT_FACTOR_M1			(0x7 << 5)
+
+#define s_RGA3_OVLP_BOT_ALPHA_SW_BOT_ALPHA_M1(x)		((x & 0x1) << 1)
+#define s_RGA3_OVLP_BOT_ALPHA_SW_BOT_BLEND_M1(x)		((x & 0x3) << 2)
+#define s_RGA3_OVLP_BOT_ALPHA_SW_BOT_ALPHA_CAL_M1(x)		((x & 0x1) << 4)
+#define s_RGA3_OVLP_BOT_ALPHA_SW_BOT_FACTOR_M1(x)		((x & 0x7) << 5)
+
+/* RGA3_WR_CTRL */
+#define m_RGA3_WR_CTRL_SW_WR_MODE				(0x3 << 0)
+#define m_RGA3_WR_CTRL_SW_WR_FBCE_SPARSE_EN			(0x1 << 2)
+#define m_RGA3_WR_CTRL_SW_WR_PIC_FORMAT				(0xf << 4)
+#define m_RGA3_WR_CTRL_SW_WR_FORMAT				(0x3 << 8)
+#define m_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT			(0x1 << 10)
+#define m_RGA3_WR_CTRL_SW_WR_ENDIAN_MODE			(0x1 << 11)
+#define m_RGA3_WR_CTRL_SW_WR_PIX_SWAP				(0x1 << 12)
+#define m_RGA3_WR_CTRL_SW_OUTSTANDING_MAX			(0x3f << 13)
+#define m_RGA3_WR_CTRL_SW_WR_YC_SWAP				(0x1 << 20)
+
+#define s_RGA3_WR_CTRL_SW_WR_MODE(x)				((x & 0x3) << 0)
+#define s_RGA3_WR_CTRL_SW_WR_FBCE_SPARSE_EN(x)			((x & 0x1) << 2)
+#define s_RGA3_WR_CTRL_SW_WR_PIC_FORMAT(x)			((x & 0xf) << 4)
+#define s_RGA3_WR_CTRL_SW_WR_FORMAT(x)				((x & 0x3) << 8)
+#define s_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT(x)			((x & 0x1) << 10)
+#define s_RGA3_WR_CTRL_SW_WR_ENDIAN_MODE(x)			((x & 0x1) << 11)
+#define s_RGA3_WR_CTRL_SW_WR_PIX_SWAP(x)			((x & 0x1) << 12)
+#define s_RGA3_WR_CTRL_SW_OUTSTANDING_MAX(x)			((x & 0x3f) << 13)
+#define s_RGA3_WR_CTRL_SW_WR_YC_SWAP(x)				((x & 0x1) << 20)
+
+/* RGA3_WR_FBCE_CTRL */
+#define m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_BLKBD_OPT_DIS		(0x1 << 0)
+#define m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_HOFF_DISS		(0x1 << 1)
+#define m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_PL_FIFO0_WATERMARK	(0x3f << 2)
+#define m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_PL_FIFO1_WATERMARK	(0x3f << 8)
+#define m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_SIZE_ALIGN_DIS		(0x1 << 31)
+
+#define s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_BLKBD_OPT_DIS(x)		((x & 0x1) << 0)
+#define s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_HOFF_DISS(x)		((x & 0x1) << 1)
+#define s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_PL_FIFO0_WATERMARK(x)	((x & 0x3f) << 2)
+#define s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_PL_FIFO1_WATERMARK(x)	((x & 0x3f) << 8)
+#define s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_SIZE_ALIGN_DIS(x)	((x & 0x1) << 31)
+
+/* RGA3_MMU_STATUS read_only */
+#define m_RGA3_MMU_STATUS_PAGING_ENABLED			(0x1 << 0)
+#define m_RGA3_MMU_STATUS_PAGE_FAULT_ACTIVE			(0x1 << 1)
+#define m_RGA3_MMU_STATUS_STAIL_ACTIVE				(0x1 << 2)
+#define m_RGA3_MMU_STATUS_MMU_IDLE				(0x1 << 3)
+#define m_RGA3_MMU_STATUS_REPLAY_BUFFER_EMPTY			(0x1 << 4)
+#define m_RGA3_MMU_STATUS_PAGE_FAULT_IS_WRITE			(0x1 << 5)
+#define m_RGA3_MMU_STATUS_PAGE_FAULT_BUS_ID			(0x1f << 6)
+
+/* RGA3_MMU_INT_RAWSTAT read_only */
+#define m_RGA3_MMU_INT_RAWSTAT_READ_BUS_ERROR			(0x1 << 0)
+#define m_RGA3_MMU_INT_RAWSTAT_PAGE_FAULT			(0x1 << 1)
+
+/* RGA3_MMU_INT_CLEAR write_only */
+#define m_RGA3_MMU_INT_CLEAR_READ_BUS_ERROR			(0x1 << 0)
+#define m_RGA3_MMU_INT_CLEAR_PAGE_FAULT				(0x1 << 1)
+
+#define s_RGA3_MMU_INT_CLEAR_READ_BUS_ERROR(x)			((x & 0x1) << 0)
+#define s_RGA3_MMU_INT_CLEAR_PAGE_FAULT(x)			((x & 0x1) << 1)
+
+/* RGA3_MMU_INT_MASK */
+#define m_RGA3_MMU_INT_MASK_READ_BUS_ERROR			(0x1 << 0)
+#define m_RGA3_MMU_INT_MASK_PAGE_FAULT				(0x1 << 1)
+
+#define s_RGA3_MMU_INT_MASK_READ_BUS_ERROR(x)			((x & 0x1) << 0)
+#define s_RGA3_MMU_INT_MASK_PAGE_FAULT(x)			((x & 0x1) << 1)
+
+/* RGA3_MMU_INT_STATUS read_only */
+#define m_RGA3_MMU_INT_STATUS_READ_BUS_ERROR			(0x1 << 0)
+#define m_RGA3_MMU_INT_STATUS_PAGE_FAULT			(0x1 << 1)
+
+/* RGA3_MMU_AUTO_GATING */
+#define m_RGA3_MMU_AUTO_GATING_MMU_AUTO_GATING			(0x1 << 1)
+#define m_RGA3_MMU_AUTO_GATING_MMU_CFG_MODE			(0x1 << 1)
+#define m_RGA3_MMU_AUTO_GATING_MMU_BUG_FIXED_DISABLE		(0x1 << 31)
+
+#define s_RGA3_MMU_AUTO_GATING_MMU_AUTO_GATING(x)		((x & 0x1) << 1)
+#define s_RGA3_MMU_AUTO_GATING_MMU_BUG_FIXED_DISABLE(x)		((x & 0x1) << 31)
+
+#define RGA3_ROT_BIT_ROT_90			BIT(0)
+#define RGA3_ROT_BIT_X_MIRROR			BIT(1)
+#define RGA3_ROT_BIT_Y_MIRROR			BIT(2)
+
+union rga3_color_ctrl {
+	uint32_t value;
+	struct {
+		uint32_t color_mode:1;
+		uint32_t alpha_mode:1;
+		uint32_t blend_mode:2;
+		uint32_t alpha_cal_mode:1;
+		uint32_t factor_mode:3;
+
+		uint32_t reserved:8;
+
+		uint32_t global_alpha:8;
+	} bits;
+};
+
+union rga3_alpha_ctrl {
+	uint32_t value;
+	struct {
+		uint32_t reserved:1;
+		uint32_t alpha_mode:1;
+		uint32_t blend_mode:2;
+		uint32_t alpha_cal_mode:1;
+		uint32_t factor_mode:3;
+	} bits;
+};
+
+extern const struct rga_backend_ops rga3_ops;
+
+#endif
+
diff --git a/drivers/video/rockchip/rga3/include/rga_common.h b/drivers/video/rockchip/rga3/include/rga_common.h
new file mode 100644
index 0000000000000..6c64f74019c97
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_common.h
@@ -0,0 +1,86 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *  Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKRGA_COMMON_H_
+#define __LINUX_RKRGA_COMMON_H_
+
+#include "rga_drv.h"
+#include "rga_hw_config.h"
+
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+
+#define pr_fmt(fmt) "%s: " fmt, "rga"
+
+#define rga_log(fmt, args...) \
+	pr_info("%-6d %-6d: " fmt, get_current()->tgid, get_current()->pid, ##args)
+#define rga_err(fmt, args...) \
+	pr_err("%-6d %-6d: " fmt, get_current()->tgid, get_current()->pid, ##args)
+
+#define rga_dev_log(dev, fmt, args...) \
+	dev_info(dev, "%-6d %-6d: " fmt, current->tgid, current->pid, ##args)
+#define rga_dev_err(dev, fmt, args...) \
+	dev_err(dev, "%-6d %-6d: " fmt, current->tgid, current->pid, ##args)
+
+#define rga_job_log(job, fmt, args...) \
+	pr_info("%-6d %-6d: ID[%d]: " fmt, job->session->tgid, job->pid, job->request_id, ##args)
+#define rga_job_err(job, fmt, args...) \
+	pr_err("%-6d %-6d: ID[%d]: " fmt, job->session->tgid, job->pid, job->request_id, ##args)
+
+#define rga_req_log(request, fmt, args...) \
+	pr_info("%-6d %-6d: ID[%d]: " fmt, \
+	request->session->tgid, request->pid, request->id, ##args)
+#define rga_req_err(request, fmt, args...) \
+	pr_err("%-6d %-6d: ID[%d]: " fmt, request->session->tgid, request->pid, request->id, ##args)
+
+#define rga_buf_log(buf, fmt, args...) \
+	pr_info("%-6d %-6d: handle[%d]: " fmt, \
+	buf->session->tgid, current->pid, buf->handle, ##args)
+#define rga_buf_err(buf, fmt, args...) \
+	pr_err("%-6d %-6d: handle[%d]: " fmt, buf->session->tgid, current->pid, buf->handle, ##args)
+
+#define RGA_GET_PAGE_COUNT(size) (((size) >> PAGE_SHIFT) + (((size) & (~PAGE_MASK)) ? 1 : 0))
+
+bool rga_is_rgb_format(uint32_t format);
+bool rga_is_yuv_format(uint32_t format);
+bool rga_is_alpha_format(uint32_t format);
+bool rga_is_yuv420_packed_format(uint32_t format);
+bool rga_is_yuv420_planar_format(uint32_t format);
+bool rga_is_yuv420_semi_planar_format(uint32_t format);
+bool rga_is_yuv422_packed_format(uint32_t format);
+bool rga_is_yuv422_planar_format(uint32_t format);
+bool rga_is_yuv422_semi_planar_format(uint32_t format);
+bool rga_is_yuv8bit_format(uint32_t format);
+bool rga_is_yuv10bit_format(uint32_t format);
+bool rga_is_yuv422p_format(uint32_t format);
+bool rga_is_only_y_format(uint32_t format);
+
+int rga_get_format_bits(uint32_t format);
+int rga_get_pixel_stride_from_format(uint32_t format);
+
+const char *rga_get_format_name(uint32_t format);
+const char *rga_get_render_mode_str(uint8_t mode);
+const char *rga_get_store_mode_str(uint32_t mode);
+const char *rga_get_interp_str(uint8_t interp);
+const char *rga_get_rotate_mode_str(uint8_t mode);
+const char *rga_get_blend_mode_str(enum rga_alpha_blend_mode mode);
+const char *rga_get_memory_type_str(uint8_t type);
+const char *rga_get_mmu_type_str(enum rga_mmu mmu_type);
+const char *rga_get_dma_data_direction_str(enum dma_data_direction dir);
+const char *rga_get_core_name(enum RGA_SCHEDULER_CORE core);
+
+void rga_convert_addr(struct rga_img_info_t *img, bool before_vir_get_channel);
+void rga_swap_pd_mode(struct rga_req *req_rga);
+int rga_image_size_cal(int w, int h, int format,
+		       int *yrgb_size, int *uv_size, int *v_size);
+void rga_dump_memory_parm(struct rga_memory_parm *parm);
+void rga_dump_external_buffer(struct rga_external_buffer *buffer);
+void rga_dump_req(struct rga_request *request, struct rga_req *req);
+
+#endif
diff --git a/drivers/video/rockchip/rga3/include/rga_debugger.h b/drivers/video/rockchip/rga3/include/rga_debugger.h
new file mode 100644
index 0000000000000..cc2966ec554dc
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_debugger.h
@@ -0,0 +1,145 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *	Cerf Yu <cerf.yu@rock-chips.com>
+ *	Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef _RGA_DEBUGGER_H_
+#define _RGA_DEBUGGER_H_
+
+#include "rga_drv.h"
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER
+
+extern int RGA_DEBUG_REG;
+extern int RGA_DEBUG_MSG;
+extern int RGA_DEBUG_TIME;
+extern int RGA_DEBUG_INT_FLAG;
+extern int RGA_DEBUG_MM;
+extern int RGA_DEBUG_CHECK_MODE;
+extern int RGA_DEBUG_INTERNAL_MODE;
+extern int RGA_DEBUG_NONUSE;
+extern int RGA_DEBUG_DUMP_IMAGE;
+
+#define DEBUGGER_EN(name) (unlikely(RGA_DEBUG_##name ? true : false))
+
+/*
+ * struct rga_debugger - RGA debugger information
+ *
+ * This structure represents a debugger to be created by the rga driver
+ * or core.
+ */
+struct rga_debugger {
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS
+	/* Directory of debugfs file */
+	struct dentry *debugfs_dir;
+	struct list_head debugfs_entry_list;
+	struct mutex debugfs_lock;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_PROC_FS
+	/* Directory of procfs file */
+	struct proc_dir_entry *procfs_dir;
+	struct list_head procfs_entry_list;
+	struct mutex procfs_lock;
+#endif
+};
+
+/*
+ * struct rga_debugger_list - debugfs/procfs info list entry
+ *
+ * This structure represents a debugfs/procfs file to be created by the rga
+ * driver or core.
+ */
+struct rga_debugger_list {
+	/* File name */
+	const char *name;
+	/*
+	 * Show callback. &seq_file->private will be set to the &struct
+	 * rga_debugger_node corresponding to the instance of this info
+	 * on a given &struct rga_debugger.
+	 */
+	int (*show)(struct seq_file *seq, void *data);
+	/*
+	 * Write callback. &seq_file->private will be set to the &struct
+	 * rga_debugger_node corresponding to the instance of this info
+	 * on a given &struct rga_debugger.
+	 */
+	ssize_t (*write)(struct file *file, const char __user *ubuf,
+		size_t len, loff_t *offp);
+	/* Procfs/Debugfs private data. */
+	void *data;
+};
+
+/*
+ * struct rga_debugger_node - Nodes for debugfs/procfs
+ *
+ * This structure represents each instance of procfs/debugfs created from the
+ * template.
+ */
+struct rga_debugger_node {
+	struct rga_debugger *debugger;
+
+	/* template for this node. */
+	const struct rga_debugger_list *info_ent;
+
+	/* Each Procfs/Debugfs file. */
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS
+	struct dentry *dent;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_PROC_FS
+	struct proc_dir_entry *pent;
+#endif
+
+	struct list_head list;
+};
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS
+int rga_debugfs_init(void);
+int rga_debugfs_remove(void);
+#else
+static inline int rga_debugfs_remove(void)
+{
+	return 0;
+}
+static inline int rga_debugfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RGA_PROC_FS
+int rga_procfs_remove(void);
+int rga_procfs_init(void);
+#else
+static inline int rga_procfs_remove(void)
+{
+	return 0;
+}
+static inline int rga_procfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA_PROC_FS */
+
+#else
+
+#define DEBUGGER_EN(name) (unlikely(false))
+
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER */
+
+void rga_request_task_debug_info(struct seq_file *m, struct rga_req *req);
+#ifdef CONFIG_NO_GKI
+void rga_dump_job_image(struct rga_job *dump_job);
+#else
+static inline void rga_dump_job_image(struct rga_job *dump_job)
+{
+}
+#endif /* #ifdef CONFIG_NO_GKI */
+
+#endif /* #ifndef _RGA_DEBUGGER_H_ */
+
diff --git a/drivers/video/rockchip/rga3/include/rga_dma_buf.h b/drivers/video/rockchip/rga3/include/rga_dma_buf.h
new file mode 100644
index 0000000000000..fefea23914e2d
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_dma_buf.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *  Cerf Yu <cerf.yu@rock-chips.com>
+ *  Huang Lee <Putin.li@rock-chips.com>
+ */
+#ifndef __RGA3_DMA_BUF_H__
+#define __RGA3_DMA_BUF_H__
+
+#include "rga_drv.h"
+
+#ifndef for_each_sgtable_sg
+/*
+ * Loop over each sg element in the given sg_table object.
+ */
+#define for_each_sgtable_sg(sgt, sg, i)		\
+	for_each_sg((sgt)->sgl, sg, (sgt)->orig_nents, i)
+#endif
+
+int rga_buf_size_cal(unsigned long yrgb_addr, unsigned long uv_addr,
+		      unsigned long v_addr, int format, uint32_t w,
+		      uint32_t h, unsigned long *StartAddr, unsigned long *size);
+
+int rga_virtual_memory_check(void *vaddr, u32 w, u32 h, u32 format, int fd);
+int rga_dma_memory_check(struct rga_dma_buffer *rga_dma_buffer, struct rga_img_info_t *img);
+
+int rga_iommu_map_sgt(struct sg_table *sgt, size_t size,
+		      struct rga_dma_buffer *buffer,
+		      struct device *rga_dev);
+int rga_iommu_map(phys_addr_t paddr, size_t size,
+		  struct rga_dma_buffer *buffer,
+		  struct device *rga_dev);
+void rga_iommu_unmap(struct rga_dma_buffer *buffer);
+
+int rga_dma_map_buf(struct dma_buf *dma_buf, struct rga_dma_buffer *rga_dma_buffer,
+		    enum dma_data_direction dir, struct device *rga_dev);
+int rga_dma_map_fd(int fd, struct rga_dma_buffer *rga_dma_buffer,
+		   enum dma_data_direction dir, struct device *rga_dev);
+void rga_dma_unmap_buf(struct rga_dma_buffer *rga_dma_buffer);
+
+void rga_dma_sync_flush_range(void *pstart, void *pend, struct rga_scheduler_t *scheduler);
+
+struct rga_dma_buffer *rga_dma_alloc_coherent(struct rga_scheduler_t *scheduler, int size);
+int rga_dma_free(struct rga_dma_buffer *buffer);
+
+#endif /* #ifndef __RGA3_DMA_BUF_H__ */
+
diff --git a/drivers/video/rockchip/rga3/include/rga_drv.h b/drivers/video/rockchip/rga3/include/rga_drv.h
new file mode 100644
index 0000000000000..e0c34bb4aa7ed
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_drv.h
@@ -0,0 +1,485 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RGA_DRV_H_
+#define __LINUX_RGA_DRV_H_
+
+#include <linux/clk.h>
+#include <linux/completion.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/fb.h>
+#include <linux/fdtable.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
+#include <linux/kref.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regulator/consumer.h>
+#include <linux/scatterlist.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/syscalls.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/uaccess.h>
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/pm_runtime.h>
+#include <linux/sched/mm.h>
+#include <linux/string_helpers.h>
+
+#include <asm/cacheflush.h>
+
+#include <linux/iommu.h>
+#include <linux/iova.h>
+#include <linux/pagemap.h>
+
+#ifdef CONFIG_DMABUF_CACHE
+#include <linux/dma-buf-cache.h>
+#else
+#include <linux/dma-buf.h>
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0))
+#include <linux/dma-map-ops.h>
+#endif
+
+#include <linux/hrtimer.h>
+
+#include "rga.h"
+
+#define RGA_CORE_REG_OFFSET 0x10000
+
+/* load interval: 1000ms */
+#define RGA_LOAD_INTERVAL_US 1000000
+#define RGA_LOAD_ACTIVE_MAX_US 5000000
+
+/* timer interval: 1000ms */
+#define RGA_TIMER_INTERVAL_NS 1000000000
+
+#if ((defined(CONFIG_RK_IOMMU) || defined(CONFIG_ROCKCHIP_IOMMU)) \
+	&& defined(CONFIG_ION_ROCKCHIP))
+#define CONFIG_RGA_IOMMU
+#endif
+
+/* Driver information */
+#define DRIVER_DESC		"RGA multicore Device Driver"
+#define DRIVER_NAME		"rga_multicore"
+
+#define STR_HELPER(x) #x
+#define STR(x) STR_HELPER(x)
+
+#define DRIVER_MAJOR_VERISON		1
+#define DRIVER_MINOR_VERSION		3
+#define DRIVER_REVISION_VERSION		7
+#define DRIVER_PATCH_VERSION
+
+#define DRIVER_VERSION (STR(DRIVER_MAJOR_VERISON) "." STR(DRIVER_MINOR_VERSION) \
+			"." STR(DRIVER_REVISION_VERSION) STR(DRIVER_PATCH_VERSION))
+
+/* time limit */
+#define RGA_JOB_TIMEOUT_DELAY		HZ
+#define RGA_RESET_TIMEOUT			1000
+
+#define RGA_MAX_SCHEDULER	RGA_HW_SIZE
+#define RGA_MAX_BUS_CLK		10
+
+#define RGA_BUFFER_POOL_MAX_SIZE	64
+
+#ifndef ABS
+#define ABS(X)			 (((X) < 0) ? (-(X)) : (X))
+#endif
+
+#ifndef CLIP
+#define CLIP(x, a, b)	 (((x) < (a)) \
+	? (a) : (((x) > (b)) ? (b) : (x)))
+#endif
+
+extern struct rga_drvdata_t *rga_drvdata;
+
+enum {
+	RGA_CMD_SLAVE		= 1,
+	RGA_CMD_MASTER		= 2,
+};
+
+enum iommu_dma_cookie_type {
+	IOMMU_DMA_IOVA_COOKIE,
+	IOMMU_DMA_MSI_COOKIE,
+};
+
+enum rga_scheduler_status {
+	RGA_SCHEDULER_IDLE = 0,
+	RGA_SCHEDULER_WORKING,
+	RGA_SCHEDULER_ABORT,
+};
+
+enum rga_job_state {
+	RGA_JOB_STATE_PENDING = 0,
+	RGA_JOB_STATE_PREPARE,
+	RGA_JOB_STATE_RUNNING,
+	RGA_JOB_STATE_FINISH,
+	RGA_JOB_STATE_DONE,
+	RGA_JOB_STATE_INTR_ERR,
+	RGA_JOB_STATE_HW_TIMEOUT,
+	RGA_JOB_STATE_ABORT,
+};
+
+enum RGA_DEVICE_TYPE {
+	RGA_DEVICE_RGA2 = 0,
+	RGA_DEVICE_RGA3,
+	RGA_DEVICE_BUTT,
+};
+
+struct rga_iommu_dma_cookie {
+	enum iommu_dma_cookie_type  type;
+
+	/* Full allocator for IOMMU_DMA_IOVA_COOKIE */
+	struct iova_domain  iovad;
+};
+
+struct rga_iommu_info {
+	struct device *dev;
+	struct device *default_dev;		/* for dma-buf_api */
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+};
+
+struct rga_dma_buffer {
+	/* DMABUF information */
+	struct dma_buf *dma_buf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	void *vaddr;
+
+	struct iommu_domain *domain;
+
+	enum dma_data_direction dir;
+
+	dma_addr_t iova;
+	dma_addr_t dma_addr;
+	unsigned long size;
+	/*
+	 * The offset of the first page of the sgt.
+	 * Since alloc iova must be page aligned, the offset of the first page is
+	 * identified separately.
+	 */
+	size_t offset;
+
+	/* The scheduler of the mapping */
+	struct rga_scheduler_t *scheduler;
+};
+
+struct rga_virt_addr {
+	uint64_t addr;
+
+	struct page **pages;
+	int pages_order;
+	int page_count;
+	unsigned long size;
+
+	/* The offset of the first page of the virtual address */
+	size_t offset;
+
+	int result;
+};
+
+struct rga_internal_buffer {
+	/* DMA buffer */
+	struct rga_dma_buffer *dma_buffer;
+
+	/* virtual address */
+	struct rga_virt_addr *virt_addr;
+
+	/* physical address */
+	uint64_t phys_addr;
+
+	/* buffer size */
+	unsigned long size;
+
+	struct rga_memory_parm memory_parm;
+
+
+	struct mm_struct *current_mm;
+
+	/* memory type. */
+	uint32_t type;
+
+	uint32_t handle;
+
+	uint32_t mm_flag;
+
+	struct kref refcount;
+	struct rga_session *session;
+};
+
+struct rga_scheduler_t;
+
+struct rga_session {
+	int id;
+
+	pid_t tgid;
+
+	char *pname;
+
+	ktime_t last_active;
+};
+
+struct rga_job_buffer {
+	union {
+		struct {
+			struct rga_external_buffer *ex_y_addr;
+			struct rga_external_buffer *ex_uv_addr;
+			struct rga_external_buffer *ex_v_addr;
+		};
+		struct rga_external_buffer *ex_addr;
+	};
+
+	union {
+		struct {
+			struct rga_internal_buffer *y_addr;
+			struct rga_internal_buffer *uv_addr;
+			struct rga_internal_buffer *v_addr;
+		};
+		struct rga_internal_buffer *addr;
+	};
+
+	uint32_t *page_table;
+	int order;
+	int page_count;
+};
+
+struct rga_job_timestamp {
+	ktime_t init;
+	ktime_t insert;
+	ktime_t hw_execute;
+	ktime_t hw_done;
+	ktime_t done;
+
+	/* The time only for hrtimer to calculate the load */
+	ktime_t hw_recode;
+};
+
+struct rga_job {
+	struct list_head head;
+
+	struct rga_scheduler_t *scheduler;
+	struct rga_session *session;
+
+	struct rga_req rga_command_base;
+	struct rga_dma_buffer *cmd_buf;
+	struct rga_full_csc full_csc;
+	struct rga_csc_clip full_csc_clip;
+	struct rga_pre_intr_info pre_intr_info;
+
+	struct rga_job_buffer src_buffer;
+	struct rga_job_buffer src1_buffer;
+	struct rga_job_buffer dst_buffer;
+	/* used by rga2 */
+	struct rga_job_buffer els_buffer;
+
+	/* for rga2 virtual_address */
+	struct mm_struct *mm;
+
+	/* job time stamp */
+	struct rga_job_timestamp timestamp;
+
+	unsigned int flags;
+	int request_id;
+	int priority;
+	int core;
+	int ret;
+	pid_t pid;
+	bool use_batch_mode;
+
+	struct kref refcount;
+	unsigned long state;
+	uint32_t intr_status;
+	uint32_t hw_status;
+	uint32_t cmd_status;
+
+	uint32_t work_cycle;
+};
+
+struct rga_backend_ops {
+	int (*get_version)(struct rga_scheduler_t *scheduler);
+	int (*set_reg)(struct rga_job *job, struct rga_scheduler_t *scheduler);
+	int (*init_reg)(struct rga_job *job);
+	void (*soft_reset)(struct rga_scheduler_t *scheduler);
+	int (*read_back_reg)(struct rga_job *job, struct rga_scheduler_t *scheduler);
+	int (*read_status)(struct rga_job *job, struct rga_scheduler_t *scheduler);
+	int (*irq)(struct rga_scheduler_t *scheduler);
+	int (*isr_thread)(struct rga_job *job, struct rga_scheduler_t *scheduler);
+};
+
+struct rga_timer {
+	u32 busy_time;
+	u32 busy_time_record;
+};
+
+struct rga_scheduler_t {
+	struct device *dev;
+	void __iomem *rga_base;
+	struct rga_iommu_info *iommu_info;
+
+	struct clk_bulk_data *clks;
+	int num_clks;
+
+	enum rga_scheduler_status status;
+	int pd_refcount;
+
+	struct rga_job *running_job;
+	struct list_head todo_list;
+	spinlock_t irq_lock;
+	wait_queue_head_t job_done_wq;
+
+	const struct rga_backend_ops *ops;
+	const struct rga_hw_data *data;
+	unsigned long hw_issues_mask;
+
+	int job_count;
+	int irq;
+	struct rga_version_t version;
+	int core;
+
+	struct rga_timer timer;
+};
+
+struct rga_request {
+	struct rga_req *task_list;
+	int task_count;
+	uint32_t finished_task_count;
+	uint32_t failed_task_count;
+
+	bool use_batch_mode;
+	bool is_running;
+	bool is_done;
+	int ret;
+	uint32_t sync_mode;
+
+	int32_t acquire_fence_fd;
+	int32_t release_fence_fd;
+	struct dma_fence *release_fence;
+	spinlock_t fence_lock;
+	struct work_struct fence_work;
+
+	wait_queue_head_t finished_wq;
+
+	int flags;
+	uint8_t mpi_config_flags;
+	int id;
+	struct rga_session *session;
+
+	spinlock_t lock;
+	struct kref refcount;
+
+	pid_t pid;
+
+	/*
+	 * The mapping of virtual addresses to obtain physical addresses requires
+	 * the memory mapping information of the current process.
+	 */
+	struct mm_struct *current_mm;
+
+	struct rga_feature feature;
+	/* TODO: add some common work */
+};
+
+struct rga_pending_request_manager {
+	struct mutex lock;
+
+	/*
+	 * @request_idr:
+	 *
+	 * Mapping of request id to object pointers. Used by the GEM
+	 * subsystem. Protected by @lock.
+	 */
+	struct idr request_idr;
+
+	int request_count;
+};
+
+struct rga_session_manager {
+	struct mutex lock;
+
+	struct idr ctx_id_idr;
+
+	int session_cnt;
+};
+
+struct rga_drvdata_t {
+	/* used by rga2's mmu lock */
+	struct mutex lock;
+
+	struct rga_scheduler_t *scheduler[RGA_MAX_SCHEDULER];
+	int num_of_scheduler;
+	int device_count[RGA_DEVICE_BUTT];
+	/* The scheduler_index used by default for memory mapping. */
+	int map_scheduler_index;
+	struct rga_mmu_base *mmu_base;
+
+	struct delayed_work power_off_work;
+
+	struct rga_mm *mm;
+
+	/* rga_job pending manager, import by RGA_START_CONFIG */
+	struct rga_pending_request_manager *pend_request_manager;
+
+	struct rga_session_manager *session_manager;
+
+#ifdef CONFIG_ROCKCHIP_RGA_ASYNC
+	struct rga_fence_context *fence_ctx;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER
+	struct rga_debugger *debugger;
+#endif
+
+	bool shutdown;
+	struct rw_semaphore rwsem;
+};
+
+struct rga_irqs_data_t {
+	const char *name;
+	irqreturn_t (*irq_hdl)(int irq, void *ctx);
+	irqreturn_t (*irq_thread)(int irq, void *ctx);
+};
+
+struct rga_match_data_t {
+	enum RGA_DEVICE_TYPE device_type;
+
+	const struct rga_backend_ops *ops;
+};
+
+static inline int rga_read(int offset, struct rga_scheduler_t *scheduler)
+{
+	return readl(scheduler->rga_base + offset);
+}
+
+static inline void rga_write(int value, int offset, struct rga_scheduler_t *scheduler)
+{
+	writel(value, scheduler->rga_base + offset);
+}
+
+int rga_power_enable(struct rga_scheduler_t *scheduler);
+int rga_power_disable(struct rga_scheduler_t *scheduler);
+
+int rga_kernel_commit(struct rga_req *cmd);
+
+#endif /* __LINUX_RGA_FENCE_H_ */
diff --git a/drivers/video/rockchip/rga3/include/rga_fence.h b/drivers/video/rockchip/rga3/include/rga_fence.h
new file mode 100644
index 0000000000000..75c7c22926a2e
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_fence.h
@@ -0,0 +1,101 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RGA_FENCE_H_
+#define __LINUX_RGA_FENCE_H_
+
+struct rga_fence_context {
+	unsigned int context;
+	unsigned int seqno;
+	spinlock_t spinlock;
+};
+
+struct rga_fence_waiter {
+	/* Base sync driver waiter structure */
+	struct dma_fence_cb waiter;
+
+	void *private;
+};
+
+#ifdef CONFIG_ROCKCHIP_RGA_ASYNC
+int rga_fence_context_init(struct rga_fence_context **ctx);
+void rga_fence_context_remove(struct rga_fence_context **ctx);
+
+struct dma_fence *rga_dma_fence_alloc(void);
+int rga_dma_fence_get_fd(struct dma_fence *fence);
+struct dma_fence *rga_get_dma_fence_from_fd(int fence_fd);
+int rga_dma_fence_wait(struct dma_fence *fence);
+int rga_dma_fence_add_callback(struct dma_fence *fence, dma_fence_func_t func, void *private);
+
+
+static inline void rga_dma_fence_put(struct dma_fence *fence)
+{
+	if (fence)
+		dma_fence_put(fence);
+}
+
+static inline void rga_dma_fence_signal(struct dma_fence *fence, int error)
+{
+	if (fence) {
+		if (error != 0)
+			dma_fence_set_error(fence, error);
+		dma_fence_signal(fence);
+	}
+}
+
+static inline int rga_dma_fence_get_status(struct dma_fence *fence)
+{
+	if (fence)
+		return dma_fence_get_status(fence);
+	else
+		return 1;
+}
+
+#else
+static inline struct dma_fence *rga_dma_fence_alloc(void)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+static inline int rga_dma_fence_get_fd(struct dma_fence *fence)
+{
+	return -1;
+}
+
+static inline struct dma_fence *rga_get_dma_fence_from_fd(int fence_fd)
+{
+	return NULL;
+}
+
+static inline int rga_dma_fence_wait(struct dma_fence *fence)
+{
+	return 0;
+}
+
+static inline int rga_dma_fence_add_callback(struct dma_fence *fence,
+					     dma_fence_func_t func,
+					     void *private)
+{
+	return -EINVAL;
+}
+
+static inline void rga_dma_fence_put(struct dma_fence *fence)
+{
+}
+
+static inline void rga_dma_fence_signal(struct dma_fence *fence, int error)
+{
+}
+
+static inline int rga_dma_fence_get_status(struct dma_fence *fence)
+{
+	return -EINVAL;
+}
+
+#endif /* #ifdef CONFIG_SYNC_FILE */
+
+#endif /* __LINUX_RGA_FENCE_H_ */
diff --git a/drivers/video/rockchip/rga3/include/rga_hw_config.h b/drivers/video/rockchip/rga3/include/rga_hw_config.h
new file mode 100644
index 0000000000000..c4982e037e42d
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_hw_config.h
@@ -0,0 +1,94 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RGA_HW_CONFIG_H_
+#define __LINUX_RGA_HW_CONFIG_H_
+
+#include "rga_drv.h"
+
+enum rga_mmu {
+	RGA_NONE_MMU	= 0,
+	RGA_MMU		= 1,
+	RGA_IOMMU	= 2,
+};
+
+enum rga_hw_support_format_index {
+	RGA_RASTER_INDEX,
+	RGA_AFBC16x16_INDEX,
+	RGA_TILE8x8_INDEX,
+	RGA_TILE4x4_INDEX,
+	RGA_RKFBC64x4_INDEX,
+	RGA_AFBC32x8_INDEX,
+	RGA_FORMAT_INDEX_BUTT,
+};
+
+enum rga_hw_issue {
+	RGA_HW_ISSUE_DIS_AUTO_RST,
+};
+
+struct rga_win_data {
+	const char *name;
+	const uint32_t *formats[RGA_FORMAT_INDEX_BUTT];
+	uint32_t formats_count[RGA_FORMAT_INDEX_BUTT];
+
+	uint32_t supported_rotations;
+	uint32_t scale_up_mode;
+	uint32_t scale_down_mode;
+	uint32_t rd_mode;
+};
+
+struct rga_rect {
+	int width;
+	int height;
+};
+
+struct rga_rect_range {
+	struct rga_rect min;
+	struct rga_rect max;
+};
+
+struct rga_hw_data {
+	uint32_t version;
+	uint32_t feature;
+
+	uint32_t csc_r2y_mode;
+	uint32_t csc_y2r_mode;
+
+	struct rga_rect_range input_range;
+	struct rga_rect_range output_range;
+
+	unsigned int max_upscale_factor;
+	unsigned int max_downscale_factor;
+
+	uint32_t byte_stride_align;
+	uint32_t max_byte_stride;
+
+	const struct rga_win_data *win;
+	unsigned int win_size;
+
+	enum rga_mmu mmu;
+};
+
+extern const struct rga_hw_data rga3_data;
+extern const struct rga_hw_data rga2e_data;
+extern const struct rga_hw_data rga2e_1106_data;
+extern const struct rga_hw_data rga2e_3506_data;
+extern const struct rga_hw_data rga2e_iommu_data;
+extern const struct rga_hw_data rga2p_iommu_data;
+extern const struct rga_hw_data rga2p_lite_1103b_data;
+
+#define rga_hw_has_issue(scheduler, issue) test_bit(issue, &((scheduler)->hw_issues_mask))
+#define rga_hw_set_issue_mask(scheduler, issue) set_bit(issue, &((scheduler)->hw_issues_mask))
+
+/* Returns false if in range, true otherwise */
+static inline bool rga_hw_out_of_range(const struct rga_rect_range *range, int width, int height)
+{
+	return (width > range->max.width || height > range->max.height ||
+		width < range->min.width || height < range->min.height);
+}
+
+#endif /* __LINUX_RGA_HW_CONFIG_H_ */
diff --git a/drivers/video/rockchip/rga3/include/rga_iommu.h b/drivers/video/rockchip/rga3/include/rga_iommu.h
new file mode 100644
index 0000000000000..b80a1f48bb25b
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_iommu.h
@@ -0,0 +1,79 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RGA_MMU_INFO_H__
+#define __RGA_MMU_INFO_H__
+
+#include "rga_drv.h"
+
+/* RGA_IOMMU register offsets */
+#define RGA_IOMMU_BASE				0xf00
+#define RGA_IOMMU_DTE_ADDR			(RGA_IOMMU_BASE + 0x00) /* Directory table address */
+#define RGA_IOMMU_STATUS			(RGA_IOMMU_BASE + 0x04)
+#define RGA_IOMMU_COMMAND			(RGA_IOMMU_BASE + 0x08)
+#define RGA_IOMMU_PAGE_FAULT_ADDR		(RGA_IOMMU_BASE + 0x0C) /* IOVA of last page fault */
+#define RGA_IOMMU_ZAP_ONE_LINE			(RGA_IOMMU_BASE + 0x10) /* Shootdown one IOTLB entry */
+#define RGA_IOMMU_INT_RAWSTAT			(RGA_IOMMU_BASE + 0x14) /* IRQ status ignoring mask */
+#define RGA_IOMMU_INT_CLEAR			(RGA_IOMMU_BASE + 0x18) /* Acknowledge and re-arm irq */
+#define RGA_IOMMU_INT_MASK			(RGA_IOMMU_BASE + 0x1C) /* IRQ enable */
+#define RGA_IOMMU_INT_STATUS			(RGA_IOMMU_BASE + 0x20) /* IRQ status after masking */
+#define RGA_IOMMU_AUTO_GATING			(RGA_IOMMU_BASE + 0x24)
+
+/* RGA_IOMMU_STATUS fields */
+#define RGA_IOMMU_STATUS_PAGING_ENABLED		BIT(0)
+#define RGA_IOMMU_STATUS_PAGE_FAULT_ACTIVE	BIT(1)
+#define RGA_IOMMU_STATUS_STALL_ACTIVE		BIT(2)
+#define RGA_IOMMU_STATUS_IDLE			BIT(3)
+#define RGA_IOMMU_STATUS_REPLAY_BUFFER_EMPTY	BIT(4)
+#define RGA_IOMMU_STATUS_PAGE_FAULT_IS_WRITE	BIT(5)
+#define RGA_IOMMU_STATUS_STALL_NOT_ACTIVE	BIT(31)
+
+/* RGA_IOMMU_COMMAND command values */
+#define RGA_IOMMU_CMD_ENABLE_PAGING		0 /* Enable memory translation */
+#define RGA_IOMMU_CMD_DISABLE_PAGING		1 /* Disable memory translation */
+#define RGA_IOMMU_CMD_ENABLE_STALL		2 /* Stall paging to allow other cmds */
+#define RGA_IOMMU_CMD_DISABLE_STALL		3 /* Stop stall re-enables paging */
+#define RGA_IOMMU_CMD_ZAP_CACHE			4 /* Shoot down entire IOTLB */
+#define RGA_IOMMU_CMD_PAGE_FAULT_DONE		5 /* Clear page fault */
+#define RGA_IOMMU_CMD_FORCE_RESET		6 /* Reset all registers */
+
+/* RGA_IOMMU_INT_* register fields */
+#define RGA_IOMMU_IRQ_PAGE_FAULT		0x01 /* page fault */
+#define RGA_IOMMU_IRQ_BUS_ERROR			0x02 /* bus read error */
+#define RGA_IOMMU_IRQ_MASK			(RGA_IOMMU_IRQ_PAGE_FAULT | RGA_IOMMU_IRQ_BUS_ERROR)
+
+/*
+ * The maximum input is 8192*8192, the maximum output is 4096*4096
+ * The size of physical pages requested is:
+ * (( maximum_input_value *
+ *         maximum_input_value * format_bpp ) / 4K_page_size) + 1
+ */
+#define RGA2_PHY_PAGE_SIZE	 (((8192 * 8192 * 4) / 4096) + 1)
+
+struct rga_mmu_base {
+	unsigned int *buf_virtual;
+	struct page **pages;
+	u8 buf_order;
+	u8 pages_order;
+
+	int32_t front;
+	int32_t back;
+	int32_t size;
+	int32_t curr;
+};
+
+int rga_user_memory_check(struct page **pages, u32 w, u32 h, u32 format, int flag);
+int rga_set_mmu_base(struct rga_job *job, struct rga2_req *req);
+unsigned int *rga_mmu_buf_get(struct rga_mmu_base *mmu_base, uint32_t size);
+
+struct rga_mmu_base *rga_mmu_base_init(size_t size);
+void rga_mmu_base_free(struct rga_mmu_base **mmu_base);
+
+int rga_iommu_detach(struct rga_iommu_info *info);
+int rga_iommu_attach(struct rga_iommu_info *info);
+struct rga_iommu_info *rga_iommu_probe(struct device *dev);
+int rga_iommu_remove(struct rga_iommu_info *info);
+
+int rga_iommu_bind(void);
+void rga_iommu_unbind(void);
+
+#endif
+
diff --git a/drivers/video/rockchip/rga3/include/rga_job.h b/drivers/video/rockchip/rga3/include/rga_job.h
new file mode 100644
index 0000000000000..28bf35bacc55c
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_job.h
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKRGA_JOB_H_
+#define __LINUX_RKRGA_JOB_H_
+
+#include <linux/spinlock.h>
+#include <linux/dma-fence.h>
+
+#include "rga_drv.h"
+
+#define RGA_CMD_REG_SIZE 256 /* 32 * 8 bit */
+
+enum job_flags {
+	RGA_JOB_DONE			= 1 << 0,
+	RGA_JOB_ASYNC			= 1 << 1,
+	RGA_JOB_SYNC			= 1 << 2,
+	RGA_JOB_USE_HANDLE		= 1 << 3,
+	RGA_JOB_UNSUPPORT_RGA_MMU	= 1 << 4,
+	RGA_JOB_DEBUG_FAKE_BUFFER	= 1 << 5,
+};
+
+void rga_job_scheduler_dump_info(struct rga_scheduler_t *scheduler);
+void rga_job_next(struct rga_scheduler_t *scheduler);
+struct rga_job *rga_job_done(struct rga_scheduler_t *scheduler);
+int rga_job_commit(struct rga_req *rga_command_base, struct rga_request *request);
+int rga_job_mpi_commit(struct rga_req *rga_command_base, struct rga_request *request);
+
+int rga_job_assign(struct rga_job *job);
+
+
+int rga_request_check(struct rga_user_request *req);
+struct rga_request *rga_request_lookup(struct rga_pending_request_manager *request_manager,
+				       uint32_t id);
+
+int rga_request_commit(struct rga_request *user_request);
+void rga_request_scheduler_shutdown(struct rga_scheduler_t *scheduler);
+void rga_request_scheduler_abort(struct rga_scheduler_t *scheduler);
+void rga_request_session_destroy_abort(struct rga_session *session);
+int rga_request_put(struct rga_request *request);
+void rga_request_get(struct rga_request *request);
+int rga_request_free(struct rga_request *request);
+int rga_request_alloc(uint32_t flags, struct rga_session *session);
+
+struct rga_request *rga_request_config(struct rga_user_request *user_request);
+struct rga_request *rga_request_kernel_config(struct rga_user_request *user_request);
+int rga_request_submit(struct rga_request *request);
+int rga_request_mpi_submit(struct rga_req *req, struct rga_request *request);
+int rga_request_release_signal(struct rga_scheduler_t *scheduler, struct rga_job *job);
+
+int rga_request_manager_init(struct rga_pending_request_manager **request_manager_session);
+int rga_request_manager_remove(struct rga_pending_request_manager **request_manager_session);
+
+#endif /* __LINUX_RKRGA_JOB_H_ */
diff --git a/drivers/video/rockchip/rga3/include/rga_mm.h b/drivers/video/rockchip/rga3/include/rga_mm.h
new file mode 100644
index 0000000000000..d68fd75dcb453
--- /dev/null
+++ b/drivers/video/rockchip/rga3/include/rga_mm.h
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *  Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKRGA_MM_H_
+#define __LINUX_RKRGA_MM_H_
+
+#include "rga_drv.h"
+
+enum rga_mm_flag {
+	/* It will identify whether the buffer is within 0 ~ 4G. */
+	RGA_MEM_UNDER_4G		= 1 << 0,
+	/* Logo enable IOMMU */
+	RGA_MEM_NEED_USE_IOMMU		= 1 << 1,
+	/* Flag this is a physical contiguous memory. */
+	RGA_MEM_PHYSICAL_CONTIGUOUS	= 1 << 2,
+	/* need force flush cache */
+	RGA_MEM_FORCE_FLUSH_CACHE	= 1 << 3,
+};
+
+struct rga_mm {
+	struct mutex lock;
+
+	/*
+	 * @memory_idr:
+	 *
+	 * Mapping of memory object handles to object pointers. Used by the GEM
+	 * subsystem. Protected by @memory_lock.
+	 */
+	struct idr memory_idr;
+
+	/* the count of buffer in the cached_list */
+	int buffer_count;
+};
+
+static inline bool rga_mm_is_invalid_dma_buffer(struct rga_dma_buffer *buffer)
+{
+	if (buffer == NULL)
+		return true;
+
+	return buffer->scheduler == NULL ? true : false;
+}
+
+struct rga_internal_buffer *rga_mm_lookup_handle(struct rga_mm *mm_session, uint32_t handle);
+int rga_mm_lookup_flag(struct rga_mm *mm_session, uint64_t handle);
+dma_addr_t rga_mm_lookup_iova(struct rga_internal_buffer *buffer);
+struct sg_table *rga_mm_lookup_sgt(struct rga_internal_buffer *buffer);
+
+void rga_mm_dump_buffer(struct rga_internal_buffer *dump_buffer);
+void rga_mm_dump_info(struct rga_mm *session);
+
+int rga_mm_map_job_info(struct rga_job *job);
+void rga_mm_unmap_job_info(struct rga_job *job);
+
+int rga_mm_import_buffer(struct rga_external_buffer *external_buffer,
+			 struct rga_session *session);
+int rga_mm_release_buffer(uint32_t handle);
+int rga_mm_session_release_buffer(struct rga_session *session);
+
+int rga_mm_init(struct rga_mm **session);
+int rga_mm_remove(struct rga_mm **session);
+
+#endif
diff --git a/drivers/video/rockchip/rga3/rga2_reg_info.c b/drivers/video/rockchip/rga3/rga2_reg_info.c
new file mode 100644
index 0000000000000..7a35594e79265
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga2_reg_info.c
@@ -0,0 +1,3273 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga2_reg_info.h"
+#include "rga_dma_buf.h"
+#include "rga_iommu.h"
+#include "rga_common.h"
+#include "rga_hw_config.h"
+#include "rga_debugger.h"
+
+enum rga2_scale_mode_reg {
+	RGA2_SCALE_BYPASS = 0x0,
+	RGA2_SCALE_DOWN = 0x1,
+	RGA2_SCALE_UP = 0x2,
+	RGA2_SCALE_FORCE_TILE = 0x3,
+};
+
+unsigned int rga2_rop_code[256] = {
+	0x00000007, 0x00000451, 0x00006051, 0x00800051,
+	0x00007041, 0x00800041, 0x00804830, 0x000004f0,//0
+	0x00800765, 0x000004b0, 0x00000065, 0x000004f4,
+	0x00000075, 0x000004e6, 0x00804850, 0x00800005,
+
+	0x00006850, 0x00800050, 0x00805028, 0x00000568,
+	0x00804031, 0x00000471, 0x002b6071, 0x018037aa,//1
+	0x008007aa, 0x00036071, 0x00002c6a, 0x00803631,
+	0x00002d68, 0x00802721, 0x008002d0, 0x000006d0,
+
+	0x0080066e, 0x00000528, 0x00000066, 0x0000056c,
+	0x018007aa, 0x0002e06a, 0x00003471, 0x00834031,//2
+	0x00800631, 0x0002b471, 0x00006071, 0x008037aa,
+	0x000036d0, 0x008002d4, 0x00002d28, 0x000006d4,
+
+	0x0000006e, 0x00000565, 0x00003451, 0x00800006,
+	0x000034f0, 0x00834830, 0x00800348, 0x00000748,//3
+	0x00002f48, 0x0080034c, 0x000034b0, 0x0000074c,
+	0x00000031, 0x00834850, 0x000034e6, 0x00800071,
+
+	0x008006f4, 0x00000431, 0x018007a1, 0x00b6e870,
+	0x00000074, 0x0000046e, 0x00002561, 0x00802f28,//4
+	0x00800728, 0x0002a561, 0x000026c2, 0x008002c6,
+	0x00007068, 0x018035aa, 0x00002c2a, 0x000006c6,
+
+	0x0000006c, 0x00000475, 0x000024e2, 0x008036b0,
+	0x00804051, 0x00800004, 0x00800251, 0x00000651,
+	0x00002e4a, 0x0080024e, 0x00000028, 0x00824842,
+	0x000024a2, 0x0000064e, 0x000024f4, 0x00800068,//5
+
+	0x008006b0, 0x000234f0, 0x00002741, 0x00800345,
+	0x00003651, 0x00800255, 0x00000030, 0x00834051,
+	0x00a34842, 0x000002b0, 0x00800271, 0x0002b651,
+	0x00800368, 0x0002a741, 0x0000364e, 0x00806830,//6
+
+	0x00006870, 0x008037a2, 0x00003431, 0x00000745,
+	0x00002521, 0x00000655, 0x0000346e, 0x00800062,
+	0x008002f0, 0x000236d0, 0x000026d4, 0x00807028,
+	0x000036c6, 0x00806031, 0x008005aa, 0x00000671,//7
+
+	0x00800671, 0x000005aa, 0x00006031, 0x008036c6,
+	0x00007028, 0x00802e55, 0x008236d0, 0x000002f0,
+	0x00000070, 0x0080346e, 0x00800655, 0x00802521,
+	0x00800745, 0x00803431, 0x000037a2, 0x00806870,//8
+
+	0x00006830, 0x0080364e, 0x00822f48, 0x00000361,
+	0x0082b651, 0x00000271, 0x00800231, 0x002b4051,
+	0x00034051, 0x00800030, 0x0080026e, 0x00803651,
+	0x0080036c, 0x00802741, 0x008234f0, 0x000006b0,//9
+
+	0x00000068, 0x00802c75, 0x0080064e, 0x008024a2,
+	0x0002c04a, 0x00800021, 0x00800275, 0x00802e51,
+	0x00800651, 0x00000251, 0x00800000, 0x00004051,
+	0x000036b0, 0x008024e2, 0x00800475, 0x00000045,//a
+
+	0x008006c6, 0x00802c2a, 0x000035aa, 0x00807068,
+	0x008002f4, 0x008026c2, 0x00822d68, 0x00000728,
+	0x00002f28, 0x00802561, 0x0080046e, 0x00000046,
+	0x00836870, 0x000007a2, 0x00800431, 0x00004071,//b
+
+	0x00000071, 0x008034e6, 0x00034850, 0x00800031,
+	0x0080074c, 0x008034b0, 0x00800365, 0x00802f48,
+	0x00800748, 0x00000341, 0x000026a2, 0x008034f0,
+	0x00800002, 0x00005048, 0x00800565, 0x00000055,//c
+
+	0x008006d4, 0x00802d28, 0x008002e6, 0x008036d0,
+	0x000037aa, 0x00806071, 0x0082b471, 0x00000631,
+	0x00002e2a, 0x00803471, 0x00826862, 0x010007aa,
+	0x0080056c, 0x00000054, 0x00800528, 0x00005068,//d
+
+	0x008006d0, 0x000002d0, 0x00002721, 0x00802d68,
+	0x00003631, 0x00802c6a, 0x00836071, 0x000007aa,
+	0x010037aa, 0x00a36870, 0x00800471, 0x00004031,
+	0x00800568, 0x00005028, 0x00000050, 0x00800545,//e
+
+	0x00800001, 0x00004850, 0x008004e6, 0x0000004e,
+	0x008004f4, 0x0000004c, 0x008004b0, 0x00004870,
+	0x008004f0, 0x00004830, 0x00000048, 0x0080044e,
+	0x00000051, 0x008004d4, 0x00800451, 0x00800007,//f
+};
+
+static void rga2_scale_down_bilinear_protect(u32 *param_fix, u32 *src_fix,
+					     u32 param, u32 offset, u32 src, u32 dst)
+{
+	int final_coor, final_diff, final_steps;
+
+	while (1) {
+		final_coor = offset + param * (dst - 1);
+		final_diff = (src - 1) * (1 << RGA2_BILINEAR_PREC) - final_coor;
+
+		/*
+		 *   The hardware requires that the last point of the dst map on
+		 * src must not exceed the range of src.
+		 */
+		if (final_diff <= 0)
+			param = param - 1;
+		else
+			break;
+	}
+
+	/*
+	 *   The hardware requires that the last point of dst mapping on
+	 * src be between the last two points of each row/column, so
+	 * actual width/height needs to be modified.
+	 */
+	final_steps = (final_coor & ((1 << RGA2_BILINEAR_PREC) - 1)) ?
+		((final_coor >> RGA2_BILINEAR_PREC) + 1) :
+		(final_coor >> RGA2_BILINEAR_PREC);
+
+	*param_fix = param;
+	*src_fix = final_steps + 1;
+}
+
+static void RGA2_reg_get_param(unsigned char *base, struct rga2_req *msg)
+{
+	u32 *bRGA_SRC_X_FACTOR;
+	u32 *bRGA_SRC_Y_FACTOR;
+	u32 *bRGA_SRC_ACT_INFO;
+	u32 sw, sh;
+	u32 dw, dh;
+	u32 param_x, param_y;
+	u32 scale_x_offset, scale_y_offset;
+	u32 src_fix, param_fix;
+
+	bRGA_SRC_X_FACTOR = (u32 *) (base + RGA2_SRC_X_FACTOR_OFFSET);
+	bRGA_SRC_Y_FACTOR = (u32 *) (base + RGA2_SRC_Y_FACTOR_OFFSET);
+	bRGA_SRC_ACT_INFO = (u32 *) (base + RGA2_SRC_ACT_INFO_OFFSET);
+
+	if (((msg->rotate_mode & 0x3) == 1) ||
+		((msg->rotate_mode & 0x3) == 3)) {
+		dw = msg->dst.act_h;
+		dh = msg->dst.act_w;
+	} else {
+		dw = msg->dst.act_w;
+		dh = msg->dst.act_h;
+	}
+
+	sw = msg->src.act_w;
+	sh = msg->src.act_h;
+
+	if (sw > dw) {
+		if (msg->interp.horiz == RGA_INTERP_LINEAR) {
+			/* default to half_pixel mode. */
+			param_x = (sw << RGA2_BILINEAR_PREC) / dw;
+			scale_x_offset = (1 << RGA2_BILINEAR_PREC) >> 1;
+
+			rga2_scale_down_bilinear_protect(&param_fix, &src_fix,
+							 param_x, scale_x_offset, sw, dw);
+			if (DEBUGGER_EN(MSG)) {
+				if (param_x != param_fix)
+					rga_log("scale: Bi-linear horiz factor %#x fix to %#x\n",
+						param_x, param_fix);
+				if (sw != src_fix)
+					rga_log("scale: Bi-linear src_width %d -> %d\n",
+						sw, src_fix);
+			}
+
+			*bRGA_SRC_X_FACTOR = ((param_fix & 0xffff) | ((scale_x_offset) << 16));
+			*bRGA_SRC_ACT_INFO =
+				((*bRGA_SRC_ACT_INFO & (~m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH)) |
+				s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_WIDTH((src_fix - 1)));
+		} else {
+			param_x = ((dw << 16) + (sw / 2)) / sw;
+
+			*bRGA_SRC_X_FACTOR |= ((param_x & 0xffff) << 0);
+		}
+	} else if (sw < dw) {
+#if SCALE_UP_LARGE
+		param_x = ((sw - 1) << 16) / (dw - 1);
+#else
+		param_x = ((sw) << 16) / (dw);
+#endif
+		*bRGA_SRC_X_FACTOR |= ((param_x & 0xffff) << 16);
+	} else {
+		*bRGA_SRC_X_FACTOR = 0;	//((1 << 14) << 16) | (1 << 14);
+	}
+
+	if (sh > dh) {
+		if (msg->interp.verti == RGA_INTERP_LINEAR) {
+			/* default to half_pixel mode. */
+			param_y = (sh << RGA2_BILINEAR_PREC) / dh;
+			scale_y_offset = (1 << RGA2_BILINEAR_PREC) >> 1;
+
+			rga2_scale_down_bilinear_protect(&param_fix, &src_fix,
+							 param_y, scale_y_offset, sh, dh);
+			if (DEBUGGER_EN(MSG)) {
+				if (param_y != param_fix)
+					rga_log("scale: Bi-linear verti factor %#x fix to %#x\n",
+						param_y, param_fix);
+				if (sh != src_fix)
+					rga_log("scale: Bi-linear src_height %d fix to %d\n",
+						sh, src_fix);
+			}
+
+			*bRGA_SRC_Y_FACTOR = ((param_fix & 0xffff) | ((scale_y_offset) << 16));
+			*bRGA_SRC_ACT_INFO =
+				((*bRGA_SRC_ACT_INFO & (~m_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT)) |
+				s_RGA2_SRC_ACT_INFO_SW_SRC_ACT_HEIGHT((src_fix - 1)));
+		} else {
+			param_y = ((dh << 16) + (sh / 2)) / sh;
+
+			*bRGA_SRC_Y_FACTOR |= ((param_y & 0xffff) << 0);
+		}
+	} else if (sh < dh) {
+#if SCALE_UP_LARGE
+		param_y = ((sh - 1) << 16) / (dh - 1);
+#else
+		param_y = ((sh) << 16) / (dh);
+#endif
+		*bRGA_SRC_Y_FACTOR |= ((param_y & 0xffff) << 16);
+	} else {
+		*bRGA_SRC_Y_FACTOR = 0;	//((1 << 14) << 16) | (1 << 14);
+	}
+}
+
+static void RGA2_set_mode_ctrl(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_MODE_CTL;
+	u32 reg = 0;
+	u32 render_mode = msg->render_mode;
+
+	bRGA_MODE_CTL = (u32 *) (base + RGA2_MODE_CTRL_OFFSET);
+
+	if (msg->render_mode == UPDATE_PALETTE_TABLE_MODE)
+		render_mode = 0x3;
+
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_RENDER_MODE)) |
+		 (s_RGA2_MODE_CTRL_SW_RENDER_MODE(render_mode)));
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_BITBLT_MODE)) |
+		 (s_RGA2_MODE_CTRL_SW_BITBLT_MODE(msg->bitblt_mode)));
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_CF_ROP4_PAT)) |
+		 (s_RGA2_MODE_CTRL_SW_CF_ROP4_PAT(msg->color_fill_mode)));
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET)) |
+		 (s_RGA2_MODE_CTRL_SW_ALPHA_ZERO_KET(msg->alpha_zero_key)));
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_GRADIENT_SAT)) |
+		 (s_RGA2_MODE_CTRL_SW_GRADIENT_SAT(msg->alpha_rop_flag >> 7)));
+	reg =
+		((reg & (~m_RGA2_MODE_CTRL_SW_INTR_CF_E)) |
+		 (s_RGA2_MODE_CTRL_SW_INTR_CF_E(msg->CMD_fin_int_enable)));
+
+	reg = ((reg & (~m_RGA2_MODE_CTRL_SW_MOSAIC_EN)) |
+	       (s_RGA2_MODE_CTRL_SW_MOSAIC_EN(msg->mosaic_info.enable)));
+
+	reg = ((reg & (~m_RGA2_MODE_CTRL_SW_YIN_YOUT_EN)) |
+	       (s_RGA2_MODE_CTRL_SW_YIN_YOUT_EN(msg->yin_yout_en)));
+
+	if (msg->src.rd_mode == RGA_TILE4x4_MODE)
+		reg = ((reg & (~m_RGA2_MODE_CTRL_SW_TILE4x4_IN_EN)) |
+		       (s_RGA2_MODE_CTRL_SW_TILE4x4_IN_EN(1)));
+
+	if (msg->dst.rd_mode == RGA_TILE4x4_MODE)
+		reg = ((reg & (~m_RGA2_MODE_CTRL_SW_TILE4x4_OUT_EN)) |
+		       (s_RGA2_MODE_CTRL_SW_TILE4x4_OUT_EN(1)));
+
+	if (msg->src.rd_mode == RGA_RKFBC_MODE || msg->src.rd_mode == RGA_AFBC32x8_MODE)
+		reg = ((reg & (~m_RGA2_MODE_CTRL_SW_FBC_IN_EN)) |
+		       (s_RGA2_MODE_CTRL_SW_FBC_IN_EN(1)));
+
+	reg = ((reg & (~m_RGA2_MODE_CTRL_SW_OSD_E)) |
+	       (s_RGA2_MODE_CTRL_SW_OSD_E(msg->osd_info.enable)));
+
+	if (msg->gauss_config.size > 0)
+		reg = ((reg & (~m_RGA2_MODE_CTRL_SW_SRC_GAUSS_EN)) |
+			(s_RGA2_MODE_CTRL_SW_SRC_GAUSS_EN(1)));
+
+	*bRGA_MODE_CTL = reg;
+}
+
+static void RGA2_set_reg_src_info(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_SRC_INFO;
+	u32 *bRGA_SRC_BASE0, *bRGA_SRC_BASE1, *bRGA_SRC_BASE2;
+	u32 *bRGA_SRC_VIR_INFO;
+	u32 *bRGA_SRC_ACT_INFO;
+	u32 *bRGA_MASK_ADDR;
+	u32 *bRGA_SRC_TR_COLOR0, *bRGA_SRC_TR_COLOR1;
+	u32 *bRGA_FBCIN_HEAD_BASE;
+	u32 *bRGA_FBCIN_PAYL_BASE;
+	u32 *bRGA_FBCIN_OFF;
+	u32 *bRGA_FBCIN_HEAD_VIR_INFO;
+
+	u8 disable_uv_channel_en = 0;
+
+	u32 reg = 0;
+	u8 src0_format = 0;
+
+	u8 src0_rb_swp = 0;
+	u8 src0_alpha_swp = 0;
+
+	u8 src0_cbcr_swp = 0;
+	u8 pixel_width = 1;
+	u8 plane_width = 0;
+	u8 pixel_depth = 8;
+	u32 stride = 0;
+	u32 uv_stride = 0;
+	u32 mask_stride = 0;
+	u32 byte_stride = 0;
+	u32 ydiv = 1, xdiv = 2;
+	u8 yuv10 = 0;
+
+	u32 sw, sh;
+	u32 dw, dh;
+	u8 rotate_mode;
+	u8 vsp_scale_mode = 0;
+	u8 vsd_scale_mode = 0;
+	u8 hsp_scale_mode = 0;
+	u8 hsd_scale_mode = 0;
+	u8 scale_w_flag, scale_h_flag;
+	u32 yrgb_offset = 0, uv_offset = 0, v_offset = 0;
+	u32 tile_x_offset = 0;
+	u32 tile_y_offset = 0;
+	u32 tile_block_size;
+
+	u32 fbc_fmt = 0, fbc_mode = 0;
+	u32 head_base_addr, payload_base_addr;
+
+	bRGA_SRC_INFO = (u32 *) (base + RGA2_SRC_INFO_OFFSET);
+
+	bRGA_SRC_BASE0 = (u32 *) (base + RGA2_SRC_BASE0_OFFSET);
+	bRGA_SRC_BASE1 = (u32 *) (base + RGA2_SRC_BASE1_OFFSET);
+	bRGA_SRC_BASE2 = (u32 *) (base + RGA2_SRC_BASE2_OFFSET);
+
+	bRGA_SRC_VIR_INFO = (u32 *) (base + RGA2_SRC_VIR_INFO_OFFSET);
+	bRGA_SRC_ACT_INFO = (u32 *) (base + RGA2_SRC_ACT_INFO_OFFSET);
+
+	bRGA_MASK_ADDR = (u32 *) (base + RGA2_MASK_BASE_OFFSET);
+
+	bRGA_SRC_TR_COLOR0 = (u32 *) (base + RGA2_SRC_TR_COLOR0_OFFSET);
+	bRGA_SRC_TR_COLOR1 = (u32 *) (base + RGA2_SRC_TR_COLOR1_OFFSET);
+
+	bRGA_FBCIN_HEAD_BASE = (u32 *) (base + RGA2_FBCIN_HEAD_BASE_OFFSET);
+	bRGA_FBCIN_PAYL_BASE = (u32 *) (base + RGA2_FBCIN_PAYL_BASE_OFFSET);
+	bRGA_FBCIN_OFF = (u32 *) (base + RGA2_FBCIN_OFF_OFFSET);
+	bRGA_FBCIN_HEAD_VIR_INFO = (u32 *) (base + RGA2_FBCIN_HEAD_VIR_INFO_OFFSET);
+
+	{
+		rotate_mode = msg->rotate_mode & 0x3;
+
+		sw = msg->src.act_w;
+		sh = msg->src.act_h;
+
+		if ((rotate_mode == 1) | (rotate_mode == 3)) {
+			dw = msg->dst.act_h;
+			dh = msg->dst.act_w;
+		} else {
+			dw = msg->dst.act_w;
+			dh = msg->dst.act_h;
+		}
+
+		if (sw > dw)
+			scale_w_flag = RGA2_SCALE_DOWN;
+		else if (sw < dw)
+			scale_w_flag = RGA2_SCALE_UP;
+		else {
+			scale_w_flag = RGA2_SCALE_BYPASS;
+			if (msg->rotate_mode >> 6)
+				scale_w_flag = 3;
+		}
+
+		if (sh > dh)
+			scale_h_flag = RGA2_SCALE_DOWN;
+		else if (sh < dh)
+			scale_h_flag = RGA2_SCALE_UP;
+		else {
+			scale_h_flag = RGA2_SCALE_BYPASS;
+			if (msg->rotate_mode >> 6)
+				scale_h_flag = 3;
+		}
+
+		/* uvvds need to force tile mode. */
+		if (msg->uvvds_mode && scale_w_flag == 0)
+			scale_w_flag = 3;
+	}
+
+	if (scale_h_flag == RGA2_SCALE_UP) {
+		switch (msg->interp.verti) {
+		case RGA_INTERP_BICUBIC:
+			/*
+			 * VSP scale mode select, HSD > VSD > VSP > HSP
+			 * After HSD, VSP needs to check dst_width.
+			 */
+			if (((scale_w_flag == RGA2_SCALE_DOWN) && (dw < RGA2_VSP_BICUBIC_LIMIT)) ||
+			    (sw < RGA2_VSP_BICUBIC_LIMIT)) {
+				vsp_scale_mode = 0x0;
+			} else {
+				/* force select bi-linear */
+				rga_err("Horizontal scaling will be forcibly switched to bilinear.\n");
+				vsp_scale_mode = 0x1;
+			}
+			break;
+		case RGA_INTERP_LINEAR:
+			vsp_scale_mode = 1;
+			break;
+		}
+
+	} else if (scale_h_flag == RGA2_SCALE_DOWN) {
+		switch (msg->interp.verti) {
+		case RGA_INTERP_AVERAGE:
+			vsd_scale_mode = 0;
+			break;
+		case RGA_INTERP_LINEAR:
+				vsd_scale_mode = 1;
+
+			break;
+		}
+	}
+
+	if (scale_w_flag == RGA2_SCALE_UP) {
+		switch (msg->interp.horiz) {
+		case RGA_INTERP_BICUBIC:
+			hsp_scale_mode = 0;
+			break;
+		case RGA_INTERP_LINEAR:
+			hsp_scale_mode = 1;
+			break;
+		}
+	} else if (scale_w_flag == RGA2_SCALE_DOWN) {
+		switch (msg->interp.horiz) {
+		case RGA_INTERP_AVERAGE:
+			hsd_scale_mode = 0;
+			break;
+		case RGA_INTERP_LINEAR:
+			hsd_scale_mode = 1;
+			break;
+		}
+	}
+
+	switch (msg->src.format) {
+	case RGA_FORMAT_RGBA_8888:
+		src0_format = 0x0;
+		pixel_width = 4;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		src0_format = 0x0;
+		src0_rb_swp = 0x1;
+		pixel_width = 4;
+		break;
+	case RGA_FORMAT_RGBX_8888:
+		src0_format = 0x1;
+		pixel_width = 4;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_BGRX_8888:
+		src0_format = 0x1;
+		src0_rb_swp = 0x1;
+		pixel_width = 4;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_RGB_888:
+		src0_format = 0x2;
+		pixel_width = 3;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_BGR_888:
+		src0_format = 0x2;
+		src0_rb_swp = 1;
+		pixel_width = 3;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_RGB_565:
+		src0_format = 0x4;
+		pixel_width = 2;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_BGR_565:
+		src0_format = 0x4;
+		pixel_width = 2;
+		msg->src_trans_mode &= 0x07;
+		src0_rb_swp = 0x1;
+		break;
+
+		/* ARGB */
+		/*
+		 * In colorkey mode, xrgb/xbgr does not
+		 * need to enable the alpha channel
+		 */
+	case RGA_FORMAT_ARGB_8888:
+		src0_format = 0x0;
+		pixel_width = 4;
+		src0_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_ABGR_8888:
+		src0_format = 0x0;
+		pixel_width = 4;
+		src0_alpha_swp = 1;
+		src0_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_XRGB_8888:
+		src0_format = 0x1;
+		pixel_width = 4;
+		src0_alpha_swp = 1;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_XBGR_8888:
+		src0_format = 0x1;
+		pixel_width = 4;
+		src0_alpha_swp = 1;
+		src0_rb_swp = 0x1;
+		msg->src_trans_mode &= 0x07;
+		break;
+	case RGA_FORMAT_ARGB_5551:
+		src0_format = 0x5;
+		pixel_width = 2;
+		break;
+	case RGA_FORMAT_ABGR_5551:
+		src0_format = 0x5;
+		pixel_width = 2;
+		src0_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_ARGB_4444:
+		src0_format = 0x6;
+		pixel_width = 2;
+		break;
+	case RGA_FORMAT_ABGR_4444:
+		src0_format = 0x6;
+		pixel_width = 2;
+		src0_rb_swp = 0x1;
+		break;
+
+	case RGA_FORMAT_YVYU_422:
+		src0_format = 0x7;
+		pixel_width = 2;
+		src0_cbcr_swp = 1;
+		src0_rb_swp = 0x1;
+		break;		//rbswap=ycswap
+	case RGA_FORMAT_VYUY_422:
+		src0_format = 0x7;
+		pixel_width = 2;
+		src0_cbcr_swp = 1;
+		src0_rb_swp = 0x0;
+		break;
+	case RGA_FORMAT_YUYV_422:
+		src0_format = 0x7;
+		pixel_width = 2;
+		src0_cbcr_swp = 0;
+		src0_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_UYVY_422:
+		src0_format = 0x7;
+		pixel_width = 2;
+		src0_cbcr_swp = 0;
+		src0_rb_swp = 0x0;
+		break;
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		src0_format = 0x8;
+		plane_width = 2;
+		xdiv = 2;
+		ydiv = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_P:
+		src0_format = 0x9;
+		plane_width = 1;
+		xdiv = 2;
+		ydiv = 1;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+		src0_format = 0xa;
+		plane_width = 2;
+		xdiv = 2;
+		ydiv = 2;
+		break;
+	case RGA_FORMAT_YCbCr_420_P:
+		src0_format = 0xb;
+		plane_width = 1;
+		xdiv = 2;
+		ydiv = 2;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP:
+		src0_format = 0x8;
+		plane_width = 2;
+		xdiv = 2;
+		ydiv = 1;
+		src0_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_P:
+		src0_format = 0x9;
+		plane_width = 1;
+		xdiv = 2;
+		ydiv = 1;
+		src0_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP:
+		src0_format = 0xa;
+		plane_width = 2;
+		xdiv = 2;
+		ydiv = 2;
+		src0_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_P:
+		src0_format = 0xb;
+		plane_width = 1;
+		xdiv = 2;
+		ydiv = 2;
+		src0_cbcr_swp = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+		src0_format = 0xa;
+		plane_width = 2;
+		pixel_depth = 10;
+		xdiv = 2;
+		ydiv = 2;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		src0_format = 0xa;
+		plane_width = 2;
+		pixel_depth = 10;
+		xdiv = 2;
+		ydiv = 2;
+		src0_cbcr_swp = 1;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+		src0_format = 0x8;
+		plane_width = 2;
+		pixel_depth = 10;
+		xdiv = 2;
+		ydiv = 1;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		src0_format = 0x8;
+		plane_width = 2;
+		pixel_depth = 10;
+		xdiv = 2;
+		ydiv = 1;
+		src0_cbcr_swp = 1;
+		yuv10 = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_400:
+		src0_format = 0x8;
+		/* When Yin_Yout is enabled, no need to go through the software. */
+		disable_uv_channel_en = msg->yin_yout_en ? false : true;
+		xdiv = 1;
+		ydiv = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_444_SP:
+		src0_format = 0x3;
+		plane_width = 2;
+		xdiv = 1;
+		ydiv = 1;
+		break;
+	case RGA_FORMAT_YCrCb_444_SP:
+		src0_format = 0x3;
+		plane_width = 2;
+		xdiv = 1;
+		ydiv = 1;
+		src0_cbcr_swp = 1;
+		break;
+	};
+
+	switch (msg->src.rd_mode) {
+	case RGA_RASTER_MODE:
+		if (msg->src.format == RGA_FORMAT_YCbCr_420_SP_10B ||
+		    msg->src.format == RGA_FORMAT_YCrCb_420_SP_10B ||
+		    msg->src.format == RGA_FORMAT_YCbCr_422_SP_10B ||
+		    msg->src.format == RGA_FORMAT_YCrCb_422_SP_10B)
+			/*
+			 * Legacy: implicit semantics exist here, 10bit format
+			 * width_stride equals byte_stride.
+			 */
+			byte_stride = msg->src.vir_w;
+		else
+			byte_stride = msg->src.vir_w * pixel_width * pixel_depth / 8;
+
+		stride = ALIGN(byte_stride, 4);
+		uv_stride = ALIGN(msg->src.vir_w / xdiv * plane_width, 4);
+
+		yrgb_offset = msg->src.y_offset * stride +
+			msg->src.x_offset * pixel_width * pixel_depth / 8;
+		uv_offset = (msg->src.y_offset / ydiv) * uv_stride +
+			    (msg->src.x_offset / xdiv * plane_width * pixel_depth / 8);
+		v_offset = uv_offset;
+
+		break;
+
+	case RGA_TILE4x4_MODE:
+		switch (msg->src.format) {
+		case RGA_FORMAT_YCbCr_400:
+			tile_block_size = 16;
+			break;
+		case RGA_FORMAT_YCbCr_420_SP:
+		case RGA_FORMAT_YCrCb_420_SP:
+			tile_block_size = 24;
+			break;
+		case RGA_FORMAT_YCbCr_422_SP:
+		case RGA_FORMAT_YCrCb_422_SP:
+			tile_block_size = 32;
+			break;
+		case RGA_FORMAT_YCbCr_444_SP:
+		case RGA_FORMAT_YCrCb_444_SP:
+			tile_block_size = 48;
+			break;
+		case RGA_FORMAT_YCbCr_420_SP_10B:
+		case RGA_FORMAT_YCrCb_420_SP_10B:
+			tile_block_size = 30;
+			break;
+		case RGA_FORMAT_YCbCr_422_SP_10B:
+		case RGA_FORMAT_YCrCb_422_SP_10B:
+			tile_block_size = 40;
+			break;
+		default:
+			tile_block_size = 16;
+			break;
+		}
+
+		stride = ALIGN((u32)((msg->src.vir_w * pixel_width) * (tile_block_size / 4)), 4);
+
+		yrgb_offset = (u32)((msg->src.y_offset / 4) * stride +
+			      (msg->src.x_offset / 4) * pixel_width * tile_block_size);
+		uv_offset = 0;
+		v_offset = 0;
+
+		tile_x_offset = (msg->src.x_offset % 4) & 0x3;
+		tile_y_offset = (msg->src.y_offset % 4) & 0x3;
+
+		break;
+
+	case RGA_RKFBC_MODE:
+		switch (msg->src.format) {
+		case RGA_FORMAT_YCbCr_420_SP:
+		case RGA_FORMAT_YCrCb_420_SP:
+		case RGA_FORMAT_YCbCr_420_SP_10B:
+		case RGA_FORMAT_YCrCb_420_SP_10B:
+			fbc_fmt = 0x0;
+			break;
+		case RGA_FORMAT_YCbCr_422_SP:
+		case RGA_FORMAT_YCrCb_422_SP:
+		case RGA_FORMAT_YCbCr_422_SP_10B:
+		case RGA_FORMAT_YCrCb_422_SP_10B:
+			fbc_fmt = 0x1;
+			break;
+		case RGA_FORMAT_YCbCr_444_SP:
+		case RGA_FORMAT_YCrCb_444_SP:
+			fbc_fmt = 0x2;
+			break;
+		}
+
+		fbc_mode = 0x0;
+		head_base_addr = msg->src.yrgb_addr;
+		payload_base_addr = head_base_addr;
+		stride = ALIGN(msg->src.vir_w, 64) / 64 * 4;
+
+		break;
+
+	case RGA_AFBC32x8_MODE:
+		switch (msg->src.format) {
+		case RGA_FORMAT_RGBA_8888:
+		case RGA_FORMAT_BGRA_8888:
+		case RGA_FORMAT_ARGB_8888:
+		case RGA_FORMAT_ABGR_8888:
+		case RGA_FORMAT_RGBX_8888:
+		case RGA_FORMAT_BGRX_8888:
+		case RGA_FORMAT_XRGB_8888:
+		case RGA_FORMAT_XBGR_8888:
+			fbc_fmt = 0x0;
+			break;
+		case RGA_FORMAT_RGB_888:
+		case RGA_FORMAT_BGR_888:
+			fbc_fmt = 0x1;
+			break;
+		}
+
+		fbc_mode = 0x1;
+		head_base_addr = msg->src.yrgb_addr;
+		payload_base_addr = head_base_addr;
+		stride = ALIGN(msg->src.vir_w, 32) / 32 * 4;
+
+		break;
+	}
+
+	if (msg->src.rd_mode == RGA_RKFBC_MODE || msg->src.rd_mode == RGA_AFBC32x8_MODE) {
+		reg = ((reg & (~m_RGA2_SRC_INFO_SW_FBCIN_MODE)) |
+		       (s_RGA2_SRC_INFO_SW_FBCIN_MODE(fbc_mode)));
+
+		reg = ((reg & (~m_RGA2_SRC_INFO_SW_FBCIN_FMT)) |
+		       (s_RGA2_SRC_INFO_SW_FBCIN_FMT(fbc_fmt)));
+	} else {
+		reg = ((reg & (~m_RGA2_SRC_INFO_SW_SRC_FMT)) |
+		       (s_RGA2_SRC_INFO_SW_SRC_FMT(src0_format)));
+	}
+
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_RB_SWAP(src0_rb_swp)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_ALPHA_SWAP(src0_alpha_swp)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_UV_SWAP(src0_cbcr_swp)));
+
+	if (msg->src1.format == RGA_FORMAT_RGBA_2BPP)
+		reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_CP_ENDIAN)) |
+		       (s_RGA2_SRC_INFO_SW_SW_CP_ENDAIN(msg->osd_info.bpp2_info.endian_swap & 1)));
+
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_CSC_MODE(msg->yuv2rgb_mode)));
+
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_ROT_MODE(msg->rotate_mode & 0x3)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_MIR_MODE
+		 ((msg->rotate_mode >> 4) & 0x3)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_HSCL_MODE((scale_w_flag))));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_VSCL_MODE((scale_h_flag))));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_SCL_FILTER((
+			msg->scale_bicu_mode))));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_MODE(msg->src_trans_mode)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_TRANS_E(msg->src_trans_mode >> 1)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E)) |
+		 (s_RGA2_SRC_INFO_SW_SW_SRC_DITHER_UP_E
+		 ((msg->alpha_rop_flag >> 4) & 0x1)));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL)) |
+		 (s_RGA2_SRC_INFO_SW_SW_VSP_MODE_SEL((vsp_scale_mode))));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_YUV10_E)) |
+		 (s_RGA2_SRC_INFO_SW_SW_YUV10_E((yuv10))));
+
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E)) |
+		 (s_RGA2_SRC_INFO_SW_SW_YUV10_ROUND_E((yuv10))));
+	reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_VSD_MODE_SEL)) |
+	       (s_RGA2_SRC_INFO_SW_SW_VSD_MODE_SEL((vsd_scale_mode))));
+	reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_HSP_MODE_SEL)) |
+	       (s_RGA2_SRC_INFO_SW_SW_HSP_MODE_SEL((hsp_scale_mode))));
+	reg = ((reg & (~m_RGA2_SRC_INFO_SW_SW_HSD_MODE_SEL)) |
+	       (s_RGA2_SRC_INFO_SW_SW_HSD_MODE_SEL((hsd_scale_mode))));
+
+	*bRGA_SRC_INFO = reg;
+	if (msg->src.rd_mode == RGA_RKFBC_MODE || msg->src.rd_mode == RGA_AFBC32x8_MODE) {
+		*bRGA_FBCIN_HEAD_BASE = head_base_addr;
+		*bRGA_FBCIN_PAYL_BASE = payload_base_addr;
+		*bRGA_FBCIN_HEAD_VIR_INFO = stride;
+		*bRGA_FBCIN_OFF = msg->src.x_offset | (msg->src.y_offset << 16);
+		*bRGA_SRC_ACT_INFO = (msg->src.act_w - 1) | ((msg->src.act_h - 1) << 16);
+	} else {
+		*bRGA_SRC_BASE0 = (u32)(msg->src.yrgb_addr + yrgb_offset);
+		if (disable_uv_channel_en == 1) {
+			/*
+			 * When Y400 as the input format, because the current RGA does
+			 * not support closing the access of the UV channel, the address
+			 * of the UV channel access is equal to the address of
+			 * the Y channel access to ensure that the UV channel can access,
+			 * preventing the RGA hardware from reporting errors.
+			 */
+			*bRGA_SRC_BASE1 = *bRGA_SRC_BASE0;
+			*bRGA_SRC_BASE2 = *bRGA_SRC_BASE0;
+		} else {
+			*bRGA_SRC_BASE1 = (u32)(msg->src.uv_addr + uv_offset);
+			*bRGA_SRC_BASE2 = (u32)(msg->src.v_addr + v_offset);
+		}
+
+		//mask_stride = ((msg->src0_act.width + 31) & ~31) >> 5;
+		mask_stride = msg->rop_mask_stride;
+		*bRGA_SRC_VIR_INFO = (stride >> 2) | (mask_stride << 16);
+
+		*bRGA_SRC_ACT_INFO =
+			(msg->src.act_w - 1) | ((msg->src.act_h - 1) << 16) |
+			tile_x_offset << 14 | tile_y_offset << 30;
+
+		*bRGA_MASK_ADDR = (u32) msg->rop_mask_addr;
+	}
+
+	RGA2_reg_get_param(base, msg);
+
+	*bRGA_SRC_TR_COLOR0 = msg->color_key_min;
+	*bRGA_SRC_TR_COLOR1 = msg->color_key_max;
+}
+
+static void RGA2_set_reg_dst_info(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_SRC_FG_COLOR;
+	u32 *bRGA_SRC_BG_COLOR;
+	u32 *bRGA_DST_INFO;
+	u32 *bRGA_DST_BASE0, *bRGA_DST_BASE1, *bRGA_DST_BASE2,
+		*bRGA_SRC_BASE3;
+	u32 *bRGA_TILE4x4_OUT_BASE;
+	u32 *bRGA_DST_VIR_INFO;
+	u32 *bRGA_DST_ACT_INFO;
+
+	u32 *RGA_DST_Y4MAP_LUT0;	//Y4 LUT0
+	u32 *RGA_DST_Y4MAP_LUT1;	//Y4 LUT1
+	u32 *RGA_DST_NN_QUANTIZE_SCALE;
+	u32 *RGA_DST_NN_QUANTIZE_OFFSET;
+
+	u32 line_width_real;
+
+	u8 ydither_en = 0;
+
+	u8 src1_format = 0;
+	u8 src1_rb_swp = 0;
+	u8 src1_alpha_swp = 0;
+
+	u8 dst_format = 0;
+	u8 dst_rb_swp = 0;
+	u8 dst_cbcr_swp = 0;
+	u8 dst_alpha_swp = 0;
+
+	u8 dst_fmt_yuv400_en = 0;
+	u8 dst_fmt_y4_en = 0;
+	u8 dst_fmt_y4_lut_en = 0;
+	u8 dst_nn_quantize_en = 0;
+
+	u32 reg = 0;
+	u8 spw, dpw;
+	u8 plane_width = 0;
+	u8 bbp_shift = 0;
+	u32 s_stride = 0, d_stride = 0;
+	u32 x_mirr, y_mirr, rot_90_flag;
+	u32 yrgb_addr, u_addr, v_addr, s_yrgb_addr;
+	u32 d_uv_stride, x_div, y_div;
+	u32 y_lt_addr = 0, y_ld_addr = 0, y_rt_addr = 0, y_rd_addr = 0;
+	u32 u_lt_addr = 0, u_ld_addr = 0, u_rt_addr = 0, u_rd_addr = 0;
+	u32 v_lt_addr = 0, v_ld_addr = 0, v_rt_addr = 0, v_rd_addr = 0;
+	u32 yrgb_offset = 0, uv_offset = 0, v_offset = 0;
+	u32 tile_x_offset = 0;
+	u32 tile_y_offset = 0;
+	u32 tile_block_size;
+
+	dpw = 1;
+	x_div = y_div = 1;
+
+	dst_nn_quantize_en = (msg->alpha_rop_flag >> 8) & 0x1;
+
+	bRGA_SRC_FG_COLOR = (u32 *) (base + RGA2_SRC_FG_COLOR_OFFSET);
+	bRGA_SRC_BG_COLOR = (u32 *) (base + RGA2_SRC_BG_COLOR_OFFSET);
+
+	bRGA_DST_INFO = (u32 *) (base + RGA2_DST_INFO_OFFSET);
+	bRGA_DST_BASE0 = (u32 *) (base + RGA2_DST_BASE0_OFFSET);
+	bRGA_DST_BASE1 = (u32 *) (base + RGA2_DST_BASE1_OFFSET);
+	bRGA_DST_BASE2 = (u32 *) (base + RGA2_DST_BASE2_OFFSET);
+	bRGA_TILE4x4_OUT_BASE = (u32 *) (base + RGA2_TILE4x4_OUT_BASE_OFFSET);
+
+	bRGA_SRC_BASE3 = (u32 *) (base + RGA2_SRC_BASE3_OFFSET);
+
+	bRGA_DST_VIR_INFO = (u32 *) (base + RGA2_DST_VIR_INFO_OFFSET);
+	bRGA_DST_ACT_INFO = (u32 *) (base + RGA2_DST_ACT_INFO_OFFSET);
+
+	RGA_DST_Y4MAP_LUT0 = (u32 *) (base + RGA2_DST_Y4MAP_LUT0_OFFSET);
+	RGA_DST_Y4MAP_LUT1 = (u32 *) (base + RGA2_DST_Y4MAP_LUT1_OFFSET);
+	RGA_DST_NN_QUANTIZE_SCALE =
+		(u32 *) (base + RGA2_DST_QUANTIZE_SCALE_OFFSET);
+	RGA_DST_NN_QUANTIZE_OFFSET =
+		(u32 *) (base + RGA2_DST_QUANTIZE_OFFSET_OFFSET);
+
+	switch (msg->src1.format) {
+	case RGA_FORMAT_RGBA_8888:
+		src1_format = 0x0;
+		spw = 4;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		src1_format = 0x0;
+		src1_rb_swp = 0x1;
+		spw = 4;
+		break;
+	case RGA_FORMAT_RGBX_8888:
+		src1_format = 0x1;
+		spw = 4;
+		break;
+	case RGA_FORMAT_BGRX_8888:
+		src1_format = 0x1;
+		src1_rb_swp = 0x1;
+		spw = 4;
+		break;
+	case RGA_FORMAT_RGB_888:
+		src1_format = 0x2;
+		spw = 3;
+		break;
+	case RGA_FORMAT_BGR_888:
+		src1_format = 0x2;
+		src1_rb_swp = 1;
+		spw = 3;
+		break;
+	case RGA_FORMAT_RGB_565:
+		src1_format = 0x4;
+		spw = 2;
+		break;
+	case RGA_FORMAT_BGR_565:
+		src1_format = 0x4;
+		spw = 2;
+		src1_rb_swp = 0x1;
+		break;
+
+		/* ARGB */
+	case RGA_FORMAT_ARGB_8888:
+		src1_format = 0x0;
+		spw = 4;
+		src1_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_ABGR_8888:
+		src1_format = 0x0;
+		spw = 4;
+		src1_alpha_swp = 1;
+		src1_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_XRGB_8888:
+		src1_format = 0x1;
+		spw = 4;
+		src1_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_XBGR_8888:
+		src1_format = 0x1;
+		spw = 4;
+		src1_alpha_swp = 1;
+		src1_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_ARGB_5551:
+		src1_format = 0x5;
+		spw = 2;
+		break;
+	case RGA_FORMAT_ABGR_5551:
+		src1_format = 0x5;
+		spw = 2;
+		src1_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_ARGB_4444:
+		src1_format = 0x6;
+		spw = 2;
+		break;
+	case RGA_FORMAT_ABGR_4444:
+		src1_format = 0x6;
+		spw = 2;
+		src1_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_RGBA_2BPP:
+		src1_format = 0x0;
+		spw = 1;
+		/* 2BPP = 8 >> 2 = 2bit */
+		bbp_shift = 2;
+		src1_alpha_swp = msg->osd_info.bpp2_info.ac_swap;
+		break;
+	case RGA_FORMAT_A8:
+		src1_format = 0x3;
+		spw = 1;
+		break;
+	default:
+		spw = 4;
+		break;
+	};
+
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_SRC1_FMT)) |
+		 (s_RGA2_DST_INFO_SW_SRC1_FMT(src1_format)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_SRC1_RB_SWP)) |
+		 (s_RGA2_DST_INFO_SW_SRC1_RB_SWP(src1_rb_swp)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP)) |
+		 (s_RGA2_DST_INFO_SW_SRC1_ALPHA_SWP(src1_alpha_swp)));
+
+	if (msg->rgba5551_alpha.flags & 0x1) {
+		reg = ((reg & (~m_RGA2_DST_INFO_SW_SRC1_A1555_ACONFIG_EN)) |
+		       (s_RGA2_DST_INFO_SW_SRC1_A1555_ACONFIG_EN(1)));
+
+		*bRGA_SRC_FG_COLOR = (msg->rgba5551_alpha.alpha1 & 0xff) << 24;
+		*bRGA_SRC_BG_COLOR = (msg->rgba5551_alpha.alpha0 & 0xff) << 24;
+	}
+
+	switch (msg->dst.format) {
+	case RGA_FORMAT_RGBA_8888:
+		dst_format = 0x0;
+		dpw = 4;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		dst_format = 0x0;
+		dst_rb_swp = 0x1;
+		dpw = 4;
+		break;
+	case RGA_FORMAT_RGBX_8888:
+		dst_format = 0x1;
+		dpw = 4;
+		break;
+	case RGA_FORMAT_BGRX_8888:
+		dst_format = 0x1;
+		dst_rb_swp = 0x1;
+		dpw = 4;
+		break;
+	case RGA_FORMAT_RGB_888:
+		dst_format = 0x2;
+		dpw = 3;
+		break;
+	case RGA_FORMAT_BGR_888:
+		dst_format = 0x2;
+		dst_rb_swp = 1;
+		dpw = 3;
+		break;
+	case RGA_FORMAT_RGB_565:
+		dst_format = 0x4;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_RGBA_5551:
+		dst_format = 0x5;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_RGBA_4444:
+		dst_format = 0x6;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_BGR_565:
+		dst_format = 0x4;
+		dpw = 2;
+		dst_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_BGRA_5551:
+		dst_format = 0x5;
+		dpw = 2;
+		dst_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_BGRA_4444:
+		dst_format = 0x6;
+		dpw = 2;
+		dst_rb_swp = 0x1;
+		break;
+
+		/* ARGB */
+	case RGA_FORMAT_ARGB_8888:
+		dst_format = 0x0;
+		dpw = 4;
+		dst_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_ABGR_8888:
+		dst_format = 0x0;
+		dpw = 4;
+		dst_alpha_swp = 1;
+		dst_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_XRGB_8888:
+		dst_format = 0x1;
+		dpw = 4;
+		dst_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_XBGR_8888:
+		dst_format = 0x1;
+		dpw = 4;
+		dst_alpha_swp = 1;
+		dst_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_ARGB_5551:
+		dst_format = 0x5;
+		dpw = 2;
+		dst_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_ABGR_5551:
+		dst_format = 0x5;
+		dpw = 2;
+		dst_alpha_swp = 1;
+		dst_rb_swp = 0x1;
+		break;
+	case RGA_FORMAT_ARGB_4444:
+		dst_format = 0x6;
+		dpw = 2;
+		dst_alpha_swp = 1;
+		break;
+	case RGA_FORMAT_ABGR_4444:
+		dst_format = 0x6;
+		dpw = 2;
+		dst_alpha_swp = 1;
+		dst_rb_swp = 0x1;
+		break;
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		dst_format = 0x8;
+		plane_width = 2;
+		x_div = 2;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_P:
+		dst_format = 0x9;
+		plane_width = 1;
+		x_div = 2;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+		dst_format = 0xa;
+		plane_width = 2;
+		x_div = 2;
+		y_div = 2;
+		break;
+	case RGA_FORMAT_YCbCr_420_P:
+		dst_format = 0xb;
+		dst_cbcr_swp = 1;
+		plane_width = 1;
+		x_div = 2;
+		y_div = 2;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP:
+		dst_format = 0x8;
+		dst_cbcr_swp = 1;
+		plane_width = 2;
+		x_div = 2;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_P:
+		dst_format = 0x9;
+		dst_cbcr_swp = 1;
+		plane_width = 1;
+		x_div = 2;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP:
+		dst_format = 0xa;
+		dst_cbcr_swp = 1;
+		plane_width = 2;
+		x_div = 2;
+		y_div = 2;
+		break;
+	case RGA_FORMAT_YCrCb_420_P:
+		dst_format = 0xb;
+		plane_width = 1;
+		x_div = 2;
+		y_div = 2;
+		break;
+
+	case RGA_FORMAT_YCbCr_400:
+		dst_format = 0x8;
+		dst_fmt_yuv400_en = 1;
+		x_div = 1;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_Y4:
+		dst_format = 0x8;
+		dst_fmt_y4_en = 1;
+		dst_fmt_y4_lut_en = 1;
+		dst_fmt_yuv400_en = 1;
+		x_div = 1;
+		y_div = 1;
+		break;
+
+	case RGA_FORMAT_Y8:
+		dst_format = 0x8;
+		dst_fmt_y4_lut_en = 1;
+		dst_fmt_yuv400_en = 1;
+		x_div = 1;
+		y_div = 1;
+		break;
+
+	case RGA_FORMAT_YUYV_422:
+		dst_format = 0xe;
+		dpw = 2;
+		dst_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_YVYU_422:
+		dst_format = 0xe;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_YUYV_420:
+		dst_format = 0xf;
+		dpw = 2;
+		dst_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_YVYU_420:
+		dst_format = 0xf;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_UYVY_422:
+		dst_format = 0xc;
+		dpw = 2;
+		dst_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_VYUY_422:
+		dst_format = 0xc;
+		dpw = 2;
+		break;
+	case RGA_FORMAT_UYVY_420:
+		dst_format = 0xd;
+		dpw = 2;
+		dst_cbcr_swp = 1;
+		break;
+	case RGA_FORMAT_VYUY_420:
+		dst_format = 0xd;
+		dpw = 2;
+		break;
+
+	case RGA_FORMAT_YCbCr_444_SP:
+		dst_format = 0x3;
+		plane_width = 2;
+		x_div = 1;
+		y_div = 1;
+		break;
+	case RGA_FORMAT_YCrCb_444_SP:
+		dst_format = 0x3;
+		plane_width = 2;
+		x_div = 1;
+		y_div = 1;
+		dst_cbcr_swp = 1;
+		break;
+	};
+
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_FMT)) |
+		 (s_RGA2_DST_INFO_SW_DST_FMT(dst_format)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_RB_SWAP)) |
+		 (s_RGA2_DST_INFO_SW_DST_RB_SWAP(dst_rb_swp)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_ALPHA_SWAP)) |
+		 (s_RGA2_DST_INFO_SW_ALPHA_SWAP(dst_alpha_swp)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_UV_SWAP)) |
+		 (s_RGA2_DST_INFO_SW_DST_UV_SWAP(dst_cbcr_swp)));
+
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN)) |
+		 (s_RGA2_DST_INFO_SW_DST_FMT_YUV400_EN(dst_fmt_yuv400_en)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_FMT_Y4_EN)) |
+		 (s_RGA2_DST_INFO_SW_DST_FMT_Y4_EN(dst_fmt_y4_en)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN)) |
+		 (s_RGA2_DST_INFO_SW_DST_NN_QUANTIZE_EN(dst_nn_quantize_en)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DITHER_UP_E)) |
+		 (s_RGA2_DST_INFO_SW_DITHER_UP_E(msg->alpha_rop_flag >> 5)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DITHER_DOWN_E)) |
+		 (s_RGA2_DST_INFO_SW_DITHER_DOWN_E(msg->alpha_rop_flag >> 6)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DITHER_MODE)) |
+		 (s_RGA2_DST_INFO_SW_DITHER_MODE(msg->dither_mode)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_CSC_MODE)) |
+		 (s_RGA2_DST_INFO_SW_DST_CSC_MODE(msg->yuv2rgb_mode >> 2)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_CSC_CLIP_MODE)) |
+		 (s_RGA2_DST_INFO_SW_CSC_CLIP_MODE(msg->yuv2rgb_mode >> 4)));
+	/* full csc enable */
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_DST_CSC_MODE_2)) |
+		 (s_RGA2_DST_INFO_SW_DST_CSC_MODE_2(msg->full_csc_en)));
+	/*
+	 * Some older chips do not support src1 csc mode,
+	 * they do not have these two registers.
+	 */
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_SRC1_CSC_MODE)) |
+		 (s_RGA2_DST_INFO_SW_SRC1_CSC_MODE(msg->yuv2rgb_mode >> 5)));
+	reg =
+		((reg & (~m_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE)) |
+		 (s_RGA2_DST_INFO_SW_SRC1_CSC_CLIP_MODE(
+			msg->yuv2rgb_mode >> 7)));
+
+	reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_UVHDS_MODE)) |
+	       (s_RGA2_DST_INFO_SW_DST_UVHDS_MODE(msg->uvhds_mode)));
+	reg = ((reg & (~m_RGA2_DST_INFO_SW_DST_UVVDS_MODE)) |
+	       (s_RGA2_DST_INFO_SW_DST_UVVDS_MODE(msg->uvvds_mode)));
+
+	ydither_en = (msg->dst.format == RGA_FORMAT_Y4 ||
+		      msg->dst.format == RGA_FORMAT_Y8)
+		&& ((msg->alpha_rop_flag >> 6) & 0x1);
+
+	*bRGA_DST_INFO = reg;
+
+	if (((msg->rotate_mode & 0xf) == 0) ||
+	    ((msg->rotate_mode & 0xf) == 1)) {
+		x_mirr = 0;
+		y_mirr = 0;
+	} else {
+		x_mirr = 1;
+		y_mirr = 1;
+	}
+
+	rot_90_flag = msg->rotate_mode & 1;
+	x_mirr = (x_mirr + ((msg->rotate_mode >> 4) & 1)) & 1;
+	y_mirr = (y_mirr + ((msg->rotate_mode >> 5) & 1)) & 1;
+
+	if (ydither_en) {
+		if (x_mirr && y_mirr) {
+			rga_err("ydither mode do not support rotate x_mirr=%d,y_mirr=%d\n",
+				x_mirr, y_mirr);
+		}
+
+		if (msg->dst.act_w != msg->src.act_w)
+			rga_err("ydither mode do not support x dir scale\n");
+
+		if (msg->dst.act_h != msg->src.act_h)
+			rga_err("ydither mode do not support y dir scale\n");
+	}
+
+	if (dst_fmt_y4_lut_en) {
+		*RGA_DST_Y4MAP_LUT0 = (msg->gr_color.gr_x_r & 0xffff) |
+				      (msg->gr_color.gr_x_g << 16);
+		*RGA_DST_Y4MAP_LUT1 = (msg->gr_color.gr_y_r & 0xffff) |
+				      (msg->gr_color.gr_y_g << 16);
+	}
+
+	if (dst_nn_quantize_en) {
+		*RGA_DST_NN_QUANTIZE_SCALE = (msg->gr_color.gr_x_r & 0xffff) |
+			(msg->gr_color.gr_x_g << 10) |
+			(msg->gr_color.gr_x_b << 20);
+		*RGA_DST_NN_QUANTIZE_OFFSET = (msg->gr_color.gr_y_r & 0xffff) |
+			(msg->gr_color.gr_y_g << 10) |
+			(msg->gr_color.gr_y_b << 20);
+	}
+
+	s_stride = (((msg->src1.vir_w * spw >> bbp_shift) + 3) & ~3);
+
+	s_yrgb_addr =
+		(u32) msg->src1.yrgb_addr + (msg->src1.y_offset * s_stride) +
+		(msg->src1.x_offset * spw >> bbp_shift);
+	*bRGA_SRC_BASE3 = s_yrgb_addr;
+
+	/* Warning */
+	line_width_real = dst_fmt_y4_en ? ((msg->dst.act_w) >> 1) : msg->dst.act_w;
+
+	switch (msg->dst.rd_mode) {
+	case RGA_RASTER_MODE:
+		d_stride = ALIGN(msg->dst.vir_w * dpw, 4);
+		/* Y4 output will HALF */
+		if (dst_fmt_y4_en)
+			d_stride = ALIGN(d_stride, 2) >> 1;
+		d_uv_stride = ALIGN(d_stride / x_div * plane_width, 4);
+
+		yrgb_offset = msg->dst.y_offset * d_stride + msg->dst.x_offset * dpw;
+		uv_offset = (msg->dst.y_offset / y_div) * d_uv_stride +
+			    (msg->dst.x_offset / x_div * plane_width);
+		v_offset = uv_offset;
+
+		yrgb_addr = (u32)msg->dst.yrgb_addr + yrgb_offset;
+		u_addr = (u32)msg->dst.uv_addr + uv_offset;
+		v_addr = (u32)msg->dst.v_addr + v_offset;
+
+		y_lt_addr = yrgb_addr;
+		u_lt_addr = u_addr;
+		v_lt_addr = v_addr;
+
+		/*
+		 * YUV packet mode is a new format, and the write behavior during
+		 * rotation is different from the old format.
+		 */
+		if (rga_is_yuv422_packed_format(msg->dst.format)) {
+			y_ld_addr = yrgb_addr + (msg->dst.act_h - 1) * (d_stride);
+			y_rt_addr = yrgb_addr + (msg->dst.act_w * 2 - 1);
+			y_rd_addr = y_ld_addr + (msg->dst.act_w * 2 - 1);
+		} else if (rga_is_yuv420_packed_format(msg->dst.format)) {
+			y_ld_addr = (u32)msg->dst.yrgb_addr +
+				((msg->dst.y_offset + (msg->dst.act_h - 1)) * d_stride) +
+				msg->dst.x_offset;
+			y_rt_addr = yrgb_addr + (msg->dst.act_w * 2 - 1);
+			y_rd_addr = y_ld_addr + (msg->dst.act_w - 1);
+		} else {
+			/* 270 degree & Mirror V */
+			y_ld_addr = yrgb_addr + (msg->dst.act_h - 1) * (d_stride);
+			/* 90 degree & Mirror H */
+			y_rt_addr = yrgb_addr + (line_width_real - 1) * dpw;
+			/* 180 degree */
+			y_rd_addr = y_ld_addr + (line_width_real - 1) * dpw;
+		}
+
+		u_ld_addr = u_addr + ((msg->dst.act_h / y_div) - 1) * (d_uv_stride);
+		v_ld_addr = v_addr + ((msg->dst.act_h / y_div) - 1) * (d_uv_stride);
+
+		u_rt_addr = u_addr + (msg->dst.act_w / x_div * plane_width) - 1;
+		v_rt_addr = v_addr + (msg->dst.act_w / x_div * plane_width) - 1;
+
+		u_rd_addr = u_ld_addr + (msg->dst.act_w / x_div * plane_width) - 1;
+		v_rd_addr = v_ld_addr + (msg->dst.act_w / x_div * plane_width) - 1;
+
+		break;
+
+	case RGA_TILE4x4_MODE:
+		switch (msg->dst.format) {
+		case RGA_FORMAT_YCbCr_400:
+			tile_block_size = 16;
+			break;
+		case RGA_FORMAT_YCbCr_420_SP:
+		case RGA_FORMAT_YCrCb_420_SP:
+			tile_block_size = 24;
+			break;
+		case RGA_FORMAT_YCbCr_422_SP:
+		case RGA_FORMAT_YCrCb_422_SP:
+			tile_block_size = 32;
+			break;
+		case RGA_FORMAT_YCbCr_444_SP:
+		case RGA_FORMAT_YCrCb_444_SP:
+			tile_block_size = 48;
+			break;
+		case RGA_FORMAT_YCbCr_420_SP_10B:
+		case RGA_FORMAT_YCrCb_420_SP_10B:
+			tile_block_size = 30;
+			break;
+		case RGA_FORMAT_YCbCr_422_SP_10B:
+		case RGA_FORMAT_YCrCb_422_SP_10B:
+			tile_block_size = 40;
+			break;
+		default:
+			tile_block_size = 16;
+			break;
+		}
+
+		d_stride = ALIGN((u32)((msg->dst.vir_w * dpw) * (tile_block_size / 4)), 4);
+
+		yrgb_offset = (u32)((msg->dst.y_offset / 4) * d_stride +
+			      (msg->dst.x_offset / 4) * dpw * tile_block_size);
+
+		tile_x_offset = (msg->dst.x_offset % 4) & 0x3;
+		tile_y_offset = (msg->dst.y_offset % 4) & 0x3;
+
+		y_lt_addr = (u32)msg->dst.yrgb_addr + yrgb_offset;
+		y_ld_addr = y_lt_addr +
+			    (msg->dst.act_h / 4 - ((msg->dst.act_h % 4 == 0) ? 1 : 0)) * d_stride;
+		y_rt_addr = y_lt_addr +
+			    (line_width_real / 4 - ((msg->dst.act_w % 4 == 0) ? 0 : 1)) * dpw *
+			    tile_block_size;
+		y_rd_addr = y_rt_addr +
+			    (msg->dst.act_h / 4 - ((msg->dst.act_h % 4 == 0) ? 1 : 0)) * d_stride;
+
+		u_lt_addr = 0;
+		u_ld_addr = 0;
+		u_rt_addr = 0;
+		u_rd_addr = 0;
+
+		v_lt_addr = 0;
+		v_ld_addr = 0;
+		v_rt_addr = 0;
+		v_rd_addr = 0;
+
+		break;
+	}
+
+	*bRGA_DST_VIR_INFO = (d_stride >> 2) | ((s_stride >> 2) << 16);
+
+	if ((msg->dst.vir_w % 2 != 0) &&
+	    (msg->dst.act_w == msg->src.act_w) && (msg->dst.act_h == msg->src.act_h) &&
+	    (msg->dst.format == RGA_FORMAT_BGR_888 || msg->dst.format == RGA_FORMAT_RGB_888))
+		*bRGA_DST_ACT_INFO =
+			(msg->dst.act_w) | ((msg->dst.act_h - 1) << 16) |
+			tile_x_offset << 14 | tile_y_offset << 30;
+	else
+		*bRGA_DST_ACT_INFO =
+			(msg->dst.act_w - 1) | ((msg->dst.act_h - 1) << 16) |
+			tile_x_offset << 14 | tile_y_offset << 30;
+
+	if (rot_90_flag == 0) {
+		if (y_mirr == 1) {
+			if (x_mirr == 1) {
+				yrgb_addr = y_rd_addr;
+				u_addr = u_rd_addr;
+				v_addr = v_rd_addr;
+			} else {
+				yrgb_addr = y_ld_addr;
+				u_addr = u_ld_addr;
+				v_addr = v_ld_addr;
+			}
+		} else {
+			if (x_mirr == 1) {
+				yrgb_addr = y_rt_addr;
+				u_addr = u_rt_addr;
+				v_addr = v_rt_addr;
+			} else {
+				yrgb_addr = y_lt_addr;
+				u_addr = u_lt_addr;
+				v_addr = v_lt_addr;
+			}
+		}
+	} else {
+		if (y_mirr == 1) {
+			if (x_mirr == 1) {
+				yrgb_addr = y_ld_addr;
+				u_addr = u_ld_addr;
+				v_addr = v_ld_addr;
+			} else {
+				yrgb_addr = y_rd_addr;
+				u_addr = u_rd_addr;
+				v_addr = v_rd_addr;
+			}
+		} else {
+			if (x_mirr == 1) {
+				yrgb_addr = y_lt_addr;
+				u_addr = u_lt_addr;
+				v_addr = v_lt_addr;
+			} else {
+				yrgb_addr = y_rt_addr;
+				u_addr = u_rt_addr;
+				v_addr = v_rt_addr;
+			}
+		}
+	}
+
+	*bRGA_DST_BASE0 = (u32) yrgb_addr;
+
+	switch (msg->dst.rd_mode) {
+	case RGA_RASTER_MODE:
+		if ((msg->dst.format == RGA_FORMAT_YCbCr_420_P) ||
+		    (msg->dst.format == RGA_FORMAT_YCrCb_420_P)) {
+			if (dst_cbcr_swp == 0) {
+				*bRGA_DST_BASE1 = (u32) v_addr;
+				*bRGA_DST_BASE2 = (u32) u_addr;
+			} else {
+				*bRGA_DST_BASE1 = (u32) u_addr;
+				*bRGA_DST_BASE2 = (u32) v_addr;
+			}
+		} else {
+			*bRGA_DST_BASE1 = (u32) u_addr;
+			*bRGA_DST_BASE2 = (u32) v_addr;
+		}
+
+		break;
+	case RGA_TILE4x4_MODE:
+		*bRGA_TILE4x4_OUT_BASE = yrgb_addr;
+
+		break;
+	}
+
+	if (rot_90_flag == 1) {
+		if (y_mirr == 1) {
+			msg->iommu_prefetch.y_threshold = y_lt_addr;
+			msg->iommu_prefetch.uv_threshold = u_lt_addr;
+		} else {
+			msg->iommu_prefetch.y_threshold = y_rd_addr;
+			msg->iommu_prefetch.uv_threshold = u_rd_addr;
+		}
+	}
+}
+
+static void RGA2_set_reg_alpha_info(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_ALPHA_CTRL0;
+	u32 *bRGA_ALPHA_CTRL1;
+	u32 *bRGA_FADING_CTRL;
+	u32 reg = 0;
+	union rga2_color_ctrl color_ctrl;
+	union rga2_alpha_ctrl alpha_ctrl;
+	struct rga_alpha_config *config;
+
+	bRGA_ALPHA_CTRL0 = (u32 *) (base + RGA2_ALPHA_CTRL0_OFFSET);
+	bRGA_ALPHA_CTRL1 = (u32 *) (base + RGA2_ALPHA_CTRL1_OFFSET);
+	bRGA_FADING_CTRL = (u32 *) (base + RGA2_FADING_CTRL_OFFSET);
+
+	color_ctrl.value = 0;
+	alpha_ctrl.value = 0;
+	config = &msg->alpha_config;
+
+	color_ctrl.bits.src_color_mode =
+		config->fg_pre_multiplied ? RGA_ALPHA_PRE_MULTIPLIED : RGA_ALPHA_NO_PRE_MULTIPLIED;
+	color_ctrl.bits.dst_color_mode =
+		config->bg_pre_multiplied ? RGA_ALPHA_PRE_MULTIPLIED : RGA_ALPHA_NO_PRE_MULTIPLIED;
+
+	if (config->fg_pixel_alpha_en)
+		color_ctrl.bits.src_blend_mode =
+			config->fg_global_alpha_en ? RGA_ALPHA_PER_PIXEL_GLOBAL :
+			RGA_ALPHA_PER_PIXEL;
+	else
+		color_ctrl.bits.src_blend_mode = RGA_ALPHA_GLOBAL;
+
+	if (config->bg_pixel_alpha_en)
+		color_ctrl.bits.dst_blend_mode =
+			config->bg_global_alpha_en ? RGA_ALPHA_PER_PIXEL_GLOBAL :
+			RGA_ALPHA_PER_PIXEL;
+	else
+		color_ctrl.bits.dst_blend_mode = RGA_ALPHA_GLOBAL;
+
+	/*
+	 * Since the hardware uses 256 as 1, the original alpha value needs to
+	 * be + (alpha >> 7).
+	 */
+	color_ctrl.bits.src_alpha_cal_mode = RGA_ALPHA_SATURATION;
+	color_ctrl.bits.dst_alpha_cal_mode = RGA_ALPHA_SATURATION;
+
+	/* porter duff alpha enable */
+	switch (config->mode) {
+	case RGA_ALPHA_BLEND_SRC:
+		/*
+		 * SRC mode:
+		 *	Sf = 1, Df = 0；
+		 *	[Rc,Ra] = [Sc,Sa]；
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ONE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST:
+		/*
+		 * SRC mode:
+		 *	Sf = 0, Df = 1；
+		 *	[Rc,Ra] = [Dc,Da]；
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ZERO;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ONE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_OVER:
+		/*
+		 * SRC-OVER mode:
+		 *	Sf = 1, Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc + (1 - Sa) * Dc, Sa + (1 - Sa) * Da ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ONE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_OVER:
+		/*
+		 * DST-OVER mode:
+		 *	Sf = (1 - Da) , Df = 1
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc, Sa * (1 - Da) + Da ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ONE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_IN:
+		/*
+		 * SRC-IN mode:
+		 *	Sf = Da , Df = 0
+		 *	[Rc,Ra] = [ Sc * Da, Sa * Da ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_IN:
+		/*
+		 * DST-IN mode:
+		 *	Sf = 0 , Df = Sa
+		 *	[Rc,Ra] = [ Dc * Sa, Da * Sa ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ZERO;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_OUT:
+		/*
+		 * SRC-OUT mode:
+		 *	Sf = (1 - Da) , Df = 0
+		 *	[Rc,Ra] = [ Sc * (1 - Da), Sa * (1 - Da) ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_OUT:
+		/*
+		 * DST-OUT mode:
+		 *	Sf = 0 , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Dc * (1 - Sa), Da * (1 - Sa) ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ZERO;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_ATOP:
+		/*
+		 * SRC-ATOP mode:
+		 *	Sf = Da , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc * Da + Dc * (1 - Sa), Sa * Da + Da * (1 - Sa) ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_ATOP:
+		/*
+		 * DST-ATOP mode:
+		 *	Sf = (1 - Da) , Df = Sa
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc * Sa, Sa * (1 - Da) + Da * Sa ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_XOR:
+		/*
+		 * DST-XOR mode:
+		 *	Sf = (1 - Da) , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc * (1 - Sa), Sa * (1 - Da) + Da * (1 - Sa) ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_CLEAR:
+		/*
+		 * DST-CLEAR mode:
+		 *	Sf = 0 , Df = 0
+		 *	[Rc,Ra] = [ 0, 0 ]
+		 */
+		color_ctrl.bits.src_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.src_factor_mode = RGA_ALPHA_ZERO;
+
+		color_ctrl.bits.dst_alpha_mode = RGA_ALPHA_STRAIGHT;
+		color_ctrl.bits.dst_factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	default:
+		break;
+	}
+
+	alpha_ctrl.bits.src_blend_mode = color_ctrl.bits.src_blend_mode;
+	alpha_ctrl.bits.dst_blend_mode = color_ctrl.bits.dst_blend_mode;
+
+	alpha_ctrl.bits.src_alpha_cal_mode = color_ctrl.bits.src_alpha_cal_mode;
+	alpha_ctrl.bits.dst_alpha_cal_mode = color_ctrl.bits.dst_alpha_cal_mode;
+
+	alpha_ctrl.bits.src_alpha_mode = color_ctrl.bits.src_alpha_mode;
+	alpha_ctrl.bits.src_factor_mode = color_ctrl.bits.src_factor_mode;
+
+	alpha_ctrl.bits.dst_alpha_mode = color_ctrl.bits.dst_alpha_mode;
+	alpha_ctrl.bits.dst_factor_mode = color_ctrl.bits.dst_factor_mode;
+
+	reg =
+		((reg & (~m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0)) |
+		 (s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_0(msg->alpha_rop_flag)));
+	reg =
+		((reg & (~m_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL)) |
+		 (s_RGA2_ALPHA_CTRL0_SW_ALPHA_ROP_SEL
+		 (msg->alpha_rop_flag >> 1)));
+	reg =
+		((reg & (~m_RGA2_ALPHA_CTRL0_SW_ROP_MODE)) |
+		 (s_RGA2_ALPHA_CTRL0_SW_ROP_MODE(msg->rop_mode)));
+	reg =
+		((reg & (~m_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA)) |
+		 (s_RGA2_ALPHA_CTRL0_SW_SRC_GLOBAL_ALPHA
+		 ((uint8_t)config->fg_global_alpha_value)));
+	reg =
+		((reg & (~m_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA)) |
+		 (s_RGA2_ALPHA_CTRL0_SW_DST_GLOBAL_ALPHA
+		 ((uint8_t)config->bg_global_alpha_value)));
+
+	*bRGA_ALPHA_CTRL0 = reg;
+	*bRGA_ALPHA_CTRL1 = color_ctrl.value | (alpha_ctrl.value << 16);
+
+
+	if ((msg->alpha_rop_flag >> 2) & 1) {
+		*bRGA_FADING_CTRL = (1 << 24) | (msg->fading_b_value << 16) |
+			(msg->fading_g_value << 8) | (msg->fading_r_value);
+	}
+}
+
+static void RGA2_set_reg_rop_info(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_ALPHA_CTRL0;
+	u32 *bRGA_ROP_CTRL0;
+	u32 *bRGA_ROP_CTRL1;
+	u32 *bRGA_MASK_ADDR;
+	u32 *bRGA_FG_COLOR;
+	u32 *bRGA_PAT_CON;
+
+	u32 rop_code0 = 0;
+	u32 rop_code1 = 0;
+
+	bRGA_ALPHA_CTRL0 = (u32 *) (base + RGA2_ALPHA_CTRL0_OFFSET);
+	bRGA_ROP_CTRL0 = (u32 *) (base + RGA2_ROP_CTRL0_OFFSET);
+	bRGA_ROP_CTRL1 = (u32 *) (base + RGA2_ROP_CTRL1_OFFSET);
+	bRGA_MASK_ADDR = (u32 *) (base + RGA2_MASK_BASE_OFFSET);
+	bRGA_FG_COLOR = (u32 *) (base + RGA2_SRC_FG_COLOR_OFFSET);
+	bRGA_PAT_CON = (u32 *) (base + RGA2_PAT_CON_OFFSET);
+
+	if (msg->rop_mode == 0) {
+		rop_code0 = rga2_rop_code[(msg->rop_code & 0xff)];
+	} else if (msg->rop_mode == 1) {
+		rop_code0 = rga2_rop_code[(msg->rop_code & 0xff)];
+	} else if (msg->rop_mode == 2) {
+		rop_code0 = rga2_rop_code[(msg->rop_code & 0xff)];
+		rop_code1 = rga2_rop_code[(msg->rop_code & 0xff00) >> 8];
+	}
+
+	*bRGA_ROP_CTRL0 = rop_code0;
+	*bRGA_ROP_CTRL1 = rop_code1;
+	*bRGA_FG_COLOR = msg->fg_color;
+	*bRGA_MASK_ADDR = (u32) msg->rop_mask_addr;
+	*bRGA_PAT_CON = (msg->pat.act_w - 1) | ((msg->pat.act_h - 1) << 8)
+		| (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+	*bRGA_ALPHA_CTRL0 =
+		*bRGA_ALPHA_CTRL0 | (((msg->endian_mode >> 1) & 1) << 20);
+
+}
+
+static void RGA_set_reg_mosaic(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_MOSAIC_MODE;
+
+	bRGA_MOSAIC_MODE = (u32 *)(base + RGA2_MOSAIC_MODE_OFFSET);
+
+	*bRGA_MOSAIC_MODE = (u32)(msg->mosaic_info.mode & 0x7);
+}
+
+static int RGA_set_reg_gauss(u8 *base, struct rga2_req *msg)
+{
+	uint32_t *bRGA_GAUSS_COE;
+	uint32_t reg = 0;
+	uint32_t *coe;
+
+	bRGA_GAUSS_COE = (u32 *)(base + RGA2_GAUSS_COE_OFFSET);
+
+	if (msg->gauss_config.size != 3) {
+		rga_err("Gaussian blur only support 3x3\n");
+		return -EINVAL;
+	}
+
+	coe = kmalloc(sizeof(uint32_t) * msg->gauss_config.size, GFP_KERNEL);
+	if (coe == NULL) {
+		rga_err("Gaussian blur alloc coe buffer error!\n");
+		return -ENOMEM;
+	}
+
+	if (unlikely(copy_from_user(coe,
+				    u64_to_user_ptr(msg->gauss_config.coe_ptr),
+				    sizeof(uint32_t) * msg->gauss_config.size))) {
+		rga_err("Gaussian blur coe copy_from_user failed\n");
+
+		kfree(coe);
+		return -EFAULT;
+	}
+
+	reg = ((reg & (~m_RGA2_GAUSS_COE_SW_COE0)) |
+	       (s_RGA2_GAUSS_COE_SW_COE0(coe[0])));
+
+	reg = ((reg & (~m_RGA2_GAUSS_COE_SW_COE1)) |
+	       (s_RGA2_GAUSS_COE_SW_COE1(coe[1])));
+
+	reg = ((reg & (~m_RGA2_GAUSS_COE_SW_COE2)) |
+	       (s_RGA2_GAUSS_COE_SW_COE2(coe[2])));
+
+	*bRGA_GAUSS_COE = reg;
+
+	kfree(coe);
+
+	return 0;
+}
+
+static void RGA2_set_reg_osd(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_OSD_CTRL0;
+	u32 *bRGA_OSD_CTRL1;
+	u32 *bRGA_OSD_INVERTSION_CAL0;
+	u32 *bRGA_OSD_INVERTSION_CAL1;
+	u32 *bRGA_OSD_COLOR0;
+	u32 *bRGA_OSD_COLOR1;
+	u32 *bRGA_OSD_LAST_FLAGS0;
+	u32 *bRGA_OSD_LAST_FLAGS1;
+	u32 reg;
+	u8 rgba2bpp_en = 0;
+	u8 block_num;
+	u16 fix_width;
+
+
+	bRGA_OSD_CTRL0 = (u32 *)(base + RGA2_OSD_CTRL0_OFFSET);
+	bRGA_OSD_CTRL1 = (u32 *)(base + RGA2_OSD_CTRL1_OFFSET);
+	bRGA_OSD_INVERTSION_CAL0 = (u32 *)(base + RGA2_OSD_INVERTSION_CAL0_OFFSET);
+	bRGA_OSD_INVERTSION_CAL1 = (u32 *)(base + RGA2_OSD_INVERTSION_CAL1_OFFSET);
+	bRGA_OSD_COLOR0 = (u32 *)(base + RGA2_OSD_COLOR0_OFFSET);
+	bRGA_OSD_COLOR1 = (u32 *)(base + RGA2_OSD_COLOR1_OFFSET);
+	bRGA_OSD_LAST_FLAGS0 = (u32 *)(base + RGA2_OSD_LAST_FLAGS0_OFFSET);
+	bRGA_OSD_LAST_FLAGS1 = (u32 *)(base + RGA2_OSD_LAST_FLAGS1_OFFSET);
+
+	/* To save the number of register bits. */
+	fix_width = msg->osd_info.mode_ctrl.block_fix_width / 2 - 1;
+
+	/* The register is '0' as the first. */
+	block_num = msg->osd_info.mode_ctrl.block_num - 1;
+
+	if (msg->src1.format == RGA_FORMAT_RGBA_2BPP)
+		rgba2bpp_en = 1;
+
+	reg = 0;
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_MODE)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_MODE(msg->osd_info.mode_ctrl.mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_VER_MODE)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_VER_MODE(msg->osd_info.mode_ctrl.direction_mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_WIDTH_MODE)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_WIDTH_MODE(msg->osd_info.mode_ctrl.width_mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_BLK_NUM)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_BLK_NUM(block_num)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_FLAGS_INDEX)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_FLAGS_INDEX(msg->osd_info.mode_ctrl.flags_index)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_FIX_WIDTH)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_FIX_WIDTH(fix_width)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL0_SW_OSD_2BPP_MODE)) |
+	       (s_RGA2_OSD_CTRL0_SW_OSD_2BPP_MODE(rgba2bpp_en)));
+	*bRGA_OSD_CTRL0 = reg;
+
+	reg = 0;
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_COLOR_SEL)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_COLOR_SEL(msg->osd_info.mode_ctrl.color_mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_FLAG_SEL)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_FLAG_SEL(msg->osd_info.mode_ctrl.invert_flags_mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_DEFAULT_COLOR)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_DEFAULT_COLOR(msg->osd_info.mode_ctrl.default_color_sel)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_AUTO_INVERST_MODE)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_AUTO_INVERST_MODE(msg->osd_info.mode_ctrl.invert_mode)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_THRESH)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_THRESH(msg->osd_info.mode_ctrl.invert_thresh)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_INVERT_A_EN)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_INVERT_A_EN(msg->osd_info.mode_ctrl.invert_enable)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_INVERT_Y_DIS)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_INVERT_Y_DIS(msg->osd_info.mode_ctrl.invert_enable >> 1)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_INVERT_C_DIS)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_INVERT_C_DIS(msg->osd_info.mode_ctrl.invert_enable >> 2)));
+	reg = ((reg & (~m_RGA2_OSD_CTRL1_SW_OSD_UNFIX_INDEX)) |
+	       (s_RGA2_OSD_CTRL1_SW_OSD_UNFIX_INDEX(msg->osd_info.mode_ctrl.unfix_index)));
+	*bRGA_OSD_CTRL1 = reg;
+
+	*bRGA_OSD_INVERTSION_CAL0 = ((msg->osd_info.cal_factor.crb_max) << 24) |
+				    ((msg->osd_info.cal_factor.crb_min) << 16) |
+				    ((msg->osd_info.cal_factor.yg_max) << 8) |
+				    ((msg->osd_info.cal_factor.yg_min) << 0);
+	*bRGA_OSD_INVERTSION_CAL1 = ((msg->osd_info.cal_factor.alpha_max) << 8) |
+				    ((msg->osd_info.cal_factor.alpha_min) << 0);
+
+	*bRGA_OSD_LAST_FLAGS0 = (msg->osd_info.last_flags0);
+	*bRGA_OSD_LAST_FLAGS1 = (msg->osd_info.last_flags1);
+
+	if (msg->osd_info.mode_ctrl.color_mode == 1) {
+		*bRGA_OSD_COLOR0 = (msg->osd_info.bpp2_info.color0.value & 0xffffff);
+		*bRGA_OSD_COLOR1 = (msg->osd_info.bpp2_info.color1.value & 0xffffff);
+	}
+
+	if (rgba2bpp_en) {
+		*bRGA_OSD_COLOR0 = msg->osd_info.bpp2_info.color0.value;
+		*bRGA_OSD_COLOR1 = msg->osd_info.bpp2_info.color1.value;
+	}
+}
+
+static void RGA2_set_reg_color_palette(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_SRC_BASE0, *bRGA_SRC_INFO, *bRGA_SRC_VIR_INFO,
+		*bRGA_SRC_ACT_INFO, *bRGA_SRC_FG_COLOR, *bRGA_SRC_BG_COLOR;
+	u32 *p;
+	short x_off, y_off;
+	u16 src_stride;
+	u8 shift;
+	u32 sw;
+	u32 byte_num;
+	u32 reg;
+
+	bRGA_SRC_BASE0 = (u32 *) (base + RGA2_SRC_BASE0_OFFSET);
+	bRGA_SRC_INFO = (u32 *) (base + RGA2_SRC_INFO_OFFSET);
+	bRGA_SRC_VIR_INFO = (u32 *) (base + RGA2_SRC_VIR_INFO_OFFSET);
+	bRGA_SRC_ACT_INFO = (u32 *) (base + RGA2_SRC_ACT_INFO_OFFSET);
+	bRGA_SRC_FG_COLOR = (u32 *) (base + RGA2_SRC_FG_COLOR_OFFSET);
+	bRGA_SRC_BG_COLOR = (u32 *) (base + RGA2_SRC_BG_COLOR_OFFSET);
+
+	reg = 0;
+
+	shift = 3 - msg->palette_mode;
+
+	x_off = msg->src.x_offset;
+	y_off = msg->src.y_offset;
+
+	sw = msg->src.vir_w;
+	byte_num = sw >> shift;
+
+	src_stride = (byte_num + 3) & (~3);
+
+	p = (u32 *) ((unsigned long)msg->src.yrgb_addr);
+
+	p = p + (x_off >> shift) + y_off * src_stride;
+
+	*bRGA_SRC_BASE0 = (unsigned long)p;
+
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SRC_FMT)) |
+		 (s_RGA2_SRC_INFO_SW_SRC_FMT((msg->palette_mode | 0xc))));
+	reg =
+		((reg & (~m_RGA2_SRC_INFO_SW_SW_CP_ENDIAN)) |
+		 (s_RGA2_SRC_INFO_SW_SW_CP_ENDAIN(msg->endian_mode & 1)));
+	*bRGA_SRC_VIR_INFO = src_stride >> 2;
+	*bRGA_SRC_ACT_INFO =
+		(msg->src.act_w - 1) | ((msg->src.act_h - 1) << 16);
+	*bRGA_SRC_INFO = reg;
+
+	*bRGA_SRC_FG_COLOR = msg->fg_color;
+	*bRGA_SRC_BG_COLOR = msg->bg_color;
+
+}
+
+static void RGA2_set_reg_color_fill(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_CF_GR_A;
+	u32 *bRGA_CF_GR_B;
+	u32 *bRGA_CF_GR_G;
+	u32 *bRGA_CF_GR_R;
+	u32 *bRGA_SRC_FG_COLOR;
+	u32 *bRGA_MASK_ADDR;
+	u32 *bRGA_PAT_CON;
+
+	u32 mask_stride;
+	u32 *bRGA_SRC_VIR_INFO;
+
+	bRGA_SRC_FG_COLOR = (u32 *) (base + RGA2_SRC_FG_COLOR_OFFSET);
+
+	bRGA_CF_GR_A = (u32 *) (base + RGA2_CF_GR_A_OFFSET);
+	bRGA_CF_GR_B = (u32 *) (base + RGA2_CF_GR_B_OFFSET);
+	bRGA_CF_GR_G = (u32 *) (base + RGA2_CF_GR_G_OFFSET);
+	bRGA_CF_GR_R = (u32 *) (base + RGA2_CF_GR_R_OFFSET);
+
+	bRGA_MASK_ADDR = (u32 *) (base + RGA2_MASK_BASE_OFFSET);
+	bRGA_PAT_CON = (u32 *) (base + RGA2_PAT_CON_OFFSET);
+
+	bRGA_SRC_VIR_INFO = (u32 *) (base + RGA2_SRC_VIR_INFO_OFFSET);
+
+	mask_stride = msg->rop_mask_stride;
+
+	if (msg->color_fill_mode == 0) {
+		/* solid color */
+		*bRGA_CF_GR_A = (msg->gr_color.gr_x_a & 0xffff) |
+			(msg->gr_color.gr_y_a << 16);
+		*bRGA_CF_GR_B = (msg->gr_color.gr_x_b & 0xffff) |
+			(msg->gr_color.gr_y_b << 16);
+		*bRGA_CF_GR_G = (msg->gr_color.gr_x_g & 0xffff) |
+			(msg->gr_color.gr_y_g << 16);
+		*bRGA_CF_GR_R = (msg->gr_color.gr_x_r & 0xffff) |
+			(msg->gr_color.gr_y_r << 16);
+
+		*bRGA_SRC_FG_COLOR = msg->fg_color;
+	} else {
+		/* pattern color */
+		*bRGA_MASK_ADDR = (u32) msg->pat.yrgb_addr;
+		*bRGA_PAT_CON =
+			(msg->pat.act_w - 1) | ((msg->pat.act_h - 1) << 8)
+			| (msg->pat.x_offset << 16) | (msg->pat.y_offset << 24);
+	}
+	*bRGA_SRC_VIR_INFO = mask_stride << 16;
+}
+
+static void RGA2_set_reg_update_palette_table(u8 *base,
+						 struct rga2_req *msg)
+{
+	u32 *bRGA_MASK_BASE;
+	u32 *bRGA_FADING_CTRL;
+
+	bRGA_MASK_BASE = (u32 *) (base + RGA2_MASK_BASE_OFFSET);
+	bRGA_FADING_CTRL = (u32 *) (base + RGA2_FADING_CTRL_OFFSET);
+
+	*bRGA_FADING_CTRL = msg->fading_g_value << 8;
+	*bRGA_MASK_BASE = (u32) msg->pat.yrgb_addr;
+}
+
+static void RGA2_set_reg_update_patten_buff(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_PAT_MST;
+	u32 *bRGA_PAT_CON;
+	u32 *bRGA_PAT_START_POINT;
+	u32 *bRGA_FADING_CTRL;
+	u32 reg = 0;
+	struct rga_img_info_t *pat;
+
+	u32 num, offset;
+
+	pat = &msg->pat;
+
+	num = (pat->act_w * pat->act_h) - 1;
+
+	offset = pat->act_w * pat->y_offset + pat->x_offset;
+
+	bRGA_PAT_START_POINT = (u32 *) (base + RGA2_FADING_CTRL_OFFSET);
+	bRGA_PAT_MST = (u32 *) (base + RGA2_MASK_BASE_OFFSET);
+	bRGA_PAT_CON = (u32 *) (base + RGA2_PAT_CON_OFFSET);
+	bRGA_FADING_CTRL = (u32 *) (base + RGA2_FADING_CTRL_OFFSET);
+
+	*bRGA_PAT_MST = (u32) msg->pat.yrgb_addr;
+	*bRGA_PAT_START_POINT = (pat->act_w * pat->y_offset) + pat->x_offset;
+
+	reg = (pat->act_w - 1) | ((pat->act_h - 1) << 8) |
+		(pat->x_offset << 16) | (pat->y_offset << 24);
+	*bRGA_PAT_CON = reg;
+
+	*bRGA_FADING_CTRL = (num << 8) | offset;
+}
+
+static void RGA2_set_pat_info(u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_PAT_CON;
+	u32 *bRGA_FADING_CTRL;
+	u32 reg = 0;
+	struct rga_img_info_t *pat;
+
+	u32 num, offset;
+
+	pat = &msg->pat;
+
+	num = ((pat->act_w * pat->act_h) - 1) & 0xff;
+
+	offset = (pat->act_w * pat->y_offset) + pat->x_offset;
+
+	bRGA_PAT_CON = (u32 *) (base + RGA2_PAT_CON_OFFSET);
+	bRGA_FADING_CTRL = (u32 *) (base + RGA2_FADING_CTRL_OFFSET);
+
+	reg = (pat->act_w - 1) | ((pat->act_h - 1) << 8) |
+		(pat->x_offset << 16) | (pat->y_offset << 24);
+	*bRGA_PAT_CON = reg;
+	*bRGA_FADING_CTRL = (num << 8) | offset;
+}
+
+static void RGA2_set_mmu_reg_info(struct rga_scheduler_t *scheduler, u8 *base, struct rga2_req *msg)
+{
+	u32 *bRGA_MMU_CTRL1;
+	u32 *bRGA_MMU_SRC_BASE;
+	u32 *bRGA_MMU_SRC1_BASE;
+	u32 *bRGA_MMU_DST_BASE;
+	u32 *bRGA_MMU_ELS_BASE;
+	u32 *RGA_PREFETCH_ADDR_TH;
+
+	u32 reg;
+
+	switch (scheduler->data->mmu) {
+	case RGA_MMU:
+		bRGA_MMU_CTRL1 = (u32 *) (base + RGA2_MMU_CTRL1_OFFSET);
+		bRGA_MMU_SRC_BASE = (u32 *) (base + RGA2_MMU_SRC_BASE_OFFSET);
+		bRGA_MMU_SRC1_BASE = (u32 *) (base + RGA2_MMU_SRC1_BASE_OFFSET);
+		bRGA_MMU_DST_BASE = (u32 *) (base + RGA2_MMU_DST_BASE_OFFSET);
+		bRGA_MMU_ELS_BASE = (u32 *) (base + RGA2_MMU_ELS_BASE_OFFSET);
+
+		reg = (msg->mmu_info.src0_mmu_flag & 0xf) |
+			((msg->mmu_info.src1_mmu_flag & 0xf) << 4) |
+			((msg->mmu_info.dst_mmu_flag & 0xf) << 8) |
+			((msg->mmu_info.els_mmu_flag & 0x3) << 12);
+
+		*bRGA_MMU_CTRL1 = reg;
+		*bRGA_MMU_SRC_BASE = (u32) (msg->mmu_info.src0_base_addr) >> 4;
+		*bRGA_MMU_SRC1_BASE = (u32) (msg->mmu_info.src1_base_addr) >> 4;
+		*bRGA_MMU_DST_BASE = (u32) (msg->mmu_info.dst_base_addr) >> 4;
+		*bRGA_MMU_ELS_BASE = (u32) (msg->mmu_info.els_base_addr) >> 4;
+
+		break;
+	case RGA_IOMMU:
+		RGA_PREFETCH_ADDR_TH = (u32 *)(base + RGA2_PREFETCH_ADDR_TH_OFFSET);
+
+		*RGA_PREFETCH_ADDR_TH = (msg->iommu_prefetch.y_threshold >> 16) |
+					((msg->iommu_prefetch.uv_threshold >> 16) << 16);
+		break;
+	default:
+		break;
+	}
+}
+
+static int rga2_gen_reg_info(struct rga_scheduler_t *scheduler, u8 *base, struct rga2_req *msg)
+{
+	int ret;
+	u8 dst_nn_quantize_en = 0;
+
+	RGA2_set_mode_ctrl(base, msg);
+
+	RGA2_set_pat_info(base, msg);
+
+	switch (msg->render_mode) {
+	case BITBLT_MODE:
+		RGA2_set_reg_src_info(base, msg);
+		RGA2_set_reg_dst_info(base, msg);
+		dst_nn_quantize_en = (msg->alpha_rop_flag >> 8) & 0x1;
+		if (dst_nn_quantize_en != 1) {
+			if ((msg->dst.format != RGA_FORMAT_Y4) &&
+			    (msg->dst.format != RGA_FORMAT_Y8)) {
+				RGA2_set_reg_alpha_info(base, msg);
+				if (msg->rgba5551_alpha.flags != 1)
+					RGA2_set_reg_rop_info(base, msg);
+			}
+		}
+		if (msg->mosaic_info.enable)
+			RGA_set_reg_mosaic(base, msg);
+		if (msg->osd_info.enable)
+			RGA2_set_reg_osd(base, msg);
+		if (msg->gauss_config.size > 0) {
+			ret = RGA_set_reg_gauss(base, msg);
+			if (ret < 0)
+				return ret;
+		}
+
+		break;
+	case COLOR_FILL_MODE:
+		RGA2_set_reg_color_fill(base, msg);
+		/* tile4x4 need a fake input */
+		if (msg->dst.rd_mode == RGA_TILE4x4_MODE) {
+			msg->src.act_w = msg->dst.act_w;
+			msg->src.act_h = msg->dst.act_h;
+			msg->src.vir_w = msg->dst.vir_w;
+			msg->src.vir_h = msg->dst.vir_h;
+			msg->src.format = RGA_FORMAT_RGBA_8888;
+			msg->src.rd_mode = RGA_RASTER_MODE;
+
+			RGA2_set_reg_src_info(base, msg);
+		}
+		RGA2_set_reg_dst_info(base, msg);
+		RGA2_set_reg_alpha_info(base, msg);
+		break;
+	case COLOR_PALETTE_MODE:
+		RGA2_set_reg_color_palette(base, msg);
+		RGA2_set_reg_dst_info(base, msg);
+		break;
+	case UPDATE_PALETTE_TABLE_MODE:
+		RGA2_set_reg_update_palette_table(base, msg);
+		break;
+	case UPDATE_PATTEN_BUF_MODE:
+		RGA2_set_reg_update_patten_buff(base, msg);
+		break;
+	default:
+		rga_err("ERROR msg render mode %d\n", msg->render_mode);
+		return -EINVAL;
+	}
+
+	RGA2_set_mmu_reg_info(scheduler, base, msg);
+
+	return 0;
+}
+
+static void rga_cmd_to_rga2_cmd(struct rga_scheduler_t *scheduler,
+				struct rga_req *req_rga, struct rga2_req *req)
+{
+	if (req_rga->render_mode == 6)
+		req->render_mode = UPDATE_PALETTE_TABLE_MODE;
+	else if (req_rga->render_mode == 7)
+		req->render_mode = UPDATE_PATTEN_BUF_MODE;
+	else if (req_rga->render_mode == 5)
+		req->render_mode = BITBLT_MODE;
+	else
+		req->render_mode = req_rga->render_mode;
+
+	memcpy(&req->src, &req_rga->src, sizeof(req_rga->src));
+	memcpy(&req->dst, &req_rga->dst, sizeof(req_rga->dst));
+	/* The application will only import pat or src1. */
+	if (req->render_mode == UPDATE_PALETTE_TABLE_MODE)
+		memcpy(&req->pat, &req_rga->pat, sizeof(req_rga->pat));
+	else
+		memcpy(&req->src1, &req_rga->pat, sizeof(req_rga->pat));
+
+	req->src.format = req_rga->src.format;
+	req->dst.format = req_rga->dst.format;
+	req->src1.format = req_rga->pat.format;
+
+	switch (req_rga->rotate_mode & 0x0F) {
+	case 1:
+		if (req_rga->sina == 0 && req_rga->cosa == 65536) {
+			/* rotate 0 */
+			req->rotate_mode = 0;
+		} else if (req_rga->sina == 65536 && req_rga->cosa == 0) {
+			/* rotate 90 */
+			req->rotate_mode = 1;
+			req->dst.x_offset = req_rga->dst.x_offset;
+			req->dst.act_w = req_rga->dst.act_h;
+			req->dst.act_h = req_rga->dst.act_w;
+		} else if (req_rga->sina == 0 && req_rga->cosa == -65536) {
+			/* rotate 180 */
+			req->rotate_mode = 2;
+			req->dst.x_offset = req_rga->dst.x_offset;
+			req->dst.y_offset = req_rga->dst.y_offset;
+		} else if (req_rga->sina == -65536 && req_rga->cosa == 0) {
+			/* totate 270 */
+			req->rotate_mode = 3;
+			req->dst.y_offset = req_rga->dst.y_offset;
+			req->dst.act_w = req_rga->dst.act_h;
+			req->dst.act_h = req_rga->dst.act_w;
+		}
+		break;
+	case 2:
+		//x_mirror
+		req->rotate_mode |= (1 << 4);
+		break;
+	case 3:
+		//y_mirror
+		req->rotate_mode |= (2 << 4);
+		break;
+	case 4:
+		//x_mirror+y_mirror
+		req->rotate_mode |= (3 << 4);
+		break;
+	default:
+		req->rotate_mode = 0;
+		break;
+	}
+
+	switch ((req_rga->rotate_mode & 0xF0) >> 4) {
+	case 2:
+		//x_mirror
+		req->rotate_mode |= (1 << 4);
+		break;
+	case 3:
+		//y_mirror
+		req->rotate_mode |= (2 << 4);
+		break;
+	case 4:
+		//x_mirror+y_mirror
+		req->rotate_mode |= (3 << 4);
+		break;
+	}
+
+	if ((req->src.act_w == req->dst.act_w) &&
+	    (req->src.act_h == req->dst.act_h) &&
+	    (req->rotate_mode == 0)) {
+		if (req->src.format == RGA_FORMAT_YCbCr_420_SP_10B ||
+		    req->src.format == RGA_FORMAT_YCrCb_420_SP_10B ||
+		    req->src.format == RGA_FORMAT_YCbCr_444_SP ||
+		    req->src.format == RGA_FORMAT_YCrCb_444_SP ||
+		    req->dst.format == RGA_FORMAT_YCbCr_444_SP ||
+		    req->dst.format == RGA_FORMAT_YCrCb_444_SP)
+			/* force select to tile mode */
+			req->rotate_mode = 1 << 6;
+	}
+
+	if (req->src.rd_mode == RGA_TILE4x4_MODE ||
+	    req->dst.rd_mode == RGA_TILE4x4_MODE ||
+	    req->src.rd_mode == RGA_RKFBC_MODE ||
+	    req->src.rd_mode == RGA_AFBC32x8_MODE)
+		/* force select to tile mode */
+		req->rotate_mode |= 1 << 6;
+
+	req->interp = req_rga->interp;
+	req->LUT_addr = req_rga->LUT_addr;
+	req->rop_mask_addr = req_rga->rop_mask_addr;
+
+	req->bitblt_mode = req_rga->bsfilter_flag;
+
+	req->src_a_global_val = req_rga->alpha_global_value;
+	req->dst_a_global_val = req_rga->alpha_global_value;
+	req->rop_code = req_rga->rop_code;
+	req->rop_mode = req_rga->alpha_rop_mode;
+
+	req->color_fill_mode = req_rga->color_fill_mode;
+	req->alpha_zero_key = req_rga->alpha_rop_mode >> 4;
+	req->src_trans_mode = req_rga->src_trans_mode;
+	req->color_key_min = req_rga->color_key_min;
+	req->color_key_max = req_rga->color_key_max;
+
+	req->fg_color = req_rga->fg_color;
+	req->bg_color = req_rga->bg_color;
+	memcpy(&req->gr_color, &req_rga->gr_color, sizeof(req_rga->gr_color));
+
+	req->palette_mode = req_rga->palette_mode;
+	req->yuv2rgb_mode = req_rga->yuv2rgb_mode;
+	if (req_rga->full_csc.flag & 0x1)
+		req->full_csc_en = 1;
+	req->endian_mode = req_rga->endian_mode;
+	req->rgb2yuv_mode = 0;
+
+	req->fading_alpha_value = 0;
+	req->fading_r_value = req_rga->fading.r;
+	req->fading_g_value = req_rga->fading.g;
+	req->fading_b_value = req_rga->fading.b;
+
+	/* alpha mode set */
+	req->alpha_rop_flag = 0;
+	/* alpha_rop_enable */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag & 1)));
+	/* rop_enable */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 1) & 1) << 1);
+	/* fading_enable */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 2) & 1) << 2);
+	/* alpha_cal_mode_sel */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 4) & 1) << 3);
+	/* dst_dither_down */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 5) & 1) << 6);
+	/* gradient fill mode sel */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 6) & 1) << 7);
+	/* RGA_NN_QUANTIZE */
+	req->alpha_rop_flag |= (((req_rga->alpha_rop_flag >> 8) & 1) << 8);
+	req->dither_mode = req_rga->dither_mode;
+
+	/* RGA2 1106 add */
+	memcpy(&req->mosaic_info, &req_rga->mosaic_info, sizeof(req_rga->mosaic_info));
+
+	memcpy(&req->gauss_config, &req_rga->gauss_config, sizeof(req_rga->gauss_config));
+
+	if ((scheduler->data->feature & RGA_YIN_YOUT) &&
+	    rga_is_only_y_format(req->src.format) &&
+	    rga_is_only_y_format(req->dst.format))
+		req->yin_yout_en = true;
+
+	req->uvhds_mode = req_rga->uvhds_mode;
+	req->uvvds_mode = req_rga->uvvds_mode;
+
+	memcpy(&req->osd_info, &req_rga->osd_info, sizeof(req_rga->osd_info));
+
+	memcpy(&req->rgba5551_alpha, &req_rga->rgba5551_alpha, sizeof(req_rga->rgba5551_alpha));
+
+	if (((req_rga->alpha_rop_flag) & 1)) {
+		if ((req_rga->alpha_rop_flag >> 3) & 1) {
+			req->alpha_config.enable = true;
+
+			if ((req_rga->alpha_rop_flag >> 9) & 1) {
+				req->alpha_config.fg_pre_multiplied = false;
+				req->alpha_config.bg_pre_multiplied = false;
+			} else if (req->osd_info.enable) {
+				req->alpha_config.fg_pre_multiplied = true;
+				/* set dst(osd_block) real color mode */
+				req->alpha_config.bg_pre_multiplied = false;
+			} else {
+				req->alpha_config.fg_pre_multiplied = true;
+				req->alpha_config.bg_pre_multiplied = true;
+			}
+
+			req->alpha_config.fg_pixel_alpha_en = rga_is_alpha_format(req->src.format);
+			if (req->bitblt_mode)
+				req->alpha_config.bg_pixel_alpha_en =
+					rga_is_alpha_format(req->src1.format);
+			else
+				req->alpha_config.bg_pixel_alpha_en =
+					rga_is_alpha_format(req->dst.format);
+
+			if (req_rga->feature.global_alpha_en) {
+				if (req_rga->fg_global_alpha < 0xff) {
+					req->alpha_config.fg_global_alpha_en = true;
+					req->alpha_config.fg_global_alpha_value =
+						req_rga->fg_global_alpha;
+				} else if (!req->alpha_config.fg_pixel_alpha_en) {
+					req->alpha_config.fg_global_alpha_en = true;
+					req->alpha_config.fg_global_alpha_value = 0xff;
+				}
+
+				if (req_rga->bg_global_alpha < 0xff) {
+					req->alpha_config.bg_global_alpha_en = true;
+					req->alpha_config.bg_global_alpha_value =
+						req_rga->bg_global_alpha;
+				} else if (!req->alpha_config.bg_pixel_alpha_en) {
+					req->alpha_config.bg_global_alpha_en = true;
+					req->alpha_config.bg_global_alpha_value = 0xff;
+				}
+			} else {
+				req->alpha_config.bg_global_alpha_value = 0xff;
+				req->alpha_config.bg_global_alpha_value = 0xff;
+			}
+
+			req->alpha_config.mode = req_rga->PD_mode;
+		}
+	} else if (req_rga->gauss_config.size > 0) {
+		if (req_rga->feature.global_alpha_en) {
+			req->alpha_config.fg_global_alpha_en = true;
+			req->alpha_config.fg_global_alpha_value = req_rga->fg_global_alpha;
+		} else {
+			req->alpha_config.fg_global_alpha_value = 0xff;
+		}
+	}
+
+	if (req_rga->mmu_info.mmu_en && (req_rga->mmu_info.mmu_flag & 1) == 1) {
+		req->mmu_info.src0_mmu_flag = 1;
+		req->mmu_info.dst_mmu_flag = 1;
+
+		if (req_rga->mmu_info.mmu_flag >> 31) {
+			req->mmu_info.src0_mmu_flag =
+				((req_rga->mmu_info.mmu_flag >> 8) & 1);
+			req->mmu_info.src1_mmu_flag =
+				((req_rga->mmu_info.mmu_flag >> 9) & 1);
+			req->mmu_info.dst_mmu_flag =
+				((req_rga->mmu_info.mmu_flag >> 10) & 1);
+			req->mmu_info.els_mmu_flag =
+				((req_rga->mmu_info.mmu_flag >> 11) & 1);
+		} else {
+			if (req_rga->src.yrgb_addr >= 0xa0000000) {
+				req->mmu_info.src0_mmu_flag = 0;
+				req->src.yrgb_addr =
+					req_rga->src.yrgb_addr - 0x60000000;
+				req->src.uv_addr =
+					req_rga->src.uv_addr - 0x60000000;
+				req->src.v_addr =
+					req_rga->src.v_addr - 0x60000000;
+			}
+
+			if (req_rga->dst.yrgb_addr >= 0xa0000000) {
+				req->mmu_info.dst_mmu_flag = 0;
+				req->dst.yrgb_addr =
+					req_rga->dst.yrgb_addr - 0x60000000;
+			}
+
+			if (req_rga->pat.yrgb_addr >= 0xa0000000) {
+				req->mmu_info.src1_mmu_flag = 0;
+				req->src1.yrgb_addr =
+					req_rga->pat.yrgb_addr - 0x60000000;
+			}
+		}
+	}
+}
+
+static void rga2_soft_reset(struct rga_scheduler_t *scheduler)
+{
+	u32 i;
+	u32 reg;
+	u32 iommu_dte_addr = 0;
+
+	if (scheduler->data->mmu == RGA_IOMMU)
+		iommu_dte_addr = rga_read(RGA_IOMMU_DTE_ADDR, scheduler);
+
+	rga_write(m_RGA2_SYS_CTRL_ACLK_SRESET_P | m_RGA2_SYS_CTRL_CCLK_SRESET_P |
+		  m_RGA2_SYS_CTRL_RST_PROTECT_P,
+		  RGA2_SYS_CTRL, scheduler);
+
+	for (i = 0; i < RGA_RESET_TIMEOUT; i++) {
+		/* RGA_SYS_CTRL */
+		reg = rga_read(RGA2_SYS_CTRL, scheduler) & 1;
+
+		if (reg == 0)
+			break;
+
+		udelay(1);
+	}
+
+	if (scheduler->data->mmu == RGA_IOMMU) {
+		rga_write(iommu_dte_addr, RGA_IOMMU_DTE_ADDR, scheduler);
+		/* enable iommu */
+		rga_write(RGA_IOMMU_CMD_ENABLE_PAGING, RGA_IOMMU_COMMAND, scheduler);
+	}
+
+	if (i == RGA_RESET_TIMEOUT)
+		rga_err("%s[%#x] soft reset timeout.\n",
+			rga_get_core_name(scheduler->core), scheduler->core);
+	else
+		rga_log("%s[%#x] soft reset complete.\n",
+			rga_get_core_name(scheduler->core), scheduler->core);
+}
+
+static int rga2_check_param(struct rga_job *job,
+			    const struct rga_hw_data *data, const struct rga2_req *req)
+{
+	if (!((req->render_mode == COLOR_FILL_MODE))) {
+		if (unlikely(rga_hw_out_of_range(&data->input_range,
+						 req->src.act_w, req->src.act_h))) {
+			rga_job_err(job, "invalid src resolution act_w = %d, act_h = %d\n",
+				 req->src.act_w, req->src.act_h);
+			return -EINVAL;
+		}
+
+		if (unlikely(req->src.vir_w * rga_get_pixel_stride_from_format(req->src.format) >
+			     data->max_byte_stride * 8)) {
+			rga_job_err(job, "invalid src stride, stride = %d, max_byte_stride = %d\n",
+			       req->src.vir_w, data->max_byte_stride);
+			return -EINVAL;
+		}
+
+		if (unlikely(req->src.vir_w < req->src.act_w)) {
+			rga_job_err(job, "invalid src_vir_w act_w = %d, vir_w = %d\n",
+			       req->src.act_w, req->src.vir_w);
+			return -EINVAL;
+		}
+	}
+
+	if (unlikely(rga_hw_out_of_range(&data->output_range, req->dst.act_w, req->dst.act_h))) {
+		rga_job_err(job, "invalid dst resolution act_w = %d, act_h = %d\n",
+		       req->dst.act_w, req->dst.act_h);
+		return -EINVAL;
+	}
+
+	if (unlikely(req->dst.vir_w * rga_get_pixel_stride_from_format(req->dst.format) >
+		     data->max_byte_stride * 8)) {
+		rga_err("invalid dst stride, stride = %d, max_byte_stride = %d\n",
+		       req->dst.vir_w, data->max_byte_stride);
+		return -EINVAL;
+	}
+
+	if (unlikely(req->dst.vir_w < req->dst.act_w)) {
+		if (req->rotate_mode != 1) {
+			rga_err("invalid dst_vir_w act_h = %d, vir_h = %d\n",
+			       req->dst.act_w, req->dst.vir_w);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int rga2_align_check(struct rga_job *job, struct rga2_req *req)
+{
+	if (rga_is_yuv10bit_format(req->src.format))
+		if ((req->src.x_offset % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.act_w % 2))
+			rga_job_log(job, "err src wstride, 10bit yuv\n");
+	if (rga_is_yuv10bit_format(req->dst.format))
+		if ((req->dst.x_offset % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.act_w % 2))
+			rga_job_log(job, "err dst wstride, 10bit yuv\n");
+	if (rga_is_yuv8bit_format(req->src.format))
+		if ((req->src.x_offset % 2) || (req->src.y_offset % 2) ||
+		    (req->src.act_w % 2) || (req->src.act_w % 2))
+			rga_job_log(job, "err src wstride, 8bit yuv\n");
+	if (rga_is_yuv8bit_format(req->dst.format))
+		if ((req->dst.x_offset % 2) || (req->dst.y_offset % 2) ||
+		    (req->dst.act_w % 2) || (req->dst.act_w % 2))
+			rga_job_log(job, "err dst wstride, 8bit yuv\n");
+
+	return 0;
+}
+
+static void print_debug_info(struct rga_job *job, struct rga2_req *req)
+{
+	rga_job_log(job, "render_mode:%s,bitblit_mode=%d,rotate_mode:%s\n",
+		rga_get_render_mode_str(req->render_mode), req->bitblt_mode,
+		rga_get_rotate_mode_str(req->rotate_mode));
+
+	rga_job_log(job, "src: y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d\n",
+		 (unsigned long)req->src.yrgb_addr,
+		 (unsigned long)req->src.uv_addr,
+		 (unsigned long)req->src.v_addr,
+		 req->src.act_w, req->src.act_h,
+		 req->src.vir_w, req->src.vir_h);
+	rga_job_log(job, "src: xoff=%d yoff=%d format=%s\n",
+		req->src.x_offset, req->src.y_offset,
+		 rga_get_format_name(req->src.format));
+
+	if (req->src1.yrgb_addr != 0 || req->src1.uv_addr != 0
+		|| req->src1.v_addr != 0) {
+		rga_job_log(job, "src1: y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d\n",
+			 (unsigned long)req->src1.yrgb_addr,
+			 (unsigned long)req->src1.uv_addr,
+			 (unsigned long)req->src1.v_addr,
+			 req->src1.act_w, req->src1.act_h,
+			 req->src1.vir_w, req->src1.vir_h);
+		rga_job_log(job, "src1: xoff=%d yoff=%d format=%s\n",
+			req->src1.x_offset, req->src1.y_offset,
+			 rga_get_format_name(req->src1.format));
+	}
+
+	rga_job_log(job, "dst: y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d\n",
+		 (unsigned long)req->dst.yrgb_addr,
+		 (unsigned long)req->dst.uv_addr,
+		 (unsigned long)req->dst.v_addr,
+		 req->dst.act_w, req->dst.act_h,
+		 req->dst.vir_w, req->dst.vir_h);
+	rga_job_log(job, "dst: xoff=%d yoff=%d format=%s\n",
+		req->dst.x_offset, req->dst.y_offset,
+		 rga_get_format_name(req->dst.format));
+
+	rga_job_log(job, "mmu: src=%.2x src1=%.2x dst=%.2x els=%.2x\n",
+		req->mmu_info.src0_mmu_flag, req->mmu_info.src1_mmu_flag,
+		req->mmu_info.dst_mmu_flag, req->mmu_info.els_mmu_flag);
+	rga_job_log(job, "alpha: flag %x mode=%s\n",
+		req->alpha_rop_flag, rga_get_blend_mode_str(req->alpha_config.mode));
+	rga_job_log(job, "alpha: pre_multi=[%d,%d] pixl=[%d,%d] glb=[%d,%d]\n",
+		req->alpha_config.fg_pre_multiplied, req->alpha_config.bg_pre_multiplied,
+		req->alpha_config.fg_pixel_alpha_en, req->alpha_config.bg_pixel_alpha_en,
+		req->alpha_config.fg_global_alpha_en, req->alpha_config.bg_global_alpha_en);
+	rga_job_log(job, "alpha: fg_global_alpha=%x bg_global_alpha=%x\n",
+		req->alpha_config.fg_global_alpha_value, req->alpha_config.bg_global_alpha_value);
+	rga_job_log(job, "yuv2rgb mode is %x\n", req->yuv2rgb_mode);
+}
+
+static int rga2_init_reg(struct rga_job *job)
+{
+	struct rga2_req req;
+	int ret = 0;
+	struct rga_scheduler_t *scheduler = NULL;
+	ktime_t timestamp = ktime_get();
+
+	scheduler = job->scheduler;
+	if (unlikely(scheduler == NULL)) {
+		rga_job_err(job, "failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	memset(&req, 0x0, sizeof(req));
+
+	rga_cmd_to_rga2_cmd(scheduler, &job->rga_command_base, &req);
+	if (req.full_csc_en) {
+		memcpy(&job->full_csc, &job->rga_command_base.full_csc, sizeof(job->full_csc));
+		if (job->rga_command_base.feature.full_csc_clip_en) {
+			memcpy(&job->full_csc_clip, &job->rga_command_base.full_csc_clip,
+			       sizeof(job->full_csc_clip));
+		} else {
+			job->full_csc_clip.y.max = 0xff;
+			job->full_csc_clip.y.min = 0x0;
+			job->full_csc_clip.uv.max = 0xff;
+			job->full_csc_clip.uv.min = 0x0;
+		}
+
+	} else {
+		job->full_csc_clip.y.max = 0xff;
+		job->full_csc_clip.y.min = 0x0;
+		job->full_csc_clip.uv.max = 0xff;
+		job->full_csc_clip.uv.min = 0x0;
+	}
+	memcpy(&job->pre_intr_info, &job->rga_command_base.pre_intr_info,
+	       sizeof(job->pre_intr_info));
+
+	/* check value if legal */
+	ret = rga2_check_param(job, scheduler->data, &req);
+	if (ret == -EINVAL) {
+		rga_job_err(job, "req argument is inval\n");
+		return ret;
+	}
+
+	rga2_align_check(job, &req);
+
+	/* for debug */
+	if (DEBUGGER_EN(MSG))
+		print_debug_info(job, &req);
+
+	/* RGA2 mmu set */
+	if ((req.mmu_info.src0_mmu_flag & 1) || (req.mmu_info.src1_mmu_flag & 1) ||
+	    (req.mmu_info.dst_mmu_flag & 1) || (req.mmu_info.els_mmu_flag & 1)) {
+		if (scheduler->data->mmu != RGA_MMU) {
+			rga_job_err(job, "core[%d] has no MMU, please use physically contiguous memory.\n",
+				scheduler->core);
+			rga_job_err(job, "mmu_flag[src, src1, dst, els] = [0x%x, 0x%x, 0x%x, 0x%x]\n",
+				req.mmu_info.src0_mmu_flag, req.mmu_info.src1_mmu_flag,
+				req.mmu_info.dst_mmu_flag, req.mmu_info.els_mmu_flag);
+			return -EINVAL;
+		}
+
+		ret = rga_set_mmu_base(job, &req);
+		if (ret < 0) {
+			rga_job_err(job, "%s, [%d] set mmu info error\n", __func__,
+				 __LINE__);
+			return -EFAULT;
+		}
+	}
+
+	/* In slave mode, the current frame completion interrupt must be enabled. */
+	if (scheduler->data->mmu == RGA_IOMMU)
+		req.CMD_fin_int_enable = 1;
+
+	ret = rga2_gen_reg_info(scheduler, (uint8_t *)job->cmd_buf->vaddr, &req);
+	if (ret < 0) {
+		rga_job_err(job, "gen reg info error\n");
+		return -EINVAL;
+	}
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "generate register cost time %lld us\n",
+			ktime_us_delta(ktime_get(), timestamp));
+
+	return ret;
+}
+
+static void rga2_dump_read_back_sys_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	unsigned long flags;
+	uint32_t sys_reg[24] = {0};
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	for (i = 0; i < 24; i++)
+		sys_reg[i] = rga_read(RGA2_SYS_REG_BASE + i * 4, scheduler);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rga_job_log(job, "SYS_READ_BACK_REG\n");
+	for (i = 0; i < 6; i++)
+		rga_job_log(job, "0x%04x : %.8x %.8x %.8x %.8x\n",
+			RGA2_SYS_REG_BASE + i * 0x10,
+			sys_reg[0 + i * 4], sys_reg[1 + i * 4],
+			sys_reg[2 + i * 4], sys_reg[3 + i * 4]);
+}
+
+static void rga2_dump_read_back_csc_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	unsigned long flags;
+	uint32_t csc_reg[12] = {0};
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	for (i = 0; i < 12; i++)
+		csc_reg[i] = rga_read(RGA2_CSC_REG_BASE + i * 4, scheduler);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rga_job_log(job, "CSC_READ_BACK_REG\n");
+	for (i = 0; i < 3; i++)
+		rga_job_log(job, "0x%04x : %.8x %.8x %.8x %.8x\n",
+			RGA2_CSC_REG_BASE + i * 0x10,
+			csc_reg[0 + i * 4], csc_reg[1 + i * 4],
+			csc_reg[2 + i * 4], csc_reg[3 + i * 4]);
+}
+
+static void rga2_dump_read_back_cmd_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	unsigned long flags;
+	uint32_t cmd_reg[32] = {0};
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	for (i = 0; i < 32; i++)
+		cmd_reg[i] = rga_read(RGA2_CMD_REG_BASE + i * 4, scheduler);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rga_job_log(job, "CMD_READ_BACK_REG\n");
+	for (i = 0; i < 8; i++)
+		rga_job_log(job, "0x%04x : %.8x %.8x %.8x %.8x\n",
+			RGA2_CMD_REG_BASE + i * 0x10,
+			cmd_reg[0 + i * 4], cmd_reg[1 + i * 4],
+			cmd_reg[2 + i * 4], cmd_reg[3 + i * 4]);
+}
+
+static void rga2_dump_read_back_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	rga2_dump_read_back_sys_reg(job, scheduler);
+	rga2_dump_read_back_csc_reg(job, scheduler);
+	rga2_dump_read_back_cmd_reg(job, scheduler);
+}
+
+static void rga2_set_pre_intr_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	uint32_t reg;
+
+	if (job->pre_intr_info.read_intr_en) {
+		reg = s_RGA2_READ_LINE_SW_INTR_LINE_RD_TH(job->pre_intr_info.read_threshold);
+		rga_write(reg, RGA2_READ_LINE_CNT, scheduler);
+	}
+
+	if (job->pre_intr_info.write_intr_en) {
+		reg = s_RGA2_WRITE_LINE_SW_INTR_LINE_WR_START(job->pre_intr_info.write_start);
+		reg = ((reg & (~m_RGA2_WRITE_LINE_SW_INTR_LINE_WR_STEP)) |
+		       (s_RGA2_WRITE_LINE_SW_INTR_LINE_WR_STEP(job->pre_intr_info.write_step)));
+		rga_write(reg, RGA2_WRITE_LINE_CNT, scheduler);
+	}
+
+	reg = rga_read(RGA2_SYS_CTRL, scheduler);
+	reg = ((reg & (~m_RGA2_SYS_CTRL_HOLD_MODE_EN)) |
+	       (s_RGA2_SYS_CTRL_HOLD_MODE_EN(job->pre_intr_info.read_hold_en)));
+	rga_write(reg, RGA2_SYS_CTRL, scheduler);
+
+	reg = rga_read(RGA2_INT, scheduler);
+	reg = (reg | s_RGA2_INT_LINE_RD_CLEAR(0x1) | s_RGA2_INT_LINE_WR_CLEAR(0x1));
+	reg = ((reg & (~m_RGA2_INT_LINE_RD_EN)) |
+	       (s_RGA2_INT_LINE_RD_EN(job->pre_intr_info.read_intr_en)));
+	reg = ((reg & (~m_RGA2_INT_LINE_WR_EN)) |
+	       (s_RGA2_INT_LINE_WR_EN(job->pre_intr_info.write_intr_en)));
+	rga_write(reg, RGA2_INT, scheduler);
+}
+
+static void rga2_set_reg_full_csc(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	/* full csc coefficient */
+	/* Y coefficient */
+	rga_write(job->full_csc.coe_y.r_v |
+		  (job->full_csc_clip.y.max << 16) | (job->full_csc_clip.y.min << 24),
+		  RGA2_DST_CSC_00, scheduler);
+	rga_write(job->full_csc.coe_y.g_y |
+		  (job->full_csc_clip.uv.max << 16) | (job->full_csc_clip.uv.min << 24),
+		  RGA2_DST_CSC_01, scheduler);
+	rga_write(job->full_csc.coe_y.b_u, RGA2_DST_CSC_02, scheduler);
+	rga_write(job->full_csc.coe_y.off, RGA2_DST_CSC_OFF0, scheduler);
+
+	/* U coefficient */
+	rga_write(job->full_csc.coe_u.r_v, RGA2_DST_CSC_10, scheduler);
+	rga_write(job->full_csc.coe_u.g_y, RGA2_DST_CSC_11, scheduler);
+	rga_write(job->full_csc.coe_u.b_u, RGA2_DST_CSC_12, scheduler);
+	rga_write(job->full_csc.coe_u.off, RGA2_DST_CSC_OFF1, scheduler);
+
+	/* V coefficient */
+	rga_write(job->full_csc.coe_v.r_v, RGA2_DST_CSC_20, scheduler);
+	rga_write(job->full_csc.coe_v.g_y, RGA2_DST_CSC_21, scheduler);
+	rga_write(job->full_csc.coe_v.b_u, RGA2_DST_CSC_22, scheduler);
+	rga_write(job->full_csc.coe_v.off, RGA2_DST_CSC_OFF2, scheduler);
+}
+
+static int rga2_set_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	bool master_mode_en;
+	uint32_t sys_ctrl;
+	uint32_t *cmd;
+	unsigned long flags;
+	ktime_t now = ktime_get();
+
+	cmd = job->cmd_buf->vaddr;
+
+	/*
+	 * Currently there is no iova allocated for storing cmd for the IOMMU device,
+	 * so the iommu device needs to use the slave mode.
+	 */
+	if (scheduler->data->mmu != RGA_IOMMU)
+		master_mode_en = true;
+	else
+		master_mode_en = false;
+
+	if (DEBUGGER_EN(REG)) {
+		rga2_dump_read_back_sys_reg(job, scheduler);
+		rga2_dump_read_back_csc_reg(job, scheduler);
+
+		rga_job_log(job, "CMD_REG\n");
+		for (i = 0; i < 8; i++)
+			rga_job_log(job, "0x%04x : %.8x %.8x %.8x %.8x\n",
+				RGA2_CMD_REG_BASE + i * 0x10,
+				cmd[0 + i * 4], cmd[1 + i * 4],
+				cmd[2 + i * 4], cmd[3 + i * 4]);
+	}
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	/* sys_reg init */
+	sys_ctrl = m_RGA2_SYS_CTRL_AUTO_CKG |
+		   m_RGA2_SYS_CTRL_DST_WR_OPT_DIS |
+		   m_RGA2_SYS_CTRL_SRC0YUV420SP_RD_OPT_DIS;
+
+	if (rga_hw_has_issue(scheduler, RGA_HW_ISSUE_DIS_AUTO_RST)) {
+		/*
+		 *   when RGA is running continuously, disabling auto_rst
+		 * requires resetting core_clk.
+		 */
+		rga_write(m_RGA2_SYS_CTRL_AUTO_CKG | m_RGA2_SYS_CTRL_CCLK_SRESET_P,
+			  RGA2_SYS_CTRL, scheduler);
+	} else {
+		sys_ctrl |= m_RGA2_SYS_CTRL_AUTO_RST;
+	}
+
+	if (job->pre_intr_info.enable)
+		rga2_set_pre_intr_reg(job, scheduler);
+
+	if (job->full_csc.flag)
+		rga2_set_reg_full_csc(job, scheduler);
+
+	/* All CMD finish int */
+	rga_write(rga_read(RGA2_INT, scheduler) |
+		  m_RGA2_INT_ERROR_ENABLE_MASK | m_RGA2_INT_ALL_CMD_DONE_INT_EN,
+		  RGA2_INT, scheduler);
+
+	if (master_mode_en) {
+		/* master mode */
+		sys_ctrl |= s_RGA2_SYS_CTRL_CMD_MODE(1);
+
+		/* set cmd_addr */
+		rga_write(job->cmd_buf->dma_addr, RGA2_CMD_BASE, scheduler);
+		rga_write(sys_ctrl, RGA2_SYS_CTRL, scheduler);
+		rga_write(rga_read(RGA2_CMD_CTRL, scheduler) | m_RGA2_CMD_CTRL_CMD_LINE_ST_P,
+			  RGA2_CMD_CTRL, scheduler);
+	} else {
+		/* slave mode */
+		sys_ctrl |= s_RGA2_SYS_CTRL_CMD_MODE(0) | m_RGA2_SYS_CTRL_CMD_OP_ST_P;
+
+		/* set cmd_reg */
+		for (i = 0; i <= 32; i++)
+			rga_write(cmd[i], 0x100 + i * 4, scheduler);
+
+		rga_write(sys_ctrl, RGA2_SYS_CTRL, scheduler);
+	}
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "set register cost time %lld us\n",
+			ktime_us_delta(ktime_get(), now));
+
+	job->timestamp.hw_execute = now;
+	job->timestamp.hw_recode = now;
+	job->session->last_active = now;
+
+	if (DEBUGGER_EN(REG))
+		rga2_dump_read_back_reg(job, scheduler);
+
+	return 0;
+}
+
+static int rga2_get_version(struct rga_scheduler_t *scheduler)
+{
+	u32 major_version, minor_version, svn_version;
+	u32 reg_version;
+
+	if (!scheduler) {
+		rga_err("scheduler is null\n");
+		return -EINVAL;
+	}
+
+	reg_version = rga_read(RGA2_VERSION_NUM, scheduler);
+
+	major_version = (reg_version & RGA2_MAJOR_VERSION_MASK) >> 24;
+	minor_version = (reg_version & RGA2_MINOR_VERSION_MASK) >> 20;
+	svn_version = (reg_version & RGA2_SVN_VERSION_MASK);
+
+	/*
+	 * some old rga ip has no rga version register, so force set to 2.00
+	 */
+	if (!major_version && !minor_version)
+		major_version = 2;
+
+	snprintf(scheduler->version.str, 10, "%x.%01x.%05x", major_version,
+		 minor_version, svn_version);
+
+	scheduler->version.major = major_version;
+	scheduler->version.minor = minor_version;
+	scheduler->version.revision = svn_version;
+
+	return 0;
+}
+
+static int rga2_read_back_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	if (job->rga_command_base.osd_info.enable) {
+		job->rga_command_base.osd_info.cur_flags0 = rga_read(RGA2_OSD_CUR_FLAGS0,
+								     scheduler);
+		job->rga_command_base.osd_info.cur_flags1 = rga_read(RGA2_OSD_CUR_FLAGS1,
+								     scheduler);
+	}
+
+	return 0;
+}
+
+static int rga2_read_status(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	job->intr_status = rga_read(RGA2_INT, scheduler);
+	job->hw_status = rga_read(RGA2_STATUS2, scheduler);
+	job->cmd_status = rga_read(RGA2_STATUS1, scheduler);
+	job->work_cycle = rga_read(RGA2_WORK_CNT, scheduler);
+
+	return 0;
+}
+
+static void rga2_clear_intr(struct rga_scheduler_t *scheduler)
+{
+	rga_write(rga_read(RGA2_INT, scheduler) |
+		  (m_RGA2_INT_ERROR_CLEAR_MASK |
+		   m_RGA2_INT_ALL_CMD_DONE_INT_CLEAR | m_RGA2_INT_NOW_CMD_DONE_INT_CLEAR |
+		   m_RGA2_INT_LINE_RD_CLEAR | m_RGA2_INT_LINE_WR_CLEAR),
+		  RGA2_INT, scheduler);
+}
+
+static int rga2_irq(struct rga_scheduler_t *scheduler)
+{
+	struct rga_job *job = scheduler->running_job;
+
+	/* The hardware interrupt top-half don't need to lock the scheduler. */
+	if (job == NULL) {
+		rga2_clear_intr(scheduler);
+		rga_err("core[%d], invalid job, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x], WORK_CYCLE[0x%x(%d)]\n",
+			scheduler->core, rga_read(RGA2_INT, scheduler),
+			rga_read(RGA2_STATUS2, scheduler), rga_read(RGA2_STATUS1, scheduler),
+			rga_read(RGA2_WORK_CNT, scheduler), rga_read(RGA2_WORK_CNT, scheduler));
+
+		return IRQ_HANDLED;
+	}
+
+	if (test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		rga2_clear_intr(scheduler);
+
+		return IRQ_WAKE_THREAD;
+	}
+
+	scheduler->ops->read_status(job, scheduler);
+
+	if (DEBUGGER_EN(INT_FLAG))
+		rga_job_log(job, "irq handler, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x], WORK_CYCLE[0x%x(%d)]\n",
+			job->intr_status, job->hw_status, job->cmd_status,
+			job->work_cycle, job->work_cycle);
+
+	if (job->intr_status &
+	    (m_RGA2_INT_CUR_CMD_DONE_INT_FLAG | m_RGA2_INT_ALL_CMD_DONE_INT_FLAG)) {
+		set_bit(RGA_JOB_STATE_FINISH, &job->state);
+	} else if (job->intr_status & m_RGA2_INT_ERROR_FLAG_MASK) {
+		set_bit(RGA_JOB_STATE_INTR_ERR, &job->state);
+
+		rga_job_err(job, "irq handler err! INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x], WORK_CYCLE[0x%x(%d)]\n",
+			job->intr_status, job->hw_status, job->cmd_status,
+			job->work_cycle, job->work_cycle);
+
+		scheduler->ops->soft_reset(scheduler);
+	}
+
+	rga2_clear_intr(scheduler);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int rga2_isr_thread(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	if (DEBUGGER_EN(INT_FLAG))
+		rga_job_log(job, "isr thread, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x]\n",
+			rga_read(RGA2_INT, scheduler),
+			rga_read(RGA2_STATUS2, scheduler),
+			rga_read(RGA2_STATUS1, scheduler));
+
+	if (test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		if (job->hw_status & m_RGA2_STATUS2_RPP_ERROR)
+			rga_job_err(job, "RGA current status: rpp error!\n");
+		if (job->hw_status & m_RGA2_STATUS2_BUS_ERROR)
+			rga_job_err(job, "RGA current status: bus error!\n");
+
+		if (job->intr_status & m_RGA2_INT_ERROR_INT_FLAG) {
+			rga_job_err(job, "RGA bus error intr, please check your configuration and buffer.\n");
+			job->ret = -EFAULT;
+		} else if (job->intr_status & m_RGA2_INT_MMU_INT_FLAG) {
+			rga_job_err(job, "mmu failed, please check size of the buffer or whether the buffer has been freed.\n");
+			job->ret = -EACCES;
+		}
+
+		if (job->ret == 0) {
+			rga_job_err(job, "rga intr error[0x%x]!\n", job->intr_status);
+			job->ret = -EFAULT;
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+const struct rga_backend_ops rga2_ops = {
+	.get_version = rga2_get_version,
+	.set_reg = rga2_set_reg,
+	.init_reg = rga2_init_reg,
+	.soft_reset = rga2_soft_reset,
+	.read_back_reg = rga2_read_back_reg,
+	.read_status = rga2_read_status,
+	.irq = rga2_irq,
+	.isr_thread = rga2_isr_thread,
+};
diff --git a/drivers/video/rockchip/rga3/rga3_reg_info.c b/drivers/video/rockchip/rga3/rga3_reg_info.c
new file mode 100644
index 0000000000000..9973ea684db4b
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga3_reg_info.c
@@ -0,0 +1,2246 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga3_reg_info.h"
+#include "rga_dma_buf.h"
+#include "rga_iommu.h"
+#include "rga_common.h"
+#include "rga_debugger.h"
+#include "rga_hw_config.h"
+
+#define FACTOR_MAX ((int)(2 << 15))
+
+static void RGA3_set_reg_win0_info(u8 *base, struct rga3_req *msg)
+{
+	u32 *bRGA3_WIN0_RD_CTRL;
+	u32 *bRGA3_WIN0_Y_BASE, *bRGA3_WIN0_U_BASE, *bRGA3_WIN0_V_BASE;
+	u32 *bRGA3_WIN0_VIR_STRIDE;
+	u32 *bRGA3_WIN0_UV_VIR_STRIDE;
+	u32 *bRGA3_WIN0_SRC_SIZE;
+	u32 *bRGA3_WIN0_ACT_OFF;
+	u32 *bRGA3_WIN0_ACT_SIZE;
+	u32 *bRGA3_WIN0_DST_SIZE;
+
+	u32 *bRGA3_WIN0_SCL_FAC;
+	/* Not used yet. */
+	// u32 *bRGA3_WIN0_FBC_OFF;
+
+	u32 sw = 0, sh = 0;
+	u32 dw = 0, dh = 0;
+	u32 param_x = 0, param_y = 0;
+	u8 x_up = 0, y_up = 0, x_by = 0, y_by = 0;
+
+	u32 reg = 0;
+
+	u8 win_format = 0;
+	u8 win_yc_swp = 0;
+
+	/* rb swap on RGB, uv swap on YUV */
+	u8 win_pix_swp = 0;
+
+	/*
+	 * 1: Semi planar, for yuv 4:2:x
+	 * 2: Interleaved (yuyv), for yuv422 8bit only ，RGB
+	 */
+	u8 win_interleaved = 1;
+
+	/* enable r2y or y2r */
+	u8 win_r2y = 0;
+	u8 win_y2r = 0;
+
+	u8 rotate_mode = 0;
+	u8 xmirror = 0;
+	u8 ymirror = 0;
+
+	u8 pixel_width = 1;
+	u8 yuv10 = 0;
+
+	u32 stride = 0;
+	u32 uv_stride = 0;
+
+	bRGA3_WIN0_RD_CTRL = (u32 *) (base + RGA3_WIN0_RD_CTRL_OFFSET);
+
+	bRGA3_WIN0_Y_BASE = (u32 *) (base + RGA3_WIN0_Y_BASE_OFFSET);
+	bRGA3_WIN0_U_BASE = (u32 *) (base + RGA3_WIN0_U_BASE_OFFSET);
+	bRGA3_WIN0_V_BASE = (u32 *) (base + RGA3_WIN0_V_BASE_OFFSET);
+
+	bRGA3_WIN0_VIR_STRIDE = (u32 *) (base + RGA3_WIN0_VIR_STRIDE_OFFSET);
+	bRGA3_WIN0_UV_VIR_STRIDE =
+		(u32 *) (base + RGA3_WIN0_UV_VIR_STRIDE_OFFSET);
+
+	/* Not used yet. */
+	// bRGA3_WIN0_FBC_OFF = (u32 *) (base + RGA3_WIN0_FBC_OFF_OFFSET);
+	bRGA3_WIN0_ACT_OFF = (u32 *) (base + RGA3_WIN0_ACT_OFF_OFFSET);
+	bRGA3_WIN0_SRC_SIZE = (u32 *) (base + RGA3_WIN0_SRC_SIZE_OFFSET);
+	bRGA3_WIN0_ACT_SIZE = (u32 *) (base + RGA3_WIN0_ACT_SIZE_OFFSET);
+	bRGA3_WIN0_DST_SIZE = (u32 *) (base + RGA3_WIN0_DST_SIZE_OFFSET);
+
+	bRGA3_WIN0_SCL_FAC = (u32 *) (base + RGA3_WIN0_SCL_FAC_OFFSET);
+
+	if (msg->win0.rotate_mode != 0) {
+		rotate_mode = msg->rotate_mode & RGA3_ROT_BIT_ROT_90 ? 1 : 0;
+		xmirror = msg->rotate_mode & RGA3_ROT_BIT_X_MIRROR ? 1 : 0;
+		ymirror = msg->rotate_mode & RGA3_ROT_BIT_Y_MIRROR ? 1 : 0;
+	}
+
+	/* scale */
+	dw = msg->win0.dst_act_w;
+	dh = msg->win0.dst_act_h;
+
+	if (rotate_mode) {
+		sh = msg->win0.src_act_w;
+		sw = msg->win0.src_act_h;
+	} else {
+		sw = msg->win0.src_act_w;
+		sh = msg->win0.src_act_h;
+	}
+
+	if (sw > dw) {
+		x_up = 0;
+		x_by = 0;
+	} else if (sw < dw) {
+		x_up = 1;
+		x_by = 0;
+	} else {
+		x_up = 0;
+		x_by = 1;
+	}
+
+	if (sh > dh) {
+		y_up = 0;
+		y_by = 0;
+	} else if (sh < dh) {
+		y_up = 1;
+		y_by = 0;
+	} else {
+		y_up = 0;
+		y_by = 1;
+	}
+
+	if (x_by == 1 && x_up == 0)
+		param_x = 0;
+	else if (x_up == 1 && x_by == 0) {
+		param_x = FACTOR_MAX * (sw - 1) / (dw - 1);
+		/* even multiples of 128 require a scaling factor -1 */
+		if ((FACTOR_MAX * (sw - 1)) % (dw - 1) == 0)
+			param_x = param_x - 1;
+	} else
+		param_x = FACTOR_MAX * (dw - 1) / (sw - 1) + 1;
+
+	if (y_by == 1 && y_up == 0)
+		param_y = 0;
+	else if (y_up == 1 && y_by == 0) {
+		param_y = FACTOR_MAX * (sh - 1) / (dh - 1);
+		/* even multiples of 128 require a scaling factor -1 */
+		if ((FACTOR_MAX * (sh - 1)) % (dh - 1) == 0)
+			param_y = param_y - 1;
+	} else
+		param_y = FACTOR_MAX * (dh - 1) / (sh - 1) + 1;
+
+	switch (msg->win0.format) {
+	case RGA_FORMAT_RGBA_8888:
+		win_format = 0x8;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		win_format = 0x6;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_ARGB_8888:
+		win_format = 0x9;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_ABGR_8888:
+		win_format = 0x7;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_888:
+		win_format = 0x5;
+		pixel_width = 3;
+		win_interleaved = 2;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_888:
+		win_format = 0x5;
+		pixel_width = 3;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_565:
+		win_format = 0x4;
+		pixel_width = 2;
+		win_interleaved = 2;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_565:
+		win_format = 0x4;
+		pixel_width = 2;
+		win_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YVYU_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 1;
+		win_yc_swp = 1;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_VYUY_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 1;
+		win_yc_swp = 0;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_YUYV_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 0;
+		win_yc_swp = 1;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_UYVY_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 0;
+		win_yc_swp = 0;
+		win_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		win_format = 0x1;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+		win_format = 0x0;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP:
+		win_format = 0x1;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP:
+		win_format = 0x0;
+		win_pix_swp = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+		win_format = 0x2;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		win_format = 0x2;
+		yuv10 = 1;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+		win_format = 0x3;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		win_format = 0x3;
+		yuv10 = 1;
+		win_pix_swp = 1;
+		break;
+	};
+
+	if (rga_is_rgb_format(msg->win0.format) &&
+	    rga_is_yuv_format(msg->wr.format))
+		win_r2y = 1;
+	if (rga_is_yuv_format(msg->win0.format) &&
+	    rga_is_rgb_format(msg->wr.format))
+		win_y2r = 1;
+
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_R2Y_EN)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_R2Y_EN(win_r2y)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_Y2R_EN)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_Y2R_EN(win_y2r)));
+
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_PIC_FORMAT)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_PIC_FORMAT(win_format)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_PIX_SWAP)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_PIX_SWAP(win_pix_swp)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_YC_SWAP)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_YC_SWAP(win_yc_swp)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_FORMAT)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_FORMAT(win_interleaved)));
+
+	if (win_r2y == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE(msg->win0.r2y_mode)));
+	} else if (win_y2r == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE(msg->win0.y2r_mode)));
+	}
+
+	/* rotate & mirror */
+	if (msg->win0.rotate_mode == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_ROT)) |
+			 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_ROT(rotate_mode)));
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_XMIRROR)) |
+			 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_XMIRROR(xmirror)));
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_YMIRROR)) |
+			 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_YMIRROR(ymirror)));
+	}
+
+	/* scale */
+	*bRGA3_WIN0_SCL_FAC = param_x | param_y << 16;
+
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_BY)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_BY(x_by)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_UP)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_HOR_UP(x_up)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_BY)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_BY(y_by)));
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_UP)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_VER_UP(y_up)));
+
+	/* rd_mode */
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_MODE)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_RD_MODE(msg->win0.rd_mode)));
+	/* win0 enable */
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_ENABLE)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_ENABLE(msg->win0.enable)));
+
+	reg =
+		((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT)) |
+		 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT(1)));
+
+	/* Only on raster mode, yuv 10bit can change to compact or set endian */
+	if (msg->win0.rd_mode == RGA_RASTER_MODE && yuv10 == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT)) |
+			 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_YUV10B_COMPACT
+			 (msg->win0.is_10b_compact)));
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_ENDIAN_MODE)) |
+			 (s_RGA3_WIN0_RD_CTRL_SW_WIN0_ENDIAN_MODE
+			 (msg->win0.is_10b_endian)));
+	}
+
+	*bRGA3_WIN0_RD_CTRL = reg;
+
+	switch (msg->win0.rd_mode) {
+	case 0: /* raster */
+		stride = (((msg->win0.vir_w * pixel_width) + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win0.format))
+			uv_stride = ((msg->win0.vir_w + 15) & ~15) >> 2;
+		else
+			uv_stride = stride;
+		break;
+
+	case 1: /* fbc */
+		stride = ((msg->win0.vir_w + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win0.format))
+			uv_stride = ((msg->win0.vir_w + 15) & ~15) >> 2;
+		else
+			uv_stride = stride;
+		break;
+
+	case 2: /* tile 8*8 */
+		/*
+		 * tile 8*8 mode 8 lines of data are read/written at one time,
+		 * so stride needs * 8. YUV420 only has 4 lines of UV data, so
+		 * it needs to >>1.
+		 */
+		stride = (((msg->win0.vir_w * pixel_width * 8) + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win0.format))
+			uv_stride = ((((msg->win0.vir_w * 8) + 15) & ~15) >> 1) >> 2;
+		else
+			uv_stride = stride;
+		break;
+	}
+
+	*bRGA3_WIN0_Y_BASE = (u32) msg->win0.yrgb_addr;
+	*bRGA3_WIN0_U_BASE = (u32) msg->win0.uv_addr;
+	*bRGA3_WIN0_V_BASE = (u32) msg->win0.v_addr;
+
+	*bRGA3_WIN0_VIR_STRIDE = stride;
+	*bRGA3_WIN0_UV_VIR_STRIDE = uv_stride;
+
+	*bRGA3_WIN0_ACT_OFF = msg->win0.x_offset | (msg->win0.y_offset << 16);
+	/* fbcd offset */
+	/*
+	 *	*bRGA3_WIN0_FBC_OFF = msg->win0.fbc_x_offset |
+	 *		 (msg->win0.fbc_y_offset << 16);
+	 */
+
+	/* do not use win0 src size except fbcd */
+	/* in FBCD, src_width needs to be aligned at 16 */
+	*bRGA3_WIN0_SRC_SIZE = ALIGN(msg->win0.src_act_w + msg->win0.x_offset, 16) |
+			       (ALIGN(msg->win0.y_offset + msg->win0.src_act_h, 16) << 16);
+	*bRGA3_WIN0_ACT_SIZE =
+		msg->win0.src_act_w | (msg->win0.src_act_h << 16);
+	*bRGA3_WIN0_DST_SIZE =
+		msg->win0.dst_act_w | (msg->win0.dst_act_h << 16);
+}
+
+static void RGA3_set_reg_win1_info(u8 *base, struct rga3_req *msg)
+{
+	u32 *bRGA3_WIN1_RD_CTRL;
+	u32 *bRGA3_WIN1_Y_BASE, *bRGA3_WIN1_U_BASE, *bRGA3_WIN1_V_BASE;
+	u32 *bRGA3_WIN1_VIR_STRIDE;
+	u32 *bRGA3_WIN1_UV_VIR_STRIDE;
+	u32 *bRGA3_WIN1_SRC_SIZE;
+	u32 *bRGA3_WIN1_ACT_OFF;
+	u32 *bRGA3_WIN1_ACT_SIZE;
+	u32 *bRGA3_WIN1_DST_SIZE;
+
+	u32 *bRGA3_WIN1_SCL_FAC;
+	/* Not used yet. */
+	// u32 *bRGA3_WIN1_FBC_OFF;
+
+	u32 sw = 0, sh = 0;
+	u32 dw = 0, dh = 0;
+	u32 param_x = 0, param_y = 0;
+	u8 x_up = 0, y_up = 0, x_by = 0, y_by = 0;
+
+	u32 reg = 0;
+
+	u8 win_format = 0;
+	u8 win_yc_swp = 0;
+
+	/* rb swap on RGB, uv swap on YUV */
+	u8 win_pix_swp = 0;
+
+	/*
+	 * 1: Semi planar, for yuv 4:2:x
+	 * 2: Interleaved (yuyv), for yuv422 8bit only ，RGB
+	 */
+	u8 win_interleaved = 1;
+
+	u8 pixel_width = 1;
+	u8 yuv10 = 0;
+
+	/* enable r2y or y2r */
+	u8 win_r2y = 0;
+	u8 win_y2r = 0;
+
+	u8 rotate_mode = 0;
+	u8 xmirror = 0;
+	u8 ymirror = 0;
+
+	u32 stride = 0;
+	u32 uv_stride = 0;
+
+	bRGA3_WIN1_RD_CTRL = (u32 *) (base + RGA3_WIN1_RD_CTRL_OFFSET);
+
+	bRGA3_WIN1_Y_BASE = (u32 *) (base + RGA3_WIN1_Y_BASE_OFFSET);
+	bRGA3_WIN1_U_BASE = (u32 *) (base + RGA3_WIN1_U_BASE_OFFSET);
+	bRGA3_WIN1_V_BASE = (u32 *) (base + RGA3_WIN1_V_BASE_OFFSET);
+
+	bRGA3_WIN1_VIR_STRIDE = (u32 *) (base + RGA3_WIN1_VIR_STRIDE_OFFSET);
+	bRGA3_WIN1_UV_VIR_STRIDE =
+		(u32 *) (base + RGA3_WIN1_UV_VIR_STRIDE_OFFSET);
+
+	/* Not used yet. */
+	// bRGA3_WIN1_FBC_OFF = (u32 *) (base + RGA3_WIN1_FBC_OFF_OFFSET);
+	bRGA3_WIN1_ACT_OFF = (u32 *) (base + RGA3_WIN1_ACT_OFF_OFFSET);
+	bRGA3_WIN1_SRC_SIZE = (u32 *) (base + RGA3_WIN1_SRC_SIZE_OFFSET);
+	bRGA3_WIN1_ACT_SIZE = (u32 *) (base + RGA3_WIN1_ACT_SIZE_OFFSET);
+	bRGA3_WIN1_DST_SIZE = (u32 *) (base + RGA3_WIN1_DST_SIZE_OFFSET);
+
+	bRGA3_WIN1_SCL_FAC = (u32 *) (base + RGA3_WIN1_SCL_FAC_OFFSET);
+
+	if (msg->win1.rotate_mode != 0) {
+		rotate_mode = msg->rotate_mode & RGA3_ROT_BIT_ROT_90 ? 1 : 0;
+		xmirror = msg->rotate_mode & RGA3_ROT_BIT_X_MIRROR ? 1 : 0;
+		ymirror = msg->rotate_mode & RGA3_ROT_BIT_Y_MIRROR ? 1 : 0;
+	}
+
+	/* scale */
+	dw = msg->win1.dst_act_w;
+	dh = msg->win1.dst_act_h;
+
+	if (rotate_mode) {
+		sh = msg->win1.src_act_w;
+		sw = msg->win1.src_act_h;
+	} else {
+		sw = msg->win1.src_act_w;
+		sh = msg->win1.src_act_h;
+	}
+
+	if (sw > dw) {
+		x_up = 0;
+		x_by = 0;
+	} else if (sw < dw) {
+		x_up = 1;
+		x_by = 0;
+	} else {
+		x_up = 0;
+		x_by = 1;
+	}
+
+	if (sh > dh) {
+		y_up = 0;
+		y_by = 0;
+	} else if (sh < dh) {
+		y_up = 1;
+		y_by = 0;
+	} else {
+		y_up = 0;
+		y_by = 1;
+	}
+
+	if (x_by == 1)
+		param_x = 0;
+	else if (x_up == 1) {
+		param_x = (FACTOR_MAX * (sw - 1)) / (dw - 1);
+		/* even multiples of 128 require a scaling factor -1 */
+		if ((FACTOR_MAX * (sw - 1)) % (dw - 1) == 0)
+			param_x = param_x - 1;
+	} else
+		param_x = (FACTOR_MAX * (dw - 1)) / (sw - 1) + 1;
+
+	if (y_by == 1)
+		param_y = 0;
+	else if (y_up == 1) {
+		param_y = (FACTOR_MAX * (sh - 1)) / (dh - 1);
+		/* even multiples of 128 require a scaling factor -1 */
+		if ((FACTOR_MAX * (sh - 1)) % (dh - 1) == 0)
+			param_y = param_y - 1;
+	} else
+		param_y = (FACTOR_MAX * (dh - 1)) / (sh - 1) + 1;
+
+	switch (msg->win1.format) {
+	case RGA_FORMAT_RGBA_8888:
+		win_format = 0x8;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		win_format = 0x6;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_ARGB_8888:
+		win_format = 0x9;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_ABGR_8888:
+		win_format = 0x7;
+		pixel_width = 4;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_888:
+		win_format = 0x5;
+		pixel_width = 3;
+		win_interleaved = 2;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_888:
+		win_format = 0x5;
+		pixel_width = 3;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_565:
+		win_format = 0x4;
+		pixel_width = 2;
+		win_interleaved = 2;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_565:
+		win_format = 0x4;
+		pixel_width = 2;
+		win_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YVYU_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 1;
+		win_yc_swp = 1;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_VYUY_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 1;
+		win_yc_swp = 0;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_YUYV_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 0;
+		win_yc_swp = 1;
+		win_interleaved = 2;
+		break;
+	case RGA_FORMAT_UYVY_422:
+		win_format = 0x1;
+		pixel_width = 2;
+		win_pix_swp = 0;
+		win_yc_swp = 0;
+		win_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		win_format = 0x1;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+		win_format = 0x0;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP:
+		win_format = 0x1;
+		win_pix_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP:
+		win_format = 0x0;
+		win_pix_swp = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+		win_format = 0x2;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		win_format = 0x2;
+		win_pix_swp = 1;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+		win_format = 0x3;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		win_format = 0x3;
+		win_pix_swp = 1;
+		yuv10 = 1;
+		break;
+	};
+
+	if (rga_is_rgb_format(msg->win1.format) &&
+	    rga_is_yuv_format(msg->wr.format))
+		win_r2y = 1;
+	if (rga_is_yuv_format(msg->win1.format) &&
+	    rga_is_rgb_format(msg->wr.format))
+		win_y2r = 1;
+
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_R2Y_EN)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_R2Y_EN(win_r2y)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_Y2R_EN)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_Y2R_EN(win_y2r)));
+
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_PIC_FORMAT)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_PIC_FORMAT(win_format)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_PIX_SWAP)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_PIX_SWAP(win_pix_swp)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_YC_SWAP)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_YC_SWAP(win_yc_swp)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_FORMAT)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_FORMAT(win_interleaved)));
+
+	if (win_r2y == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE(msg->win1.r2y_mode)));
+	} else if (win_y2r == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE)) |
+			(s_RGA3_WIN0_RD_CTRL_SW_WIN0_CSC_MODE(msg->win1.y2r_mode)));
+	}
+
+	/* rotate & mirror */
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_ROT)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_ROT(rotate_mode)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_XMIRROR)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_XMIRROR(xmirror)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_YMIRROR)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_YMIRROR(ymirror)));
+	//warning: TRM not complete
+	/* scale */
+	*bRGA3_WIN1_SCL_FAC = param_x | param_y << 16;
+
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_BY)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_BY(x_by)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_UP)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_HOR_UP(x_up)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_BY)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_BY(y_by)));
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_UP)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_VER_UP(y_up)));
+
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT(1)));
+
+	/* Only on roster mode, yuv 10bit can change to compact or set endian */
+	if (msg->win1.rd_mode == RGA_RASTER_MODE && yuv10 == 1) {
+		reg =
+			((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT)) |
+			 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_YUV10B_COMPACT
+			 (msg->win1.is_10b_compact)));
+		reg =
+			((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_ENDIAN_MODE)) |
+			 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_ENDIAN_MODE
+			 (msg->win1.is_10b_endian)));
+	}
+
+	/* rd_mode */
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_MODE)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_RD_MODE(msg->win1.rd_mode)));
+	/* win1 enable */
+	reg =
+		((reg & (~m_RGA3_WIN1_RD_CTRL_SW_WIN1_ENABLE)) |
+		 (s_RGA3_WIN1_RD_CTRL_SW_WIN1_ENABLE(msg->win1.enable)));
+
+	*bRGA3_WIN1_RD_CTRL = reg;
+
+	switch (msg->win1.rd_mode) {
+	case 0: /* raster */
+		stride = (((msg->win1.vir_w * pixel_width) + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win1.format))
+			uv_stride = ((msg->win1.vir_w + 15) & ~15) >> 2;
+		else
+			uv_stride = stride;
+		break;
+
+	case 1: /* fbc */
+		stride = ((msg->win1.vir_w + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win1.format))
+			uv_stride = ((msg->win1.vir_w + 15) & ~15) >> 2;
+		else
+			uv_stride = stride;
+		break;
+
+	case 2: /* tile 8*8 */
+		stride = (((msg->win1.vir_w * pixel_width * 8) + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win1.format))
+			uv_stride = ((((msg->win1.vir_w * 8) + 15) & ~15) >> 1) >> 2;
+		else
+			uv_stride = stride;
+		break;
+	}
+
+	*bRGA3_WIN1_Y_BASE = (u32) msg->win1.yrgb_addr;
+	*bRGA3_WIN1_U_BASE = (u32) msg->win1.uv_addr;
+	*bRGA3_WIN1_V_BASE = (u32) msg->win1.v_addr;
+
+	*bRGA3_WIN1_VIR_STRIDE = stride;
+	*bRGA3_WIN1_UV_VIR_STRIDE = uv_stride;
+
+	*bRGA3_WIN1_ACT_OFF = msg->win1.x_offset | (msg->win1.y_offset << 16);
+	/* fbcd offset */
+	/*
+	 *		 *bRGA3_WIN1_FBC_OFF = msg->win1.fbc_x_offset |
+	 *			(msg->win1.fbc_y_offset << 16);
+	 */
+
+	/* do not use win1 src size except fbcd */
+	*bRGA3_WIN1_SRC_SIZE = (msg->win1.src_act_w +
+		msg->win1.x_offset) | ((msg->win1.src_act_h +
+		msg->win1.y_offset) << 16);
+	*bRGA3_WIN1_ACT_SIZE =
+		msg->win1.src_act_w | (msg->win1.src_act_h << 16);
+	*bRGA3_WIN1_DST_SIZE =
+		msg->win1.dst_act_w | (msg->win1.dst_act_h << 16);
+}
+
+static void RGA3_set_reg_wr_info(u8 *base, struct rga3_req *msg)
+{
+	u32 *bRGA3_WR_RD_CTRL;
+	u32 *bRGA3_WR_Y_BASE, *bRGA3_WR_U_BASE, *bRGA3_WR_V_BASE;
+	u32 *bRGA3_WR_VIR_STRIDE;
+	u32 *bRGA3_WR_PL_VIR_STRIDE;
+	u32 *bRGA3_WR_FBCD_CTRL;
+
+	u32 reg = 0;
+	u32 fbcd_reg = 0;
+
+	u8 wr_format = 0;
+	u8 wr_yc_swp = 0;
+
+	/* rb swap on RGB, uv swap on YUV */
+	u8 wr_pix_swp = 0;
+
+	u8 pixel_width = 1;
+	u8 yuv10 = 0;
+
+	/*
+	 * 1: Semi planar, for yuv 4:2:x
+	 * 2: Interleaved (yuyv), for yuv422 8bit only ，RGB
+	 */
+	u8 wr_interleaved = 1;
+
+	u32 stride = 0;
+	u32 uv_stride = 0;
+
+	u32 vir_h = 0;
+
+	bRGA3_WR_RD_CTRL = (u32 *) (base + RGA3_WR_CTRL_OFFSET);
+	bRGA3_WR_FBCD_CTRL = (u32 *) (base + RGA3_WR_FBCE_CTRL_OFFSET);
+
+	bRGA3_WR_Y_BASE = (u32 *) (base + RGA3_WR_Y_BASE_OFFSET);
+	bRGA3_WR_U_BASE = (u32 *) (base + RGA3_WR_U_BASE_OFFSET);
+	bRGA3_WR_V_BASE = (u32 *) (base + RGA3_WR_V_BASE_OFFSET);
+
+	bRGA3_WR_VIR_STRIDE = (u32 *) (base + RGA3_WR_VIR_STRIDE_OFFSET);
+	bRGA3_WR_PL_VIR_STRIDE =
+		(u32 *) (base + RGA3_WR_PL_VIR_STRIDE_OFFSET);
+
+	switch (msg->wr.format) {
+	case RGA_FORMAT_RGBA_8888:
+		wr_format = 0x6;
+		pixel_width = 4;
+		wr_interleaved = 2;
+		wr_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGRA_8888:
+		wr_format = 0x6;
+		pixel_width = 4;
+		wr_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_888:
+		wr_format = 0x5;
+		pixel_width = 3;
+		wr_interleaved = 2;
+		wr_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_888:
+		wr_format = 0x5;
+		pixel_width = 3;
+		wr_interleaved = 2;
+		break;
+	case RGA_FORMAT_RGB_565:
+		wr_format = 0x4;
+		pixel_width = 2;
+		wr_interleaved = 2;
+		wr_pix_swp = 1;
+		break;
+	case RGA_FORMAT_BGR_565:
+		wr_format = 0x4;
+		pixel_width = 2;
+		wr_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YVYU_422:
+		wr_format = 0x1;
+		pixel_width = 2;
+		wr_pix_swp = 1;
+		wr_yc_swp = 1;
+		wr_interleaved = 2;
+		break;
+	case RGA_FORMAT_VYUY_422:
+		wr_format = 0x1;
+		pixel_width = 2;
+		wr_pix_swp = 1;
+		wr_yc_swp = 0;
+		wr_interleaved = 2;
+		break;
+	case RGA_FORMAT_YUYV_422:
+		wr_format = 0x1;
+		pixel_width = 2;
+		wr_pix_swp = 0;
+		wr_yc_swp = 1;
+		wr_interleaved = 2;
+		break;
+	case RGA_FORMAT_UYVY_422:
+		wr_format = 0x1;
+		pixel_width = 2;
+		wr_pix_swp = 0;
+		wr_yc_swp = 0;
+		wr_interleaved = 2;
+		break;
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		wr_format = 0x1;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+		wr_format = 0x0;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP:
+		wr_format = 0x1;
+		wr_pix_swp = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP:
+		wr_format = 0x0;
+		wr_pix_swp = 1;
+		break;
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+		wr_format = 0x2;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		wr_format = 0x2;
+		wr_pix_swp = 1;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+		wr_format = 0x3;
+		yuv10 = 1;
+		break;
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		wr_format = 0x3;
+		wr_pix_swp = 1;
+		yuv10 = 1;
+		break;
+	};
+
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_PIC_FORMAT)) |
+		 (s_RGA3_WR_CTRL_SW_WR_PIC_FORMAT(wr_format)));
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_PIX_SWAP)) |
+		 (s_RGA3_WR_CTRL_SW_WR_PIX_SWAP(wr_pix_swp)));
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_YC_SWAP)) |
+		 (s_RGA3_WR_CTRL_SW_WR_YC_SWAP(wr_yc_swp)));
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_FORMAT)) |
+		 (s_RGA3_WR_CTRL_SW_WR_FORMAT(wr_interleaved)));
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_FBCE_SPARSE_EN)) |
+		 (s_RGA3_WR_CTRL_SW_WR_FBCE_SPARSE_EN(1)));
+
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_OUTSTANDING_MAX)) |
+		 (s_RGA3_WR_CTRL_SW_OUTSTANDING_MAX(0xf)));
+
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT)) |
+		 (s_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT(1)));
+
+	/* Only on roster mode, yuv 10bit can change to compact or set endian */
+	if (msg->wr.rd_mode == 0 && yuv10 == 1) {
+		reg =
+			((reg & (~m_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT)) |
+			 (s_RGA3_WR_CTRL_SW_WR_YUV10B_COMPACT
+			 (msg->wr.is_10b_compact)));
+		reg =
+			((reg & (~m_RGA3_WR_CTRL_SW_WR_ENDIAN_MODE)) |
+			 (s_RGA3_WR_CTRL_SW_WR_ENDIAN_MODE
+			 (msg->wr.is_10b_endian)));
+	}
+
+	/* rd_mode */
+	reg =
+		((reg & (~m_RGA3_WR_CTRL_SW_WR_MODE)) |
+		 (s_RGA3_WR_CTRL_SW_WR_MODE(msg->wr.rd_mode)));
+
+	fbcd_reg = ((fbcd_reg & (~m_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_HOFF_DISS)) |
+		 (s_RGA3_WR_FBCE_CTRL_SW_WR_FBCE_HOFF_DISS(0)));
+
+	*bRGA3_WR_RD_CTRL = reg;
+	*bRGA3_WR_FBCD_CTRL = fbcd_reg;
+
+	switch (msg->wr.rd_mode) {
+	case 0: /* raster */
+		stride = (((msg->wr.vir_w * pixel_width) + 15) & ~15) >> 2;
+		uv_stride = ((msg->wr.vir_w + 15) & ~15) >> 2;
+
+		*bRGA3_WR_U_BASE = (u32) msg->wr.uv_addr;
+
+		break;
+
+	case 1: /* fbc */
+		stride = ((msg->wr.vir_w + 15) & ~15) >> 2;
+		/* need to calculate fbcd header size */
+		vir_h = ((msg->wr.vir_h + 15) & ~15);
+
+		/* RGBA8888 */
+		if (wr_format == 0x6)
+			uv_stride = ((msg->wr.vir_w + 15) & ~15);
+		/* RGB888 */
+		else if (wr_format == 0x5)
+			uv_stride = (((msg->wr.vir_w + 15) & ~15) >> 2) * 3;
+		/* RGB565, yuv422 8bit, yuv420 10bit */
+		else if (wr_format == 0x4 || wr_format == 0x1 || wr_format == 0x2)
+			uv_stride = ((msg->wr.vir_w + 15) & ~15) >> 1;
+		/* yuv420 8bit */
+		else if (wr_format == 0x0)
+			uv_stride = (((msg->wr.vir_w + 15) & ~15) >> 3) * 3;
+		/* yuv422 10bit */
+		else if (wr_format == 0x3)
+			uv_stride = (((msg->wr.vir_w + 15) & ~15) >> 3) * 5;
+
+		*bRGA3_WR_U_BASE = (u32) (msg->wr.uv_addr + ((stride * vir_h)>>2));
+
+		break;
+
+	case 2: /* tile 8*8 */
+		stride = (((msg->wr.vir_w * pixel_width * 8) + 15) & ~15) >> 2;
+		if (rga_is_yuv420_semi_planar_format(msg->win0.format))
+			uv_stride = ((((msg->wr.vir_w * 8) + 15) & ~15) >> 1) >> 2;
+		else
+			uv_stride = stride;
+
+		*bRGA3_WR_U_BASE = (u32) msg->wr.uv_addr;
+		break;
+	}
+
+	*bRGA3_WR_Y_BASE = (u32) msg->wr.yrgb_addr;
+	*bRGA3_WR_V_BASE = (u32) msg->wr.v_addr;
+
+	*bRGA3_WR_VIR_STRIDE = stride;
+	*bRGA3_WR_PL_VIR_STRIDE = uv_stride;
+}
+
+static void RGA3_set_reg_overlap_info(u8 *base, struct rga3_req *msg)
+{
+	u32 *bRGA_OVERLAP_TOP_CTRL;
+	u32 *bRGA_OVERLAP_BOT_CTRL;
+	u32 *bRGA_OVERLAP_TOP_ALPHA;
+	u32 *bRGA_OVERLAP_BOT_ALPHA;
+	u32 *bRGA_OVERLAP_TOP_KEY_MIN;
+	u32 *bRGA_OVERLAP_TOP_KEY_MAX;
+
+	u32 *bRGA_OVERLAP_CTRL;
+	u32 *bRGA3_OVLP_OFF;
+
+	u32 reg;
+	union rga3_color_ctrl top_color_ctrl, bottom_color_ctrl;
+	union rga3_alpha_ctrl top_alpha_ctrl, bottom_alpha_ctrl;
+	struct rga_alpha_config *config;
+
+	bRGA_OVERLAP_TOP_CTRL = (u32 *) (base + RGA3_OVLP_TOP_CTRL_OFFSET);
+	bRGA_OVERLAP_BOT_CTRL = (u32 *) (base + RGA3_OVLP_BOT_CTRL_OFFSET);
+	bRGA_OVERLAP_TOP_ALPHA = (u32 *) (base + RGA3_OVLP_TOP_ALPHA_OFFSET);
+	bRGA_OVERLAP_BOT_ALPHA = (u32 *) (base + RGA3_OVLP_BOT_ALPHA_OFFSET);
+
+	bRGA_OVERLAP_CTRL = (u32 *) (base + RGA3_OVLP_CTRL_OFFSET);
+	bRGA3_OVLP_OFF = (u32 *) (base + RGA3_OVLP_OFF_OFFSET);
+
+	/* Alpha blend */
+	/*bot -> win0(dst), top -> win1(src). */
+	top_color_ctrl.value = 0;
+	bottom_color_ctrl.value = 0;
+	top_alpha_ctrl.value = 0;
+	bottom_alpha_ctrl.value = 0;
+	config = &msg->alpha_config;
+
+	if (config->fg_pixel_alpha_en)
+		top_color_ctrl.bits.blend_mode =
+			config->fg_global_alpha_en ? RGA_ALPHA_PER_PIXEL_GLOBAL :
+			RGA_ALPHA_PER_PIXEL;
+	else
+		top_color_ctrl.bits.blend_mode = RGA_ALPHA_GLOBAL;
+
+	if (config->bg_pixel_alpha_en)
+		bottom_color_ctrl.bits.blend_mode =
+			config->bg_global_alpha_en ? RGA_ALPHA_PER_PIXEL_GLOBAL :
+			RGA_ALPHA_PER_PIXEL;
+	else
+		bottom_color_ctrl.bits.blend_mode = RGA_ALPHA_GLOBAL;
+
+	/*
+	 * Since the hardware uses 256 as 1, the original alpha value needs to
+	 * be + (alpha >> 7).
+	 */
+	top_color_ctrl.bits.alpha_cal_mode = RGA_ALPHA_SATURATION;
+	bottom_color_ctrl.bits.alpha_cal_mode = RGA_ALPHA_SATURATION;
+
+	top_color_ctrl.bits.global_alpha = config->fg_global_alpha_value;
+	bottom_color_ctrl.bits.global_alpha = config->bg_global_alpha_value;
+
+	/* porter duff alpha enable */
+	switch (config->mode) {
+	case RGA_ALPHA_BLEND_SRC:
+		/*
+		 * SRC mode:
+		 *	Sf = 1, Df = 0；
+		 *	[Rc,Ra] = [Sc,Sa]；
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ONE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST:
+		/*
+		 * SRC mode:
+		 *	Sf = 0, Df = 1；
+		 *	[Rc,Ra] = [Dc,Da]；
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ONE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_OVER:
+		/*
+		 * SRC-OVER mode:
+		 *	Sf = 1, Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc + (1 - Sa) * Dc, Sa + (1 - Sa) * Da ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ONE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_OVER:
+		/*
+		 * DST-OVER mode:
+		 *	Sf = (1 - Da) , Df = 1
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc, Sa * (1 - Da) + Da ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ONE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_IN:
+		/*
+		 * SRC-IN mode:
+		 *	Sf = Da , Df = 0
+		 *	[Rc,Ra] = [ Sc * Da, Sa * Da ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_IN:
+		/*
+		 * DST-IN mode:
+		 *	Sf = 0 , Df = Sa
+		 *	[Rc,Ra] = [ Dc * Sa, Da * Sa ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_OUT:
+		/*
+		 * SRC-OUT mode:
+		 *	Sf = (1 - Da) , Df = 0
+		 *	[Rc,Ra] = [ Sc * (1 - Da), Sa * (1 - Da) ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_OUT:
+		/*
+		 * DST-OUT mode:
+		 *	Sf = 0 , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Dc * (1 - Sa), Da * (1 - Sa) ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_SRC_ATOP:
+		/*
+		 * SRC-ATOP mode:
+		 *	Sf = Da , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc * Da + Dc * (1 - Sa), Sa * Da + Da * (1 - Sa) ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_DST_ATOP:
+		/*
+		 * DST-ATOP mode:
+		 *	Sf = (1 - Da) , Df = Sa
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc * Sa, Sa * (1 - Da) + Da * Sa ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_XOR:
+		/*
+		 * DST-XOR mode:
+		 *	Sf = (1 - Da) , Df = (1 - Sa)
+		 *	[Rc,Ra] = [ Sc * (1 - Da) + Dc * (1 - Sa), Sa * (1 - Da) + Da * (1 - Sa) ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_OPPOSITE_INVERSE;
+
+		break;
+
+	case RGA_ALPHA_BLEND_CLEAR:
+		/*
+		 * DST-CLEAR mode:
+		 *	Sf = 0 , Df = 0
+		 *	[Rc,Ra] = [ 0, 0 ]
+		 */
+		top_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		top_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		bottom_color_ctrl.bits.alpha_mode = RGA_ALPHA_STRAIGHT;
+		bottom_color_ctrl.bits.factor_mode = RGA_ALPHA_ZERO;
+
+		break;
+
+	default:
+		break;
+	}
+
+	if (!config->enable && msg->abb_alpha_pass) {
+		/*
+		 * enabled by default bot_blend_m1 && bot_alpha_cal_m1 for src channel(win0)
+		 * In ABB mode, the number will be fetched according to 16*16, so it needs to
+		 * be enabled top_blend_m1 && top_alpha_cal_m1 for dst channel(wr).
+		 */
+		top_color_ctrl.bits.color_mode = RGA_ALPHA_PRE_MULTIPLIED;
+
+		top_alpha_ctrl.bits.blend_mode = RGA_ALPHA_PER_PIXEL;
+		top_alpha_ctrl.bits.alpha_cal_mode = RGA_ALPHA_NO_SATURATION;
+
+		bottom_color_ctrl.bits.color_mode = RGA_ALPHA_PRE_MULTIPLIED;
+
+		bottom_alpha_ctrl.bits.blend_mode = RGA_ALPHA_PER_PIXEL;
+		bottom_alpha_ctrl.bits.alpha_cal_mode = RGA_ALPHA_NO_SATURATION;
+	} else {
+		top_color_ctrl.bits.color_mode =
+			config->fg_pre_multiplied ?
+				RGA_ALPHA_PRE_MULTIPLIED : RGA_ALPHA_NO_PRE_MULTIPLIED;
+
+		top_alpha_ctrl.bits.blend_mode = top_color_ctrl.bits.blend_mode;
+		top_alpha_ctrl.bits.alpha_cal_mode = top_color_ctrl.bits.alpha_cal_mode;
+		top_alpha_ctrl.bits.alpha_mode = top_color_ctrl.bits.alpha_mode;
+		top_alpha_ctrl.bits.factor_mode = top_color_ctrl.bits.factor_mode;
+
+		bottom_color_ctrl.bits.color_mode =
+			config->bg_pre_multiplied ?
+				RGA_ALPHA_PRE_MULTIPLIED : RGA_ALPHA_NO_PRE_MULTIPLIED;
+
+		bottom_alpha_ctrl.bits.blend_mode = bottom_color_ctrl.bits.blend_mode;
+		bottom_alpha_ctrl.bits.alpha_cal_mode = bottom_color_ctrl.bits.alpha_cal_mode;
+		bottom_alpha_ctrl.bits.alpha_mode = bottom_color_ctrl.bits.alpha_mode;
+		bottom_alpha_ctrl.bits.factor_mode = bottom_color_ctrl.bits.factor_mode;
+	}
+
+	*bRGA_OVERLAP_TOP_CTRL = top_color_ctrl.value;
+	*bRGA_OVERLAP_BOT_CTRL = bottom_color_ctrl.value;
+	*bRGA_OVERLAP_TOP_ALPHA = top_alpha_ctrl.value;
+	*bRGA_OVERLAP_BOT_ALPHA = bottom_alpha_ctrl.value;
+
+	/* set RGA_OVERLAP_CTRL */
+	reg = 0;
+	/* color key */
+	bRGA_OVERLAP_TOP_KEY_MIN =
+		(u32 *) (base + RGA3_OVLP_TOP_KEY_MIN_OFFSET);
+	bRGA_OVERLAP_TOP_KEY_MAX =
+		(u32 *) (base + RGA3_OVLP_TOP_KEY_MAX_OFFSET);
+
+	/*
+	 * YG : value		 (0:9)
+	 * UB : value >> 10	 (10:19)
+	 * VG : value >> 20	 (20:29)
+	 */
+	if (msg->color_key_min > 0 || msg->color_key_max > 0) {
+		*bRGA_OVERLAP_TOP_KEY_MIN = msg->color_key_min;
+		*bRGA_OVERLAP_TOP_KEY_MAX = msg->color_key_max;
+		reg = ((reg & (~m_RGA3_OVLP_CTRL_SW_TOP_KEY_EN)) |
+			 (s_RGA3_OVLP_CTRL_SW_TOP_KEY_EN(1)));
+	}
+
+	/* 1: ABB mode, 0: ABC mode， ABB cannot support fbc in&out */
+	if (msg->win0.yrgb_addr == msg->wr.yrgb_addr)
+		reg = ((reg & (~m_RGA3_OVLP_CTRL_SW_OVLP_MODE)) |
+			(s_RGA3_OVLP_CTRL_SW_OVLP_MODE(1)));
+
+	/* 1: yuv field, 0: rgb field */
+	if (rga_is_yuv_format(msg->wr.format))
+		reg = ((reg & (~m_RGA3_OVLP_CTRL_SW_OVLP_FIELD)) |
+			 (s_RGA3_OVLP_CTRL_SW_OVLP_FIELD(1)));
+
+	/*
+	 * warning: if m1 & m0 need config split，need to redesign
+	 * this judge, which consider RGBA8888 format
+	 */
+	reg = ((reg & (~m_RGA3_OVLP_CTRL_SW_TOP_ALPHA_EN)) |
+	       (s_RGA3_OVLP_CTRL_SW_TOP_ALPHA_EN(config->enable)));
+
+	*bRGA_OVERLAP_CTRL = reg;
+
+	*bRGA3_OVLP_OFF = msg->wr.x_offset | (msg->wr.y_offset << 16);
+}
+
+static int rga3_gen_reg_info(u8 *base, struct rga3_req *msg)
+{
+	switch (msg->render_mode) {
+	case BITBLT_MODE:
+		RGA3_set_reg_win0_info(base, msg);
+		RGA3_set_reg_win1_info(base, msg);
+		RGA3_set_reg_overlap_info(base, msg);
+		RGA3_set_reg_wr_info(base, msg);
+		break;
+	default:
+		rga_err("error msg render mode %d\n", msg->render_mode);
+		break;
+	}
+
+	return 0;
+}
+
+static void addr_copy(struct rga_win_info_t *win, struct rga_img_info_t *img)
+{
+	win->yrgb_addr = img->yrgb_addr;
+	win->uv_addr = img->uv_addr;
+	win->v_addr = img->v_addr;
+	win->enable = 1;
+}
+
+static void set_win_info(struct rga_win_info_t *win, struct rga_img_info_t *img)
+{
+	win->x_offset = img->x_offset;
+	win->y_offset = img->y_offset;
+	win->src_act_w = img->act_w;
+	win->src_act_h = img->act_h;
+	win->vir_w = img->vir_w;
+	win->vir_h = img->vir_h;
+	if (img->rd_mode == RGA_RASTER_MODE)
+		win->rd_mode = 0;
+	else if (img->rd_mode == RGA_FBC_MODE)
+		win->rd_mode = 1;
+	else if (img->rd_mode == RGA_TILE_MODE)
+		win->rd_mode = 2;
+
+	switch (img->compact_mode) {
+	case RGA_10BIT_INCOMPACT:
+		win->is_10b_compact = 0;
+		break;
+	case RGA_10BIT_COMPACT:
+	default:
+		win->is_10b_compact = 1;
+		break;
+	}
+
+	win->is_10b_endian = img->is_10b_endian;
+}
+
+static void set_wr_info(struct rga_req *req_rga, struct rga3_req *req)
+{
+	/* The output w/h are bound to the dst_act_w/h of win0. */
+	req->wr.dst_act_w = req->win0.dst_act_w;
+	req->wr.dst_act_h = req->win0.dst_act_h;
+
+	/* Some configurations need to be all equal to the output w/h. */
+	req->wr.vir_w = req_rga->dst.vir_w;
+	req->wr.vir_h = req_rga->dst.vir_h;
+
+	if (req_rga->dst.rd_mode == RGA_RASTER_MODE)
+		req->wr.rd_mode = 0;
+	else if (req_rga->dst.rd_mode == RGA_FBC_MODE)
+		req->wr.rd_mode = 1;
+	else if (req_rga->dst.rd_mode == RGA_TILE_MODE)
+		req->wr.rd_mode = 2;
+
+	switch (req_rga->dst.compact_mode) {
+	case RGA_10BIT_INCOMPACT:
+		req->wr.is_10b_compact = 0;
+		break;
+	case RGA_10BIT_COMPACT:
+	default:
+		req->wr.is_10b_compact = 1;
+		break;
+	}
+
+	req->wr.is_10b_endian = req_rga->dst.is_10b_endian;
+}
+
+/* TODO: common part */
+static void rga_cmd_to_rga3_cmd(struct rga_req *req_rga, struct rga3_req *req)
+{
+	struct rga_img_info_t tmp;
+
+	req->render_mode = BITBLT_MODE;
+
+	/* rotate & mirror */
+	switch (req_rga->rotate_mode & 0x0f) {
+	case 0x1:
+		if (req_rga->sina == 65536 && req_rga->cosa == 0) {
+			/* rot-90 */
+			req->rotate_mode = RGA3_ROT_BIT_ROT_90;
+		} else if (req_rga->sina == 0 && req_rga->cosa == -65536) {
+			/* rot-180 = X-mirror + Y-mirror */
+			req->rotate_mode = RGA3_ROT_BIT_X_MIRROR | RGA3_ROT_BIT_Y_MIRROR;
+		} else if (req_rga->sina == -65536 && req_rga->cosa == 0) {
+			/* rot-270 or -90 = rot-90 + X-mirror + Y-mirror */
+			req->rotate_mode = RGA3_ROT_BIT_X_MIRROR | RGA3_ROT_BIT_Y_MIRROR |
+					   RGA3_ROT_BIT_ROT_90;
+		} else if (req_rga->sina == 0 && req_rga->cosa == 65536) {
+			/* bypass */
+			req->rotate_mode = 0;
+		}
+		break;
+	case 0x2:
+		/* X-mirror */
+		req->rotate_mode = RGA3_ROT_BIT_X_MIRROR;
+		break;
+	case 0x3:
+		/* Y-mirror */
+		req->rotate_mode = RGA3_ROT_BIT_Y_MIRROR;
+		break;
+	case 0x4:
+		/* X-mirror + Y-mirror */
+		req->rotate_mode = RGA3_ROT_BIT_X_MIRROR | RGA3_ROT_BIT_Y_MIRROR;
+		break;
+	default:
+		req->rotate_mode = 0;
+		break;
+	}
+
+	/* The upper four bits are only allowed to configure the mirror. */
+	switch ((req_rga->rotate_mode & 0xf0) >> 4) {
+	case 2:
+		/* X-mirror */
+		req->rotate_mode ^= RGA3_ROT_BIT_X_MIRROR;
+		break;
+	case 3:
+		/* Y-mirror */
+		req->rotate_mode ^= RGA3_ROT_BIT_Y_MIRROR;
+		break;
+	case 0x4:
+		/* X-mirror + Y-mirror */
+		req->rotate_mode ^= RGA3_ROT_BIT_X_MIRROR | RGA3_ROT_BIT_Y_MIRROR;
+		break;
+	}
+
+	req->win0_a_global_val = req_rga->alpha_global_value;
+	req->win1_a_global_val = req_rga->alpha_global_value;
+
+	/* fixup yuv/rgb convert to rgba missing alpha channel */
+	if (!(req_rga->alpha_rop_flag & 1)) {
+		if (!rga_is_alpha_format(req_rga->src.format) &&
+		    rga_is_alpha_format(req_rga->dst.format)) {
+			req->alpha_config.fg_global_alpha_value = 0xff;
+			req->alpha_config.bg_global_alpha_value = 0xff;
+		}
+	}
+
+	/* simple win can not support dst offset */
+	if ((!((req_rga->alpha_rop_flag) & 1)) &&
+	    (req_rga->dst.x_offset == 0 && req_rga->dst.y_offset == 0) &&
+	    (req_rga->src.yrgb_addr != req_rga->dst.yrgb_addr)) {
+		/*
+		 * ABB mode Layer binding:
+		 *     src => win0
+		 *     dst => wr
+		 */
+
+		/*
+		 * enabled by default bot_blend_m1 && bot_alpha_cal_m1 for src channel(win0)
+		 * In ABB mode, the number will be fetched according to 16*16, so it needs to
+		 * be enabled top_blend_m1 && top_alpha_cal_m1 for dst channel(wr).
+		 */
+		if (rga_is_alpha_format(req_rga->src.format))
+			req->abb_alpha_pass = true;
+
+		set_win_info(&req->win0, &req_rga->src);
+
+		/* enable win0 rotate */
+		req->win0.rotate_mode = 1;
+
+		/* set win dst size */
+		req->win0.dst_act_w = req_rga->dst.act_w;
+		req->win0.dst_act_h = req_rga->dst.act_h;
+
+		addr_copy(&req->win0, &req_rga->src);
+		addr_copy(&req->wr, &req_rga->dst);
+
+		req->win0.format = req_rga->src.format;
+		req->wr.format = req_rga->dst.format;
+	} else {
+		/*
+		 * ABC mode Layer binding:
+		 *     src => win1
+		 *     src1/dst => win0
+		 *     dst => wr
+		 */
+
+		/*
+		 * enabled by default top_blend_m1 && top_alpha_cal_m1 for src channel(win1)
+		 * In ABB mode, the number will be fetched according to 16*16, so it needs to
+		 * be enabled bot_blend_m1 && bot_alpha_cal_m1 for src1/dst channel(win0).
+		 */
+		if (rga_is_alpha_format(req_rga->src.format))
+			req->abb_alpha_pass = true;
+
+		if (req_rga->pat.yrgb_addr != 0) {
+			if (req_rga->src.yrgb_addr == req_rga->dst.yrgb_addr) {
+				/* Convert ABC mode to ABB mode. */
+				memcpy(&req_rga->src, &req_rga->pat, sizeof(req_rga->src));
+				memset(&req_rga->pat, 0x0, sizeof(req_rga->pat));
+				req_rga->bsfilter_flag = 0;
+
+				rga_swap_pd_mode(req_rga);
+			} else if ((req_rga->dst.x_offset + req_rga->src.act_w >
+				    req_rga->pat.act_w) ||
+				   (req_rga->dst.y_offset + req_rga->src.act_h >
+				    req_rga->pat.act_h)) {
+				/* wr_offset + win1.act_size need > win0.act_size */
+				memcpy(&tmp, &req_rga->src, sizeof(tmp));
+				memcpy(&req_rga->src, &req_rga->pat, sizeof(req_rga->src));
+				memcpy(&req_rga->pat, &tmp, sizeof(req_rga->pat));
+
+				rga_swap_pd_mode(req_rga);
+			}
+		}
+
+		set_win_info(&req->win1, &req_rga->src);
+
+		/* enable win1 rotate */
+		req->win1.rotate_mode = 1;
+
+		addr_copy(&req->win1, &req_rga->src);
+		addr_copy(&req->wr, &req_rga->dst);
+
+		req->win1.format = req_rga->src.format;
+		req->wr.format = req_rga->dst.format;
+
+		if (req_rga->pat.yrgb_addr != 0) {
+			/* A+B->C mode */
+			set_win_info(&req->win0, &req_rga->pat);
+			addr_copy(&req->win0, &req_rga->pat);
+			req->win0.format = req_rga->pat.format;
+
+			/* set win0 dst size */
+			if (req->win0.x_offset || req->win0.y_offset) {
+				req->win0.src_act_w = req->win0.src_act_w + req->win0.x_offset;
+				req->win0.src_act_h = req->win0.src_act_h + req->win0.y_offset;
+				req->win0.dst_act_w = req_rga->dst.act_w + req->win0.x_offset;
+				req->win0.dst_act_h = req_rga->dst.act_h + req->win0.y_offset;
+
+				req->win0.x_offset = 0;
+				req->win0.y_offset = 0;
+			} else {
+				req->win0.dst_act_w = req_rga->dst.act_w;
+				req->win0.dst_act_h = req_rga->dst.act_h;
+			}
+			/* set win1 dst size */
+			req->win1.dst_act_w = req_rga->dst.act_w;
+			req->win1.dst_act_h = req_rga->dst.act_h;
+		} else {
+			/* A+B->B mode */
+			set_win_info(&req->win0, &req_rga->dst);
+			addr_copy(&req->win0, &req_rga->dst);
+			req->win0.format = req_rga->dst.format;
+
+			/* only win1 && wr support fbcd, win0 default raster */
+			req->win0.rd_mode = 0;
+
+			/* set win0 dst size */
+			req->win0.dst_act_w = req_rga->dst.act_w;
+			req->win0.dst_act_h = req_rga->dst.act_h;
+			/* set win1 dst size */
+			req->win1.dst_act_w = req_rga->dst.act_w;
+			req->win1.dst_act_h = req_rga->dst.act_h;
+		}
+
+		/* dst offset need to config overlap offset */
+		req->wr.x_offset = req_rga->dst.x_offset;
+		req->wr.y_offset = req_rga->dst.y_offset;
+	}
+	set_wr_info(req_rga, req);
+
+	if (req->rotate_mode & RGA3_ROT_BIT_ROT_90) {
+		if (req->win1.yrgb_addr != 0) {
+			/* ABB */
+			if (req->win0.yrgb_addr == req->wr.yrgb_addr) {
+				req->win1.dst_act_w = req_rga->dst.act_h;
+				req->win1.dst_act_h = req_rga->dst.act_w;
+
+				/* win0 do not need rotate, but net equal to wr */
+				req->win0.dst_act_w = req_rga->dst.act_h;
+				req->win0.dst_act_h = req_rga->dst.act_w;
+				req->win0.src_act_w = req_rga->dst.act_h;
+				req->win0.src_act_h = req_rga->dst.act_w;
+			}
+		} else {
+			req->win0.rotate_mode = 1;
+			req->win0.dst_act_w = req_rga->dst.act_h;
+			req->win0.dst_act_h = req_rga->dst.act_w;
+		}
+	}
+
+	/* overlap */
+	/* Alpha blend mode */
+	if (((req_rga->alpha_rop_flag) & 1)) {
+		if ((req_rga->alpha_rop_flag >> 3) & 1) {
+			req->alpha_config.enable = true;
+
+			if ((req_rga->alpha_rop_flag >> 9) & 1) {
+				req->alpha_config.fg_pre_multiplied = false;
+				req->alpha_config.bg_pre_multiplied = false;
+			} else {
+				req->alpha_config.fg_pre_multiplied = true;
+				req->alpha_config.bg_pre_multiplied = true;
+			}
+
+			req->alpha_config.fg_pixel_alpha_en = rga_is_alpha_format(req->win1.format);
+			req->alpha_config.bg_pixel_alpha_en = rga_is_alpha_format(req->win0.format);
+
+			if (req_rga->feature.global_alpha_en) {
+				if (req_rga->fg_global_alpha < 0xff) {
+					req->alpha_config.fg_global_alpha_en = true;
+					req->alpha_config.fg_global_alpha_value =
+						req_rga->fg_global_alpha;
+				} else if (!req->alpha_config.fg_pixel_alpha_en) {
+					req->alpha_config.fg_global_alpha_en = true;
+					req->alpha_config.fg_global_alpha_value = 0xff;
+				}
+
+				if (req_rga->bg_global_alpha < 0xff) {
+					req->alpha_config.bg_global_alpha_en = true;
+					req->alpha_config.bg_global_alpha_value =
+						req_rga->bg_global_alpha;
+				} else if (!req->alpha_config.bg_pixel_alpha_en) {
+					req->alpha_config.bg_global_alpha_en = true;
+					req->alpha_config.bg_global_alpha_value = 0xff;
+				}
+			} else {
+				req->alpha_config.bg_global_alpha_value = 0xff;
+				req->alpha_config.bg_global_alpha_value = 0xff;
+			}
+
+			req->alpha_config.mode = req_rga->PD_mode;
+		}
+	}
+
+	/* yuv to rgb */
+	/* 601 limit */
+	if (req_rga->yuv2rgb_mode == 1) {
+		req->win0.y2r_mode = 0;
+		req->win1.y2r_mode = 0;
+	/* 601 full */
+	} else if (req_rga->yuv2rgb_mode == 2) {
+		req->win0.y2r_mode = 2;
+		req->win1.y2r_mode = 2;
+	/* 709 limit */
+	} else if (req_rga->yuv2rgb_mode == 3) {
+		req->win0.y2r_mode = 1;
+		req->win1.y2r_mode = 1;
+	}
+
+	/* rgb to yuv */
+	/* 601 limit */
+	if ((req_rga->yuv2rgb_mode >> 2) == 2) {
+		req->win0.r2y_mode = 0;
+		req->win1.r2y_mode = 0;
+	/* 601 full */
+	} else if ((req_rga->yuv2rgb_mode >> 2) == 1) {
+		req->win0.r2y_mode = 2;
+		req->win1.r2y_mode = 2;
+	/* 709 limit */
+	} else if ((req_rga->yuv2rgb_mode >> 2) == 3) {
+		req->win0.r2y_mode = 1;
+		req->win1.r2y_mode = 1;
+	}
+
+	/* color key: 8bit->10bit */
+	req->color_key_min = (req_rga->color_key_min & 0xff) << 22 |
+			     ((req_rga->color_key_min >> 8) & 0xff) << 2 |
+			     ((req_rga->color_key_min >> 16) & 0xff) << 12;
+	req->color_key_max = (req_rga->color_key_max & 0xff) << 22 |
+			     ((req_rga->color_key_max >> 8) & 0xff) << 2 |
+			     ((req_rga->color_key_max >> 16) & 0xff) << 12;
+
+	if (req_rga->mmu_info.mmu_en && (req_rga->mmu_info.mmu_flag & 1) == 1) {
+		req->mmu_info.src0_mmu_flag = 1;
+		req->mmu_info.src1_mmu_flag = 1;
+		req->mmu_info.dst_mmu_flag = 1;
+	}
+}
+
+static void rga3_soft_reset(struct rga_scheduler_t *scheduler)
+{
+	u32 i;
+	u32 iommu_dte_addr = 0;
+
+	if (scheduler->data->mmu == RGA_IOMMU)
+		iommu_dte_addr = rga_read(RGA_IOMMU_DTE_ADDR, scheduler);
+
+	rga_write(s_RGA3_SYS_CTRL_CCLK_SRESET(1) | s_RGA3_SYS_CTRL_ACLK_SRESET(1),
+		  RGA3_SYS_CTRL, scheduler);
+
+	for (i = 0; i < RGA_RESET_TIMEOUT; i++) {
+		if (rga_read(RGA3_RO_SRST, scheduler) & m_RGA3_RO_SRST_RO_RST_DONE)
+			break;
+
+		udelay(1);
+	}
+
+	rga_write(s_RGA3_SYS_CTRL_CCLK_SRESET(0) | s_RGA3_SYS_CTRL_ACLK_SRESET(0),
+		  RGA3_SYS_CTRL, scheduler);
+
+	if (scheduler->data->mmu == RGA_IOMMU) {
+		rga_write(iommu_dte_addr, RGA_IOMMU_DTE_ADDR, scheduler);
+		/* enable iommu */
+		rga_write(RGA_IOMMU_CMD_ENABLE_PAGING, RGA_IOMMU_COMMAND, scheduler);
+	}
+
+	if (i == RGA_RESET_TIMEOUT)
+		rga_err("%s[%#x] soft reset timeout. SYS_CTRL[0x%x], RO_SRST[0x%x]\n",
+			rga_get_core_name(scheduler->core), scheduler->core,
+			rga_read(RGA3_SYS_CTRL, scheduler),
+			rga_read(RGA3_RO_SRST, scheduler));
+	else
+		rga_log("%s[%#x] soft reset complete.\n",
+			rga_get_core_name(scheduler->core), scheduler->core);
+}
+
+static int rga3_scale_check(struct rga_job *job, const struct rga3_req *req)
+{
+	u32 win0_saw, win0_sah, win0_daw, win0_dah;
+	u32 win1_saw, win1_sah, win1_daw, win1_dah;
+
+	if (req->rotate_mode & RGA3_ROT_BIT_ROT_90) {
+		if (req->win1.yrgb_addr != 0) {
+			/* ABB */
+			if (req->win0.yrgb_addr == req->wr.yrgb_addr) {
+				/* win0 do not need rotate, but net equal to wr */
+				win0_saw = req->win0.src_act_h;
+				win0_sah = req->win0.src_act_w;
+				win0_daw = req->win0.dst_act_h;
+				win0_dah = req->win0.dst_act_w;
+
+				win1_saw = req->win1.dst_act_w;
+				win1_sah = req->win1.dst_act_h;
+				win1_daw = req->win1.dst_act_h;
+				win1_dah = req->win1.dst_act_w;
+			} else {
+				win0_saw = req->win0.src_act_w;
+				win0_sah = req->win0.src_act_h;
+				win0_daw = req->win0.dst_act_w;
+				win0_dah = req->win0.dst_act_h;
+
+				win1_saw = req->win1.src_act_w;
+				win1_sah = req->win1.src_act_h;
+				win1_daw = req->win1.dst_act_w;
+				win1_dah = req->win1.dst_act_h;
+			}
+		} else {
+			win0_saw = req->win0.src_act_w;
+			win0_sah = req->win0.src_act_h;
+			win0_daw = req->win0.dst_act_h;
+			win0_dah = req->win0.dst_act_w;
+		}
+	} else {
+		win0_saw = req->win0.src_act_w;
+		win0_sah = req->win0.src_act_h;
+		win0_daw = req->win0.dst_act_w;
+		win0_dah = req->win0.dst_act_h;
+
+		if (req->win1.yrgb_addr != 0) {
+			win1_saw = req->win1.src_act_w;
+			win1_sah = req->win1.src_act_h;
+			win1_daw = req->win1.dst_act_w;
+			win1_dah = req->win1.dst_act_h;
+		}
+	}
+
+	if (((win0_saw >> 3) > win0_daw) || ((win0_sah >> 3) > win0_dah)) {
+		rga_job_log(job, "win0 unsupported to scaling less than 1/8 times. src[%d, %d], dst[%d, %d]\n",
+			win0_saw, win0_sah, win0_daw, win0_dah);
+		return -EINVAL;
+	}
+	if (((win0_daw >> 3) > win0_saw) || ((win0_dah >> 3) > win0_sah)) {
+		rga_job_log(job, "win0 unsupported to scaling more than 8 times. src[%d, %d], dst[%d, %d]\n",
+			win0_saw, win0_sah, win0_daw, win0_dah);
+		return -EINVAL;
+	}
+
+	if (req->win1.yrgb_addr != 0) {
+		if (((win1_saw >> 3) > win1_daw) || ((win1_sah >> 3) > win1_dah)) {
+			rga_job_log(job, "win1 unsupported to scaling less than 1/8 times. src[%d, %d], dst[%d, %d]\n",
+				win1_saw, win1_sah, win1_daw, win1_dah);
+			return -EINVAL;
+		}
+		if (((win1_daw >> 3) > win1_saw) || ((win1_dah >> 3) > win1_sah)) {
+			rga_job_log(job, "win1 unsupported to scaling more than 8 times. src[%d, %d], dst[%d, %d]\n",
+				win1_saw, win1_sah, win1_daw, win1_dah);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int rga3_check_param(struct rga_job *job, const struct rga_hw_data *data,
+			    const struct rga3_req *req)
+{
+	if (unlikely(rga_hw_out_of_range(&(data->input_range),
+					 req->win0.src_act_w, req->win0.src_act_h) ||
+		     rga_hw_out_of_range(&(data->input_range),
+					 req->win0.dst_act_w, req->win0.dst_act_h) ||
+		     rga_hw_out_of_range(&(data->input_range),
+					 req->win0.src_act_w + req->win0.x_offset,
+					 req->win0.src_act_h + req->win0.y_offset))) {
+		rga_job_err(job, "invalid win0, src[w,h] = [%d, %d], dst[w,h] = [%d, %d], off[x,y] = [%d,%d]\n",
+			req->win0.src_act_w, req->win0.src_act_h,
+			req->win0.dst_act_w, req->win0.dst_act_h,
+			req->win0.x_offset, req->win0.y_offset);
+		return -EINVAL;
+	}
+
+	if (unlikely(req->win0.vir_w * rga_get_pixel_stride_from_format(req->win0.format) >
+		     data->max_byte_stride * 8)) {
+		rga_job_err(job, "invalid win0 stride, stride = %d, pixel_stride = %d, max_byte_stride = %d\n",
+			req->win0.vir_w, rga_get_pixel_stride_from_format(req->win0.format),
+			data->max_byte_stride);
+		return -EINVAL;
+	}
+
+	if (unlikely(rga_hw_out_of_range(&(data->output_range),
+					 req->wr.dst_act_w, req->wr.dst_act_h))) {
+		rga_job_err(job, "invalid wr, [w,h] = [%d, %d]\n", req->wr.dst_act_w, req->wr.dst_act_h);
+		return -EINVAL;
+	}
+
+	if (unlikely(req->wr.vir_w * rga_get_pixel_stride_from_format(req->wr.format) >
+		     data->max_byte_stride * 8)) {
+		rga_job_err(job, "invalid wr stride, stride = %d, pixel_stride = %d, max_byte_stride = %d\n",
+			req->wr.vir_w, rga_get_pixel_stride_from_format(req->wr.format),
+			data->max_byte_stride);
+		return -EINVAL;
+	}
+
+	if (req->win1.yrgb_addr != 0) {
+		if (unlikely(rga_hw_out_of_range(&(data->input_range),
+						 req->win1.src_act_w, req->win1.src_act_h) ||
+			     rga_hw_out_of_range(&(data->input_range),
+						 req->win1.dst_act_w, req->win1.dst_act_h) ||
+			     rga_hw_out_of_range(&(data->input_range),
+						 req->win1.src_act_w + req->win1.x_offset,
+						 req->win1.src_act_h + req->win1.y_offset))) {
+			rga_job_err(job, "invalid win1, src[w,h] = [%d, %d], dst[w,h] = [%d, %d], off[x,y] = [%d,%d]\n",
+				req->win1.src_act_w, req->win1.src_act_h,
+				req->win1.dst_act_w, req->win1.dst_act_h,
+				req->win1.x_offset, req->win1.y_offset);
+			return -EINVAL;
+		}
+
+		if (unlikely(req->win1.vir_w * rga_get_pixel_stride_from_format(req->win1.format) >
+			     data->max_byte_stride * 8)) {
+			rga_job_err(job, "invalid win1 stride, stride = %d, pixel_stride = %d, max_byte_stride = %d\n",
+				req->win1.vir_w, rga_get_pixel_stride_from_format(req->win1.format),
+				data->max_byte_stride);
+			return -EINVAL;
+		}
+
+		/* warning: rotate mode skip this judge */
+		if (req->rotate_mode == 0) {
+			/* check win0 dst size > win1 dst size */
+			if (unlikely((req->win1.dst_act_w > req->win0.dst_act_w) ||
+				     (req->win1.dst_act_h > req->win0.dst_act_h))) {
+				rga_job_err(job, "invalid output param win0[w,h] = [%d, %d], win1[w,h] = [%d, %d]\n",
+					req->win0.dst_act_w, req->win0.dst_act_h,
+					req->win1.dst_act_w, req->win1.dst_act_h);
+				return -EINVAL;
+			}
+		}
+	}
+
+	if (rga3_scale_check(job, req) < 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void print_debug_info(struct rga_job *job, struct rga3_req *req)
+{
+	rga_job_log(job, "render_mode:%s, bitblit_mode=%d, rotate_mode:%x\n",
+		rga_get_render_mode_str(req->render_mode), req->bitblt_mode,
+		req->rotate_mode);
+	rga_job_log(job, "win0: y = %lx uv = %lx v = %lx src_w = %d src_h = %d\n",
+		req->win0.yrgb_addr, req->win0.uv_addr, req->win0.v_addr,
+		req->win0.src_act_w, req->win0.src_act_h);
+	rga_job_log(job, "win0: vw = %d vh = %d xoff = %d yoff = %d format = %s\n",
+		req->win0.vir_w, req->win0.vir_h,
+		req->win0.x_offset, req->win0.y_offset,
+		rga_get_format_name(req->win0.format));
+	rga_job_log(job, "win0: dst_w = %d, dst_h = %d, rd_mode = %d\n",
+		req->win0.dst_act_w, req->win0.dst_act_h, req->win0.rd_mode);
+	rga_job_log(job, "win0: rot_mode = %d, en = %d, compact = %d, endian = %d\n",
+		req->win0.rotate_mode, req->win0.enable,
+		req->win0.is_10b_compact, req->win0.is_10b_endian);
+
+	if (req->win1.yrgb_addr != 0 || req->win1.uv_addr != 0
+		|| req->win1.v_addr != 0) {
+		rga_job_log(job, "win1: y = %lx uv = %lx v = %lx src_w = %d src_h = %d\n",
+			req->win1.yrgb_addr, req->win1.uv_addr,
+			req->win1.v_addr, req->win1.src_act_w,
+			req->win1.src_act_h);
+		rga_job_log(job, "win1: vw = %d vh = %d xoff = %d yoff = %d format = %s\n",
+			req->win1.vir_w, req->win1.vir_h,
+			req->win1.x_offset, req->win1.y_offset,
+			rga_get_format_name(req->win1.format));
+		rga_job_log(job, "win1: dst_w = %d, dst_h = %d, rd_mode = %d\n",
+			req->win1.dst_act_w, req->win1.dst_act_h,
+			req->win1.rd_mode);
+		rga_job_log(job, "win1: rot_mode = %d, en = %d, compact = %d, endian = %d\n",
+			req->win1.rotate_mode, req->win1.enable,
+			req->win1.is_10b_compact, req->win1.is_10b_endian);
+	}
+
+	rga_job_log(job, "wr: y = %lx uv = %lx v = %lx vw = %d vh = %d\n",
+		req->wr.yrgb_addr, req->wr.uv_addr, req->wr.v_addr,
+		req->wr.vir_w, req->wr.vir_h);
+	rga_job_log(job, "wr: ovlp_xoff = %d ovlp_yoff = %d format = %s rdmode = %d\n",
+		req->wr.x_offset, req->wr.y_offset,
+		rga_get_format_name(req->wr.format), req->wr.rd_mode);
+
+	rga_job_log(job, "mmu: win0 = %.2x win1 = %.2x wr = %.2x\n",
+		req->mmu_info.src0_mmu_flag, req->mmu_info.src1_mmu_flag,
+		req->mmu_info.dst_mmu_flag);
+	rga_job_log(job, "alpha: flag %x mode=%s\n",
+		req->alpha_rop_flag, rga_get_blend_mode_str(req->alpha_config.mode));
+	rga_job_log(job, "alpha: pre_multi=[%d,%d] pixl=[%d,%d] glb=[%d,%d]\n",
+		req->alpha_config.fg_pre_multiplied, req->alpha_config.bg_pre_multiplied,
+		req->alpha_config.fg_pixel_alpha_en, req->alpha_config.bg_pixel_alpha_en,
+		req->alpha_config.fg_global_alpha_en, req->alpha_config.bg_global_alpha_en);
+	rga_job_log(job, "alpha: fg_global_alpha=%x bg_global_alpha=%x\n",
+		req->alpha_config.fg_global_alpha_value, req->alpha_config.bg_global_alpha_value);
+	rga_job_log(job, "yuv2rgb mode is %x\n", req->yuv2rgb_mode);
+}
+
+static int rga3_align_check(struct rga_job *job, struct rga3_req *req)
+{
+	if (rga_is_yuv10bit_format(req->win0.format))
+		if ((req->win0.x_offset % 4) || (req->win0.y_offset % 2) ||
+			(req->win0.src_act_w % 4) || (req->win0.src_act_h % 2))
+			rga_job_log(job, "yuv10bit err win0 wstride is not align\n");
+	if (rga_is_yuv10bit_format(req->win1.format))
+		if ((req->win1.x_offset % 4) || (req->win1.y_offset % 2) ||
+			(req->win1.src_act_w % 4) || (req->win1.src_act_h % 2))
+			rga_job_log(job, "yuv10bit err win1 wstride is not align\n");
+	if (rga_is_yuv8bit_format(req->win0.format))
+		if ((req->win0.x_offset % 2) || (req->win0.y_offset % 2) ||
+			(req->win0.src_act_w % 2) || (req->win0.src_act_h % 2))
+			rga_job_log(job, "yuv8bit err win0 wstride is not align\n");
+	if (rga_is_yuv8bit_format(req->win1.format))
+		if ((req->win1.x_offset % 2) || (req->win1.y_offset % 2) ||
+			(req->win1.src_act_w % 2) || (req->win1.src_act_h % 2))
+			rga_job_log(job, "yuv8bit err win1 wstride is not align\n");
+	return 0;
+}
+
+static int rga3_init_reg(struct rga_job *job)
+{
+	struct rga3_req req;
+	int ret = 0;
+	struct rga_scheduler_t *scheduler = NULL;
+	ktime_t timestamp = ktime_get();
+
+	scheduler = job->scheduler;
+	if (unlikely(scheduler == NULL)) {
+		rga_job_err(job, "failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	memset(&req, 0x0, sizeof(req));
+
+	rga_cmd_to_rga3_cmd(&job->rga_command_base, &req);
+
+	/* check value if legal */
+	ret = rga3_check_param(job, scheduler->data, &req);
+	if (ret == -EINVAL) {
+		rga_job_err(job, "req argument is inval\n");
+		return ret;
+	}
+
+	rga3_align_check(job, &req);
+
+	/* for debug */
+	if (DEBUGGER_EN(MSG))
+		print_debug_info(job, &req);
+
+	if (rga3_gen_reg_info((uint8_t *) job->cmd_buf->vaddr, &req) == -1) {
+		rga_job_err(job, "RKA: gen reg info error\n");
+		return -EINVAL;
+	}
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "generate register cost time %lld us\n",
+			ktime_us_delta(ktime_get(), timestamp));
+
+	return ret;
+}
+
+static void rga3_dump_read_back_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	unsigned long flags;
+	uint32_t cmd_reg[48] = {0};
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	for (i = 0; i < 48; i++)
+		cmd_reg[i] = rga_read(0x100 + i * 4, scheduler);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rga_job_log(job, "CMD_READ_BACK_REG\n");
+	for (i = 0; i < 12; i++)
+		rga_job_log(job, "i = %x : %.8x %.8x %.8x %.8x\n", i,
+			cmd_reg[0 + i * 4], cmd_reg[1 + i * 4],
+			cmd_reg[2 + i * 4], cmd_reg[3 + i * 4]);
+}
+
+static int rga3_set_reg(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int i;
+	bool master_mode_en;
+	uint32_t sys_ctrl;
+	uint32_t *cmd;
+	ktime_t now = ktime_get();
+
+	cmd = job->cmd_buf->vaddr;
+
+	/*
+	 * Currently there is no iova allocated for storing cmd for the IOMMU device,
+	 * so the iommu device needs to use the slave mode.
+	 */
+	if (scheduler->data->mmu != RGA_IOMMU)
+		master_mode_en = true;
+	else
+		master_mode_en = false;
+
+	if (DEBUGGER_EN(REG)) {
+		rga_job_log(job, "CMD_REG\n");
+		for (i = 0; i < 12; i++)
+			rga_job_log(job, "i = %x : %.8x %.8x %.8x %.8x\n", i,
+				cmd[0 + i * 4], cmd[1 + i * 4],
+				cmd[2 + i * 4], cmd[3 + i * 4]);
+	}
+
+	/* All CMD finish int */
+	rga_write(m_RGA3_INT_FRM_DONE | m_RGA3_INT_CMD_LINE_FINISH | m_RGA3_INT_ERROR_MASK,
+		  RGA3_INT_EN, scheduler);
+
+	if (master_mode_en) {
+		/* master mode */
+		sys_ctrl = s_RGA3_SYS_CTRL_CMD_MODE(1);
+
+		rga_write(job->cmd_buf->dma_addr, RGA3_CMD_ADDR, scheduler);
+		rga_write(sys_ctrl, RGA3_SYS_CTRL, scheduler);
+		rga_write(m_RGA3_CMD_CTRL_CMD_LINE_ST_P, RGA3_CMD_CTRL, scheduler);
+	} else {
+		/* slave mode */
+		sys_ctrl = s_RGA3_SYS_CTRL_CMD_MODE(0) | m_RGA3_SYS_CTRL_RGA_SART;
+
+		for (i = 0; i <= 50; i++)
+			rga_write(cmd[i], 0x100 + i * 4, scheduler);
+
+		rga_write(sys_ctrl, RGA3_SYS_CTRL, scheduler);
+	}
+
+	if (DEBUGGER_EN(REG)) {
+		rga_job_log(job, "sys_ctrl = 0x%x, int_en = 0x%x, int_raw = 0x%x\n",
+			rga_read(RGA3_SYS_CTRL, scheduler),
+			rga_read(RGA3_INT_EN, scheduler),
+			rga_read(RGA3_INT_RAW, scheduler));
+
+		rga_job_log(job, "hw_status = 0x%x, cmd_status = 0x%x\n",
+			rga_read(RGA3_STATUS0, scheduler),
+			rga_read(RGA3_CMD_STATE, scheduler));
+	}
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "set register cost time %lld us\n",
+			ktime_us_delta(ktime_get(), now));
+
+	job->timestamp.hw_execute = now;
+	job->timestamp.hw_recode = now;
+	job->session->last_active = now;
+
+	if (DEBUGGER_EN(REG))
+		rga3_dump_read_back_reg(job, scheduler);
+
+	return 0;
+}
+
+static int rga3_get_version(struct rga_scheduler_t *scheduler)
+{
+	u32 major_version, minor_version, svn_version;
+	u32 reg_version;
+
+	if (!scheduler) {
+		rga_err("scheduler is null\n");
+		return -EINVAL;
+	}
+
+	reg_version = rga_read(RGA3_VERSION_NUM, scheduler);
+
+	major_version = (reg_version & RGA3_MAJOR_VERSION_MASK) >> 28;
+	minor_version = (reg_version & RGA3_MINOR_VERSION_MASK) >> 20;
+	svn_version = (reg_version & RGA3_SVN_VERSION_MASK);
+
+	snprintf(scheduler->version.str, 10, "%x.%01x.%05x", major_version,
+		 minor_version, svn_version);
+
+	scheduler->version.major = major_version;
+	scheduler->version.minor = minor_version;
+	scheduler->version.revision = svn_version;
+
+	return 0;
+}
+
+static int rga3_read_status(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	job->intr_status = rga_read(RGA3_INT_RAW, scheduler);
+	job->hw_status = rga_read(RGA3_STATUS0, scheduler);
+	job->cmd_status = rga_read(RGA3_CMD_STATE, scheduler);
+	job->work_cycle = 0;
+
+	return 0;
+}
+
+static void rga3_clear_intr(struct rga_scheduler_t *scheduler)
+{
+	rga_write(m_RGA3_INT_FRM_DONE | m_RGA3_INT_CMD_LINE_FINISH | m_RGA3_INT_ERROR_MASK,
+		  RGA3_INT_CLR, scheduler);
+}
+
+static int rga3_irq(struct rga_scheduler_t *scheduler)
+{
+	struct rga_job *job = scheduler->running_job;
+
+	if (job == NULL) {
+		rga3_clear_intr(scheduler);
+		rga_err("core[%d], invalid job, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x]\n",
+			scheduler->core, rga_read(RGA3_INT_RAW, scheduler),
+			rga_read(RGA3_STATUS0, scheduler), rga_read(RGA3_CMD_STATE, scheduler));
+
+		return IRQ_HANDLED;
+	}
+
+	if (test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		rga3_clear_intr(scheduler);
+		return IRQ_WAKE_THREAD;
+	}
+
+	scheduler->ops->read_status(job, scheduler);
+
+	if (DEBUGGER_EN(INT_FLAG))
+		rga_job_log(job, "irq handler, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x]\n",
+			job->intr_status, job->hw_status, job->cmd_status);
+
+	if (job->intr_status & (m_RGA3_INT_FRM_DONE | m_RGA3_INT_CMD_LINE_FINISH)) {
+		set_bit(RGA_JOB_STATE_FINISH, &job->state);
+	} else if (job->intr_status & m_RGA3_INT_ERROR_MASK) {
+		set_bit(RGA_JOB_STATE_INTR_ERR, &job->state);
+
+		rga_job_err(job, "irq handler err! INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x]\n",
+		       job->intr_status, job->hw_status, job->cmd_status);
+		scheduler->ops->soft_reset(scheduler);
+	}
+
+	rga3_clear_intr(scheduler);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static int rga3_isr_thread(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	if (DEBUGGER_EN(INT_FLAG))
+		rga_job_log(job, "isr thread, INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x]\n",
+			rga_read(RGA3_INT_RAW, scheduler),
+			rga_read(RGA3_STATUS0, scheduler),
+			rga_read(RGA3_CMD_STATE, scheduler));
+
+	if (test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		if (job->intr_status & m_RGA3_INT_RAG_MI_RD_BUS_ERR) {
+			rga_job_err(job, "DMA read bus error, please check size of the input_buffer or whether the buffer has been freed.\n");
+			job->ret = -EFAULT;
+		} else if (job->intr_status & m_RGA3_INT_WIN0_FBCD_DEC_ERR) {
+			rga_job_err(job, "win0 FBC decoder error, please check the fbc image of the source.\n");
+			job->ret = -EFAULT;
+		} else if (job->intr_status & m_RGA3_INT_WIN1_FBCD_DEC_ERR) {
+			rga_job_err(job, "win1 FBC decoder error, please check the fbc image of the source.\n");
+			job->ret = -EFAULT;
+		} else if (job->intr_status & m_RGA3_INT_RGA_MI_WR_BUS_ERR) {
+			rga_job_err(job, "wr buss error, please check size of the output_buffer or whether the buffer has been freed.\n");
+			job->ret = -EFAULT;
+		}
+
+		if (job->ret == 0) {
+			rga_job_err(job, "rga intr error[0x%x]!\n", job->intr_status);
+			job->ret = -EFAULT;
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+const struct rga_backend_ops rga3_ops = {
+	.get_version = rga3_get_version,
+	.set_reg = rga3_set_reg,
+	.init_reg = rga3_init_reg,
+	.soft_reset = rga3_soft_reset,
+	.read_back_reg = NULL,
+	.read_status = rga3_read_status,
+	.irq = rga3_irq,
+	.isr_thread = rga3_isr_thread,
+};
diff --git a/drivers/video/rockchip/rga3/rga_common.c b/drivers/video/rockchip/rga3/rga_common.c
new file mode 100644
index 0000000000000..a8af724303030
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_common.c
@@ -0,0 +1,927 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#include "rga.h"
+#include "rga_common.h"
+
+bool rga_is_rgb_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_RGBX_8888:
+	case RGA_FORMAT_RGB_888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_BGRX_8888:
+	case RGA_FORMAT_BGR_888:
+	case RGA_FORMAT_RGB_565:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGR_565:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_XRGB_8888:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_XBGR_8888:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_Y4:
+	case RGA_FORMAT_Y8:
+	case RGA_FORMAT_YCbCr_400:
+
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_422_SP:
+	case RGA_FORMAT_YCrCb_422_P:
+	case RGA_FORMAT_YCrCb_420_SP:
+	case RGA_FORMAT_YCrCb_420_P:
+
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_422:
+	case RGA_FORMAT_UYVY_420:
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+
+	case RGA_FORMAT_YCbCr_444_SP:
+	case RGA_FORMAT_YCrCb_444_SP:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_alpha_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+
+	case RGA_FORMAT_A8:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv420_packed_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_420:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv420_planar_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_420_P:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv420_semi_planar_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCrCb_420_SP:
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv422_packed_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_UYVY_422:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv422_planar_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_P:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv422_semi_planar_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCrCb_422_SP:
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv8bit_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_Y4:
+	case RGA_FORMAT_Y8:
+	case RGA_FORMAT_YCbCr_400:
+
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_422_SP:
+	case RGA_FORMAT_YCrCb_422_P:
+	case RGA_FORMAT_YCrCb_420_SP:
+	case RGA_FORMAT_YCrCb_420_P:
+
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_422:
+	case RGA_FORMAT_UYVY_420:
+
+	case RGA_FORMAT_YCbCr_444_SP:
+	case RGA_FORMAT_YCrCb_444_SP:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv10bit_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_yuv422p_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_P:
+		return true;
+	default:
+		return false;
+	}
+}
+
+bool rga_is_only_y_format(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_YCbCr_400:
+	case RGA_FORMAT_Y4:
+	case RGA_FORMAT_Y8:
+		return true;
+	default:
+		return false;
+	}
+}
+
+const char *rga_get_format_name(uint32_t format)
+{
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+		return "RGBA8888";
+	case RGA_FORMAT_RGBX_8888:
+		return "RGBX8888";
+	case RGA_FORMAT_RGB_888:
+		return "RGB888";
+	case RGA_FORMAT_BGRA_8888:
+		return "BGRA8888";
+	case RGA_FORMAT_BGRX_8888:
+		return "BGRX8888";
+	case RGA_FORMAT_BGR_888:
+		return "BGR888";
+	case RGA_FORMAT_RGB_565:
+		return "RGB565";
+	case RGA_FORMAT_RGBA_5551:
+		return "RGBA5551";
+	case RGA_FORMAT_RGBA_4444:
+		return "RGBA4444";
+	case RGA_FORMAT_BGR_565:
+		return "BGR565";
+	case RGA_FORMAT_BGRA_5551:
+		return "BGRA5551";
+	case RGA_FORMAT_BGRA_4444:
+		return "BGRA4444";
+
+	case RGA_FORMAT_YCbCr_422_SP:
+		return "YCbCr422SP";
+	case RGA_FORMAT_YCbCr_422_P:
+		return "YCbCr422P";
+	case RGA_FORMAT_YCbCr_420_SP:
+		return "YCbCr420SP";
+	case RGA_FORMAT_YCbCr_420_P:
+		return "YCbCr420P";
+	case RGA_FORMAT_YCrCb_422_SP:
+		return "YCrCb422SP";
+	case RGA_FORMAT_YCrCb_422_P:
+		return "YCrCb422P";
+	case RGA_FORMAT_YCrCb_420_SP:
+		return "YCrCb420SP";
+	case RGA_FORMAT_YCrCb_420_P:
+		return "YCrCb420P";
+
+	case RGA_FORMAT_YVYU_422:
+		return "YVYU422";
+	case RGA_FORMAT_YVYU_420:
+		return "YVYU420";
+	case RGA_FORMAT_VYUY_422:
+		return "VYUY422";
+	case RGA_FORMAT_VYUY_420:
+		return "VYUY420";
+	case RGA_FORMAT_YUYV_422:
+		return "YUYV422";
+	case RGA_FORMAT_YUYV_420:
+		return "YUYV420";
+	case RGA_FORMAT_UYVY_422:
+		return "UYVY422";
+	case RGA_FORMAT_UYVY_420:
+		return "UYVY420";
+
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+		return "YCrCb420SP10B";
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		return "YCbCr420SP10B";
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+		return "YCbCr422SP10B";
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		return "YCrCb422SP10B";
+	case RGA_FORMAT_BPP1:
+		return "BPP1";
+	case RGA_FORMAT_BPP2:
+		return "BPP2";
+	case RGA_FORMAT_BPP4:
+		return "BPP4";
+	case RGA_FORMAT_BPP8:
+		return "BPP8";
+	case RGA_FORMAT_YCbCr_400:
+		return "YCbCr400";
+	case RGA_FORMAT_Y4:
+		return "Y4";
+
+	case RGA_FORMAT_ARGB_8888:
+		return "ARGB8888";
+	case RGA_FORMAT_XRGB_8888:
+		return "XRGB8888";
+	case RGA_FORMAT_ARGB_5551:
+		return "ARGB5551";
+	case RGA_FORMAT_ARGB_4444:
+		return "ARGB4444";
+	case RGA_FORMAT_ABGR_8888:
+		return "ABGR8888";
+	case RGA_FORMAT_XBGR_8888:
+		return "XBGR8888";
+	case RGA_FORMAT_ABGR_5551:
+		return "ABGR5551";
+	case RGA_FORMAT_ABGR_4444:
+		return "ABGR4444";
+
+	case RGA_FORMAT_RGBA_2BPP:
+		return "RGBA2BPP";
+
+	case RGA_FORMAT_A8:
+		return "alpha-8";
+	case RGA_FORMAT_YCbCr_444_SP:
+		return "YCbCr444SP";
+	case RGA_FORMAT_YCrCb_444_SP:
+		return "YCrCb444SP";
+
+	case RGA_FORMAT_Y8:
+		return "Y8";
+
+	default:
+		return "UNF";
+	}
+}
+
+int rga_get_format_bits(uint32_t format)
+{
+	int bits = 0;
+
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_RGBX_8888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_BGRX_8888:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_XRGB_8888:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_XBGR_8888:
+		bits = 32;
+		break;
+	case RGA_FORMAT_RGB_888:
+	case RGA_FORMAT_BGR_888:
+	case RGA_FORMAT_YCbCr_444_SP:
+	case RGA_FORMAT_YCrCb_444_SP:
+		bits = 24;
+		break;
+	case RGA_FORMAT_RGB_565:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGR_565:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_SP:
+	case RGA_FORMAT_YCrCb_422_P:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_UYVY_422:
+	case RGA_FORMAT_VYUY_422:
+	/* YUV 420 packed according to the arrangement of YUV422 packed. */
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_UYVY_420:
+	case RGA_FORMAT_VYUY_420:
+		bits = 16;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_420_SP:
+	case RGA_FORMAT_YCrCb_420_P:
+		bits = 12;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		bits = 15;
+		break;
+	case RGA_FORMAT_YCbCr_400:
+	case RGA_FORMAT_BPP8:
+	case RGA_FORMAT_A8:
+	case RGA_FORMAT_Y8:
+		bits = 8;
+		break;
+	case RGA_FORMAT_Y4:
+	case RGA_FORMAT_BPP4:
+		bits = 4;
+		break;
+	case RGA_FORMAT_BPP2:
+		bits = 2;
+		break;
+	case RGA_FORMAT_BPP1:
+		bits = 1;
+		break;
+	default:
+		rga_err("unknown format [0x%x]\n", format);
+		return -1;
+	}
+
+	return bits;
+}
+
+int rga_get_pixel_stride_from_format(uint32_t format)
+{
+	int pixel_stride = 0;
+
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_RGBX_8888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_BGRX_8888:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_XRGB_8888:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_XBGR_8888:
+		pixel_stride = 32;
+		break;
+	case RGA_FORMAT_RGB_888:
+	case RGA_FORMAT_BGR_888:
+	case RGA_FORMAT_YCbCr_444_SP:
+	case RGA_FORMAT_YCrCb_444_SP:
+		pixel_stride = 24;
+		break;
+	case RGA_FORMAT_RGB_565:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGR_565:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_422:
+	case RGA_FORMAT_UYVY_420:
+		pixel_stride = 16;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		pixel_stride = 10;
+		break;
+	case RGA_FORMAT_BPP1:
+	case RGA_FORMAT_BPP2:
+	case RGA_FORMAT_BPP4:
+	case RGA_FORMAT_BPP8:
+	case RGA_FORMAT_YCbCr_400:
+	case RGA_FORMAT_A8:
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_420_SP:
+	case RGA_FORMAT_YCrCb_420_P:
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_SP:
+	case RGA_FORMAT_YCrCb_422_P:
+	case RGA_FORMAT_Y8:
+		pixel_stride = 8;
+		break;
+	case RGA_FORMAT_Y4:
+		pixel_stride = 4;
+		break;
+	default:
+		rga_err("unknown format [0x%x]\n", format);
+		return -1;
+	}
+
+	return pixel_stride;
+}
+
+const char *rga_get_render_mode_str(uint8_t mode)
+{
+	switch (mode) {
+	case 0x0:
+		return "bitblt";
+	case 0x1:
+		return "RGA_COLOR_PALETTE";
+	case 0x2:
+		return "RGA_COLOR_FILL";
+	case 0x3:
+		return "update_palette_table";
+	case 0x4:
+		return "update_patten_buff";
+	default:
+		return "UNF";
+	}
+}
+
+const char *rga_get_store_mode_str(uint32_t mode)
+{
+	switch (mode) {
+	case RGA_RASTER_MODE:
+		return "raster";
+	case RGA_FBC_MODE:
+		return "afbc16x16";
+	case RGA_TILE_MODE:
+		return "tile8x8";
+	case RGA_TILE4x4_MODE:
+		return "tile4x4";
+	case RGA_RKFBC_MODE:
+		return "rkfbc64x4";
+	case RGA_AFBC32x8_MODE:
+		return "afbc32x8";
+	default:
+		return "unknown";
+	}
+}
+
+const char *rga_get_interp_str(uint8_t interp)
+{
+	switch (interp) {
+	case RGA_INTERP_DEFAULT:
+		return "default";
+	case RGA_INTERP_LINEAR:
+		return "bi-linear";
+	case RGA_INTERP_BICUBIC:
+		return "bi-cubic";
+	case RGA_INTERP_AVERAGE:
+		return "average_filter";
+	default:
+		return "unknown";
+	}
+}
+
+const char *rga_get_rotate_mode_str(uint8_t mode)
+{
+	switch (mode) {
+	case 0x0:
+		return "0";
+	case 0x1:
+		return "90 degree";
+	case 0x2:
+		return "180 degree";
+	case 0x3:
+		return "270 degree";
+	case 0x10:
+		return "xmirror";
+	case 0x20:
+		return "ymirror";
+	case 0x30:
+		return "xymirror";
+	default:
+		return "UNF";
+	}
+}
+
+const char *rga_get_blend_mode_str(enum rga_alpha_blend_mode mode)
+{
+	switch (mode) {
+	case RGA_ALPHA_NONE:
+		return "no blend";
+
+	case RGA_ALPHA_BLEND_SRC:
+		return "src";
+
+	case RGA_ALPHA_BLEND_DST:
+		return "dst";
+
+	case RGA_ALPHA_BLEND_SRC_OVER:
+		return "src-over";
+
+	case RGA_ALPHA_BLEND_DST_OVER:
+		return "dst-over";
+
+	case RGA_ALPHA_BLEND_SRC_IN:
+		return "src-in";
+
+	case RGA_ALPHA_BLEND_DST_IN:
+		return "dst-in";
+
+	case RGA_ALPHA_BLEND_SRC_OUT:
+		return "src-out";
+
+	case RGA_ALPHA_BLEND_DST_OUT:
+		return "dst-out";
+
+	case RGA_ALPHA_BLEND_SRC_ATOP:
+		return "src-atop";
+
+	case RGA_ALPHA_BLEND_DST_ATOP:
+		return "dst-atop";
+
+	case RGA_ALPHA_BLEND_XOR:
+		return "xor";
+
+	case RGA_ALPHA_BLEND_CLEAR:
+		return "clear";
+
+	default:
+		return "check reg for more imformation";
+	}
+}
+
+const char *rga_get_memory_type_str(uint8_t type)
+{
+	switch (type) {
+	case RGA_DMA_BUFFER:
+		return "dma_fd";
+	case RGA_VIRTUAL_ADDRESS:
+		return "virt_addr";
+	case RGA_PHYSICAL_ADDRESS:
+		return "phys_addr";
+	case RGA_DMA_BUFFER_PTR:
+		return "dma_buf_ptr";
+	default:
+		return "UNF";
+	}
+}
+
+const char *rga_get_mmu_type_str(enum rga_mmu mmu_type)
+{
+	switch (mmu_type) {
+	case RGA_MMU:
+		return "RGA_MMU";
+	case RGA_IOMMU:
+		return "RK_IOMMU";
+	default:
+		return "NONE_MMU";
+	}
+}
+
+const char *rga_get_dma_data_direction_str(enum dma_data_direction dir)
+{
+	switch (dir) {
+	case DMA_BIDIRECTIONAL:
+		return "bidirectional";
+	case DMA_TO_DEVICE:
+		return "to_device";
+	case DMA_FROM_DEVICE:
+		return "from_device";
+	case DMA_NONE:
+		return "none";
+	default:
+		return "unknown";
+	}
+}
+
+const char *rga_get_core_name(enum RGA_SCHEDULER_CORE core)
+{
+	switch (core) {
+	case RGA3_SCHEDULER_CORE0:
+		return "RGA3_core0";
+	case RGA3_SCHEDULER_CORE1:
+		return "RGA3_core1";
+	case RGA2_SCHEDULER_CORE0:
+		return "RGA2_core0";
+	case RGA2_SCHEDULER_CORE1:
+		return "RGA2_core1";
+	default:
+		return "unknown_core";
+	}
+}
+
+void rga_convert_addr(struct rga_img_info_t *img, bool before_vir_get_channel)
+{
+	/*
+	 * If it is not using dma fd, the virtual/phyical address is assigned
+	 * to the address of the corresponding channel.
+	 */
+
+	//img->yrgb_addr = img->uv_addr;
+
+	/*
+	 * if before_vir_get_channel is true, then convert addr by default
+	 * when has iova (before_vir_get_channel is false),
+	 * need to consider whether fbc case
+	 */
+	if (img->rd_mode != RGA_FBC_MODE || before_vir_get_channel) {
+		img->uv_addr = img->yrgb_addr + (img->vir_w * img->vir_h);
+
+		//warning: rga3 may need /2 for all
+		if (rga_is_yuv422p_format(img->format))
+			img->v_addr =
+				img->uv_addr + (img->vir_w * img->vir_h) / 2;
+		else
+			img->v_addr =
+				img->uv_addr + (img->vir_w * img->vir_h) / 4;
+	} else {
+		img->uv_addr = img->yrgb_addr;
+		img->v_addr = 0;
+	}
+}
+
+void rga_swap_pd_mode(struct rga_req *req_rga)
+{
+	if (((req_rga->alpha_rop_flag) & 1)) {
+		if ((req_rga->alpha_rop_flag >> 3) & 1) {
+			if (req_rga->PD_mode == 1)
+				req_rga->PD_mode = 2;
+			else if (req_rga->PD_mode == 2)
+				req_rga->PD_mode = 1;
+			else if (req_rga->PD_mode == 3)
+				req_rga->PD_mode = 4;
+			else if (req_rga->PD_mode == 4)
+				req_rga->PD_mode = 3;
+		}
+	}
+}
+
+int rga_image_size_cal(int w, int h, int format,
+		       int *yrgb_size, int *uv_size, int *v_size)
+{
+	int yrgb = 0;
+	int uv = 0;
+	int v = 0;
+
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_RGBX_8888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_BGRX_8888:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_XRGB_8888:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_XBGR_8888:
+		yrgb = w * h * 4;
+		break;
+	case RGA_FORMAT_RGB_888:
+	case RGA_FORMAT_BGR_888:
+		yrgb = w * h * 3;
+		break;
+	case RGA_FORMAT_RGB_565:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGR_565:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_UYVY_422:
+	/* YUV 420 packed according to the arrangement of YUV422 packed. */
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_420:
+		yrgb = w * h * 2;
+		break;
+	/* YUV FORMAT */
+	case RGA_FORMAT_YCbCr_444_SP:
+	case RGA_FORMAT_YCrCb_444_SP:
+		yrgb = w * h;
+		uv = w * h;
+		v = w * h;
+		break;
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCrCb_422_SP:
+	/* 10bit format stride is externally configured. */
+	case RGA_FORMAT_YCbCr_422_SP_10B:
+	case RGA_FORMAT_YCrCb_422_SP_10B:
+		yrgb = w * h;
+		uv = w * h;
+		break;
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_P:
+		yrgb = w * h;
+		uv = (w * h) >> 1;
+		v = uv;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCrCb_420_SP:
+	/* 10bit format stride is externally configured. */
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		yrgb = w * h;
+		uv = (w * h) >> 1;
+		break;
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_420_P:
+		yrgb = w * h;
+		uv = (w * h) >> 2;
+		v = uv;
+		break;
+	case RGA_FORMAT_YCbCr_400:
+	case RGA_FORMAT_A8:
+	case RGA_FORMAT_Y8:
+		yrgb = w * h;
+		break;
+	case RGA_FORMAT_Y4:
+		yrgb = (w * h) >> 1;
+		break;
+	default:
+		rga_err("Unsuport format [0x%x]\n", format);
+		return -EFAULT;
+	}
+
+	if (yrgb_size != NULL)
+		*yrgb_size = yrgb;
+	if (uv_size != NULL)
+		*uv_size = uv;
+	if (v_size != NULL)
+		*v_size = v;
+
+	return (yrgb + uv + v);
+}
+
+void rga_dump_memory_parm(struct rga_memory_parm *parm)
+{
+	rga_log("memory param: w = %d, h = %d, f = %s(0x%x), size = %d\n",
+		parm->width, parm->height, rga_get_format_name(parm->format),
+		parm->format, parm->size);
+}
+
+void rga_dump_external_buffer(struct rga_external_buffer *buffer)
+{
+	rga_log("external: memory = 0x%lx, type = %s\n",
+		(unsigned long)buffer->memory, rga_get_memory_type_str(buffer->type));
+	rga_dump_memory_parm(&buffer->memory_parm);
+}
+
+static void rga_dump_image_info(struct rga_request *request, const char *name,
+				struct rga_img_info_t *img, uint8_t handle_flag, int need_mmu)
+{
+	if (handle_flag) {
+		if (img->uv_addr && img->v_addr)
+			rga_req_log(request, "%s: handle[y,uv,v] = [%ld(%#lx), %ld(%#lx), %ld(%#lx)], mode = %s\n",
+				name,
+				(unsigned long)img->yrgb_addr, (unsigned long)img->yrgb_addr,
+				(unsigned long)img->uv_addr, (unsigned long)img->uv_addr,
+				(unsigned long)img->v_addr, (unsigned long)img->v_addr,
+				rga_get_store_mode_str(img->rd_mode));
+		else if (img->uv_addr)
+			rga_req_log(request, "%s: handle[y,uv] = [%ld(%#lx), %ld(%#lx)], mode = %s\n",
+				name,
+				(unsigned long)img->yrgb_addr, (unsigned long)img->yrgb_addr,
+				(unsigned long)img->uv_addr, (unsigned long)img->uv_addr,
+				rga_get_store_mode_str(img->rd_mode));
+		else
+			rga_req_log(request, "%s: handle = %ld(%#lx), mode = %s\n",
+				name,
+				(unsigned long)img->yrgb_addr, (unsigned long)img->yrgb_addr,
+				rga_get_store_mode_str(img->rd_mode));
+	} else {
+		if (img->yrgb_addr)
+			rga_req_log(request, "%s: fd = %ld(%#lx), mode = %s\n",
+				name,
+				(unsigned long)img->yrgb_addr, (unsigned long)img->yrgb_addr,
+				rga_get_store_mode_str(img->rd_mode));
+		else if (img->uv_addr)
+			rga_req_log(request, "%s: %s = %#lx, mode = %s\n",
+				name,
+				need_mmu ? "virt_addr" : "phys_addr", (unsigned long)img->uv_addr,
+				rga_get_store_mode_str(img->rd_mode));
+	}
+
+	rga_req_log(request, "%s: rect[x,y,w,h] = [%d, %d, %d, %d], stride[w,h] = [%d, %d], format = %s(%#x)\n",
+		name,
+		img->x_offset, img->y_offset, img->act_w, img->act_h, img->vir_w, img->vir_h,
+		rga_get_format_name(img->format), img->format);
+}
+
+void rga_dump_req(struct rga_request *request, struct rga_req *req)
+{
+	rga_req_log(request, "render_mode = %d, bitblit_mode = %d, rotate_mode = %d\n",
+		req->render_mode, req->bsfilter_flag,
+		req->rotate_mode);
+
+	rga_dump_image_info(request, "src", &req->src, req->handle_flag,
+			    (req->mmu_info.mmu_flag >> 8) & 1);
+	if (req->pat.yrgb_addr != 0 || req->pat.uv_addr != 0 || req->pat.v_addr != 0)
+		rga_dump_image_info(request, "pat", &req->pat, req->handle_flag,
+				    (req->mmu_info.mmu_flag >> 9) & 1);
+	rga_dump_image_info(request, "dst", &req->dst, req->handle_flag,
+			    (req->mmu_info.mmu_flag >> 10) & 1);
+
+	rga_req_log(request, "mmu: mmu_flag = %#x en = %#x\n",
+		req->mmu_info.mmu_flag, req->mmu_info.mmu_en);
+	rga_req_log(request, "alpha: rop_mode = %#x\n", req->alpha_rop_mode);
+	rga_req_log(request, "csc = %#x\n", req->yuv2rgb_mode);
+	rga_req_log(request, "imterplotion: horiz = %s(%#x), verti = %s(%#x)\n",
+		rga_get_interp_str(req->interp.horiz), req->interp.horiz,
+		rga_get_interp_str(req->interp.verti), req->interp.verti);
+	rga_req_log(request, "core_mask = %#x, priority = %d, in_fence = %d(%#x)\n",
+		req->core, req->priority, req->in_fence_fd, req->in_fence_fd);
+}
diff --git a/drivers/video/rockchip/rga3/rga_debugger.c b/drivers/video/rockchip/rga3/rga_debugger.c
new file mode 100644
index 0000000000000..93c5fcf3ec48c
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_debugger.c
@@ -0,0 +1,1003 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *	Cerf Yu <cerf.yu@rock-chips.com>
+ *	Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/syscalls.h>
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include "rga.h"
+#include "rga_debugger.h"
+#include "rga_drv.h"
+#include "rga_mm.h"
+#include "rga_common.h"
+#include "rga_job.h"
+
+#define RGA_DEBUGGER_ROOT_NAME "rkrga"
+
+#define STR_ENABLE(en) (en ? "EN" : "DIS")
+
+int RGA_DEBUG_REG;
+int RGA_DEBUG_MSG;
+int RGA_DEBUG_TIME;
+int RGA_DEBUG_INT_FLAG;
+int RGA_DEBUG_MM;
+int RGA_DEBUG_CHECK_MODE;
+int RGA_DEBUG_INTERNAL_MODE;
+int RGA_DEBUG_NONUSE;
+int RGA_DEBUG_DEBUG_MODE;
+int RGA_DEBUG_DUMP_IMAGE;
+
+#ifdef CONFIG_NO_GKI
+static char g_dump_path[100] = "/data";
+#endif
+
+static int rga_debug_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "REG [%s]\n"
+		 "MSG [%s]\n"
+		 "TIME [%s]\n"
+		 "INT [%s]\n"
+		 "MM [%s]\n"
+		 "CHECK [%s]\n"
+		 "INTL [%s]\n"
+		 "STOP [%s]\n",
+		 STR_ENABLE(RGA_DEBUG_REG),
+		 STR_ENABLE(RGA_DEBUG_MSG),
+		 STR_ENABLE(RGA_DEBUG_TIME),
+		 STR_ENABLE(RGA_DEBUG_INT_FLAG),
+		 STR_ENABLE(RGA_DEBUG_MM),
+		 STR_ENABLE(RGA_DEBUG_CHECK_MODE),
+		 STR_ENABLE(RGA_DEBUG_INTERNAL_MODE),
+		 STR_ENABLE(RGA_DEBUG_NONUSE));
+
+	seq_puts(m, "\nhelp:\n");
+	seq_puts(m, " 'echo reg > debug' to enable/disable register log printing.\n");
+	seq_puts(m, " 'echo msg > debug' to enable/disable message log printing.\n");
+	seq_puts(m, " 'echo time > debug' to enable/disable time log printing.\n");
+	seq_puts(m, " 'echo int > debug' to enable/disable interruppt log printing.\n");
+	seq_puts(m, " 'echo mm > debug' to enable/disable memory manager log printing.\n");
+	seq_puts(m, " 'echo check > debug' to enable/disable check mode.\n");
+	seq_puts(m, " 'echo intl > debug' to enable/disable internal mode.\n");
+	seq_puts(m, " 'echo stop > debug' to enable/disable stop using hardware\n");
+
+	return 0;
+}
+
+static ssize_t rga_debug_write(struct file *file, const char __user *ubuf,
+				 size_t len, loff_t *offp)
+{
+	char buf[14];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strncmp(buf, "reg", 4) == 0) {
+		if (RGA_DEBUG_REG) {
+			RGA_DEBUG_REG = 0;
+			pr_info("close rga reg!\n");
+		} else {
+			RGA_DEBUG_REG = 1;
+			pr_info("open rga reg!\n");
+		}
+	} else if (strncmp(buf, "msg", 3) == 0) {
+		if (RGA_DEBUG_MSG) {
+			RGA_DEBUG_MSG = 0;
+			pr_info("close rga test MSG!\n");
+		} else {
+			RGA_DEBUG_MSG = 1;
+			pr_info("open rga test MSG!\n");
+		}
+	} else if (strncmp(buf, "time", 4) == 0) {
+		if (RGA_DEBUG_TIME) {
+			RGA_DEBUG_TIME = 0;
+			pr_info("close rga test time!\n");
+		} else {
+			RGA_DEBUG_TIME = 1;
+			pr_info("open rga test time!\n");
+		}
+	} else if (strncmp(buf, "intl", 4) == 0) {
+		if (RGA_DEBUG_INTERNAL_MODE) {
+			RGA_DEBUG_INTERNAL_MODE = 0;
+			pr_info("close rga internal flag!\n");
+		} else {
+			RGA_DEBUG_INTERNAL_MODE = 1;
+			pr_info("open rga internal flag!\n");
+		}
+	} else if (strncmp(buf, "int", 3) == 0) {
+		if (RGA_DEBUG_INT_FLAG) {
+			RGA_DEBUG_INT_FLAG = 0;
+			pr_info("close inturrupt MSG!\n");
+		} else {
+			RGA_DEBUG_INT_FLAG = 1;
+			pr_info("open inturrupt MSG!\n");
+		}
+	} else if (strncmp(buf, "mm", 2) == 0) {
+		if (RGA_DEBUG_MM) {
+			RGA_DEBUG_MM = 0;
+			pr_info("close rga mm log!\n");
+		} else {
+			RGA_DEBUG_MM = 1;
+			pr_info("open rga mm log!\n");
+		}
+	} else if (strncmp(buf, "check", 5) == 0) {
+		if (RGA_DEBUG_CHECK_MODE) {
+			RGA_DEBUG_CHECK_MODE = 0;
+			pr_info("close rga check flag!\n");
+		} else {
+			RGA_DEBUG_CHECK_MODE = 1;
+			pr_info("open rga check flag!\n");
+		}
+	} else if (strncmp(buf, "stop", 4) == 0) {
+		if (RGA_DEBUG_NONUSE) {
+			RGA_DEBUG_NONUSE = 0;
+			pr_info("using rga hardware!\n");
+		} else {
+			RGA_DEBUG_NONUSE = 1;
+			pr_info("stop using rga hardware!\n");
+		}
+	} else if (strncmp(buf, "debug", 3) == 0) {
+		if (RGA_DEBUG_DEBUG_MODE) {
+			RGA_DEBUG_REG = 0;
+			RGA_DEBUG_MSG = 0;
+			RGA_DEBUG_TIME = 0;
+			RGA_DEBUG_INT_FLAG = 0;
+
+			RGA_DEBUG_DEBUG_MODE = 0;
+			pr_info("close debug mode!\n");
+		} else {
+			RGA_DEBUG_REG = 1;
+			RGA_DEBUG_MSG = 1;
+			RGA_DEBUG_TIME = 1;
+			RGA_DEBUG_INT_FLAG = 1;
+
+			RGA_DEBUG_DEBUG_MODE = 1;
+			pr_info("open debug mode!\n");
+		}
+	} else if (strncmp(buf, "slt", 3) == 0) {
+		pr_err("Null");
+	}
+
+	return len;
+}
+
+static int rga_version_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "%s: v%s\n", DRIVER_DESC, DRIVER_VERSION);
+
+	return 0;
+}
+
+static int rga_load_show(struct seq_file *m, void *data)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	struct rga_session_manager *session_manager = NULL;
+	struct rga_session *session = NULL;
+	unsigned long flags;
+	int id = 0;
+	int i;
+	int load;
+	u32 busy_time_total;
+	ktime_t now;
+
+	session_manager = rga_drvdata->session_manager;
+
+	seq_printf(m, "num of scheduler = %d\n", rga_drvdata->num_of_scheduler);
+	seq_puts(m, "================= load ==================\n");
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		seq_printf(m, "scheduler[%d]: %s\n",
+			i, dev_driver_string(scheduler->dev));
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		busy_time_total = scheduler->timer.busy_time_record;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		load = (busy_time_total * 100 / RGA_LOAD_INTERVAL_US);
+		if (load > 100)
+			load = 100;
+
+		seq_printf(m, "\t load = %d%%\n", load);
+		seq_puts(m, "-----------------------------------\n");
+	}
+
+	seq_puts(m, "=========================================\n");
+	seq_puts(m, "<session>  <status>  <tgid>  <process>\n");
+
+	mutex_lock(&session_manager->lock);
+
+	now = ktime_get();
+	idr_for_each_entry(&session_manager->ctx_id_idr, session, id)
+		seq_printf(m, "%-9d  %-8s  %-6d  %-s\n",
+			session->id,
+			ktime_us_delta(now, session->last_active) < RGA_LOAD_ACTIVE_MAX_US ?
+				"active" : "idle",
+			session->tgid, session->pname);
+
+	mutex_unlock(&session_manager->lock);
+
+	return 0;
+}
+
+static int rga_scheduler_show(struct seq_file *m, void *data)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	int i;
+
+	seq_printf(m, "num of scheduler = %d\n", rga_drvdata->num_of_scheduler);
+	seq_printf(m, "===================================\n");
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		seq_printf(m, "scheduler[%d]: %s\n",
+			i, dev_driver_string(scheduler->dev));
+		seq_printf(m, "-----------------------------------\n");
+		seq_printf(m, "pd_ref = %d\n", scheduler->pd_refcount);
+	}
+
+	return 0;
+}
+
+static int rga_mm_session_show(struct seq_file *m, void *data)
+{
+	int id;
+	struct rga_mm *mm_session = NULL;
+	struct rga_internal_buffer *dump_buffer;
+
+	mm_session = rga_drvdata->mm;
+
+	mutex_lock(&mm_session->lock);
+
+	seq_puts(m, "rga_mm dump:\n");
+	seq_printf(m, "buffer count = %d\n", mm_session->buffer_count);
+	seq_puts(m, "===============================================================\n");
+
+	idr_for_each_entry(&mm_session->memory_idr, dump_buffer, id) {
+		seq_printf(m, "handle = %d refcount = %d mm_flag = 0x%x	tgid = %d\n",
+			   dump_buffer->handle, kref_read(&dump_buffer->refcount),
+			   dump_buffer->mm_flag, dump_buffer->session->tgid);
+
+		switch (dump_buffer->type) {
+		case RGA_DMA_BUFFER:
+		case RGA_DMA_BUFFER_PTR:
+			if (rga_mm_is_invalid_dma_buffer(dump_buffer->dma_buffer))
+				break;
+
+			seq_puts(m, "dma_buffer:\n");
+			seq_printf(m, "\t dma_buf = %p, iova = 0x%lxsgt = 0x%p, size = %ld, map_core = 0x%x\n",
+				   dump_buffer->dma_buffer->dma_buf,
+				   (unsigned long)dump_buffer->dma_buffer->iova,
+				   dump_buffer->dma_buffer->sgt,
+				   dump_buffer->dma_buffer->size,
+				   dump_buffer->dma_buffer->scheduler->core);
+
+			if (dump_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+				seq_printf(m, "\t is contiguous, pa = 0x%lx\n",
+					   (unsigned long)dump_buffer->phys_addr);
+
+			break;
+		case RGA_VIRTUAL_ADDRESS:
+			if (dump_buffer->virt_addr == NULL)
+				break;
+			seq_puts(m, "virtual address:\n");
+			seq_printf(m, "\t va = 0x%lx, pages = 0x%p, size = %ld\n",
+				   (unsigned long)dump_buffer->virt_addr->addr,
+				   dump_buffer->virt_addr->pages,
+				   dump_buffer->virt_addr->size);
+
+			if (rga_mm_is_invalid_dma_buffer(dump_buffer->dma_buffer))
+				break;
+
+			seq_printf(m, "\t iova = 0x%lx, offset = 0x%lx, sgt = 0x%p, size = %ld, map_core = 0x%x\n",
+				   (unsigned long)dump_buffer->dma_buffer->iova,
+				   (unsigned long)dump_buffer->dma_buffer->offset,
+				   dump_buffer->dma_buffer->sgt,
+				   dump_buffer->dma_buffer->size,
+				   dump_buffer->dma_buffer->scheduler->core);
+
+			if (dump_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+				seq_printf(m, "\t is contiguous, pa = 0x%lx\n",
+					   (unsigned long)dump_buffer->phys_addr);
+
+			break;
+		case RGA_PHYSICAL_ADDRESS:
+			seq_puts(m, "physical address:\n");
+			seq_printf(m, "\t pa = 0x%lx\n", (unsigned long)dump_buffer->phys_addr);
+			break;
+		default:
+			seq_puts(m, "Illegal external buffer!\n");
+			break;
+		}
+
+		seq_puts(m, "---------------------------------------------------------------\n");
+	}
+	mutex_unlock(&mm_session->lock);
+
+	return 0;
+}
+
+static int rga_request_manager_show(struct seq_file *m, void *data)
+{
+	int id, i;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+	struct rga_req *task_list;
+	unsigned long flags;
+	int task_count = 0;
+	int finished_task_count = 0, failed_task_count = 0;
+
+	request_manager = rga_drvdata->pend_request_manager;
+
+	seq_puts(m, "rga internal request dump:\n");
+	seq_printf(m, "request count = %d\n", request_manager->request_count);
+	seq_puts(m, "===============================================================\n");
+
+	mutex_lock(&request_manager->lock);
+
+	idr_for_each_entry(&request_manager->request_idr, request, id) {
+		seq_printf(m, "------------------ request: %d ------------------\n", request->id);
+
+		spin_lock_irqsave(&request->lock, flags);
+
+		task_count = request->task_count;
+		finished_task_count = request->finished_task_count;
+		failed_task_count = request->failed_task_count;
+		task_list = request->task_list;
+
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		if (task_list == NULL) {
+			seq_puts(m, "\t can not find task list from id\n");
+			continue;
+		}
+
+		seq_printf(m, "\t set cmd num: %d, finish job: %d, failed job: %d, flags = 0x%x, ref = %d\n",
+			   task_count, finished_task_count, failed_task_count,
+			   request->flags, kref_read(&request->refcount));
+
+		seq_puts(m, "\t cmd dump:\n\n");
+
+		for (i = 0; i < request->task_count; i++)
+			rga_request_task_debug_info(m, &(task_list[i]));
+	}
+
+	mutex_unlock(&request_manager->lock);
+
+	return 0;
+}
+
+#ifdef CONFIG_NO_GKI
+static int rga_dump_path_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "dump path: %s\n", g_dump_path);
+
+	return 0;
+}
+
+static ssize_t rga_dump_path_write(struct file *file, const char __user *ubuf,
+				    size_t len, loff_t *offp)
+{
+	char buf[100];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	snprintf(g_dump_path, sizeof(buf), "%s", buf);
+	pr_info("dump path change to: %s\n", g_dump_path);
+
+	return len;
+}
+
+static int rga_dump_image_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "dump image count: %d\n", RGA_DEBUG_DUMP_IMAGE);
+
+	return 0;
+}
+
+static ssize_t rga_dump_image_write(struct file *file, const char __user *ubuf,
+				    size_t len, loff_t *offp)
+{
+	int ret;
+	int dump_count = 0;
+	char buf[14];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoint(buf, 10, &dump_count);
+	if (ret) {
+		pr_err("Failed to parse str[%s]\n", buf);
+		return -EFAULT;
+	}
+
+	if (dump_count <= 0) {
+		pr_err("dump_image count is invalid [%d]!\n", dump_count);
+		return -EINVAL;
+	}
+
+	RGA_DEBUG_DUMP_IMAGE = dump_count;
+	pr_info("dump image %d\n", RGA_DEBUG_DUMP_IMAGE);
+
+	return len;
+}
+#endif /* #ifdef CONFIG_NO_GKI */
+
+static int rga_hardware_show(struct seq_file *m, void *data)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	const struct rga_hw_data *hw_data = NULL;
+	int i;
+
+	seq_puts(m, "===================================\n");
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+		hw_data = scheduler->data;
+
+		seq_printf(m, "%s, core %d: version: %s\n",
+			   dev_driver_string(scheduler->dev),
+			   scheduler->core, scheduler->version.str);
+		seq_printf(m, "input range: %dx%d ~ %dx%d\n",
+			   hw_data->input_range.min.width, hw_data->input_range.min.height,
+			   hw_data->input_range.max.width, hw_data->input_range.max.height);
+		seq_printf(m, "output range: %dx%d ~ %dx%d\n",
+			   hw_data->output_range.min.width, hw_data->output_range.min.height,
+			   hw_data->output_range.max.width, hw_data->output_range.max.height);
+		seq_printf(m, "scale limit: 1/%d ~ %d\n",
+			   (1 << hw_data->max_downscale_factor),
+			   (1 << hw_data->max_upscale_factor));
+		seq_printf(m, "byte_stride_align: %d\n", hw_data->byte_stride_align);
+		seq_printf(m, "max_byte_stride: %d\n", hw_data->max_byte_stride);
+		seq_printf(m, "csc: RGB2YUV 0x%x YUV2RGB 0x%x\n",
+			   hw_data->csc_r2y_mode, hw_data->csc_y2r_mode);
+		seq_printf(m, "feature: 0x%x\n", hw_data->feature);
+		seq_printf(m, "mmu: %s\n", rga_get_mmu_type_str(hw_data->mmu));
+		seq_puts(m, "-----------------------------------\n");
+	}
+
+	return 0;
+}
+
+static int rga_reset_show(struct seq_file *m, void *data)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	int i;
+
+	seq_puts(m, "help:\n");
+	seq_puts(m, " 'echo <core> > reset' to reset hardware.\n");
+
+	seq_puts(m, "core:\n");
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		seq_printf(m, "  %s core <%d>\n",
+			   dev_driver_string(scheduler->dev), scheduler->core);
+	}
+
+	return 0;
+}
+
+static ssize_t rga_reset_write(struct file *file, const char __user *ubuf,
+			       size_t len, loff_t *offp)
+{
+	char buf[14];
+	int i, ret;
+	int reset_core = 0;
+	int reset_done = false;
+	struct rga_scheduler_t *scheduler = NULL;
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	ret = kstrtoint(buf, 10, &reset_core);
+	if (ret < 0 || reset_core <= 0) {
+		pr_err("invalid core! failed to reset hardware, data = %s len = %zu.\n", buf, len);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		if (scheduler->core == reset_core) {
+			reset_done = true;
+			pr_info("reset hardware core[%d]!\n", reset_core);
+
+			rga_request_scheduler_abort(scheduler);
+
+			break;
+		}
+	}
+
+	if (!reset_done)
+		pr_err("cannot find core[%d]\n", reset_core);
+
+	return len;
+}
+
+static struct rga_debugger_list rga_debugger_root_list[] = {
+	{"debug", rga_debug_show, rga_debug_write, NULL},
+	{"driver_version", rga_version_show, NULL, NULL},
+	{"load", rga_load_show, NULL, NULL},
+	{"scheduler_status", rga_scheduler_show, NULL, NULL},
+	{"mm_session", rga_mm_session_show, NULL, NULL},
+	{"request_manager", rga_request_manager_show, NULL, NULL},
+#ifdef CONFIG_NO_GKI
+	{"dump_path", rga_dump_path_show, rga_dump_path_write, NULL},
+	{"dump_image", rga_dump_image_show, rga_dump_image_write, NULL},
+#endif
+	{"hardware", rga_hardware_show, NULL, NULL},
+	{"reset", rga_reset_show, rga_reset_write, NULL},
+};
+
+static ssize_t rga_debugger_write(struct file *file, const char __user *ubuf,
+				 size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rga_debugger_node *node = priv->private;
+
+	if (node->info_ent->write)
+		return node->info_ent->write(file, ubuf, len, offp);
+	else
+		return len;
+}
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS
+static int rga_debugfs_open(struct inode *inode, struct file *file)
+{
+	struct rga_debugger_node *node = inode->i_private;
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct file_operations rga_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = rga_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.write = rga_debugger_write,
+};
+
+static int rga_debugfs_remove_files(struct rga_debugger *debugger)
+{
+	struct rga_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->debugfs_lock);
+
+	/* Delete debugfs entry list */
+	entry_list = &debugger->debugfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->dent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all debugfs node in this directory */
+	debugfs_remove_recursive(debugger->debugfs_dir);
+	debugger->debugfs_dir = NULL;
+
+	mutex_unlock(&debugger->debugfs_lock);
+
+	return 0;
+}
+
+static int rga_debugfs_create_files(const struct rga_debugger_list *files,
+					int count, struct dentry *root,
+					struct rga_debugger *debugger)
+{
+	int i;
+	struct dentry *ent;
+	struct rga_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rga_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			pr_err("Cannot alloc node path /sys/kernel/debug/%pd/%s\n",
+				 root, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = debugfs_create_file(files[i].name, S_IFREG | S_IRUGO,
+					 root, tmp, &rga_debugfs_fops);
+		if (!ent) {
+			pr_err("Cannot create /sys/kernel/debug/%pd/%s\n", root,
+				 files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->dent = ent;
+
+		mutex_lock(&debugger->debugfs_lock);
+		list_add_tail(&tmp->list, &debugger->debugfs_entry_list);
+		mutex_unlock(&debugger->debugfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rga_debugfs_remove_files(debugger);
+
+	return -1;
+}
+
+int rga_debugfs_remove(void)
+{
+	struct rga_debugger *debugger;
+
+	debugger = rga_drvdata->debugger;
+
+	rga_debugfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rga_debugfs_init(void)
+{
+	int ret;
+	struct rga_debugger *debugger;
+
+	debugger = rga_drvdata->debugger;
+
+	debugger->debugfs_dir =
+		debugfs_create_dir(RGA_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->debugfs_dir)) {
+		pr_err("failed on mkdir /sys/kernel/debug/%s\n",
+			 RGA_DEBUGGER_ROOT_NAME);
+		debugger->debugfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rga_debugfs_create_files(rga_debugger_root_list, ARRAY_SIZE(rga_debugger_root_list),
+					 debugger->debugfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rga_debugger_root_list debugfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rga_debugfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RGA_PROC_FS
+static int rga_procfs_open(struct inode *inode, struct file *file)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	struct rga_debugger_node *node = pde_data(inode);
+#else
+	struct rga_debugger_node *node = PDE_DATA(inode);
+#endif
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct proc_ops rga_procfs_fops = {
+	.proc_open = rga_procfs_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = rga_debugger_write,
+};
+
+static int rga_procfs_remove_files(struct rga_debugger *debugger)
+{
+	struct rga_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->procfs_lock);
+
+	/* Delete procfs entry list */
+	entry_list = &debugger->procfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->pent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all procfs node in this directory */
+	proc_remove(debugger->procfs_dir);
+	debugger->procfs_dir = NULL;
+
+	mutex_unlock(&debugger->procfs_lock);
+
+	return 0;
+}
+
+static int rga_procfs_create_files(const struct rga_debugger_list *files,
+				 int count, struct proc_dir_entry *root,
+				 struct rga_debugger *debugger)
+{
+	int i;
+	struct proc_dir_entry *ent;
+	struct rga_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rga_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			pr_err("Cannot alloc node path for /proc/%s/%s\n",
+				 RGA_DEBUGGER_ROOT_NAME, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = proc_create_data(files[i].name, S_IFREG | S_IRUGO,
+					 root, &rga_procfs_fops, tmp);
+		if (!ent) {
+			pr_err("Cannot create /proc/%s/%s\n",
+				 RGA_DEBUGGER_ROOT_NAME, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->pent = ent;
+
+		mutex_lock(&debugger->procfs_lock);
+		list_add_tail(&tmp->list, &debugger->procfs_entry_list);
+		mutex_unlock(&debugger->procfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rga_procfs_remove_files(debugger);
+	return -1;
+}
+
+int rga_procfs_remove(void)
+{
+	struct rga_debugger *debugger;
+
+	debugger = rga_drvdata->debugger;
+
+	rga_procfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rga_procfs_init(void)
+{
+	int ret;
+	struct rga_debugger *debugger;
+
+	debugger = rga_drvdata->debugger;
+
+	debugger->procfs_dir = proc_mkdir(RGA_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->procfs_dir)) {
+		pr_err("failed on mkdir /proc/%s\n", RGA_DEBUGGER_ROOT_NAME);
+		debugger->procfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rga_procfs_create_files(rga_debugger_root_list, ARRAY_SIZE(rga_debugger_root_list),
+					 debugger->procfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rga_debugger_root_list procfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rga_procfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RGA_PROC_FS */
+
+void rga_request_task_debug_info(struct seq_file *m, struct rga_req *req)
+{
+	seq_printf(m, "\t\t rotate_mode = %d\n", req->rotate_mode);
+	seq_printf(m, "\t\t src: y = %lx uv = %lx v = %lx aw = %d ah = %d vw = %d vh = %d\n",
+		 (unsigned long)req->src.yrgb_addr, (unsigned long)req->src.uv_addr,
+		 (unsigned long)req->src.v_addr, req->src.act_w, req->src.act_h,
+		 req->src.vir_w, req->src.vir_h);
+	seq_printf(m, "\t\t src: xoff = %d, yoff = %d, format = 0x%x, rd_mode = %d\n",
+		req->src.x_offset, req->src.y_offset, req->src.format, req->src.rd_mode);
+
+	if (req->pat.yrgb_addr != 0 || req->pat.uv_addr != 0
+		|| req->pat.v_addr != 0) {
+		seq_printf(m, "\t\t pat: y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d\n",
+			 (unsigned long)req->pat.yrgb_addr, (unsigned long)req->pat.uv_addr,
+			 (unsigned long)req->pat.v_addr, req->pat.act_w, req->pat.act_h,
+			 req->pat.vir_w, req->pat.vir_h);
+		seq_printf(m, "\t\t xoff = %d yoff = %d, format = 0x%x, rd_mode = %d\n",
+			req->pat.x_offset, req->pat.y_offset, req->pat.format, req->pat.rd_mode);
+	}
+
+	seq_printf(m, "\t\t dst: y=%lx uv=%lx v=%lx aw=%d ah=%d vw=%d vh=%d\n",
+		 (unsigned long)req->dst.yrgb_addr, (unsigned long)req->dst.uv_addr,
+		 (unsigned long)req->dst.v_addr, req->dst.act_w, req->dst.act_h,
+		 req->dst.vir_w, req->dst.vir_h);
+	seq_printf(m, "\t\t dst: xoff = %d, yoff = %d, format = 0x%x, rd_mode = %d\n",
+		req->dst.x_offset, req->dst.y_offset, req->dst.format, req->dst.rd_mode);
+
+	seq_printf(m, "\t\t mmu: mmu_flag=%x en=%x\n",
+		req->mmu_info.mmu_flag, req->mmu_info.mmu_en);
+	seq_printf(m, "\t\t alpha: rop_mode = %x\n", req->alpha_rop_mode);
+	seq_printf(m, "\t\t yuv2rgb mode is %x\n", req->yuv2rgb_mode);
+	seq_printf(m, "\t\t set core = %d, priority = %d, in_fence_fd = %d\n",
+		req->core, req->priority, req->in_fence_fd);
+}
+
+#ifdef CONFIG_NO_GKI
+static int rga_dump_image_to_file(struct rga_internal_buffer *dump_buffer,
+				  const char *channel_name,
+				  int plane_id,
+				  int core)
+{
+	char file_name[100];
+	struct file *file;
+	size_t size = 0;
+	loff_t pos = 0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	int ret;
+	struct iosys_map map;
+#endif
+	void *kvaddr = NULL;
+	void *kvaddr_origin = NULL;
+
+	switch (dump_buffer->type) {
+	case RGA_DMA_BUFFER:
+	case RGA_DMA_BUFFER_PTR:
+		if (IS_ERR_OR_NULL(dump_buffer->dma_buffer->dma_buf)) {
+			pr_err("Failed to dump dma_buf 0x%px\n",
+			       dump_buffer->dma_buffer->dma_buf);
+			return -EINVAL;
+		}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		ret = dma_buf_vmap(dump_buffer->dma_buffer->dma_buf, &map);
+		kvaddr = ret ? NULL : map.vaddr;
+#else
+		kvaddr = dma_buf_vmap(dump_buffer->dma_buffer->dma_buf);
+#endif
+		if (!kvaddr) {
+			pr_err("can't vmap the dma buffer!\n");
+			return -EINVAL;
+		}
+
+		kvaddr_origin = kvaddr;
+		kvaddr += dump_buffer->dma_buffer->offset;
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		kvaddr = vmap(dump_buffer->virt_addr->pages, dump_buffer->virt_addr->page_count,
+			      VM_MAP, pgprot_writecombine(PAGE_KERNEL));
+		if (!kvaddr) {
+			pr_err("dump_vaddr vmap error!, 0x%lx\n",
+			       (unsigned long)dump_buffer->virt_addr->addr);
+			return -EFAULT;
+		}
+
+		kvaddr_origin = kvaddr;
+		kvaddr += dump_buffer->virt_addr->offset;
+		break;
+	case RGA_PHYSICAL_ADDRESS:
+		kvaddr = phys_to_virt(dump_buffer->phys_addr);
+		break;
+	default:
+		pr_err("unsupported memory type[%x]\n", dump_buffer->type);
+		return -EINVAL;
+	}
+
+	size = dump_buffer->size;
+
+	if (kvaddr == NULL) {
+		pr_err("dump addr is NULL!\n");
+		return -EFAULT;
+	}
+
+	if (size <= 0) {
+		pr_err("dump buffer size[%lx] is invalid!\n", (unsigned long)size);
+		return -EFAULT;
+	}
+
+	if (dump_buffer->memory_parm.width == 0 &&
+	    dump_buffer->memory_parm.height == 0)
+		snprintf(file_name, 100, "%s/%d_core%d_%s_plane%d_%s_size%zu_%s.bin",
+			 g_dump_path,
+			 RGA_DEBUG_DUMP_IMAGE, core, channel_name, plane_id,
+			 rga_get_memory_type_str(dump_buffer->type),
+			 size,
+			 rga_get_format_name(dump_buffer->memory_parm.format));
+	else
+		snprintf(file_name, 100, "%s/%d_core%d_%s_plane%d_%s_w%d_h%d_%s.bin",
+			 g_dump_path,
+			 RGA_DEBUG_DUMP_IMAGE, core, channel_name, plane_id,
+			 rga_get_memory_type_str(dump_buffer->type),
+			 dump_buffer->memory_parm.width,
+			 dump_buffer->memory_parm.height,
+			 rga_get_format_name(dump_buffer->memory_parm.format));
+
+	file = filp_open(file_name, O_RDWR | O_CREAT | O_TRUNC, 0600);
+	if (!IS_ERR(file)) {
+		kernel_write(file, kvaddr, size, &pos);
+		pr_info("dump image to: %s\n", file_name);
+		fput(file);
+	} else {
+		pr_info("open %s failed\n", file_name);
+	}
+
+	switch (dump_buffer->type) {
+	case RGA_DMA_BUFFER:
+	case RGA_DMA_BUFFER_PTR:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		dma_buf_vunmap(dump_buffer->dma_buffer->dma_buf, &map);
+#else
+		dma_buf_vunmap(dump_buffer->dma_buffer->dma_buf, kvaddr_origin);
+#endif
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		vunmap(kvaddr_origin);
+		break;
+	}
+
+	return 0;
+}
+
+static inline void rga_dump_channel_image(struct rga_job_buffer *job_buffer,
+					  const char *channel_name,
+					  int core)
+{
+	if (job_buffer->y_addr)
+		rga_dump_image_to_file(job_buffer->y_addr, channel_name, 0, core);
+	if (job_buffer->uv_addr)
+		rga_dump_image_to_file(job_buffer->uv_addr, channel_name, 1, core);
+	if (job_buffer->v_addr)
+		rga_dump_image_to_file(job_buffer->v_addr, channel_name, 2, core);
+}
+
+void rga_dump_job_image(struct rga_job *dump_job)
+{
+	rga_dump_channel_image(&dump_job->src_buffer, "src", dump_job->core);
+	rga_dump_channel_image(&dump_job->src1_buffer, "src1", dump_job->core);
+	rga_dump_channel_image(&dump_job->dst_buffer, "dst", dump_job->core);
+	rga_dump_channel_image(&dump_job->els_buffer, "els", dump_job->core);
+
+	if (RGA_DEBUG_DUMP_IMAGE > 0)
+		RGA_DEBUG_DUMP_IMAGE--;
+}
+#endif /* #ifdef CONFIG_NO_GKI */
diff --git a/drivers/video/rockchip/rga3/rga_dma_buf.c b/drivers/video/rockchip/rga3/rga_dma_buf.c
new file mode 100644
index 0000000000000..e72470e5c5f5d
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_dma_buf.c
@@ -0,0 +1,594 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga_dma_buf.h"
+#include "rga.h"
+#include "rga_common.h"
+#include "rga_job.h"
+#include "rga_debugger.h"
+
+static int rga_dma_info_to_prot(enum dma_data_direction dir)
+{
+	switch (dir) {
+	case DMA_BIDIRECTIONAL:
+		return IOMMU_READ | IOMMU_WRITE;
+	case DMA_TO_DEVICE:
+		return IOMMU_READ;
+	case DMA_FROM_DEVICE:
+		return IOMMU_WRITE;
+	default:
+		return 0;
+	}
+}
+
+int rga_buf_size_cal(unsigned long yrgb_addr, unsigned long uv_addr,
+		      unsigned long v_addr, int format, uint32_t w,
+		      uint32_t h, unsigned long *StartAddr, unsigned long *size)
+{
+	uint32_t size_yrgb = 0;
+	uint32_t size_uv = 0;
+	uint32_t size_v = 0;
+	uint32_t stride = 0;
+	unsigned long start, end;
+	uint32_t pageCount;
+
+	switch (format) {
+	case RGA_FORMAT_RGBA_8888:
+	case RGA_FORMAT_RGBX_8888:
+	case RGA_FORMAT_BGRA_8888:
+	case RGA_FORMAT_BGRX_8888:
+	case RGA_FORMAT_ARGB_8888:
+	case RGA_FORMAT_XRGB_8888:
+	case RGA_FORMAT_ABGR_8888:
+	case RGA_FORMAT_XBGR_8888:
+		stride = (w * 4 + 3) & (~3);
+		size_yrgb = stride * h;
+		start = yrgb_addr >> PAGE_SHIFT;
+		end = yrgb_addr + size_yrgb;
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_RGB_888:
+	case RGA_FORMAT_BGR_888:
+		stride = (w * 3 + 3) & (~3);
+		size_yrgb = stride * h;
+		start = yrgb_addr >> PAGE_SHIFT;
+		end = yrgb_addr + size_yrgb;
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_RGB_565:
+	case RGA_FORMAT_RGBA_5551:
+	case RGA_FORMAT_RGBA_4444:
+	case RGA_FORMAT_BGR_565:
+	case RGA_FORMAT_BGRA_5551:
+	case RGA_FORMAT_BGRA_4444:
+	case RGA_FORMAT_ARGB_5551:
+	case RGA_FORMAT_ARGB_4444:
+	case RGA_FORMAT_ABGR_5551:
+	case RGA_FORMAT_ABGR_4444:
+		stride = (w * 2 + 3) & (~3);
+		size_yrgb = stride * h;
+		start = yrgb_addr >> PAGE_SHIFT;
+		end = yrgb_addr + size_yrgb;
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+
+		/* YUV FORMAT */
+	case RGA_FORMAT_YCbCr_422_SP:
+	case RGA_FORMAT_YCrCb_422_SP:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = stride * h;
+		start = min(yrgb_addr, uv_addr);
+		start >>= PAGE_SHIFT;
+		end = max((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YCbCr_422_P:
+	case RGA_FORMAT_YCrCb_422_P:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = ((stride >> 1) * h);
+		size_v = ((stride >> 1) * h);
+		start = min3(yrgb_addr, uv_addr, v_addr);
+		start = start >> PAGE_SHIFT;
+		end =
+			max3((yrgb_addr + size_yrgb), (uv_addr + size_uv),
+			(v_addr + size_v));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP:
+	case RGA_FORMAT_YCrCb_420_SP:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = (stride * (h >> 1));
+		start = min(yrgb_addr, uv_addr);
+		start >>= PAGE_SHIFT;
+		end = max((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YCbCr_420_P:
+	case RGA_FORMAT_YCrCb_420_P:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = ((stride >> 1) * (h >> 1));
+		size_v = ((stride >> 1) * (h >> 1));
+		start = min3(yrgb_addr, uv_addr, v_addr);
+		start >>= PAGE_SHIFT;
+		end =
+			max3((yrgb_addr + size_yrgb), (uv_addr + size_uv),
+			(v_addr + size_v));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YCbCr_400:
+	case RGA_FORMAT_Y8:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		start = yrgb_addr >> PAGE_SHIFT;
+		end = yrgb_addr + size_yrgb;
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_Y4:
+		stride = ((w + 3) & (~3)) >> 1;
+		size_yrgb = stride * h;
+		start = yrgb_addr >> PAGE_SHIFT;
+		end = yrgb_addr + size_yrgb;
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YVYU_422:
+	case RGA_FORMAT_VYUY_422:
+	case RGA_FORMAT_YUYV_422:
+	case RGA_FORMAT_UYVY_422:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = stride * h;
+		start = min(yrgb_addr, uv_addr);
+		start >>= PAGE_SHIFT;
+		end = max((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YVYU_420:
+	case RGA_FORMAT_VYUY_420:
+	case RGA_FORMAT_YUYV_420:
+	case RGA_FORMAT_UYVY_420:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = (stride * (h >> 1));
+		start = min(yrgb_addr, uv_addr);
+		start >>= PAGE_SHIFT;
+		end = max((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	case RGA_FORMAT_YCbCr_420_SP_10B:
+	case RGA_FORMAT_YCrCb_420_SP_10B:
+		stride = (w + 3) & (~3);
+		size_yrgb = stride * h;
+		size_uv = (stride * (h >> 1));
+		start = min(yrgb_addr, uv_addr);
+		start >>= PAGE_SHIFT;
+		end = max((yrgb_addr + size_yrgb), (uv_addr + size_uv));
+		end = (end + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+		pageCount = end - start;
+		break;
+	default:
+		pageCount = 0;
+		start = 0;
+		break;
+	}
+
+	*StartAddr = start;
+
+	if (size != NULL)
+		*size = size_yrgb + size_uv + size_v;
+
+	return pageCount;
+}
+
+static dma_addr_t rga_iommu_dma_alloc_iova(struct iommu_domain *domain,
+					    size_t size, u64 dma_limit,
+					    struct device *dev)
+{
+	struct rga_iommu_dma_cookie *cookie = (void *)domain->iova_cookie;
+	struct iova_domain *iovad = &cookie->iovad;
+	unsigned long shift, iova_len, iova = 0;
+
+	shift = iova_shift(iovad);
+	iova_len = size >> shift;
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 1, 0))
+	/*
+	 * Freeing non-power-of-two-sized allocations back into the IOVA caches
+	 * will come back to bite us badly, so we have to waste a bit of space
+	 * rounding up anything cacheable to make sure that can't happen. The
+	 * order of the unadjusted size will still match upon freeing.
+	 */
+	if (iova_len < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))
+		iova_len = roundup_pow_of_two(iova_len);
+#endif
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0))
+	dma_limit = min_not_zero(dma_limit, dev->bus_dma_limit);
+#else
+	if (dev->bus_dma_mask)
+		dma_limit &= dev->bus_dma_mask;
+#endif
+
+	if (domain->geometry.force_aperture)
+		dma_limit = min(dma_limit, (u64)domain->geometry.aperture_end);
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(4, 19, 111) && \
+     LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
+	iova = alloc_iova_fast(iovad, iova_len,
+			       min_t(dma_addr_t, dma_limit >> shift, iovad->end_pfn),
+			       true);
+#else
+	iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift, true);
+#endif
+
+	return (dma_addr_t)iova << shift;
+}
+
+static void rga_iommu_dma_free_iova(struct iommu_domain *domain,
+				    dma_addr_t iova, size_t size)
+{
+	struct rga_iommu_dma_cookie *cookie = (void *)domain->iova_cookie;
+	struct iova_domain *iovad = &cookie->iovad;
+
+	free_iova_fast(iovad, iova_pfn(iovad, iova), size >> iova_shift(iovad));
+}
+
+static inline struct iommu_domain *rga_iommu_get_dma_domain(struct device *dev)
+{
+	return iommu_get_domain_for_dev(dev);
+}
+
+void rga_iommu_unmap(struct rga_dma_buffer *buffer)
+{
+	if (buffer == NULL)
+		return;
+	if (buffer->iova == 0)
+		return;
+
+	iommu_unmap(buffer->domain, buffer->iova, buffer->size);
+	rga_iommu_dma_free_iova(buffer->domain, buffer->iova, buffer->size);
+}
+
+int rga_iommu_map_sgt(struct sg_table *sgt, size_t size,
+		      struct rga_dma_buffer *buffer,
+		      struct device *rga_dev)
+{
+	struct iommu_domain *domain = NULL;
+	struct rga_iommu_dma_cookie *cookie;
+	struct iova_domain *iovad;
+	dma_addr_t iova;
+	size_t map_size;
+	unsigned long align_size;
+
+	if (sgt == NULL) {
+		rga_err("can not map iommu, because sgt is null!\n");
+		return -EINVAL;
+	}
+
+	domain = rga_iommu_get_dma_domain(rga_dev);
+	cookie = (void *)domain->iova_cookie;
+	iovad = &cookie->iovad;
+	align_size = iova_align(iovad, size);
+
+	if (DEBUGGER_EN(MSG))
+		rga_log("iova_align size = %ld", align_size);
+
+	iova = rga_iommu_dma_alloc_iova(domain, align_size, rga_dev->coherent_dma_mask, rga_dev);
+	if (!iova) {
+		rga_err("rga_iommu_dma_alloc_iova failed");
+		return -ENOMEM;
+	}
+
+	map_size = iommu_map_sg(domain, iova, sgt->sgl, sgt->orig_nents,
+				rga_dma_info_to_prot(DMA_BIDIRECTIONAL));
+	if (map_size < align_size) {
+		rga_err("iommu can not map sgt to iova");
+		rga_iommu_dma_free_iova(domain, iova, align_size);
+		return -EINVAL;
+	}
+
+	buffer->domain = domain;
+	buffer->iova = iova;
+	buffer->size = align_size;
+
+	return 0;
+}
+
+int rga_iommu_map(phys_addr_t paddr, size_t size,
+		  struct rga_dma_buffer *buffer,
+		  struct device *rga_dev)
+{
+	int ret;
+	struct iommu_domain *domain = NULL;
+	struct rga_iommu_dma_cookie *cookie;
+	struct iova_domain *iovad;
+	dma_addr_t iova;
+	unsigned long align_size;
+
+	if (paddr == 0) {
+		rga_err("can not map iommu, because phys_addr is 0!\n");
+		return -EINVAL;
+	}
+
+	domain = rga_iommu_get_dma_domain(rga_dev);
+	cookie = (void *)domain->iova_cookie;
+	iovad = &cookie->iovad;
+	align_size = iova_align(iovad, size);
+
+	if (DEBUGGER_EN(MSG))
+		rga_log("iova_align size = %ld", align_size);
+
+	iova = rga_iommu_dma_alloc_iova(domain, align_size, rga_dev->coherent_dma_mask, rga_dev);
+	if (!iova) {
+		rga_err("rga_iommu_dma_alloc_iova failed");
+		return -ENOMEM;
+	}
+
+	ret = iommu_map(domain, iova, paddr, align_size,
+			rga_dma_info_to_prot(DMA_BIDIRECTIONAL));
+	if (ret) {
+		rga_err("iommu can not map phys_addr to iova");
+		rga_iommu_dma_free_iova(domain, iova, align_size);
+		return ret;
+	}
+
+	buffer->domain = domain;
+	buffer->iova = iova;
+	buffer->size = align_size;
+
+	return 0;
+}
+
+int rga_virtual_memory_check(void *vaddr, u32 w, u32 h, u32 format, int fd)
+{
+	int bits = 32;
+	int temp_data = 0;
+	void *one_line = NULL;
+
+	bits = rga_get_format_bits(format);
+	if (bits < 0)
+		return -1;
+
+	one_line = kzalloc(w * 4, GFP_KERNEL);
+	if (!one_line) {
+		rga_err("kzalloc fail %s[%d]\n", __func__, __LINE__);
+		return 0;
+	}
+
+	temp_data = w * (h - 1) * bits >> 3;
+	if (fd > 0) {
+		rga_log("vaddr is%p, bits is %d, fd check\n", vaddr, bits);
+		memcpy(one_line, (char *)vaddr + temp_data, w * bits >> 3);
+		rga_log("fd check ok\n");
+	} else {
+		rga_log("vir addr memory check.\n");
+		memcpy((void *)((char *)vaddr + temp_data), one_line,
+			 w * bits >> 3);
+		rga_log("vir addr check ok.\n");
+	}
+
+	kfree(one_line);
+	return 0;
+}
+
+int rga_dma_memory_check(struct rga_dma_buffer *rga_dma_buffer, struct rga_img_info_t *img)
+{
+	int ret = 0;
+	void *vaddr;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	struct iosys_map map;
+#endif
+	struct dma_buf *dma_buf;
+
+	dma_buf = rga_dma_buffer->dma_buf;
+
+	if (!IS_ERR_OR_NULL(dma_buf)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		ret = dma_buf_vmap(dma_buf, &map);
+		vaddr = ret ? NULL : map.vaddr;
+#else
+		vaddr = dma_buf_vmap(dma_buf);
+#endif
+		if (vaddr) {
+			ret = rga_virtual_memory_check(vaddr, img->vir_w,
+				img->vir_h, img->format, img->yrgb_addr);
+		} else {
+			rga_err("can't vmap the dma buffer!\n");
+			return -EINVAL;
+		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		dma_buf_vunmap(dma_buf, &map);
+#else
+		dma_buf_vunmap(dma_buf, vaddr);
+#endif
+	}
+
+	return ret;
+}
+
+int rga_dma_map_buf(struct dma_buf *dma_buf, struct rga_dma_buffer *rga_dma_buffer,
+		    enum dma_data_direction dir, struct device *rga_dev)
+{
+	struct dma_buf_attachment *attach = NULL;
+	struct sg_table *sgt = NULL;
+	struct scatterlist *sg = NULL;
+	int i, ret = 0;
+
+	if (dma_buf != NULL) {
+		get_dma_buf(dma_buf);
+	} else {
+		rga_err("dma_buf is invalid[%p]\n", dma_buf);
+		return -EINVAL;
+	}
+
+	attach = dma_buf_attach(dma_buf, rga_dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		rga_err("Failed to attach dma_buf, ret[%d]\n", ret);
+		goto err_get_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, dir);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		rga_err("Failed to map attachment, ret[%d]\n", ret);
+		goto err_get_sgt;
+	}
+
+	rga_dma_buffer->dma_buf = dma_buf;
+	rga_dma_buffer->attach = attach;
+	rga_dma_buffer->sgt = sgt;
+	rga_dma_buffer->dma_addr = sg_dma_address(sgt->sgl);
+	rga_dma_buffer->dir = dir;
+	rga_dma_buffer->size = 0;
+	for_each_sgtable_sg(sgt, sg, i)
+		rga_dma_buffer->size += sg_dma_len(sg);
+
+	return ret;
+
+err_get_sgt:
+	if (attach)
+		dma_buf_detach(dma_buf, attach);
+err_get_attach:
+	if (dma_buf)
+		dma_buf_put(dma_buf);
+
+	return ret;
+}
+
+int rga_dma_map_fd(int fd, struct rga_dma_buffer *rga_dma_buffer,
+		   enum dma_data_direction dir, struct device *rga_dev)
+{
+	struct dma_buf *dma_buf = NULL;
+	struct dma_buf_attachment *attach = NULL;
+	struct sg_table *sgt = NULL;
+	struct scatterlist *sg = NULL;
+	int i, ret = 0;
+
+	dma_buf = dma_buf_get(fd);
+	if (IS_ERR(dma_buf)) {
+		ret = PTR_ERR(dma_buf);
+		rga_err("Fail to get dma_buf from fd[%d], ret[%d]\n", fd, ret);
+		return ret;
+	}
+
+	attach = dma_buf_attach(dma_buf, rga_dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		rga_err("Failed to attach dma_buf, ret[%d]\n", ret);
+		goto err_get_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, dir);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		rga_err("Failed to map attachment, ret[%d]\n", ret);
+		goto err_get_sgt;
+	}
+
+	rga_dma_buffer->dma_buf = dma_buf;
+	rga_dma_buffer->attach = attach;
+	rga_dma_buffer->sgt = sgt;
+	rga_dma_buffer->dma_addr = sg_dma_address(sgt->sgl);
+	rga_dma_buffer->dir = dir;
+	rga_dma_buffer->size = 0;
+	for_each_sgtable_sg(sgt, sg, i)
+		rga_dma_buffer->size += sg_dma_len(sg);
+
+	return ret;
+
+err_get_sgt:
+	if (attach)
+		dma_buf_detach(dma_buf, attach);
+err_get_attach:
+	if (dma_buf)
+		dma_buf_put(dma_buf);
+
+	return ret;
+}
+
+void rga_dma_unmap_buf(struct rga_dma_buffer *rga_dma_buffer)
+{
+	if (rga_dma_buffer->attach && rga_dma_buffer->sgt)
+		dma_buf_unmap_attachment(rga_dma_buffer->attach,
+					 rga_dma_buffer->sgt,
+					 rga_dma_buffer->dir);
+
+	if (rga_dma_buffer->attach) {
+		dma_buf_detach(rga_dma_buffer->dma_buf, rga_dma_buffer->attach);
+		dma_buf_put(rga_dma_buffer->dma_buf);
+	}
+}
+
+void rga_dma_sync_flush_range(void *pstart, void *pend, struct rga_scheduler_t *scheduler)
+{
+	dma_sync_single_for_device(scheduler->dev, virt_to_phys(pstart),
+				   pend - pstart, DMA_TO_DEVICE);
+}
+
+int rga_dma_free(struct rga_dma_buffer *buffer)
+{
+	if (buffer == NULL) {
+		rga_err("rga_dma_buffer is NULL.\n");
+		return -EINVAL;
+	}
+
+	dma_free_coherent(buffer->scheduler->dev, buffer->size, buffer->vaddr, buffer->dma_addr);
+	buffer->vaddr = NULL;
+	buffer->dma_addr = 0;
+	buffer->iova = 0;
+	buffer->size = 0;
+	buffer->scheduler = NULL;
+
+	kfree(buffer);
+
+	return 0;
+}
+
+struct rga_dma_buffer *rga_dma_alloc_coherent(struct rga_scheduler_t *scheduler,
+					      int size)
+{
+	size_t align_size;
+	dma_addr_t dma_addr;
+	struct  rga_dma_buffer *buffer;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (!buffer)
+		return NULL;
+
+	align_size = PAGE_ALIGN(size);
+	buffer->vaddr = dma_alloc_coherent(scheduler->dev, align_size, &dma_addr, GFP_KERNEL);
+	if (!buffer->vaddr)
+		goto fail_dma_alloc;
+
+	buffer->size = align_size;
+	buffer->dma_addr = dma_addr;
+	buffer->scheduler = scheduler;
+	if (scheduler->data->mmu == RGA_IOMMU)
+		buffer->iova = buffer->dma_addr;
+
+	return buffer;
+
+fail_dma_alloc:
+	kfree(buffer);
+
+	return NULL;
+}
diff --git a/drivers/video/rockchip/rga3/rga_drv.c b/drivers/video/rockchip/rga3/rga_drv.c
new file mode 100644
index 0000000000000..fc96dc198ac5c
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_drv.c
@@ -0,0 +1,1633 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga2_reg_info.h"
+#include "rga3_reg_info.h"
+#include "rga_dma_buf.h"
+#include "rga_mm.h"
+
+#include "rga_job.h"
+#include "rga_fence.h"
+#include "rga_hw_config.h"
+
+#include "rga_iommu.h"
+#include "rga_debugger.h"
+#include "rga_common.h"
+
+struct rga_drvdata_t *rga_drvdata;
+
+/* set hrtimer */
+static struct hrtimer timer;
+static ktime_t kt;
+
+static struct rga_session *rga_session_init(void);
+static int rga_session_deinit(struct rga_session *session);
+
+static int rga_mpi_set_channel_buffer(struct dma_buf *dma_buf,
+				      struct rga_img_info_t *channel_info,
+				      struct rga_session *session)
+{
+	struct rga_external_buffer buffer;
+
+	memset(&buffer, 0x0, sizeof(buffer));
+	buffer.memory = (unsigned long)dma_buf;
+	buffer.type = RGA_DMA_BUFFER_PTR;
+	buffer.memory_parm.width = channel_info->vir_w;
+	buffer.memory_parm.height = channel_info->vir_h;
+	buffer.memory_parm.format = channel_info->format;
+
+	buffer.handle = rga_mm_import_buffer(&buffer, session);
+	if (buffer.handle == 0) {
+		rga_err("can not import dma_buf %p\n", dma_buf);
+		return -EFAULT;
+	}
+	channel_info->yrgb_addr = buffer.handle;
+
+	return 0;
+}
+
+static void rga_mpi_set_channel_info(uint32_t flags_mask, uint32_t flags,
+				     struct rga_video_frame_info *mpi_frame,
+				     struct rga_img_info_t *channel_info,
+				     struct rga_img_info_t *cache_info)
+{
+	uint32_t fix_enable_flag, cache_info_flag;
+
+	switch (flags_mask) {
+	case RGA_CONTEXT_SRC_MASK:
+		fix_enable_flag = RGA_CONTEXT_SRC_FIX_ENABLE;
+		cache_info_flag = RGA_CONTEXT_SRC_CACHE_INFO;
+		break;
+	case RGA_CONTEXT_PAT_MASK:
+		fix_enable_flag = RGA_CONTEXT_PAT_FIX_ENABLE;
+		cache_info_flag = RGA_CONTEXT_PAT_CACHE_INFO;
+		break;
+	case RGA_CONTEXT_DST_MASK:
+		fix_enable_flag = RGA_CONTEXT_DST_FIX_ENABLE;
+		cache_info_flag = RGA_CONTEXT_DST_CACHE_INFO;
+		break;
+	default:
+		return;
+	}
+
+	if (flags & fix_enable_flag) {
+		channel_info->x_offset = mpi_frame->x_offset;
+		channel_info->y_offset = mpi_frame->y_offset;
+		channel_info->act_w = mpi_frame->width;
+		channel_info->act_h = mpi_frame->height;
+		channel_info->vir_w = mpi_frame->vir_w;
+		channel_info->vir_h = mpi_frame->vir_h;
+		channel_info->rd_mode = mpi_frame->rd_mode;
+		channel_info->format = mpi_frame->format;
+
+		if (flags & cache_info_flag) {
+			/* Replace the config of src in ctx with the config of mpi src. */
+			cache_info->x_offset = mpi_frame->x_offset;
+			cache_info->y_offset = mpi_frame->y_offset;
+			cache_info->act_w = mpi_frame->width;
+			cache_info->act_h = mpi_frame->height;
+			cache_info->vir_w = mpi_frame->vir_w;
+			cache_info->vir_h = mpi_frame->vir_h;
+			cache_info->rd_mode = mpi_frame->rd_mode;
+			cache_info->format = mpi_frame->format;
+
+		}
+	}
+}
+
+int rga_mpi_commit(struct rga_mpi_job_t *mpi_job)
+{
+	int ret = 0;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+	struct rga_req *cached_cmd;
+	struct rga_req mpi_cmd;
+	unsigned long flags;
+
+	request_manager = rga_drvdata->pend_request_manager;
+
+	mutex_lock(&request_manager->lock);
+	request = rga_request_lookup(request_manager, mpi_job->ctx_id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", mpi_job->ctx_id);
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	if (request->task_count > 1) {
+		/* TODO */
+		rga_req_err(request, "Currently request does not support multiple tasks!");
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	/* TODO: batch mode need mpi async mode */
+	request->sync_mode = RGA_BLIT_SYNC;
+
+	cached_cmd = request->task_list;
+	memcpy(&mpi_cmd, cached_cmd, sizeof(mpi_cmd));
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	/* set channel info */
+	if ((mpi_job->src != NULL) && (request->flags & RGA_CONTEXT_SRC_MASK))
+		rga_mpi_set_channel_info(RGA_CONTEXT_SRC_MASK,
+					 request->flags,
+					 mpi_job->src,
+					 &mpi_cmd.src,
+					 &cached_cmd->src);
+
+	if ((mpi_job->pat != NULL) && (request->flags & RGA_CONTEXT_PAT_MASK))
+		rga_mpi_set_channel_info(RGA_CONTEXT_PAT_MASK,
+					 request->flags,
+					 mpi_job->pat,
+					 &mpi_cmd.pat,
+					 &cached_cmd->pat);
+
+	if ((mpi_job->dst != NULL) && (request->flags & RGA_CONTEXT_DST_MASK))
+		rga_mpi_set_channel_info(RGA_CONTEXT_DST_MASK,
+					 request->flags,
+					 mpi_job->dst,
+					 &mpi_cmd.dst,
+					 &cached_cmd->dst);
+
+	/* set buffer handle */
+	if (mpi_job->dma_buf_src0 != NULL) {
+		ret = rga_mpi_set_channel_buffer(mpi_job->dma_buf_src0,
+						 &mpi_cmd.src,
+						 request->session);
+		if (ret < 0) {
+			rga_req_err(request, "src channel set buffer handle failed!\n");
+			goto err_put_request;
+		}
+	}
+
+	if (mpi_job->dma_buf_src1 != NULL) {
+		ret = rga_mpi_set_channel_buffer(mpi_job->dma_buf_src1,
+						 &mpi_cmd.pat,
+						 request->session);
+		if (ret < 0) {
+			rga_req_err(request, "src1 channel set buffer handle failed!\n");
+			goto err_put_request;
+		}
+	}
+
+	if (mpi_job->dma_buf_dst != NULL) {
+		ret = rga_mpi_set_channel_buffer(mpi_job->dma_buf_dst,
+						 &mpi_cmd.dst,
+						 request->session);
+		if (ret < 0) {
+			rga_req_err(request, "dst channel set buffer handle failed!\n");
+			goto err_put_request;
+		}
+	}
+
+	mpi_cmd.handle_flag = 1;
+	mpi_cmd.mmu_info.mmu_en = 0;
+	mpi_cmd.mmu_info.mmu_flag = 0;
+
+	if (DEBUGGER_EN(MSG))
+		rga_dump_req(request, &mpi_cmd);
+
+	ret = rga_request_mpi_submit(&mpi_cmd, request);
+	if (ret < 0) {
+		if (ret == -ERESTARTSYS) {
+			if (DEBUGGER_EN(MSG))
+				rga_req_err(request, "%s, commit mpi job failed, by a software interrupt.\n",
+					__func__);
+		} else {
+			rga_req_err(request, "%s, commit mpi job failed\n", __func__);
+		}
+
+		goto err_put_request;
+	}
+
+	/* copy dst info to mpi job for next node */
+	if (mpi_job->output != NULL) {
+		mpi_job->output->x_offset = mpi_cmd.dst.x_offset;
+		mpi_job->output->y_offset = mpi_cmd.dst.y_offset;
+		mpi_job->output->width = mpi_cmd.dst.act_w;
+		mpi_job->output->height = mpi_cmd.dst.act_h;
+		mpi_job->output->vir_w = mpi_cmd.dst.vir_w;
+		mpi_job->output->vir_h = mpi_cmd.dst.vir_h;
+		mpi_job->output->rd_mode = mpi_cmd.dst.rd_mode;
+		mpi_job->output->format = mpi_cmd.dst.format;
+	}
+
+err_put_request:
+	if ((mpi_job->dma_buf_src0 != NULL) && (mpi_cmd.src.yrgb_addr > 0))
+		rga_mm_release_buffer(mpi_cmd.src.yrgb_addr);
+	if ((mpi_job->dma_buf_src1 != NULL) && (mpi_cmd.pat.yrgb_addr > 0))
+		rga_mm_release_buffer(mpi_cmd.pat.yrgb_addr);
+	if ((mpi_job->dma_buf_dst != NULL) && (mpi_cmd.dst.yrgb_addr > 0))
+		rga_mm_release_buffer(mpi_cmd.dst.yrgb_addr);
+
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rga_mpi_commit);
+
+int rga_kernel_commit(struct rga_req *cmd)
+{
+	int ret = 0;
+	int request_id;
+	struct rga_user_request kernel_request;
+	struct rga_request *request = NULL;
+	struct rga_session *session = NULL;
+	struct rga_pending_request_manager *request_manager = rga_drvdata->pend_request_manager;
+
+	session = rga_session_init();
+	if (IS_ERR(session))
+		return PTR_ERR(session);
+
+	request_id = rga_request_alloc(0, session);
+	if (request_id < 0) {
+		rga_err("request alloc error!\n");
+		ret = request_id;
+		return ret;
+	}
+
+	memset(&kernel_request, 0, sizeof(kernel_request));
+	kernel_request.id = request_id;
+	kernel_request.task_ptr = (uint64_t)(unsigned long)cmd;
+	kernel_request.task_num = 1;
+	kernel_request.sync_mode = RGA_BLIT_SYNC;
+
+	ret = rga_request_check(&kernel_request);
+	if (ret < 0) {
+		rga_err("ID[%d]: user request check error!\n", kernel_request.id);
+		goto err_free_request_by_id;
+	}
+
+	request = rga_request_kernel_config(&kernel_request);
+	if (IS_ERR(request)) {
+		rga_err("ID[%d]: config failed!\n", kernel_request.id);
+		ret = -EFAULT;
+		goto err_free_request_by_id;
+	}
+
+	if (DEBUGGER_EN(MSG)) {
+		rga_req_log(request, "kernel blit mode:\n");
+		rga_dump_req(request, cmd);
+	}
+
+	ret = rga_request_submit(request);
+	if (ret < 0) {
+		rga_req_err(request, "submit failed!\n");
+		goto err_put_request;
+	}
+
+err_put_request:
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	rga_session_deinit(session);
+
+	return ret;
+
+err_free_request_by_id:
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, request_id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", request_id);
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	rga_request_free(request);
+
+	mutex_unlock(&request_manager->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(rga_kernel_commit);
+
+static enum hrtimer_restart hrtimer_handler(struct hrtimer *timer)
+{
+	struct rga_drvdata_t *rga = rga_drvdata;
+	struct rga_scheduler_t *scheduler = NULL;
+	struct rga_job *job = NULL;
+	unsigned long flags;
+	int i;
+
+	ktime_t now = ktime_get();
+
+	for (i = 0; i < rga->num_of_scheduler; i++) {
+		scheduler = rga->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		/* if timer action on job running */
+		job = scheduler->running_job;
+		if (job) {
+			scheduler->timer.busy_time += ktime_us_delta(now, job->timestamp.hw_recode);
+			job->timestamp.hw_recode = now;
+		}
+
+		scheduler->timer.busy_time_record = scheduler->timer.busy_time;
+		scheduler->timer.busy_time = 0;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+
+	hrtimer_forward_now(timer, kt);
+	return HRTIMER_RESTART;
+}
+
+static void rga_init_timer(void)
+{
+	kt = ktime_set(0, RGA_TIMER_INTERVAL_NS);
+	hrtimer_init(&timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+
+	timer.function = hrtimer_handler;
+
+	hrtimer_start(&timer, kt, HRTIMER_MODE_REL);
+}
+
+static void rga_cancel_timer(void)
+{
+	hrtimer_cancel(&timer);
+}
+
+#ifndef RGA_DISABLE_PM
+int rga_power_enable(struct rga_scheduler_t *scheduler)
+{
+	int ret = -EINVAL;
+	unsigned long flags;
+
+	pm_runtime_get_sync(scheduler->dev);
+	pm_stay_awake(scheduler->dev);
+
+	ret = clk_bulk_prepare_enable(scheduler->num_clks, scheduler->clks);
+	if (ret < 0)
+		goto err_enable_clk;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	scheduler->pd_refcount++;
+	if (scheduler->status == RGA_SCHEDULER_IDLE)
+		scheduler->status = RGA_SCHEDULER_WORKING;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	return 0;
+
+err_enable_clk:
+	clk_bulk_disable_unprepare(scheduler->num_clks, scheduler->clks);
+
+	pm_relax(scheduler->dev);
+	pm_runtime_put_sync_suspend(scheduler->dev);
+
+	return ret;
+}
+
+int rga_power_disable(struct rga_scheduler_t *scheduler)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	if (scheduler->status == RGA_SCHEDULER_IDLE ||
+	    scheduler->pd_refcount == 0) {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		WARN(true, "%s already idle!\n", dev_driver_string(scheduler->dev));
+		return -1;
+	}
+
+	scheduler->pd_refcount--;
+	if (scheduler->pd_refcount == 0)
+		scheduler->status = RGA_SCHEDULER_IDLE;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	clk_bulk_disable_unprepare(scheduler->num_clks, scheduler->clks);
+
+	pm_relax(scheduler->dev);
+	pm_runtime_put_sync_suspend(scheduler->dev);
+
+	return 0;
+}
+
+static void rga_power_enable_all(void)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	int ret = 0;
+	int i;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+		ret = rga_power_enable(scheduler);
+		if (ret < 0)
+			rga_err("power enable failed");
+	}
+}
+
+static void rga_power_disable_all(void)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+	int i;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+		rga_power_disable(scheduler);
+	}
+}
+
+#else
+int rga_power_enable(struct rga_scheduler_t *scheduler)
+{
+	return 0;
+}
+
+int rga_power_disable(struct rga_scheduler_t *scheduler)
+{
+	return 0;
+}
+
+static inline void rga_power_enable_all(void) {}
+static inline void rga_power_disable_all(void) {}
+#endif /* #ifndef RGA_DISABLE_PM */
+
+static int rga_session_manager_init(struct rga_session_manager **session_manager_ptr)
+{
+	struct rga_session_manager *session_manager = NULL;
+
+	*session_manager_ptr = kzalloc(sizeof(struct rga_session_manager), GFP_KERNEL);
+	if (*session_manager_ptr == NULL) {
+		pr_err("can not kzalloc for rga_session_manager\n");
+		return -ENOMEM;
+	}
+
+	session_manager = *session_manager_ptr;
+
+	mutex_init(&session_manager->lock);
+
+	idr_init_base(&session_manager->ctx_id_idr, 1);
+
+	return 0;
+}
+
+/*
+ * Called at driver close to release the rga session's id references.
+ */
+static int rga_session_free_remove_idr_cb(int id, void *ptr, void *data)
+{
+	struct rga_session *session = ptr;
+
+	idr_remove(&rga_drvdata->session_manager->ctx_id_idr, session->id);
+	kfree(session);
+
+	return 0;
+}
+
+static int rga_session_free_remove_idr(struct rga_session *session)
+{
+	struct rga_session_manager *session_manager;
+
+	session_manager = rga_drvdata->session_manager;
+
+	mutex_lock(&session_manager->lock);
+
+	session_manager->session_cnt--;
+	idr_remove(&session_manager->ctx_id_idr, session->id);
+
+	mutex_unlock(&session_manager->lock);
+
+	return 0;
+}
+
+static int rga_session_manager_remove(struct rga_session_manager **session_manager_ptr)
+{
+	struct rga_session_manager *session_manager = *session_manager_ptr;
+
+	mutex_lock(&session_manager->lock);
+
+	idr_for_each(&session_manager->ctx_id_idr, &rga_session_free_remove_idr_cb, session_manager);
+	idr_destroy(&session_manager->ctx_id_idr);
+
+	mutex_unlock(&session_manager->lock);
+
+	kfree(*session_manager_ptr);
+
+	*session_manager_ptr = NULL;
+
+	return 0;
+}
+
+static struct rga_session *rga_session_init(void)
+{
+	int new_id;
+
+	struct rga_session_manager *session_manager = NULL;
+	struct rga_session *session = NULL;
+
+	session_manager = rga_drvdata->session_manager;
+	if (session_manager == NULL) {
+		rga_err("rga_session_manager is null!\n");
+		return ERR_PTR(-EFAULT);
+	}
+
+	session = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (!session) {
+		rga_err("rga_session alloc failed\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	mutex_lock(&session_manager->lock);
+
+	idr_preload(GFP_KERNEL);
+	new_id = idr_alloc_cyclic(&session_manager->ctx_id_idr, session, 1, 0, GFP_NOWAIT);
+	idr_preload_end();
+	if (new_id < 0) {
+		mutex_unlock(&session_manager->lock);
+
+		rga_err("rga_session alloc id failed!\n");
+		kfree(session);
+		return ERR_PTR(new_id);
+	}
+
+	session->id = new_id;
+	session_manager->session_cnt++;
+
+	mutex_unlock(&session_manager->lock);
+
+	session->tgid = current->tgid;
+	session->pname = kstrdup_quotable_cmdline(current, GFP_KERNEL);
+
+	session->last_active = ktime_get();
+
+	return session;
+}
+
+static int rga_session_deinit(struct rga_session *session)
+{
+	rga_request_session_destroy_abort(session);
+	rga_mm_session_release_buffer(session);
+
+	rga_session_free_remove_idr(session);
+
+	kfree(session->pname);
+	kfree(session);
+
+	return 0;
+}
+
+static long rga_ioctl_import_buffer(unsigned long arg, struct rga_session *session)
+{
+	int i;
+	int ret = 0;
+	struct rga_buffer_pool buffer_pool;
+	struct rga_external_buffer *external_buffer = NULL;
+
+	if (unlikely(copy_from_user(&buffer_pool,
+				    (struct rga_buffer_pool *)arg,
+				    sizeof(buffer_pool)))) {
+		rga_err("rga_buffer_pool copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+	if (buffer_pool.size > RGA_BUFFER_POOL_SIZE_MAX) {
+		rga_err("Cannot import more than %d buffers at a time!\n",
+		       RGA_BUFFER_POOL_SIZE_MAX);
+		return -EFBIG;
+	}
+
+	if (buffer_pool.buffers_ptr == 0) {
+		rga_err("Import buffers is NULL!\n");
+		return -EFAULT;
+	}
+
+	external_buffer = kmalloc(sizeof(struct rga_external_buffer) * buffer_pool.size,
+				  GFP_KERNEL);
+	if (external_buffer == NULL) {
+		rga_err("external buffer list alloc error!\n");
+		return -ENOMEM;
+	}
+
+	if (unlikely(copy_from_user(external_buffer,
+				    u64_to_user_ptr(buffer_pool.buffers_ptr),
+				    sizeof(struct rga_external_buffer) * buffer_pool.size))) {
+		rga_err("rga_buffer_pool external_buffer list copy_from_user failed\n");
+		ret = -EFAULT;
+
+		goto err_free_external_buffer;
+	}
+
+	for (i = 0; i < buffer_pool.size; i++) {
+		if (DEBUGGER_EN(MSG)) {
+			rga_log("import buffer info:\n");
+			rga_dump_external_buffer(&external_buffer[i]);
+		}
+
+		ret = rga_mm_import_buffer(&external_buffer[i], session);
+		if (ret <= 0) {
+			rga_err("buffer[%d] mm import buffer failed! memory = 0x%lx, type = %s(0x%x)\n",
+			       i, (unsigned long)external_buffer[i].memory,
+			       rga_get_memory_type_str(external_buffer[i].type),
+			       external_buffer[i].type);
+
+			goto err_free_external_buffer;
+		}
+
+		external_buffer[i].handle = ret;
+	}
+
+	if (unlikely(copy_to_user(u64_to_user_ptr(buffer_pool.buffers_ptr),
+				  external_buffer,
+				  sizeof(struct rga_external_buffer) * buffer_pool.size))) {
+		rga_err("rga_buffer_pool external_buffer list copy_to_user failed\n");
+		ret = -EFAULT;
+
+		goto err_free_external_buffer;
+	}
+
+err_free_external_buffer:
+	kfree(external_buffer);
+	return ret;
+}
+
+static long rga_ioctl_release_buffer(unsigned long arg)
+{
+	int i;
+	int ret = 0;
+	struct rga_buffer_pool buffer_pool;
+	struct rga_external_buffer *external_buffer = NULL;
+
+	if (unlikely(copy_from_user(&buffer_pool,
+				    (struct rga_buffer_pool *)arg,
+				    sizeof(buffer_pool)))) {
+		rga_err("rga_buffer_pool  copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+	if (buffer_pool.size > RGA_BUFFER_POOL_SIZE_MAX) {
+		rga_err("Cannot release more than %d buffers at a time!\n",
+		       RGA_BUFFER_POOL_SIZE_MAX);
+		return -EFBIG;
+	}
+
+	if (buffer_pool.buffers_ptr == 0) {
+		rga_err("Release buffers is NULL!\n");
+		return -EFAULT;
+	}
+
+	external_buffer = kmalloc(sizeof(struct rga_external_buffer) * buffer_pool.size,
+				  GFP_KERNEL);
+	if (external_buffer == NULL) {
+		rga_err("external buffer list alloc error!\n");
+		return -ENOMEM;
+	}
+
+	if (unlikely(copy_from_user(external_buffer,
+				    u64_to_user_ptr(buffer_pool.buffers_ptr),
+				    sizeof(struct rga_external_buffer) * buffer_pool.size))) {
+		rga_err("rga_buffer_pool external_buffer list copy_from_user failed\n");
+		ret = -EFAULT;
+
+		goto err_free_external_buffer;
+	}
+
+	for (i = 0; i < buffer_pool.size; i++) {
+		if (DEBUGGER_EN(MSG))
+			rga_log("release buffer handle[%d]\n", external_buffer[i].handle);
+
+		ret = rga_mm_release_buffer(external_buffer[i].handle);
+		if (ret < 0) {
+			rga_err("buffer[%d] mm release buffer failed! handle = %d\n",
+			       i, external_buffer[i].handle);
+
+			goto err_free_external_buffer;
+		}
+	}
+
+err_free_external_buffer:
+	kfree(external_buffer);
+	return ret;
+}
+
+static long rga_ioctl_request_create(unsigned long arg, struct rga_session *session)
+{
+	uint32_t id;
+	uint32_t flags;
+
+	if (copy_from_user(&flags, (void *)arg, sizeof(uint32_t))) {
+		rga_err("%s failed to copy from user!\n", __func__);
+		return -EFAULT;
+	}
+
+	id = rga_request_alloc(flags, session);
+
+	if (copy_to_user((void *)arg, &id, sizeof(uint32_t))) {
+		rga_err("%s failed to copy to user!\n", __func__);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static long rga_ioctl_request_submit(unsigned long arg, bool run_enbale)
+{
+	int ret = 0;
+	struct rga_pending_request_manager *request_manager = NULL;
+	struct rga_user_request user_request;
+	struct rga_request *request = NULL;
+
+	request_manager = rga_drvdata->pend_request_manager;
+
+	if (unlikely(copy_from_user(&user_request,
+				    (struct rga_user_request *)arg,
+				    sizeof(user_request)))) {
+		rga_err("%s copy_from_user failed!\n", __func__);
+		return -EFAULT;
+	}
+
+	ret = rga_request_check(&user_request);
+	if (ret < 0) {
+		rga_err("user request check error!\n");
+		return ret;
+	}
+
+	if (DEBUGGER_EN(MSG))
+		rga_log("config request id = %d", user_request.id);
+
+	request = rga_request_config(&user_request);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("request[%d] config failed!\n", user_request.id);
+		return -EFAULT;
+	}
+
+	if (run_enbale) {
+		ret = rga_request_submit(request);
+		if (ret < 0) {
+			rga_err("request[%d] submit failed!\n", user_request.id);
+			return -EFAULT;
+		}
+
+		if (request->sync_mode == RGA_BLIT_ASYNC) {
+			user_request.release_fence_fd = request->release_fence_fd;
+			if (copy_to_user((struct rga_req *)arg,
+					 &user_request, sizeof(user_request))) {
+				rga_err("copy_to_user failed\n");
+				return -EFAULT;
+			}
+		}
+	}
+
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	return 0;
+}
+
+static long rga_ioctl_request_cancel(unsigned long arg)
+{
+	uint32_t id;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return -EFAULT;
+	}
+
+	if (unlikely(copy_from_user(&id, (uint32_t *)arg, sizeof(uint32_t)))) {
+		rga_err("request id copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+	if (DEBUGGER_EN(MSG))
+		rga_log("config cancel request id = %d", id);
+
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", id);
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	rga_request_put(request);
+
+	mutex_unlock(&request_manager->lock);
+
+	return 0;
+}
+
+static long rga_ioctl_blit(unsigned long arg, uint32_t cmd, struct rga_session *session)
+{
+	int ret = 0;
+	int request_id;
+	struct rga_user_request user_request;
+	struct rga_req *rga_req;
+	struct rga_request *request = NULL;
+	struct rga_pending_request_manager *request_manager = rga_drvdata->pend_request_manager;
+
+	request_id = rga_request_alloc(0, session);
+	if (request_id < 0) {
+		rga_err("request alloc error!\n");
+		ret = request_id;
+		return ret;
+	}
+
+	memset(&user_request, 0, sizeof(user_request));
+	user_request.id = request_id;
+	user_request.task_ptr = arg;
+	user_request.task_num = 1;
+	user_request.sync_mode = cmd;
+
+	ret = rga_request_check(&user_request);
+	if (ret < 0) {
+		rga_err("ID[%d]: user request check error!\n", user_request.id);
+		goto err_free_request_by_id;
+	}
+
+	request = rga_request_config(&user_request);
+	if (IS_ERR(request)) {
+		rga_err("ID[%d]: config failed!\n", user_request.id);
+		ret = -EFAULT;
+		goto err_free_request_by_id;
+	}
+
+	rga_req = request->task_list;
+	/* In the BLIT_SYNC/BLIT_ASYNC command, in_fence_fd needs to be set. */
+	request->acquire_fence_fd = rga_req->in_fence_fd;
+
+	ret = rga_request_submit(request);
+	if (ret < 0) {
+		rga_req_err(request, "submit failed!\n");
+		goto err_put_request;
+	}
+
+	if (request->sync_mode == RGA_BLIT_ASYNC) {
+		rga_req->out_fence_fd = request->release_fence_fd;
+		if (copy_to_user((struct rga_req *)arg, rga_req, sizeof(struct rga_req))) {
+			rga_req_err(request, "copy_to_user failed\n");
+			ret = -EFAULT;
+			goto err_put_request;
+		}
+	}
+
+err_put_request:
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	return ret;
+
+err_free_request_by_id:
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, request_id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", request_id);
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	rga_request_free(request);
+
+	mutex_unlock(&request_manager->lock);
+
+	return ret;
+}
+
+static long rga_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	int ret = 0;
+	int i = 0;
+	int major_version = 0, minor_version = 0;
+	char version[16] = { 0 };
+	struct rga_version_t driver_version;
+	struct rga_hw_versions_t hw_versions;
+	struct rga_drvdata_t *rga = rga_drvdata;
+	struct rga_session *session = file->private_data;
+
+	if (!rga) {
+		rga_err("rga_drvdata is null, rga is not init\n");
+		return -ENODEV;
+	}
+
+	if (DEBUGGER_EN(NONUSE))
+		return 0;
+
+	down_read(&rga_drvdata->rwsem);
+
+	if (rga_drvdata->shutdown) {
+		rga_log("driver has been shutdown\n");
+		up_read(&rga_drvdata->rwsem);
+
+		return -EBUSY;
+	}
+
+	if (cmd == RGA_BLIT_ASYNC && !IS_ENABLED(CONFIG_ROCKCHIP_RGA_ASYNC)) {
+		rga_log("The current driver does not support asynchronous mode, please enable CONFIG_ROCKCHIP_RGA_ASYNC.\n");
+		up_read(&rga_drvdata->rwsem);
+
+		return -EINVAL;
+	}
+
+	switch (cmd) {
+	case RGA_BLIT_SYNC:
+	case RGA_BLIT_ASYNC:
+		ret = rga_ioctl_blit(arg, cmd, session);
+
+		break;
+	case RGA_CACHE_FLUSH:
+	case RGA_FLUSH:
+	case RGA_GET_RESULT:
+		break;
+	case RGA_GET_VERSION:
+		sscanf(rga->scheduler[i]->version.str, "%x.%x.%*x",
+			 &major_version, &minor_version);
+		snprintf(version, 5, "%x.%02x", major_version, minor_version);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+		/* TODO: userspcae to get version */
+		if (copy_to_user((void *)arg, version, sizeof(version)))
+			ret = -EFAULT;
+#else
+		if (copy_to_user((void *)arg, RGA3_VERSION,
+				 sizeof(RGA3_VERSION)))
+			ret = -EFAULT;
+#endif
+		break;
+	case RGA2_GET_VERSION:
+		for (i = 0; i < rga->num_of_scheduler; i++) {
+			if (rga->scheduler[i]->ops == &rga2_ops) {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+				if (copy_to_user((void *)arg, rga->scheduler[i]->version.str,
+					sizeof(rga->scheduler[i]->version.str)))
+					ret = -EFAULT;
+#else
+				if (copy_to_user((void *)arg, RGA3_VERSION,
+						sizeof(RGA3_VERSION)))
+					ret = -EFAULT;
+#endif
+				else
+					ret = true;
+
+				break;
+			}
+		}
+
+		/* This will indicate that the RGA2 version number cannot be obtained. */
+		if (ret != true)
+			ret = -EFAULT;
+
+		break;
+
+	case RGA_IOC_GET_HW_VERSION:
+		/* RGA hardware version */
+		hw_versions.size = rga->num_of_scheduler > RGA_HW_SIZE ?
+			RGA_HW_SIZE : rga->num_of_scheduler;
+
+		for (i = 0; i < hw_versions.size; i++) {
+			memcpy(&hw_versions.version[i], &rga->scheduler[i]->version,
+				sizeof(rga->scheduler[i]->version));
+		}
+
+		if (copy_to_user((void *)arg, &hw_versions, sizeof(hw_versions)))
+			ret = -EFAULT;
+		else
+			ret = true;
+
+		break;
+
+	case RGA_IOC_GET_DRVIER_VERSION:
+		/* Driver version */
+		driver_version.major = DRIVER_MAJOR_VERISON;
+		driver_version.minor = DRIVER_MINOR_VERSION;
+		driver_version.revision = DRIVER_REVISION_VERSION;
+		strncpy((char *)driver_version.str, DRIVER_VERSION, sizeof(driver_version.str));
+
+		if (copy_to_user((void *)arg, &driver_version, sizeof(driver_version)))
+			ret = -EFAULT;
+		else
+			ret = true;
+
+		break;
+
+	case RGA_IOC_IMPORT_BUFFER:
+		rga_power_enable_all();
+
+		ret = rga_ioctl_import_buffer(arg, session);
+
+		rga_power_disable_all();
+
+		break;
+
+	case RGA_IOC_RELEASE_BUFFER:
+		rga_power_enable_all();
+
+		ret = rga_ioctl_release_buffer(arg);
+
+		rga_power_disable_all();
+
+		break;
+
+	case RGA_IOC_REQUEST_CREATE:
+		ret = rga_ioctl_request_create(arg, session);
+
+		break;
+
+	case RGA_IOC_REQUEST_SUBMIT:
+		ret = rga_ioctl_request_submit(arg, true);
+
+		break;
+
+	case RGA_IOC_REQUEST_CONFIG:
+		ret = rga_ioctl_request_submit(arg, false);
+
+		break;
+
+	case RGA_IOC_REQUEST_CANCEL:
+		ret = rga_ioctl_request_cancel(arg);
+
+		break;
+
+	case RGA_IMPORT_DMA:
+	case RGA_RELEASE_DMA:
+	default:
+		rga_err("unknown ioctl cmd!\n");
+		ret = -EINVAL;
+		break;
+	}
+
+	up_read(&rga_drvdata->rwsem);
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER
+static int rga_debugger_init(struct rga_debugger **debugger_p)
+{
+	struct rga_debugger *debugger;
+
+	*debugger_p = kzalloc(sizeof(struct rga_debugger), GFP_KERNEL);
+	if (*debugger_p == NULL) {
+		pr_err("can not alloc for rga debugger\n");
+		return -ENOMEM;
+	}
+
+	debugger = *debugger_p;
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUG_FS
+	mutex_init(&debugger->debugfs_lock);
+	INIT_LIST_HEAD(&debugger->debugfs_entry_list);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_PROC_FS
+	mutex_init(&debugger->procfs_lock);
+	INIT_LIST_HEAD(&debugger->procfs_entry_list);
+#endif
+
+	rga_debugfs_init();
+	rga_procfs_init();
+
+	return 0;
+}
+
+static int rga_debugger_remove(struct rga_debugger **debugger_p)
+{
+	rga_debugfs_remove();
+	rga_procfs_remove();
+
+	kfree(*debugger_p);
+	*debugger_p = NULL;
+
+	return 0;
+}
+#endif
+
+static int rga_open(struct inode *inode, struct file *file)
+{
+	struct rga_session *session = NULL;
+
+	session = rga_session_init();
+	if (IS_ERR(session))
+		return PTR_ERR(session);
+
+	file->private_data = (void *)session;
+
+	return nonseekable_open(inode, file);
+}
+
+static int rga_release(struct inode *inode, struct file *file)
+{
+	struct rga_session *session = file->private_data;
+
+	rga_session_deinit(session);
+
+	return 0;
+}
+
+static irqreturn_t rga_irq_handler(int irq, void *data)
+{
+	irqreturn_t irq_ret = IRQ_NONE;
+	struct rga_scheduler_t *scheduler = data;
+	ktime_t timestamp = ktime_get();
+
+	spin_lock(&scheduler->irq_lock);
+
+	if (scheduler->ops->irq) {
+		irq_ret = scheduler->ops->irq(scheduler);
+		if (irq_ret == IRQ_HANDLED) {
+			spin_unlock(&scheduler->irq_lock);
+			return irq_ret;
+		}
+	}
+
+	if (scheduler->running_job == NULL) {
+		spin_unlock(&scheduler->irq_lock);
+		return IRQ_HANDLED;
+	}
+
+	scheduler->running_job->timestamp.hw_done = timestamp;
+
+	spin_unlock(&scheduler->irq_lock);
+
+	return irq_ret;
+}
+
+static irqreturn_t rga_isr_thread(int irq, void *data)
+{
+	irqreturn_t irq_ret = IRQ_NONE;
+	struct rga_scheduler_t *scheduler = data;
+	struct rga_job *job;
+
+	job = rga_job_done(scheduler);
+	if (job == NULL) {
+		rga_err("isr thread invalid job!\n");
+		return IRQ_HANDLED;
+	}
+
+	if (scheduler->ops->isr_thread)
+		irq_ret = scheduler->ops->isr_thread(job, scheduler);
+
+	rga_request_release_signal(scheduler, job);
+
+	rga_job_next(scheduler);
+
+	rga_power_disable(scheduler);
+
+	return irq_ret;
+}
+
+const struct file_operations rga_fops = {
+	.owner = THIS_MODULE,
+	.open = rga_open,
+	.release = rga_release,
+	.unlocked_ioctl = rga_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = rga_ioctl,
+#endif
+};
+
+static struct miscdevice rga_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "rga",
+	.fops = &rga_fops,
+};
+
+static const struct rga_match_data_t rga2_match_data = {
+	.device_type = RGA_DEVICE_RGA2,
+	.ops = &rga2_ops,
+};
+
+static const struct rga_match_data_t rga3_match_data = {
+	.device_type = RGA_DEVICE_RGA3,
+	.ops = &rga3_ops,
+};
+
+static const struct of_device_id rga3_dt_ids[] = {
+	{
+	 .compatible = "rockchip,rga3",
+	 .data = &rga3_match_data,
+	},
+	/* legacy */
+	{
+	 .compatible = "rockchip,rga3_core0",
+	 .data = &rga3_match_data,
+	},
+	{
+	 .compatible = "rockchip,rga3_core1",
+	 .data = &rga3_match_data,
+	},
+	{},
+};
+
+static const struct of_device_id rga2_dt_ids[] = {
+	{
+	 .compatible = "rockchip,rga2",
+	 .data = &rga2_match_data,
+	},
+	/* legacy */
+	{
+	 .compatible = "rockchip,rga2_core0",
+	 .data = &rga2_match_data,
+	},
+	{},
+};
+
+static int init_scheduler(struct rga_scheduler_t *scheduler,
+			  struct device *dev,
+			  const struct rga_match_data_t *match_data,
+			  struct rga_drvdata_t *drv_data)
+{
+	switch (match_data->device_type) {
+	case RGA_DEVICE_RGA2:
+		switch (drv_data->device_count[match_data->device_type]) {
+		case 0:
+			scheduler->core = RGA2_SCHEDULER_CORE0;
+			break;
+		case 1:
+			scheduler->core = RGA2_SCHEDULER_CORE1;
+			break;
+		default:
+			pr_err("scheduler failed to match RGA2\n");
+			return -EINVAL;
+		}
+
+		break;
+	case RGA_DEVICE_RGA3:
+		switch (drv_data->device_count[match_data->device_type]) {
+		case 0:
+			scheduler->core = RGA3_SCHEDULER_CORE0;
+			break;
+		case 1:
+			scheduler->core = RGA3_SCHEDULER_CORE1;
+			break;
+		default:
+			pr_err("scheduler failed to match RGA2\n");
+			return -EINVAL;
+		}
+
+		break;
+	default:
+
+		return -EINVAL;
+	}
+
+	scheduler->ops = match_data->ops;
+	scheduler->dev = dev;
+
+	spin_lock_init(&scheduler->irq_lock);
+	INIT_LIST_HEAD(&scheduler->todo_list);
+	init_waitqueue_head(&scheduler->job_done_wq);
+
+	return 0;
+}
+
+static int rga_drv_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	int irq;
+	struct resource *res;
+	const struct rga_match_data_t *match_data;
+	const struct of_device_id *match;
+	struct rga_scheduler_t *scheduler;
+	struct device *dev = &pdev->dev;
+	struct rga_drvdata_t *data = rga_drvdata;
+
+	if (!dev->of_node)
+		return -EINVAL;
+
+	if (!strcmp(dev_driver_string(dev), "rga3"))
+		match = of_match_device(rga3_dt_ids, dev);
+	else if (!strcmp(dev_driver_string(dev), "rga2"))
+		match = of_match_device(rga2_dt_ids, dev);
+	else
+		match = NULL;
+	if (!match) {
+		dev_err(dev, "missing DT entry!\n");
+		return -EINVAL;
+	}
+
+	scheduler = devm_kzalloc(dev, sizeof(struct rga_scheduler_t), GFP_KERNEL);
+	if (scheduler == NULL) {
+		dev_err(dev, "failed to allocate scheduler.\n");
+		return -ENOMEM;
+	}
+
+	match_data = match->data;
+	ret = init_scheduler(scheduler, dev, match_data, data);
+	if (ret < 0) {
+		dev_err(dev, "init scheduler failed!\n");
+		return ret;
+	}
+
+	/* map the registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "get memory resource failed.\n");
+		return -ENXIO;
+	}
+
+	scheduler->rga_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!scheduler->rga_base) {
+		dev_err(dev, "ioremap failed\n");
+		ret = -ENOENT;
+		return ret;
+	}
+
+	/* get the IRQ */
+	/* there are irq names in dts */
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(dev, "no irq in dts\n");
+		return irq;
+	}
+
+	scheduler->irq = irq;
+
+	ret = devm_request_threaded_irq(dev, irq,
+					rga_irq_handler,
+					rga_isr_thread,
+					IRQF_SHARED,
+					dev_driver_string(dev), scheduler);
+	if (ret < 0) {
+		dev_err(dev, "request irq failed: %d\n", ret);
+		return ret;
+	}
+
+
+#ifndef RGA_DISABLE_PM
+	/* clk init */
+	ret = devm_clk_bulk_get_all(dev, &scheduler->clks);
+	if (ret < 1) {
+		dev_err(dev, "failed to get clk\n");
+		return ret < 0 ? ret : -EINVAL;
+	}
+	scheduler->num_clks = ret;
+
+	/* PM init */
+	device_init_wakeup(dev, true);
+	pm_runtime_enable(scheduler->dev);
+
+	ret = pm_runtime_get_sync(scheduler->dev);
+	if (ret < 0) {
+		dev_err(dev, "failed to get pm runtime, ret = %d\n", ret);
+		goto pm_disable;
+	}
+
+	ret = clk_bulk_prepare_enable(scheduler->num_clks, scheduler->clks);
+	if (ret < 0) {
+		dev_err(dev, "failed to enable clk\n");
+		goto pm_disable;
+	}
+#endif /* #ifndef RGA_DISABLE_PM */
+
+	scheduler->ops->get_version(scheduler);
+
+	/* TODO: get by hw version, Currently only supports judgment 1106. */
+	if (scheduler->core == RGA3_SCHEDULER_CORE0 ||
+	    scheduler->core == RGA3_SCHEDULER_CORE1) {
+		scheduler->data = &rga3_data;
+	} else if (scheduler->core == RGA2_SCHEDULER_CORE0 ||
+		   scheduler->core == RGA2_SCHEDULER_CORE1) {
+		if (!strcmp(scheduler->version.str, "3.3.87975")) {
+			scheduler->data = &rga2e_1106_data;
+		} else if (!strcmp(scheduler->version.str, "3.6.92812") ||
+			 !strcmp(scheduler->version.str, "3.7.93215")) {
+			scheduler->data = &rga2e_iommu_data;
+		} else if (!strcmp(scheduler->version.str, "3.a.07135")) {
+			scheduler->data = &rga2e_3506_data;
+		} else if (!strcmp(scheduler->version.str, "3.e.19357")) {
+			scheduler->data = &rga2p_iommu_data;
+			rga_hw_set_issue_mask(scheduler, RGA_HW_ISSUE_DIS_AUTO_RST);
+		} else if (!strcmp(scheduler->version.str, "3.f.23690")) {
+			scheduler->data = &rga2p_lite_1103b_data;
+		} else {
+			scheduler->data = &rga2e_data;
+		}
+	}
+
+	data->scheduler[data->num_of_scheduler] = scheduler;
+	data->num_of_scheduler++;
+	data->device_count[match_data->device_type]++;
+
+#ifndef RGA_DISABLE_PM
+	clk_bulk_disable_unprepare(scheduler->num_clks, scheduler->clks);
+
+	pm_runtime_put_sync(dev);
+#endif /* #ifndef RGA_DISABLE_PM */
+
+	if (scheduler->data->mmu == RGA_IOMMU) {
+		scheduler->iommu_info = rga_iommu_probe(dev);
+		if (IS_ERR(scheduler->iommu_info)) {
+			dev_err(dev, "failed to attach iommu\n");
+			scheduler->iommu_info = NULL;
+		}
+
+		dma_set_mask(dev, DMA_BIT_MASK(40));
+		dma_set_coherent_mask(dev, DMA_BIT_MASK(32));
+	} else {
+		dma_set_mask(dev, DMA_BIT_MASK(32));
+		dma_set_coherent_mask(dev, DMA_BIT_MASK(32));
+	}
+
+	platform_set_drvdata(pdev, scheduler);
+
+	dev_info(dev, "probe successfully, irq = %d, hw_version:%s\n",
+		 scheduler->irq, scheduler->version.str);
+
+	return 0;
+
+#ifndef RGA_DISABLE_PM
+pm_disable:
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+#endif /* #ifndef RGA_DISABLE_PM */
+
+	return ret;
+}
+
+static int rga_drv_remove(struct platform_device *pdev)
+{
+	struct rga_scheduler_t *scheduler = NULL;
+
+	down_write(&rga_drvdata->rwsem);
+	rga_drvdata->shutdown = true;
+
+	scheduler = (struct rga_scheduler_t *)platform_get_drvdata(pdev);
+	if (scheduler)
+		rga_request_scheduler_shutdown(scheduler);
+
+#ifndef RGA_DISABLE_PM
+	device_init_wakeup(&pdev->dev, false);
+	pm_runtime_disable(&pdev->dev);
+#endif /* #ifndef RGA_DISABLE_PM */
+
+	up_write(&rga_drvdata->rwsem);
+
+	return 0;
+}
+
+static void rga_drv_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	rga_drv_remove(pdev);
+
+	dev_info(dev, "shutdown success\n");
+}
+
+static struct platform_driver rga3_driver = {
+	.probe = rga_drv_probe,
+	.remove = rga_drv_remove,
+	.shutdown = rga_drv_shutdown,
+	.driver = {
+		 .name = "rga3",
+		 .of_match_table = of_match_ptr(rga3_dt_ids),
+		 },
+};
+
+static struct platform_driver rga2_driver = {
+	.probe = rga_drv_probe,
+	.remove = rga_drv_remove,
+	.shutdown = rga_drv_shutdown,
+	.driver = {
+		 .name = "rga2",
+		 .of_match_table = of_match_ptr(rga2_dt_ids),
+		 },
+};
+
+static int __init rga_init(void)
+{
+	int ret;
+
+	rga_drvdata = kzalloc(sizeof(struct rga_drvdata_t), GFP_KERNEL);
+	if (rga_drvdata == NULL) {
+		pr_err("failed to allocate driver data.\n");
+		return -ENOMEM;
+	}
+
+	mutex_init(&rga_drvdata->lock);
+	init_rwsem(&rga_drvdata->rwsem);
+	rga_drvdata->shutdown = false;
+
+	ret = platform_driver_register(&rga3_driver);
+	if (ret != 0) {
+		pr_err("Platform device rga3_driver register failed (%d).\n", ret);
+		goto err_free_drvdata;
+	}
+	ret = platform_driver_register(&rga2_driver);
+	if (ret != 0) {
+		pr_err("Platform device rga2_driver register failed (%d).\n", ret);
+		goto err_unregister_rga3;
+	}
+
+	ret = rga_iommu_bind();
+	if (ret < 0) {
+		pr_err("rga iommu bind failed!\n");
+		goto err_unregister_rga2;
+	}
+
+	ret = misc_register(&rga_dev);
+	if (ret) {
+		pr_err("cannot register miscdev (%d)\n", ret);
+		goto err_unbind_iommu;
+	}
+
+	rga_init_timer();
+
+	rga_mm_init(&rga_drvdata->mm);
+
+	rga_request_manager_init(&rga_drvdata->pend_request_manager);
+
+	rga_session_manager_init(&rga_drvdata->session_manager);
+
+#ifdef CONFIG_ROCKCHIP_RGA_ASYNC
+	rga_fence_context_init(&rga_drvdata->fence_ctx);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER
+	rga_debugger_init(&rga_drvdata->debugger);
+#endif
+
+	pr_info("Module initialized. v%s\n", DRIVER_VERSION);
+
+	return 0;
+
+err_unbind_iommu:
+	rga_iommu_unbind();
+
+err_unregister_rga2:
+	platform_driver_unregister(&rga2_driver);
+
+err_unregister_rga3:
+	platform_driver_unregister(&rga3_driver);
+
+err_free_drvdata:
+	kfree(rga_drvdata);
+
+	return ret;
+}
+
+static void __exit rga_exit(void)
+{
+#ifdef CONFIG_ROCKCHIP_RGA_DEBUGGER
+	rga_debugger_remove(&rga_drvdata->debugger);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RGA_ASYNC
+	rga_fence_context_remove(&rga_drvdata->fence_ctx);
+#endif
+
+	rga_mm_remove(&rga_drvdata->mm);
+
+	rga_request_manager_remove(&rga_drvdata->pend_request_manager);
+
+	rga_session_manager_remove(&rga_drvdata->session_manager);
+
+	rga_cancel_timer();
+
+	rga_iommu_unbind();
+
+	platform_driver_unregister(&rga3_driver);
+	platform_driver_unregister(&rga2_driver);
+
+	misc_deregister(&rga_dev);
+
+	kfree(rga_drvdata);
+
+	pr_info("Module exited. v%s\n", DRIVER_VERSION);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+#ifdef CONFIG_ROCKCHIP_THUNDER_BOOT
+module_init(rga_init);
+#elif defined CONFIG_VIDEO_REVERSE_IMAGE
+fs_initcall(rga_init);
+#else
+late_initcall(rga_init);
+#endif
+#else
+fs_initcall(rga_init);
+#endif
+module_exit(rga_exit);
+
+/* Module information */
+MODULE_AUTHOR("putin.li@rock-chips.com");
+MODULE_DESCRIPTION("Driver for rga device");
+MODULE_LICENSE("GPL");
+#ifdef MODULE_IMPORT_NS
+MODULE_IMPORT_NS(DMA_BUF);
+MODULE_IMPORT_NS(VFS_internal_I_am_really_a_filesystem_and_am_NOT_a_driver);
+#endif
diff --git a/drivers/video/rockchip/rga3/rga_fence.c b/drivers/video/rockchip/rga3/rga_fence.c
new file mode 100644
index 0000000000000..9c663f3be5cae
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_fence.c
@@ -0,0 +1,145 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include <linux/dma-fence.h>
+#include <linux/sync_file.h>
+#include <linux/slab.h>
+
+#include "rga_drv.h"
+#include "rga_fence.h"
+#include "rga_common.h"
+
+static const char *rga_fence_get_name(struct dma_fence *fence)
+{
+	return DRIVER_NAME;
+}
+
+static const struct dma_fence_ops rga_fence_ops = {
+	.get_driver_name = rga_fence_get_name,
+	.get_timeline_name = rga_fence_get_name,
+};
+
+int rga_fence_context_init(struct rga_fence_context **ctx)
+{
+	struct rga_fence_context *fence_ctx = NULL;
+
+	fence_ctx = kzalloc(sizeof(struct rga_fence_context), GFP_KERNEL);
+	if (!fence_ctx) {
+		pr_err("can not kzalloc for rga_fence_context!\n");
+		return -ENOMEM;
+	}
+
+	fence_ctx->context = dma_fence_context_alloc(1);
+	spin_lock_init(&fence_ctx->spinlock);
+
+	*ctx = fence_ctx;
+
+	return 0;
+}
+
+void rga_fence_context_remove(struct rga_fence_context **ctx)
+{
+	if (*ctx == NULL)
+		return;
+
+	kfree(*ctx);
+	*ctx = NULL;
+}
+
+struct dma_fence *rga_dma_fence_alloc(void)
+{
+	struct rga_fence_context *fence_ctx = rga_drvdata->fence_ctx;
+	struct dma_fence *fence = NULL;
+
+	if (fence_ctx == NULL) {
+		rga_err("fence_context is NULL!\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return ERR_PTR(-ENOMEM);
+
+	dma_fence_init(fence, &rga_fence_ops, &fence_ctx->spinlock,
+		       fence_ctx->context, ++fence_ctx->seqno);
+
+	return fence;
+}
+
+int rga_dma_fence_get_fd(struct dma_fence *fence)
+{
+	struct sync_file *sync_file = NULL;
+	int fence_fd = -1;
+
+	if (!fence)
+		return -EINVAL;
+
+	fence_fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fence_fd < 0)
+		return fence_fd;
+
+	sync_file = sync_file_create(fence);
+	if (!sync_file) {
+		put_unused_fd(fence_fd);
+		return -ENOMEM;
+	}
+
+	fd_install(fence_fd, sync_file->file);
+
+	return fence_fd;
+}
+
+struct dma_fence *rga_get_dma_fence_from_fd(int fence_fd)
+{
+	struct dma_fence *fence;
+
+	fence = sync_file_get_fence(fence_fd);
+	if (!fence)
+		rga_err("can not get fence from fd\n");
+
+	return fence;
+}
+
+int rga_dma_fence_wait(struct dma_fence *fence)
+{
+	int ret = 0;
+
+	ret = dma_fence_wait(fence, true);
+
+	dma_fence_put(fence);
+
+	return ret;
+}
+
+int rga_dma_fence_add_callback(struct dma_fence *fence, dma_fence_func_t func, void *private)
+{
+	int ret;
+	struct rga_fence_waiter *waiter = NULL;
+
+	waiter = kmalloc(sizeof(*waiter), GFP_KERNEL);
+	if (!waiter) {
+		rga_err("%s: Failed to allocate waiter\n", __func__);
+		return -ENOMEM;
+	}
+
+	waiter->private = private;
+
+	ret = dma_fence_add_callback(fence, &waiter->waiter, func);
+	if (ret == -ENOENT) {
+		rga_err("'input fence' has been already signaled.");
+		goto err_free_waiter;
+	} else if (ret == -EINVAL) {
+		rga_err("%s: failed to add callback to dma_fence, err: %d\n", __func__, ret);
+		goto err_free_waiter;
+	}
+
+	return ret;
+
+err_free_waiter:
+	kfree(waiter);
+	return ret;
+}
diff --git a/drivers/video/rockchip/rga3/rga_hw_config.c b/drivers/video/rockchip/rga3/rga_hw_config.c
new file mode 100644
index 0000000000000..6eccd4ec11489
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_hw_config.c
@@ -0,0 +1,673 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *	Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga_hw_config.h"
+
+/* RGA 1Word = 4Byte */
+#define WORD_TO_BYTE(w) ((w) * 4)
+
+const uint32_t rga3_input_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ABGR_8888,
+};
+
+const uint32_t rga3_output_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+};
+
+const uint32_t rga3_fbcd_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+};
+
+const uint32_t rga3_tile_format[] = {
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+};
+
+const uint32_t rga2e_input_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_P,
+	RGA_FORMAT_YCbCr_420_P,
+	RGA_FORMAT_YCrCb_422_P,
+	RGA_FORMAT_YCrCb_420_P,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_BPP1,
+	RGA_FORMAT_BPP2,
+	RGA_FORMAT_BPP4,
+	RGA_FORMAT_BPP8,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ARGB_5551,
+	RGA_FORMAT_ARGB_4444,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_ABGR_5551,
+	RGA_FORMAT_ABGR_4444,
+};
+
+const uint32_t rga2e_output_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_P,
+	RGA_FORMAT_YCbCr_420_P,
+	RGA_FORMAT_YCrCb_422_P,
+	RGA_FORMAT_YCrCb_420_P,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_420,
+	RGA_FORMAT_VYUY_420,
+	RGA_FORMAT_YUYV_420,
+	RGA_FORMAT_UYVY_420,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+	RGA_FORMAT_Y4,
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ARGB_5551,
+	RGA_FORMAT_ARGB_4444,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_ABGR_5551,
+	RGA_FORMAT_ABGR_4444,
+};
+
+const uint32_t rga2p_input_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_P,
+	RGA_FORMAT_YCbCr_420_P,
+	RGA_FORMAT_YCrCb_422_P,
+	RGA_FORMAT_YCrCb_420_P,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_BPP1,
+	RGA_FORMAT_BPP2,
+	RGA_FORMAT_BPP4,
+	RGA_FORMAT_BPP8,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ARGB_5551,
+	RGA_FORMAT_ARGB_4444,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_ABGR_5551,
+	RGA_FORMAT_ABGR_4444,
+	RGA_FORMAT_RGBA_2BPP,
+	RGA_FORMAT_A8,
+	RGA_FORMAT_YCbCr_444_SP,
+	RGA_FORMAT_YCrCb_444_SP,
+};
+
+const uint32_t rga2p_input1_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_ARGB_5551,
+	RGA_FORMAT_ABGR_5551,
+	RGA_FORMAT_ARGB_4444,
+	RGA_FORMAT_ABGR_4444,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_A8,
+};
+
+const uint32_t rga2p_output_raster_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+	RGA_FORMAT_RGB_565,
+	RGA_FORMAT_BGR_565,
+	RGA_FORMAT_YCbCr_422_P,
+	RGA_FORMAT_YCbCr_420_P,
+	RGA_FORMAT_YCrCb_422_P,
+	RGA_FORMAT_YCrCb_420_P,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YVYU_420,
+	RGA_FORMAT_VYUY_420,
+	RGA_FORMAT_YUYV_420,
+	RGA_FORMAT_UYVY_420,
+	RGA_FORMAT_YVYU_422,
+	RGA_FORMAT_VYUY_422,
+	RGA_FORMAT_YUYV_422,
+	RGA_FORMAT_UYVY_422,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+	RGA_FORMAT_Y4,
+	RGA_FORMAT_Y8,
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_RGBA_5551,
+	RGA_FORMAT_BGRA_5551,
+	RGA_FORMAT_RGBA_4444,
+	RGA_FORMAT_BGRA_4444,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ARGB_5551,
+	RGA_FORMAT_ARGB_4444,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_ABGR_5551,
+	RGA_FORMAT_ABGR_4444,
+	RGA_FORMAT_YCbCr_444_SP,
+	RGA_FORMAT_YCrCb_444_SP,
+};
+
+const uint32_t rga2p_tile4x4_format[] = {
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCbCr_444_SP,
+	RGA_FORMAT_YCrCb_444_SP,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+};
+
+const uint32_t rga2p_rkfbc64x4_format[] = {
+	RGA_FORMAT_YCbCr_400,
+	RGA_FORMAT_YCbCr_420_SP,
+	RGA_FORMAT_YCrCb_420_SP,
+	RGA_FORMAT_YCbCr_422_SP,
+	RGA_FORMAT_YCrCb_422_SP,
+	RGA_FORMAT_YCbCr_444_SP,
+	RGA_FORMAT_YCrCb_444_SP,
+	RGA_FORMAT_YCbCr_420_SP_10B,
+	RGA_FORMAT_YCrCb_420_SP_10B,
+	RGA_FORMAT_YCbCr_422_SP_10B,
+	RGA_FORMAT_YCrCb_422_SP_10B,
+};
+
+const uint32_t rga2p_afbc32x8_format[] = {
+	RGA_FORMAT_RGBA_8888,
+	RGA_FORMAT_BGRA_8888,
+	RGA_FORMAT_RGBX_8888,
+	RGA_FORMAT_BGRX_8888,
+	RGA_FORMAT_XRGB_8888,
+	RGA_FORMAT_XBGR_8888,
+	RGA_FORMAT_ARGB_8888,
+	RGA_FORMAT_ABGR_8888,
+	RGA_FORMAT_RGB_888,
+	RGA_FORMAT_BGR_888,
+};
+
+const struct rga_win_data rga3_win_data[] = {
+	{
+		.name = "rga3-win0",
+		.formats[RGA_RASTER_INDEX] = rga3_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga3_input_raster_format),
+		.formats[RGA_AFBC16x16_INDEX] = rga3_fbcd_format,
+		.formats_count[RGA_AFBC16x16_INDEX] = ARRAY_SIZE(rga3_fbcd_format),
+		.formats[RGA_TILE8x8_INDEX] = rga3_tile_format,
+		.formats_count[RGA_TILE8x8_INDEX] = ARRAY_SIZE(rga3_tile_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE | RGA_FBC_MODE | RGA_TILE_MODE,
+
+	},
+
+	{
+		.name = "rga3-win1",
+		.formats[RGA_RASTER_INDEX] = rga3_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga3_input_raster_format),
+		.formats[RGA_AFBC16x16_INDEX] = rga3_fbcd_format,
+		.formats_count[RGA_AFBC16x16_INDEX] = ARRAY_SIZE(rga3_fbcd_format),
+		.formats[RGA_TILE8x8_INDEX] = rga3_tile_format,
+		.formats_count[RGA_TILE8x8_INDEX] = ARRAY_SIZE(rga3_tile_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE | RGA_FBC_MODE | RGA_TILE_MODE,
+
+	},
+
+	{
+		.name = "rga3-wr",
+		.formats[RGA_RASTER_INDEX] = rga3_output_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga3_output_raster_format),
+		.formats[RGA_AFBC16x16_INDEX] = rga3_fbcd_format,
+		.formats_count[RGA_AFBC16x16_INDEX] = ARRAY_SIZE(rga3_fbcd_format),
+		.formats[RGA_TILE8x8_INDEX] = rga3_tile_format,
+		.formats_count[RGA_TILE8x8_INDEX] = ARRAY_SIZE(rga3_tile_format),
+		.supported_rotations = 0,
+		.scale_up_mode = RGA_SCALE_UP_NONE,
+		.scale_down_mode = RGA_SCALE_DOWN_NONE,
+		.rd_mode = RGA_RASTER_MODE | RGA_FBC_MODE | RGA_TILE_MODE,
+
+	},
+};
+
+const struct rga_win_data rga2e_win_data[] = {
+	{
+		.name = "rga2e-src0",
+		.formats[RGA_RASTER_INDEX] = rga2e_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_input_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+
+	{
+		.name = "rga2e-src1",
+		.formats[RGA_RASTER_INDEX] = rga2e_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_input_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+
+	{
+		.name = "rga2-dst",
+		.formats[RGA_RASTER_INDEX] = rga2e_output_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_output_raster_format),
+		.supported_rotations = 0,
+		.scale_up_mode = RGA_SCALE_UP_NONE,
+		.scale_down_mode = RGA_SCALE_DOWN_NONE,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+};
+
+const struct rga_win_data rga2e_3506_win_data[] = {
+	{
+		.name = "rga2e-src0",
+		.formats[RGA_RASTER_INDEX] = rga2e_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_input_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+
+	{
+		.name = "rga2e-src1",
+		.formats[RGA_RASTER_INDEX] = rga2p_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2p_input_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+
+	{
+		.name = "rga2-dst",
+		.formats[RGA_RASTER_INDEX] = rga2e_output_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_output_raster_format),
+		.supported_rotations = 0,
+		.scale_up_mode = RGA_SCALE_UP_NONE,
+		.scale_down_mode = RGA_SCALE_DOWN_NONE,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+};
+
+const struct rga_win_data rga2p_win_data[] = {
+	{
+		.name = "rga2p-src0",
+		.formats[RGA_RASTER_INDEX] = rga2p_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2p_input_raster_format),
+		.formats[RGA_TILE4x4_INDEX] = rga2p_tile4x4_format,
+		.formats_count[RGA_TILE4x4_INDEX] = ARRAY_SIZE(rga2p_tile4x4_format),
+		.formats[RGA_RKFBC64x4_INDEX] = rga2p_rkfbc64x4_format,
+		.formats_count[RGA_RKFBC64x4_INDEX] = ARRAY_SIZE(rga2p_rkfbc64x4_format),
+		.formats[RGA_AFBC32x8_INDEX] = rga2p_afbc32x8_format,
+		.formats_count[RGA_AFBC32x8_INDEX] = ARRAY_SIZE(rga2p_afbc32x8_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE | RGA_TILE4x4_MODE | RGA_RKFBC_MODE | RGA_AFBC32x8_MODE,
+
+	},
+
+	{
+		.name = "rga2p-src1",
+		.formats[RGA_RASTER_INDEX] = rga2p_input1_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2p_input1_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE | RGA_TILE4x4_MODE | RGA_RKFBC_MODE | RGA_AFBC32x8_MODE,
+
+	},
+
+	{
+		.name = "rga2p-dst",
+		.formats[RGA_RASTER_INDEX] = rga2p_output_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2p_output_raster_format),
+		.formats[RGA_TILE4x4_INDEX] = rga2p_tile4x4_format,
+		.formats_count[RGA_TILE4x4_INDEX] = ARRAY_SIZE(rga2p_tile4x4_format),
+		.supported_rotations = 0,
+		.scale_up_mode = RGA_SCALE_UP_NONE,
+		.scale_down_mode = RGA_SCALE_DOWN_NONE,
+		.rd_mode = RGA_RASTER_MODE | RGA_TILE4x4_MODE,
+
+	},
+};
+
+const struct rga_win_data rga2p_lite_win_data[] = {
+	{
+		.name = "rga2e-src0",
+		.formats[RGA_RASTER_INDEX] = rga2e_input_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_input_raster_format),
+		.supported_rotations = RGA_MODE_ROTATE_MASK,
+		.scale_up_mode = RGA_SCALE_UP_BIC,
+		.scale_down_mode = RGA_SCALE_DOWN_AVG,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+
+	{
+		.name = "rga2-dst",
+		.formats[RGA_RASTER_INDEX] = rga2e_output_raster_format,
+		.formats_count[RGA_RASTER_INDEX] = ARRAY_SIZE(rga2e_output_raster_format),
+		.supported_rotations = 0,
+		.scale_up_mode = RGA_SCALE_UP_NONE,
+		.scale_down_mode = RGA_SCALE_DOWN_NONE,
+		.rd_mode = RGA_RASTER_MODE,
+
+	},
+};
+
+const struct rga_hw_data rga3_data = {
+	.version = 0,
+	.input_range = {{68, 2}, {8176, 8176}},
+	.output_range = {{68, 2}, {8128, 8128}},
+
+	.win = rga3_win_data,
+	.win_size = ARRAY_SIZE(rga3_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 3,
+	.max_downscale_factor = 3,
+
+	.byte_stride_align = 16,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_KEY,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709 | RGA_MODE_CSC_BT2020,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709 | RGA_MODE_CSC_BT2020,
+	.mmu = RGA_IOMMU,
+};
+
+const struct rga_hw_data rga2e_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {8192, 8192}},
+	.output_range = {{2, 2}, {4096, 4096}},
+
+	.win = rga2e_win_data,
+	.win_size = ARRAY_SIZE(rga2e_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_COLOR_PALETTE |
+		   RGA_COLOR_KEY | RGA_ROP_CALCULATE |
+		   RGA_NN_QUANTIZE | RGA_DITHER | RGA_FULL_CSC,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_MMU,
+};
+
+const struct rga_hw_data rga2e_1106_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {8192, 8192}},
+	.output_range = {{2, 2}, {4096, 4096}},
+
+	.win = rga2e_win_data,
+	.win_size = ARRAY_SIZE(rga2e_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_COLOR_PALETTE |
+		   RGA_COLOR_KEY | RGA_ROP_CALCULATE |
+		   RGA_NN_QUANTIZE | RGA_DITHER | RGA_MOSAIC |
+		   RGA_YIN_YOUT | RGA_YUV_HDS | RGA_YUV_VDS |
+		   RGA_OSD | RGA_PRE_INTR | RGA_FULL_CSC,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_NONE_MMU,
+};
+
+const struct rga_hw_data rga2e_3506_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {1280, 1280}},
+	.output_range = {{2, 2}, {1280, 1280}},
+
+	.win = rga2e_3506_win_data,
+	.win_size = ARRAY_SIZE(rga2e_3506_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_COLOR_PALETTE |
+		   RGA_COLOR_KEY | RGA_YIN_YOUT | RGA_YUV_HDS | RGA_YUV_VDS |
+		   RGA_PRE_INTR | RGA_FULL_CSC | RGA_GAUSS,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_NONE_MMU,
+};
+
+const struct rga_hw_data rga2e_iommu_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {8192, 8192}},
+	.output_range = {{2, 2}, {4096, 4096}},
+
+	.win = rga2e_win_data,
+	.win_size = ARRAY_SIZE(rga2e_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_COLOR_PALETTE |
+		   RGA_COLOR_KEY | RGA_ROP_CALCULATE |
+		   RGA_NN_QUANTIZE | RGA_DITHER | RGA_MOSAIC |
+		   RGA_YIN_YOUT | RGA_YUV_HDS | RGA_YUV_VDS |
+		   RGA_OSD | RGA_PRE_INTR | RGA_FULL_CSC,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_IOMMU,
+};
+
+const struct rga_hw_data rga2p_iommu_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {8192, 8192}},
+	.output_range = {{2, 2}, {8192, 8192}},
+
+	.win = rga2p_win_data,
+	.win_size = ARRAY_SIZE(rga2p_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_COLOR_PALETTE |
+		   RGA_COLOR_KEY | RGA_ROP_CALCULATE |
+		   RGA_NN_QUANTIZE | RGA_DITHER | RGA_MOSAIC |
+		   RGA_YIN_YOUT | RGA_YUV_HDS | RGA_YUV_VDS |
+		   RGA_OSD | RGA_PRE_INTR | RGA_FULL_CSC,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_IOMMU,
+};
+
+const struct rga_hw_data rga2p_lite_1103b_data = {
+	.version = 0,
+	.input_range = {{2, 2}, {2880, 1620}},
+	.output_range = {{2, 2}, {2880, 1620}},
+
+	.win = rga2p_lite_win_data,
+	.win_size = ARRAY_SIZE(rga2p_lite_win_data),
+	/* 1 << factor mean real factor */
+	.max_upscale_factor = 4,
+	.max_downscale_factor = 4,
+
+	.byte_stride_align = 4,
+	.max_byte_stride = WORD_TO_BYTE(8192),
+
+	.feature = RGA_COLOR_FILL | RGA_DITHER | RGA_YIN_YOUT |
+		   RGA_YUV_HDS | RGA_YUV_VDS |
+		   RGA_PRE_INTR | RGA_FULL_CSC,
+	.csc_r2y_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.csc_y2r_mode = RGA_MODE_CSC_BT601L | RGA_MODE_CSC_BT601F |
+			RGA_MODE_CSC_BT709,
+	.mmu = RGA_NONE_MMU,
+};
diff --git a/drivers/video/rockchip/rga3/rga_iommu.c b/drivers/video/rockchip/rga3/rga_iommu.c
new file mode 100644
index 0000000000000..3ba0d58b2c206
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_iommu.c
@@ -0,0 +1,423 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga_iommu.h"
+#include "rga_dma_buf.h"
+#include "rga_mm.h"
+#include "rga_job.h"
+#include "rga_common.h"
+#include "rga_hw_config.h"
+
+int rga_user_memory_check(struct page **pages, u32 w, u32 h, u32 format, int flag)
+{
+	int bits;
+	void *vaddr = NULL;
+	int taipage_num;
+	int taidata_num;
+	int *tai_vaddr = NULL;
+
+	bits = rga_get_format_bits(format);
+	if (bits < 0)
+		return -1;
+
+	taipage_num = w * h * bits / 8 / (1024 * 4);
+	taidata_num = w * h * bits / 8 % (1024 * 4);
+	if (taidata_num == 0) {
+		vaddr = kmap(pages[taipage_num - 1]);
+		tai_vaddr = (int *)vaddr + 1023;
+	} else {
+		vaddr = kmap(pages[taipage_num]);
+		tai_vaddr = (int *)vaddr + taidata_num / 4 - 1;
+	}
+
+	if (flag == 1) {
+		rga_log("src user memory check\n");
+		rga_log("tai data is %d\n", *tai_vaddr);
+	} else {
+		rga_log("dst user memory check\n");
+		rga_log("tai data is %d\n", *tai_vaddr);
+	}
+
+	if (taidata_num == 0)
+		kunmap(pages[taipage_num - 1]);
+	else
+		kunmap(pages[taipage_num]);
+
+	return 0;
+}
+
+int rga_set_mmu_base(struct rga_job *job, struct rga2_req *req)
+{
+	if (job->src_buffer.page_table) {
+		rga_dma_sync_flush_range(job->src_buffer.page_table,
+					 (job->src_buffer.page_table +
+					  job->src_buffer.page_count),
+					 job->scheduler);
+		req->mmu_info.src0_base_addr = virt_to_phys(job->src_buffer.page_table);
+	}
+
+	if (job->src1_buffer.page_table) {
+		rga_dma_sync_flush_range(job->src1_buffer.page_table,
+					 (job->src1_buffer.page_table +
+					  job->src1_buffer.page_count),
+					 job->scheduler);
+		req->mmu_info.src1_base_addr = virt_to_phys(job->src1_buffer.page_table);
+	}
+
+	if (job->dst_buffer.page_table) {
+		rga_dma_sync_flush_range(job->dst_buffer.page_table,
+					 (job->dst_buffer.page_table +
+					  job->dst_buffer.page_count),
+					 job->scheduler);
+		req->mmu_info.dst_base_addr = virt_to_phys(job->dst_buffer.page_table);
+
+		if (((req->alpha_rop_flag & 1) == 1) && (req->bitblt_mode == 0)) {
+			req->mmu_info.src1_base_addr = req->mmu_info.dst_base_addr;
+			req->mmu_info.src1_mmu_flag = req->mmu_info.dst_mmu_flag;
+		}
+	}
+
+	if (job->els_buffer.page_table) {
+		rga_dma_sync_flush_range(job->els_buffer.page_table,
+					 (job->els_buffer.page_table +
+					  job->els_buffer.page_count),
+					 job->scheduler);
+		req->mmu_info.els_base_addr = virt_to_phys(job->els_buffer.page_table);
+	}
+
+	return 0;
+}
+
+static int rga_mmu_buf_get_try(struct rga_mmu_base *t, uint32_t size)
+{
+	int ret = 0;
+
+	if ((t->back - t->front) > t->size) {
+		if (t->front + size > t->back - t->size) {
+			rga_log("front %d, back %d dsize %d size %d",
+				t->front, t->back, t->size, size);
+			ret = -ENOMEM;
+			goto out;
+		}
+	} else {
+		if ((t->front + size) > t->back) {
+			rga_log("front %d, back %d dsize %d size %d",
+				t->front, t->back, t->size, size);
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		if (t->front + size > t->size) {
+			if (size > (t->back - t->size)) {
+				rga_log("front %d, back %d dsize %d size %d",
+					t->front, t->back, t->size, size);
+				ret = -ENOMEM;
+				goto out;
+			}
+			t->front = 0;
+		}
+	}
+out:
+	return ret;
+}
+
+unsigned int *rga_mmu_buf_get(struct rga_mmu_base *mmu_base, uint32_t size)
+{
+	int ret;
+	unsigned int *buf = NULL;
+
+	WARN_ON(!mutex_is_locked(&rga_drvdata->lock));
+
+	size = ALIGN(size, 16);
+
+	ret = rga_mmu_buf_get_try(mmu_base, size);
+	if (ret < 0) {
+		rga_err("Get MMU mem failed\n");
+		return NULL;
+	}
+
+	buf = mmu_base->buf_virtual + mmu_base->front;
+
+	mmu_base->front += size;
+
+	if (mmu_base->back + size > 2 * mmu_base->size)
+		mmu_base->back = size + mmu_base->size;
+	else
+		mmu_base->back += size;
+
+	return buf;
+}
+
+struct rga_mmu_base *rga_mmu_base_init(size_t size)
+{
+	int order = 0;
+	struct rga_mmu_base *mmu_base;
+
+	mmu_base = kzalloc(sizeof(*mmu_base), GFP_KERNEL);
+	if (mmu_base == NULL) {
+		pr_err("Cannot alloc mmu_base!\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/*
+	 * malloc pre scale mid buf mmu table:
+	 * size * channel_num * address_size
+	 */
+	order = get_order(size * 3 * sizeof(*mmu_base->buf_virtual));
+	if (order >= MAX_ORDER) {
+		pr_err("Can not alloc pages with order[%d] for mmu_page_table, max_order = %d\n",
+		       order, MAX_ORDER);
+		goto err_free_mmu_base;
+	}
+
+	mmu_base->buf_virtual = (uint32_t *) __get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+	if (mmu_base->buf_virtual == NULL) {
+		pr_err("Can not alloc pages for mmu_page_table\n");
+		goto err_free_mmu_base;
+	}
+	mmu_base->buf_order = order;
+
+	order = get_order(size * sizeof(*mmu_base->pages));
+	if (order >= MAX_ORDER) {
+		pr_err("Can not alloc pages with order[%d] for mmu_base->pages, max_order = %d\n",
+		       order, MAX_ORDER);
+		goto err_free_buf_virtual;
+	}
+
+	mmu_base->pages = (struct page **)__get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+	if (mmu_base->pages == NULL) {
+		pr_err("Can not alloc pages for mmu_base->pages\n");
+		goto err_free_buf_virtual;
+	}
+	mmu_base->pages_order = order;
+
+	mmu_base->front = 0;
+	mmu_base->back = RGA2_PHY_PAGE_SIZE * 3;
+	mmu_base->size = RGA2_PHY_PAGE_SIZE * 3;
+
+	return mmu_base;
+
+err_free_buf_virtual:
+	free_pages((unsigned long)mmu_base->buf_virtual, mmu_base->buf_order);
+	mmu_base->buf_order = 0;
+
+err_free_mmu_base:
+	kfree(mmu_base);
+
+	return ERR_PTR(-ENOMEM);
+}
+
+void rga_mmu_base_free(struct rga_mmu_base **mmu_base)
+{
+	struct rga_mmu_base *base = *mmu_base;
+
+	if (base->buf_virtual != NULL) {
+		free_pages((unsigned long)base->buf_virtual, base->buf_order);
+		base->buf_virtual = NULL;
+		base->buf_order = 0;
+	}
+
+	if (base->pages != NULL) {
+		free_pages((unsigned long)base->pages, base->pages_order);
+		base->pages = NULL;
+		base->pages_order = 0;
+	}
+
+	kfree(base);
+	*mmu_base = NULL;
+}
+
+static int rga_iommu_intr_fault_handler(struct iommu_domain *iommu, struct device *iommu_dev,
+					unsigned long iova, int status, void *arg)
+{
+	struct rga_scheduler_t *scheduler = (struct rga_scheduler_t *)arg;
+	struct rga_job *job = scheduler->running_job;
+
+	if (job == NULL)
+		return 0;
+
+	rga_err("IOMMU intr fault, IOVA[0x%lx], STATUS[0x%x]\n", iova, status);
+	if (scheduler->ops->irq)
+		scheduler->ops->irq(scheduler);
+
+	/* iommu interrupts on rga2 do not affect rga2 itself. */
+	if (!test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		set_bit(RGA_JOB_STATE_INTR_ERR, &job->state);
+		scheduler->ops->soft_reset(scheduler);
+	}
+
+	if (status & RGA_IOMMU_IRQ_PAGE_FAULT) {
+		rga_err("RGA IOMMU: page fault! Please check the memory size.\n");
+		job->ret = -EACCES;
+	} else if (status & RGA_IOMMU_IRQ_BUS_ERROR) {
+		rga_err("RGA IOMMU: bus error! Please check if the memory is invalid or has been freed.\n");
+		job->ret = -EACCES;
+	} else {
+		rga_err("RGA IOMMU: Wrong IOMMU interrupt signal!\n");
+	}
+
+	return 0;
+}
+
+int rga_iommu_detach(struct rga_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_detach_group(info->domain, info->group);
+	return 0;
+}
+
+int rga_iommu_attach(struct rga_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	return iommu_attach_group(info->domain, info->group);
+}
+
+struct rga_iommu_info *rga_iommu_probe(struct device *dev)
+{
+	int ret = 0;
+	struct rga_iommu_info *info = NULL;
+	struct iommu_domain *domain = NULL;
+	struct iommu_group *group = NULL;
+
+	group = iommu_group_get(dev);
+	if (!group)
+		return ERR_PTR(-EINVAL);
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain) {
+		ret = -EINVAL;
+		goto err_put_group;
+	}
+
+	info = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		ret = -ENOMEM;
+		goto err_put_group;
+	}
+
+	info->dev = dev;
+	info->default_dev = info->dev;
+	info->group = group;
+	info->domain = domain;
+
+	return info;
+
+err_put_group:
+	if (group)
+		iommu_group_put(group);
+
+	return ERR_PTR(ret);
+}
+
+int rga_iommu_remove(struct rga_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_group_put(info->group);
+
+	return 0;
+}
+
+int rga_iommu_bind(void)
+{
+	int i;
+	int ret;
+	struct rga_scheduler_t *scheduler = NULL;
+	struct rga_iommu_info *main_iommu = NULL;
+	int main_iommu_index = -1;
+	int main_mmu_index = -1;
+	int another_index = -1;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		switch (scheduler->data->mmu) {
+		case RGA_IOMMU:
+			if (scheduler->iommu_info == NULL)
+				continue;
+
+			if (main_iommu == NULL) {
+				main_iommu = scheduler->iommu_info;
+				main_iommu_index = i;
+				iommu_set_fault_handler(main_iommu->domain,
+							rga_iommu_intr_fault_handler,
+							(void *)scheduler);
+			} else {
+				scheduler->iommu_info->domain = main_iommu->domain;
+				scheduler->iommu_info->default_dev = main_iommu->default_dev;
+				rga_iommu_attach(scheduler->iommu_info);
+			}
+
+			break;
+
+		case RGA_MMU:
+			if (rga_drvdata->mmu_base != NULL)
+				continue;
+
+			rga_drvdata->mmu_base = rga_mmu_base_init(RGA2_PHY_PAGE_SIZE);
+			if (IS_ERR(rga_drvdata->mmu_base)) {
+				dev_err(scheduler->dev, "rga mmu base init failed!\n");
+				ret = PTR_ERR(rga_drvdata->mmu_base);
+				rga_drvdata->mmu_base = NULL;
+
+				return ret;
+			}
+
+			main_mmu_index = i;
+
+			break;
+		default:
+			if (another_index != RGA_NONE_CORE)
+				another_index = i;
+
+			break;
+		}
+	}
+
+	/*
+	 * priority order: iommu > mmu > another
+	 *   The scheduler core with IOMMU will be used preferentially as the
+	 * default memory-mapped core. This ensures that all cores can obtain
+	 * the required memory data when they are equipped with different
+	 * versions of cores.
+	 */
+	if (main_iommu_index >= 0) {
+		rga_drvdata->map_scheduler_index = main_iommu_index;
+	} else if (main_mmu_index >= 0) {
+		rga_drvdata->map_scheduler_index = main_mmu_index;
+	} else if (another_index >= 0) {
+		rga_drvdata->map_scheduler_index = another_index;
+	} else {
+		rga_drvdata->map_scheduler_index = -1;
+		pr_err("%s, binding map scheduler failed!\n", __func__);
+		return -EFAULT;
+	}
+
+	pr_info("IOMMU binding successfully, default mapping core[0x%x]\n",
+		rga_drvdata->scheduler[rga_drvdata->map_scheduler_index]->core);
+
+	return 0;
+}
+
+void rga_iommu_unbind(void)
+{
+	int i;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++)
+		if (rga_drvdata->scheduler[i]->iommu_info != NULL)
+			rga_iommu_detach(rga_drvdata->scheduler[i]->iommu_info);
+
+	if (rga_drvdata->mmu_base)
+		rga_mmu_base_free(&rga_drvdata->mmu_base);
+
+	rga_drvdata->map_scheduler_index = -1;
+}
diff --git a/drivers/video/rockchip/rga3/rga_job.c b/drivers/video/rockchip/rga3/rga_job.c
new file mode 100644
index 0000000000000..fd254b587fde1
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_job.c
@@ -0,0 +1,1551 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga_job.h"
+#include "rga_fence.h"
+#include "rga_dma_buf.h"
+#include "rga_mm.h"
+#include "rga_iommu.h"
+#include "rga_debugger.h"
+#include "rga_common.h"
+
+static void rga_job_free(struct rga_job *job)
+{
+	if (job->cmd_buf)
+		rga_dma_free(job->cmd_buf);
+
+	kfree(job);
+}
+
+static void rga_job_kref_release(struct kref *ref)
+{
+	struct rga_job *job;
+
+	job = container_of(ref, struct rga_job, refcount);
+
+	rga_job_free(job);
+}
+
+static int rga_job_put(struct rga_job *job)
+{
+	return kref_put(&job->refcount, rga_job_kref_release);
+}
+
+static void rga_job_get(struct rga_job *job)
+{
+	kref_get(&job->refcount);
+}
+
+static int rga_job_cleanup(struct rga_job *job)
+{
+	rga_job_put(job);
+
+	return 0;
+}
+
+static int rga_job_judgment_support_core(struct rga_job *job)
+{
+	int ret = 0;
+	uint32_t mm_flag;
+	struct rga_req *req;
+	struct rga_mm *mm;
+
+	req = &job->rga_command_base;
+	mm = rga_drvdata->mm;
+	if (mm == NULL) {
+		rga_job_err(job, "rga mm is null!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&mm->lock);
+
+	if (likely(req->src.yrgb_addr > 0)) {
+		ret = rga_mm_lookup_flag(mm, req->src.yrgb_addr);
+		if (ret < 0)
+			goto out_finish;
+		else
+			mm_flag = (uint32_t)ret;
+
+		if (~mm_flag & RGA_MEM_UNDER_4G) {
+			job->flags |= RGA_JOB_UNSUPPORT_RGA_MMU;
+			goto out_finish;
+		}
+	}
+
+	if (likely(req->dst.yrgb_addr > 0)) {
+		ret = rga_mm_lookup_flag(mm, req->dst.yrgb_addr);
+		if (ret < 0)
+			goto out_finish;
+		else
+			mm_flag = (uint32_t)ret;
+
+		if (~mm_flag & RGA_MEM_UNDER_4G) {
+			job->flags |= RGA_JOB_UNSUPPORT_RGA_MMU;
+			goto out_finish;
+		}
+	}
+
+	if (req->pat.yrgb_addr > 0) {
+		ret = rga_mm_lookup_flag(mm, req->pat.yrgb_addr);
+		if (ret < 0)
+			goto out_finish;
+		else
+			mm_flag = (uint32_t)ret;
+
+		if (~mm_flag & RGA_MEM_UNDER_4G) {
+			job->flags |= RGA_JOB_UNSUPPORT_RGA_MMU;
+			goto out_finish;
+		}
+	}
+
+out_finish:
+	mutex_unlock(&mm->lock);
+
+	return ret;
+}
+
+static struct rga_job *rga_job_alloc(struct rga_req *rga_command_base)
+{
+	struct rga_job *job = NULL;
+
+	job = kzalloc(sizeof(*job), GFP_KERNEL);
+	if (!job)
+		return NULL;
+
+	INIT_LIST_HEAD(&job->head);
+	kref_init(&job->refcount);
+
+	job->timestamp.init = ktime_get();
+	job->pid = current->pid;
+
+	job->rga_command_base = *rga_command_base;
+
+	if (rga_command_base->priority > 0) {
+		if (rga_command_base->priority > RGA_SCHED_PRIORITY_MAX)
+			job->priority = RGA_SCHED_PRIORITY_MAX;
+		else
+			job->priority = rga_command_base->priority;
+	}
+
+	if (DEBUGGER_EN(INTERNAL_MODE)) {
+		job->flags |= RGA_JOB_DEBUG_FAKE_BUFFER;
+
+		/* skip subsequent flag judgments. */
+		return job;
+	}
+
+	if (job->rga_command_base.handle_flag & 1) {
+		job->flags |= RGA_JOB_USE_HANDLE;
+
+		rga_job_judgment_support_core(job);
+	}
+
+	return job;
+}
+
+static int rga_job_run(struct rga_job *job, struct rga_scheduler_t *scheduler)
+{
+	int ret = 0;
+
+	/* enable power */
+	ret = rga_power_enable(scheduler);
+	if (ret < 0) {
+		rga_job_err(job, "power enable failed");
+		return ret;
+	}
+
+	ret = scheduler->ops->set_reg(job, scheduler);
+	if (ret < 0) {
+		rga_job_err(job, "set reg failed");
+		rga_power_disable(scheduler);
+		return ret;
+	}
+
+	set_bit(RGA_JOB_STATE_RUNNING, &job->state);
+
+	return ret;
+}
+
+void rga_job_next(struct rga_scheduler_t *scheduler)
+{
+	int ret;
+	struct rga_job *job = NULL;
+	unsigned long flags;
+
+next_job:
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	if (scheduler->running_job ||
+		list_empty(&scheduler->todo_list)) {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		return;
+	}
+
+	job = list_first_entry(&scheduler->todo_list, struct rga_job, head);
+
+	list_del_init(&job->head);
+
+	scheduler->job_count--;
+
+	scheduler->running_job = job;
+	set_bit(RGA_JOB_STATE_PREPARE, &job->state);
+	rga_job_get(job);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	ret = rga_job_run(job, scheduler);
+	/* If some error before hw run */
+	if (ret < 0) {
+		rga_job_err(job, "some error on rga_job_run before hw start, %s(%d)\n",
+			__func__, __LINE__);
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+		scheduler->running_job = NULL;
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		job->ret = ret;
+		rga_request_release_signal(scheduler, job);
+
+		rga_job_put(job);
+
+		goto next_job;
+	}
+
+	rga_job_put(job);
+}
+
+struct rga_job *rga_job_done(struct rga_scheduler_t *scheduler)
+{
+	struct rga_job *job;
+	unsigned long flags;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+	if (job == NULL) {
+		rga_err("%s(%#x) running job has been cleanup.\n",
+			rga_get_core_name(scheduler->core), scheduler->core);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		return NULL;
+	}
+
+	if (!test_bit(RGA_JOB_STATE_FINISH, &job->state) &&
+	    !test_bit(RGA_JOB_STATE_INTR_ERR, &job->state)) {
+		rga_err("%s(%#x) running job has not yet been completed.",
+			rga_get_core_name(scheduler->core), scheduler->core);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		return NULL;
+	}
+
+	scheduler->running_job = NULL;
+
+	scheduler->timer.busy_time +=
+		ktime_us_delta(job->timestamp.hw_done, job->timestamp.hw_recode);
+	job->session->last_active = job->timestamp.hw_done;
+	set_bit(RGA_JOB_STATE_DONE, &job->state);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	if (scheduler->ops->read_back_reg)
+		scheduler->ops->read_back_reg(job, scheduler);
+
+	if (DEBUGGER_EN(DUMP_IMAGE))
+		rga_dump_job_image(job);
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "hardware[%s] cost time %lld us, work cycle %d\n",
+			rga_get_core_name(scheduler->core),
+			ktime_us_delta(job->timestamp.hw_done, job->timestamp.hw_execute),
+			job->work_cycle);
+
+	rga_mm_unmap_job_info(job);
+
+	return job;
+}
+
+static int rga_job_timeout_query_state(struct rga_job *job, int orig_ret)
+{
+	int ret = orig_ret;
+	struct rga_scheduler_t *scheduler = job->scheduler;
+
+	if (test_bit(RGA_JOB_STATE_DONE, &job->state) &&
+	    test_bit(RGA_JOB_STATE_FINISH, &job->state)) {
+		return orig_ret;
+	} else if (!test_bit(RGA_JOB_STATE_DONE, &job->state) &&
+		   test_bit(RGA_JOB_STATE_FINISH, &job->state)) {
+		rga_job_err(job, "job hardware has finished, but the software has timeout!\n");
+
+		ret = -EBUSY;
+	} else if (!test_bit(RGA_JOB_STATE_DONE, &job->state) &&
+		   !test_bit(RGA_JOB_STATE_FINISH, &job->state)) {
+		rga_job_err(job, "job hardware has timeout.\n");
+
+		if (scheduler->ops->read_status)
+			scheduler->ops->read_status(job, scheduler);
+
+		ret = -EBUSY;
+	}
+
+	rga_job_err(job, "timeout core[%d]: INTR[0x%x], HW_STATUS[0x%x], CMD_STATUS[0x%x], WORK_CYCLE[0x%x(%d)]\n",
+		    scheduler->core,
+		    job->intr_status, job->hw_status, job->cmd_status,
+		    job->work_cycle, job->work_cycle);
+
+	return ret;
+}
+
+static void rga_job_scheduler_timeout_clean(struct rga_scheduler_t *scheduler)
+{
+	unsigned long flags;
+	struct rga_job *job = NULL;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	if (scheduler->running_job == NULL || scheduler->running_job->timestamp.hw_execute == 0) {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		return;
+	}
+
+	job = scheduler->running_job;
+	if (ktime_ms_delta(ktime_get(), job->timestamp.hw_execute) >= RGA_JOB_TIMEOUT_DELAY) {
+		job->ret = rga_job_timeout_query_state(job, job->ret);
+
+		scheduler->running_job = NULL;
+		scheduler->status = RGA_SCHEDULER_ABORT;
+		scheduler->ops->soft_reset(scheduler);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		rga_mm_unmap_job_info(job);
+		rga_request_release_signal(scheduler, job);
+
+		rga_power_disable(scheduler);
+	} else {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+}
+
+static void rga_job_insert_todo_list(struct rga_job *job)
+{
+	bool first_match = 0;
+	unsigned long flags;
+	struct rga_job *job_pos;
+	struct rga_scheduler_t *scheduler = job->scheduler;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	/* priority policy set by userspace */
+	if (list_empty(&scheduler->todo_list)
+		|| (job->priority == RGA_SCHED_PRIORITY_DEFAULT)) {
+		list_add_tail(&job->head, &scheduler->todo_list);
+	} else {
+		list_for_each_entry(job_pos, &scheduler->todo_list, head) {
+			if (job->priority > job_pos->priority &&
+					(!first_match)) {
+				list_add(&job->head, &job_pos->head);
+				first_match = true;
+			}
+
+			/*
+			 * Increase the priority of subsequent tasks
+			 * after inserting into the list
+			 */
+			if (first_match)
+				job_pos->priority++;
+		}
+
+		if (!first_match)
+			list_add_tail(&job->head, &scheduler->todo_list);
+	}
+
+	job->timestamp.insert = ktime_get();
+	scheduler->job_count++;
+	set_bit(RGA_JOB_STATE_PENDING, &job->state);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+}
+
+static struct rga_scheduler_t *rga_job_schedule(struct rga_job *job)
+{
+	int i;
+	struct rga_scheduler_t *scheduler = NULL;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+		rga_job_scheduler_timeout_clean(scheduler);
+	}
+
+	if (rga_drvdata->num_of_scheduler > 1) {
+		job->core = rga_job_assign(job);
+		if (job->core <= 0) {
+			rga_job_err(job, "job assign failed");
+			job->ret = -EINVAL;
+			return NULL;
+		}
+	} else {
+		job->core = rga_drvdata->scheduler[0]->core;
+		job->scheduler = rga_drvdata->scheduler[0];
+	}
+
+	scheduler = job->scheduler;
+	if (scheduler == NULL) {
+		rga_job_err(job, "failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+		job->ret = -EFAULT;
+		return NULL;
+	}
+
+	return scheduler;
+}
+
+int rga_job_commit(struct rga_req *rga_command_base, struct rga_request *request)
+{
+	int ret;
+	struct rga_job *job = NULL;
+	struct rga_scheduler_t *scheduler = NULL;
+
+	job = rga_job_alloc(rga_command_base);
+	if (!job) {
+		rga_err("failed to alloc rga job!\n");
+		return -ENOMEM;
+	}
+
+	job->use_batch_mode = request->use_batch_mode;
+	job->request_id = request->id;
+	job->session = request->session;
+	job->mm = request->current_mm;
+
+	scheduler = rga_job_schedule(job);
+	if (scheduler == NULL) {
+		goto err_free_job;
+	}
+
+	job->cmd_buf = rga_dma_alloc_coherent(scheduler, RGA_CMD_REG_SIZE);
+	if (job->cmd_buf == NULL) {
+		rga_job_err(job, "failed to alloc command buffer.\n");
+		goto err_free_job;
+	}
+
+	/* Memory mapping needs to keep pd enabled. */
+	if (rga_power_enable(scheduler) < 0) {
+		rga_job_err(job, "power enable failed");
+		job->ret = -EFAULT;
+		goto err_free_cmd_buf;
+	}
+
+	ret = rga_mm_map_job_info(job);
+	if (ret < 0) {
+		rga_job_err(job, "%s: failed to map job info\n", __func__);
+		job->ret = ret;
+		goto err_power_disable;
+	}
+
+	ret = scheduler->ops->init_reg(job);
+	if (ret < 0) {
+		rga_job_err(job, "%s: init reg failed", __func__);
+		job->ret = ret;
+		goto err_unmap_job_info;
+	}
+
+	rga_job_insert_todo_list(job);
+
+	rga_job_next(scheduler);
+
+	rga_power_disable(scheduler);
+
+	return 0;
+
+err_unmap_job_info:
+	rga_mm_unmap_job_info(job);
+
+err_power_disable:
+	rga_power_disable(scheduler);
+
+err_free_cmd_buf:
+	rga_dma_free(job->cmd_buf);
+	job->cmd_buf = NULL;
+
+err_free_job:
+	ret = job->ret;
+	rga_job_free(job);
+
+	return ret;
+}
+
+static bool rga_is_need_current_mm(struct rga_req *req)
+{
+	int mmu_flag;
+	struct rga_img_info_t *src0 = NULL;
+	struct rga_img_info_t *src1 = NULL;
+	struct rga_img_info_t *dst = NULL;
+	struct rga_img_info_t *els = NULL;
+
+	src0 = &req->src;
+	dst = &req->dst;
+	if (req->render_mode != UPDATE_PALETTE_TABLE_MODE)
+		src1 = &req->pat;
+	else
+		els = &req->pat;
+
+	if (likely(src0 != NULL)) {
+		mmu_flag = ((req->mmu_info.mmu_flag >> 8) & 1);
+		if (mmu_flag && src0->uv_addr)
+			return true;
+	}
+
+	if (likely(dst != NULL)) {
+		mmu_flag = ((req->mmu_info.mmu_flag >> 10) & 1);
+		if (mmu_flag && dst->uv_addr)
+			return true;
+	}
+
+	if (src1 != NULL) {
+		mmu_flag = ((req->mmu_info.mmu_flag >> 9) & 1);
+		if (mmu_flag && src1->uv_addr)
+			return true;
+	}
+
+	if (els != NULL) {
+		mmu_flag = ((req->mmu_info.mmu_flag >> 11) & 1);
+		if (mmu_flag && els->uv_addr)
+			return true;
+	}
+
+	return false;
+}
+
+static struct mm_struct *rga_request_get_current_mm(struct rga_request *request)
+{
+	int i;
+
+	for (i = 0; i < request->task_count; i++) {
+		if (rga_is_need_current_mm(&(request->task_list[i]))) {
+			mmgrab(current->mm);
+			mmget(current->mm);
+
+			return current->mm;
+		}
+	}
+
+	return NULL;
+}
+
+static void rga_request_put_current_mm(struct mm_struct *mm)
+{
+	if (mm == NULL)
+		return;
+
+	mmput(mm);
+	mmdrop(mm);
+}
+
+static int rga_request_add_acquire_fence_callback(int acquire_fence_fd,
+						  struct rga_request *request,
+						  dma_fence_func_t cb_func)
+{
+	int ret;
+	struct dma_fence *acquire_fence = NULL;
+	struct rga_pending_request_manager *request_manager = rga_drvdata->pend_request_manager;
+
+	if (DEBUGGER_EN(MSG))
+		rga_req_log(request, "acquire_fence_fd = %d", acquire_fence_fd);
+
+	acquire_fence = rga_get_dma_fence_from_fd(acquire_fence_fd);
+	if (IS_ERR_OR_NULL(acquire_fence)) {
+		rga_req_err(request, "%s: failed to get acquire dma_fence from[%d]\n",
+		       __func__, acquire_fence_fd);
+		return -EINVAL;
+	}
+
+	if (!request->feature.user_close_fence) {
+		/* close acquire fence fd */
+#ifdef CONFIG_NO_GKI
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+		close_fd(acquire_fence_fd);
+#else
+		ksys_close(acquire_fence_fd);
+#endif
+#else
+		rga_req_err(request, "Please update the driver to v1.2.28 to prevent acquire_fence_fd leaks.");
+		return -EFAULT;
+#endif
+	}
+
+
+	ret = rga_dma_fence_get_status(acquire_fence);
+	if (ret < 0) {
+		rga_req_err(request, "%s: Current acquire fence unexpectedly has error status before signal\n",
+		       __func__);
+		return ret;
+	} else if (ret > 0) {
+		/* has been signaled */
+		return ret;
+	}
+
+	/*
+	 * Ensure that the request will not be free early when
+	 * the callback is called.
+	 */
+	mutex_lock(&request_manager->lock);
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	ret = rga_dma_fence_add_callback(acquire_fence, cb_func, (void *)request);
+	if (ret < 0) {
+		if (ret != -ENOENT)
+			rga_req_err(request, "%s: failed to add fence callback\n", __func__);
+
+		mutex_lock(&request_manager->lock);
+		rga_request_put(request);
+		mutex_unlock(&request_manager->lock);
+		return ret;
+	}
+
+	return 0;
+}
+
+int rga_request_check(struct rga_user_request *req)
+{
+	if (req->id <= 0) {
+		rga_err("ID[%d]: request_id is invalid", req->id);
+		return -EINVAL;
+	}
+
+	if (req->task_num <= 0) {
+		rga_err("ID[%d]: invalid user request!\n", req->id);
+		return -EINVAL;
+	}
+
+	if (req->task_ptr == 0) {
+		rga_err("ID[%d]: task_ptr is NULL!\n", req->id);
+		return -EINVAL;
+	}
+
+	if (req->task_num > RGA_TASK_NUM_MAX) {
+		rga_err("ID[%d]: Only supports running %d tasks, now %d\n",
+			req->id, RGA_TASK_NUM_MAX, req->task_num);
+		return -EFBIG;
+	}
+
+	return 0;
+}
+
+struct rga_request *rga_request_lookup(struct rga_pending_request_manager *manager, uint32_t id)
+{
+	struct rga_request *request = NULL;
+
+	WARN_ON(!mutex_is_locked(&manager->lock));
+
+	request = idr_find(&manager->request_idr, id);
+
+	return request;
+}
+
+void rga_request_scheduler_shutdown(struct rga_scheduler_t *scheduler)
+{
+	struct rga_job *job, *job_q;
+	unsigned long flags;
+
+	rga_power_enable(scheduler);
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+	if (job) {
+		if (test_bit(RGA_JOB_STATE_INTR_ERR, &job->state) ||
+		    test_bit(RGA_JOB_STATE_FINISH, &job->state)) {
+			spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+			goto finish;
+		}
+
+		scheduler->running_job = NULL;
+		scheduler->status = RGA_SCHEDULER_ABORT;
+		scheduler->ops->soft_reset(scheduler);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		rga_mm_unmap_job_info(job);
+
+		job->ret = -EBUSY;
+		rga_request_release_signal(scheduler, job);
+
+		/*
+		 *  Since the running job was abort, turn off the power here that
+		 * should have been turned off after job done (corresponds to
+		 * power_enable in rga_job_run()).
+		 */
+		rga_power_disable(scheduler);
+	} else {
+		/* Clean up the jobs in the todo list that need to be free. */
+		list_for_each_entry_safe(job, job_q, &scheduler->todo_list, head) {
+			rga_mm_unmap_job_info(job);
+
+			job->ret = -EBUSY;
+			rga_request_release_signal(scheduler, job);
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+
+finish:
+	rga_power_disable(scheduler);
+}
+
+void rga_request_scheduler_abort(struct rga_scheduler_t *scheduler)
+{
+	struct rga_job *job;
+	unsigned long flags;
+
+	rga_power_enable(scheduler);
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+	if (job) {
+		scheduler->running_job = NULL;
+		scheduler->status = RGA_SCHEDULER_ABORT;
+		scheduler->ops->soft_reset(scheduler);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		rga_mm_unmap_job_info(job);
+
+		job->ret = -EBUSY;
+		rga_request_release_signal(scheduler, job);
+
+		rga_job_next(scheduler);
+
+		/*
+		 *  Since the running job was abort, turn off the power here that
+		 * should have been turned off after job done (corresponds to
+		 * power_enable in rga_job_run()).
+		 */
+		rga_power_disable(scheduler);
+	} else {
+		scheduler->status = RGA_SCHEDULER_ABORT;
+		scheduler->ops->soft_reset(scheduler);
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+
+	rga_power_disable(scheduler);
+}
+
+static int rga_request_scheduler_job_abort(struct rga_request *request)
+{
+	int i;
+	unsigned long flags;
+	enum rga_scheduler_status scheduler_status;
+	int running_abort_count = 0, todo_abort_count = 0, all_task_count = 0;
+	struct rga_scheduler_t *scheduler = NULL;
+	struct rga_job *job, *job_q;
+	LIST_HEAD(list_to_free);
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		list_for_each_entry_safe(job, job_q, &scheduler->todo_list, head) {
+			if (request->id == job->request_id) {
+				list_move(&job->head, &list_to_free);
+				scheduler->job_count--;
+
+				todo_abort_count++;
+			}
+		}
+
+		job = NULL;
+		if (scheduler->running_job) {
+			if (request->id == scheduler->running_job->request_id) {
+				job = scheduler->running_job;
+				scheduler_status = scheduler->status;
+				scheduler->running_job = NULL;
+				scheduler->status = RGA_SCHEDULER_ABORT;
+				list_add_tail(&job->head, &list_to_free);
+
+				if (job->timestamp.hw_execute != 0) {
+					scheduler->timer.busy_time +=
+						ktime_us_delta(ktime_get(),
+							       job->timestamp.hw_recode);
+					scheduler->ops->soft_reset(scheduler);
+				}
+				job->session->last_active = ktime_get();
+
+				rga_req_err(request, "reset core[%d] by request abort",
+					scheduler->core);
+				running_abort_count++;
+			}
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		if (job && scheduler_status == RGA_SCHEDULER_WORKING)
+			rga_power_disable(scheduler);
+	}
+
+	/* Clean up the jobs in the todo list that need to be free. */
+	list_for_each_entry_safe(job, job_q, &list_to_free, head) {
+		rga_mm_unmap_job_info(job);
+
+		job->ret = -EBUSY;
+		rga_job_cleanup(job);
+	}
+
+	all_task_count = request->finished_task_count + request->failed_task_count +
+			 running_abort_count + todo_abort_count;
+
+	/* This means it has been cleaned up. */
+	if (running_abort_count + todo_abort_count == 0 &&
+	    all_task_count == request->task_count)
+		return 1;
+
+	rga_err("request[%d] abort! finished %d failed %d running_abort %d todo_abort %d\n",
+		request->id, request->finished_task_count, request->failed_task_count,
+		running_abort_count, todo_abort_count);
+
+	return 0;
+}
+
+static void rga_request_release_abort(struct rga_request *request, int err_code)
+{
+	unsigned long flags;
+	struct mm_struct *current_mm;
+	struct rga_pending_request_manager *request_manager = rga_drvdata->pend_request_manager;
+
+	if (rga_request_scheduler_job_abort(request) > 0)
+		return;
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	if (request->is_done) {
+		spin_unlock_irqrestore(&request->lock, flags);
+		return;
+	}
+
+	request->is_running = false;
+	request->is_done = false;
+	current_mm = request->current_mm;
+	request->current_mm = NULL;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	rga_request_put_current_mm(current_mm);
+
+	rga_dma_fence_signal(request->release_fence, err_code);
+
+	mutex_lock(&request_manager->lock);
+	/* current submit request put */
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+}
+
+void rga_request_session_destroy_abort(struct rga_session *session)
+{
+	int request_id;
+	struct rga_request *request;
+	struct rga_pending_request_manager *request_manager;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return;
+	}
+
+	mutex_lock(&request_manager->lock);
+
+	idr_for_each_entry(&request_manager->request_idr, request, request_id) {
+		if (session == request->session) {
+			rga_req_err(request, "destroy when the user exits");
+			rga_request_put(request);
+		}
+	}
+
+	mutex_unlock(&request_manager->lock);
+}
+
+static int rga_request_timeout_query_state(struct rga_request *request)
+{
+	int i;
+	unsigned long flags;
+	struct rga_scheduler_t *scheduler = NULL;
+	struct rga_job *job = NULL;
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		if (scheduler->running_job) {
+			job = scheduler->running_job;
+
+			if (request->id == job->request_id) {
+				request->ret = rga_job_timeout_query_state(job, request->ret);
+
+				spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+				break;
+			}
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+
+	return request->ret;
+}
+
+static int rga_request_wait(struct rga_request *request)
+{
+	int left_time;
+	int ret;
+
+	left_time = wait_event_timeout(request->finished_wq, request->is_done,
+				       RGA_JOB_TIMEOUT_DELAY * request->task_count);
+
+	switch (left_time) {
+	case 0:
+		ret = rga_request_timeout_query_state(request);
+		break;
+	case -ERESTARTSYS:
+		ret = -ERESTARTSYS;
+		break;
+	default:
+		ret = request->ret;
+		break;
+	}
+
+	return ret;
+}
+
+int rga_request_commit(struct rga_request *request)
+{
+	int ret;
+	int i = 0;
+
+	if (DEBUGGER_EN(MSG))
+		rga_req_log(request, "commit process: %s\n", request->session->pname);
+
+	for (i = 0; i < request->task_count; i++) {
+		struct rga_req *req = &(request->task_list[i]);
+
+		if (DEBUGGER_EN(MSG)) {
+			rga_req_log(request, "commit task[%d]:\n", i);
+			rga_dump_req(request, req);
+		}
+
+		ret = rga_job_commit(req, request);
+		if (ret < 0) {
+			rga_req_err(request, "task[%d] job_commit failed.\n", i);
+
+			return ret;
+		}
+	}
+
+	if (request->sync_mode == RGA_BLIT_SYNC) {
+		ret = rga_request_wait(request);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static void rga_request_acquire_fence_work(struct work_struct *work)
+{
+	int ret;
+	unsigned long flags;
+	struct mm_struct *current_mm;
+	struct rga_request *request = container_of(work, struct rga_request, fence_work);
+	struct rga_pending_request_manager *request_manager = rga_drvdata->pend_request_manager;
+
+	ret = rga_request_commit(request);
+	if (ret < 0) {
+		rga_req_err(request, "acquire_fence callback: request commit failed!\n");
+
+		spin_lock_irqsave(&request->lock, flags);
+
+		request->is_running = false;
+		current_mm = request->current_mm;
+		request->current_mm = NULL;
+
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		rga_request_put_current_mm(current_mm);
+
+		if (rga_dma_fence_get_status(request->release_fence) == 0)
+			rga_dma_fence_signal(request->release_fence, ret);
+	}
+
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+}
+
+static void rga_request_acquire_fence_signaled_cb(struct dma_fence *fence,
+						  struct dma_fence_cb *_waiter)
+{
+	struct rga_fence_waiter *waiter = (struct rga_fence_waiter *)_waiter;
+	struct rga_request *request = (struct rga_request *)waiter->private;
+
+	queue_work(system_highpri_wq, &request->fence_work);
+	kfree(waiter);
+}
+
+int rga_request_release_signal(struct rga_scheduler_t *scheduler, struct rga_job *job)
+{
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+	struct mm_struct *current_mm;
+	int finished_count, failed_count;
+	bool is_finished = false;
+	unsigned long flags;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_job_err(job, "rga_pending_request_manager is null!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, job->request_id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_job_err(job, "can not find internal request from id[%d]", job->request_id);
+		mutex_unlock(&request_manager->lock);
+		return -EINVAL;
+	}
+
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	if (job->ret < 0) {
+		request->failed_task_count++;
+		request->ret = job->ret;
+	} else {
+		request->finished_task_count++;
+	}
+
+	failed_count = request->failed_task_count;
+	finished_count = request->finished_task_count;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	if ((failed_count + finished_count) >= request->task_count) {
+		spin_lock_irqsave(&request->lock, flags);
+
+		request->is_running = false;
+		request->is_done = true;
+		current_mm = request->current_mm;
+		request->current_mm = NULL;
+
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		rga_request_put_current_mm(current_mm);
+
+		rga_dma_fence_signal(request->release_fence, request->ret);
+
+		is_finished = true;
+		job->timestamp.done = ktime_get();
+
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "finished %d failed %d\n", finished_count, failed_count);
+
+		/* current submit request put */
+		mutex_lock(&request_manager->lock);
+		rga_request_put(request);
+		mutex_unlock(&request_manager->lock);
+	}
+
+	mutex_lock(&request_manager->lock);
+
+	if (is_finished)
+		wake_up(&request->finished_wq);
+
+	rga_request_put(request);
+
+	mutex_unlock(&request_manager->lock);
+
+	if (DEBUGGER_EN(TIME)) {
+		rga_job_log(job,
+			"stats: prepare %lld us, schedule %lld us, hardware %lld us, free %lld us\n",
+			ktime_us_delta(job->timestamp.insert, job->timestamp.init),
+			ktime_us_delta(job->timestamp.hw_execute, job->timestamp.insert),
+			ktime_us_delta(job->timestamp.hw_done, job->timestamp.hw_execute),
+			ktime_us_delta(ktime_get(), job->timestamp.hw_done));
+		rga_job_log(job, "total: job done cost %lld us, cleanup done cost %lld us\n",
+			ktime_us_delta(job->timestamp.done, job->timestamp.init),
+			ktime_us_delta(ktime_get(), job->timestamp.init));
+	}
+
+	rga_job_cleanup(job);
+
+	return 0;
+}
+
+struct rga_request *rga_request_config(struct rga_user_request *user_request)
+{
+	int ret;
+	unsigned long flags;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+	struct rga_req *task_list;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return ERR_PTR(-EFAULT);
+	}
+
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, user_request->id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", user_request->id);
+		mutex_unlock(&request_manager->lock);
+		return ERR_PTR(-EINVAL);
+	}
+
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	task_list = kmalloc_array(user_request->task_num, sizeof(struct rga_req), GFP_KERNEL);
+	if (task_list == NULL) {
+		rga_req_err(request, "task_req list alloc error!\n");
+		ret = -ENOMEM;
+		goto err_put_request;
+	}
+
+	if (unlikely(copy_from_user(task_list, u64_to_user_ptr(user_request->task_ptr),
+				    sizeof(struct rga_req) * user_request->task_num))) {
+		rga_req_err(request, "rga_user_request task list copy_from_user failed\n");
+		ret = -EFAULT;
+		goto err_free_task_list;
+	}
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	request->use_batch_mode = true;
+	request->task_list = task_list;
+	request->task_count = user_request->task_num;
+	request->sync_mode = user_request->sync_mode;
+	request->mpi_config_flags = user_request->mpi_config_flags;
+	request->acquire_fence_fd = user_request->acquire_fence_fd;
+	request->feature = task_list[0].feature;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	return request;
+
+err_free_task_list:
+	kfree(task_list);
+err_put_request:
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	return ERR_PTR(ret);
+}
+
+struct rga_request *rga_request_kernel_config(struct rga_user_request *user_request)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+	struct rga_req *task_list;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return ERR_PTR(-EFAULT);
+	}
+
+	mutex_lock(&request_manager->lock);
+
+	request = rga_request_lookup(request_manager, user_request->id);
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("can not find request from id[%d]", user_request->id);
+		mutex_unlock(&request_manager->lock);
+		return ERR_PTR(-EINVAL);
+	}
+
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	task_list = kmalloc_array(user_request->task_num, sizeof(struct rga_req), GFP_KERNEL);
+	if (task_list == NULL) {
+		rga_req_err(request, "task_req list alloc error!\n");
+		ret = -ENOMEM;
+		goto err_put_request;
+	}
+
+	memcpy(task_list, u64_to_user_ptr(user_request->task_ptr),
+	       sizeof(struct rga_req) * user_request->task_num);
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	request->use_batch_mode = true;
+	request->task_list = task_list;
+	request->task_count = user_request->task_num;
+	request->sync_mode = user_request->sync_mode;
+	request->mpi_config_flags = user_request->mpi_config_flags;
+	request->acquire_fence_fd = user_request->acquire_fence_fd;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	return request;
+
+err_put_request:
+	mutex_lock(&request_manager->lock);
+	rga_request_put(request);
+	mutex_unlock(&request_manager->lock);
+
+	return ERR_PTR(ret);
+}
+
+int rga_request_submit(struct rga_request *request)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct dma_fence *release_fence;
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	if (request->is_running) {
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		rga_req_err(request, "can not re-config when request is running\n");
+		ret = -EFAULT;
+		goto err_abort_request;
+	}
+
+	if (request->task_list == NULL) {
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		rga_req_err(request, "can not find task list\n");
+		ret = -EINVAL;
+		goto err_abort_request;
+	}
+
+	/* Reset */
+	request->is_running = true;
+	request->is_done = false;
+	request->finished_task_count = 0;
+	request->failed_task_count = 0;
+	request->ret = 0;
+
+	/* Unlock after ensuring that the current request will not be resubmitted. */
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	if (request->sync_mode == RGA_BLIT_ASYNC) {
+		release_fence = rga_dma_fence_alloc();
+		if (IS_ERR_OR_NULL(release_fence)) {
+			rga_req_err(request, "Can not alloc release fence!\n");
+			ret = IS_ERR(release_fence) ? PTR_ERR(release_fence) : -EINVAL;
+			goto err_abort_request;
+		}
+
+		request->current_mm = rga_request_get_current_mm(request);
+		request->release_fence = release_fence;
+
+		if (request->acquire_fence_fd > 0) {
+			INIT_WORK(&request->fence_work, rga_request_acquire_fence_work);
+			ret = rga_request_add_acquire_fence_callback(
+				request->acquire_fence_fd, request,
+				rga_request_acquire_fence_signaled_cb);
+			if (ret == 0) {
+				/* acquire fence active */
+				goto export_release_fence_fd;
+			} else if (ret > 0) {
+				/* acquire fence has been signaled */
+				goto request_commit;
+			} else {
+				rga_req_err(request, "Failed to add callback with acquire fence fd[%d]!\n",
+				       request->acquire_fence_fd);
+
+				rga_dma_fence_put(request->release_fence);
+				request->release_fence = NULL;
+				goto err_put_current_mm;
+			}
+		}
+	} else {
+		request->current_mm = rga_request_get_current_mm(request);
+		request->release_fence = NULL;
+	}
+
+request_commit:
+	ret = rga_request_commit(request);
+	if (ret < 0) {
+		rga_req_err(request, "request commit failed!\n");
+		goto err_put_current_mm;
+	}
+
+export_release_fence_fd:
+	if (request->release_fence != NULL) {
+		ret = rga_dma_fence_get_fd(request->release_fence);
+		if (ret < 0) {
+			rga_req_err(request, "Failed to alloc release fence fd!\n");
+			goto err_put_current_mm;
+		}
+
+		request->release_fence_fd = ret;
+	}
+
+	return 0;
+
+err_put_current_mm:
+	rga_request_put_current_mm(request->current_mm);
+	request->current_mm = NULL;
+
+err_abort_request:
+	rga_request_release_abort(request, ret);
+
+	return ret;
+}
+
+int rga_request_mpi_submit(struct rga_req *req, struct rga_request *request)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct rga_pending_request_manager *request_manager;
+
+	request_manager = rga_drvdata->pend_request_manager;
+
+	if (request->sync_mode == RGA_BLIT_ASYNC) {
+		rga_req_err(request, "mpi unsupported async mode!\n");
+		ret = -EINVAL;
+		goto err_abort_request;
+	}
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	if (request->is_running) {
+		rga_req_err(request, "can not re-config when request is running");
+		spin_unlock_irqrestore(&request->lock, flags);
+		ret = -EFAULT;
+		goto err_abort_request;
+	}
+
+	if (request->task_list == NULL) {
+		rga_req_err(request, "can not find task list");
+		spin_unlock_irqrestore(&request->lock, flags);
+		ret = -EINVAL;
+		goto err_abort_request;
+	}
+
+	/* Reset */
+	request->is_running = true;
+	request->is_done = false;
+	request->finished_task_count = 0;
+	request->failed_task_count = 0;
+	request->ret = 0;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	/*
+	 * The mpi submit will use the request repeatedly, so an additional
+	 * get() is added here.
+	 */
+	mutex_lock(&request_manager->lock);
+	rga_request_get(request);
+	mutex_unlock(&request_manager->lock);
+
+	ret = rga_job_commit(req, request);
+	if (ret < 0) {
+		rga_req_err(request, "failed to commit job!\n");
+		goto err_abort_request;
+	}
+
+	ret = rga_request_wait(request);
+	if (ret < 0)
+		goto err_abort_request;
+
+	return 0;
+
+err_abort_request:
+	rga_request_release_abort(request, ret);
+
+	return ret;
+}
+
+int rga_request_free(struct rga_request *request)
+{
+	struct rga_pending_request_manager *request_manager;
+	struct rga_req *task_list;
+	unsigned long flags;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return -EFAULT;
+	}
+
+	WARN_ON(!mutex_is_locked(&request_manager->lock));
+
+	if (IS_ERR_OR_NULL(request)) {
+		rga_err("request already freed");
+		return -EFAULT;
+	}
+
+	request_manager->request_count--;
+	idr_remove(&request_manager->request_idr, request->id);
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	task_list = request->task_list;
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	if (task_list != NULL)
+		kfree(task_list);
+
+	kfree(request);
+
+	return 0;
+}
+
+static void rga_request_kref_release(struct kref *ref)
+{
+	struct rga_request *request;
+	struct mm_struct *current_mm;
+	unsigned long flags;
+
+	request = container_of(ref, struct rga_request, refcount);
+
+	if (rga_dma_fence_get_status(request->release_fence) == 0)
+		rga_dma_fence_signal(request->release_fence, -EFAULT);
+
+	spin_lock_irqsave(&request->lock, flags);
+
+	rga_dma_fence_put(request->release_fence);
+	current_mm = request->current_mm;
+	request->current_mm = NULL;
+
+	if (!request->is_running || request->is_done) {
+		spin_unlock_irqrestore(&request->lock, flags);
+
+		rga_request_put_current_mm(current_mm);
+
+		goto free_request;
+	}
+
+	spin_unlock_irqrestore(&request->lock, flags);
+
+	rga_request_put_current_mm(current_mm);
+
+	rga_request_scheduler_job_abort(request);
+
+free_request:
+	rga_request_free(request);
+}
+
+/*
+ * Called at driver close to release the request's id references.
+ */
+static int rga_request_free_cb(int id, void *ptr, void *data)
+{
+	return rga_request_free((struct rga_request *)ptr);
+}
+
+int rga_request_alloc(uint32_t flags, struct rga_session *session)
+{
+	int new_id;
+	struct rga_pending_request_manager *request_manager;
+	struct rga_request *request;
+
+	request_manager = rga_drvdata->pend_request_manager;
+	if (request_manager == NULL) {
+		rga_err("rga_pending_request_manager is null!\n");
+		return -EFAULT;
+	}
+
+	request = kzalloc(sizeof(*request), GFP_KERNEL);
+	if (request == NULL) {
+		rga_err("can not kzalloc for rga_request\n");
+		return -ENOMEM;
+	}
+
+	spin_lock_init(&request->lock);
+	init_waitqueue_head(&request->finished_wq);
+
+	request->pid = current->pid;
+	request->flags = flags;
+	request->session = session;
+	kref_init(&request->refcount);
+
+	/*
+	 * Get the user-visible handle using idr. Preload and perform
+	 * allocation under our spinlock.
+	 */
+	mutex_lock(&request_manager->lock);
+
+	idr_preload(GFP_KERNEL);
+	new_id = idr_alloc_cyclic(&request_manager->request_idr, request, 1, 0, GFP_NOWAIT);
+	idr_preload_end();
+	if (new_id < 0) {
+		rga_err("request alloc id failed!\n");
+
+		mutex_unlock(&request_manager->lock);
+		kfree(request);
+		return new_id;
+	}
+
+	request->id = new_id;
+	request_manager->request_count++;
+
+	mutex_unlock(&request_manager->lock);
+
+	return request->id;
+}
+
+int rga_request_put(struct rga_request *request)
+{
+	return kref_put(&request->refcount, rga_request_kref_release);
+}
+
+void rga_request_get(struct rga_request *request)
+{
+	kref_get(&request->refcount);
+}
+
+int rga_request_manager_init(struct rga_pending_request_manager **request_manager_session)
+{
+	struct rga_pending_request_manager *request_manager = NULL;
+
+	*request_manager_session = kzalloc(sizeof(struct rga_pending_request_manager), GFP_KERNEL);
+	if (*request_manager_session == NULL) {
+		pr_err("can not kzalloc for rga_pending_request_manager\n");
+		return -ENOMEM;
+	}
+
+	request_manager = *request_manager_session;
+
+	mutex_init(&request_manager->lock);
+
+	idr_init_base(&request_manager->request_idr, 1);
+
+	return 0;
+}
+
+int rga_request_manager_remove(struct rga_pending_request_manager **request_manager_session)
+{
+	struct rga_pending_request_manager *request_manager = *request_manager_session;
+
+	mutex_lock(&request_manager->lock);
+
+	idr_for_each(&request_manager->request_idr, &rga_request_free_cb, request_manager);
+	idr_destroy(&request_manager->request_idr);
+
+	mutex_unlock(&request_manager->lock);
+
+	kfree(*request_manager_session);
+
+	*request_manager_session = NULL;
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/rga3/rga_mm.c b/drivers/video/rockchip/rga3/rga_mm.c
new file mode 100644
index 0000000000000..c5de4127dfb08
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_mm.c
@@ -0,0 +1,2489 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Cerf Yu <cerf.yu@rock-chips.com>
+ */
+
+#include "rga.h"
+#include "rga_job.h"
+#include "rga_mm.h"
+#include "rga_dma_buf.h"
+#include "rga_common.h"
+#include "rga_iommu.h"
+#include "rga_hw_config.h"
+#include "rga_debugger.h"
+
+static void rga_current_mm_read_lock(struct mm_struct *mm)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_lock(mm);
+#else
+	down_read(&mm->mmap_sem);
+#endif
+}
+
+static void rga_current_mm_read_unlock(struct mm_struct *mm)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	mmap_read_unlock(mm);
+#else
+	up_read(&mm->mmap_sem);
+#endif
+}
+
+static int rga_get_user_pages_from_vma(struct page **pages, unsigned long Memory,
+				       uint32_t pageCount, struct mm_struct *current_mm)
+{
+	int ret = 0;
+	int i;
+	struct vm_area_struct *vma;
+	spinlock_t *ptl;
+	pte_t *pte;
+	pgd_t *pgd;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+	p4d_t *p4d;
+#endif
+	pud_t *pud;
+	pmd_t *pmd;
+	unsigned long pfn;
+
+	for (i = 0; i < pageCount; i++) {
+		vma = find_vma(current_mm, (Memory + i) << PAGE_SHIFT);
+		if (!vma) {
+			rga_err("page[%d] failed to get vma\n", i);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+
+		pgd = pgd_offset(current_mm, (Memory + i) << PAGE_SHIFT);
+		if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd))) {
+			rga_err("page[%d] failed to get pgd\n", i);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+		/*
+		 * In the four-level page table,
+		 * it will do nothing and return pgd.
+		 */
+		p4d = p4d_offset(pgd, (Memory + i) << PAGE_SHIFT);
+		if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d))) {
+			rga_err("page[%d] failed to get p4d\n", i);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+
+		pud = pud_offset(p4d, (Memory + i) << PAGE_SHIFT);
+#else
+		pud = pud_offset(pgd, (Memory + i) << PAGE_SHIFT);
+#endif
+
+		if (pud_none(*pud) || unlikely(pud_bad(*pud))) {
+			rga_err("page[%d] failed to get pud\n", i);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+		pmd = pmd_offset(pud, (Memory + i) << PAGE_SHIFT);
+		if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd))) {
+			rga_err("page[%d] failed to get pmd\n", i);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+		pte = pte_offset_map_lock(current_mm, pmd,
+					  (Memory + i) << PAGE_SHIFT, &ptl);
+		if (pte_none(*pte)) {
+			rga_err("page[%d] failed to get pte\n", i);
+			pte_unmap_unlock(pte, ptl);
+			ret = RGA_OUT_OF_RESOURCES;
+			break;
+		}
+
+		pfn = pte_pfn(*pte);
+		pages[i] = pfn_to_page(pfn);
+		pte_unmap_unlock(pte, ptl);
+	}
+
+	if (ret == RGA_OUT_OF_RESOURCES && i > 0)
+		rga_err("Only get buffer %d byte from vma, but current image required %d byte",
+			(int)(i * PAGE_SIZE), (int)(pageCount * PAGE_SIZE));
+
+	return ret;
+}
+
+static int rga_get_user_pages(struct page **pages, unsigned long Memory,
+			      uint32_t pageCount, int writeFlag,
+			      struct mm_struct *current_mm)
+{
+	uint32_t i;
+	int32_t ret = 0;
+	int32_t result;
+
+	rga_current_mm_read_lock(current_mm);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 168) && \
+    LINUX_VERSION_CODE < KERNEL_VERSION(4, 5, 0)
+	result = get_user_pages(current, current_mm, Memory << PAGE_SHIFT,
+				pageCount, writeFlag ? FOLL_WRITE : 0,
+				pages, NULL);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
+	result = get_user_pages(current, current_mm, Memory << PAGE_SHIFT,
+				pageCount, writeFlag ? FOLL_WRITE : 0, 0, pages, NULL);
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(5, 10, 0)
+	result = get_user_pages_remote(current, current_mm,
+				       Memory << PAGE_SHIFT,
+				       pageCount, writeFlag ? FOLL_WRITE : 0, pages, NULL, NULL);
+#else
+	result = get_user_pages_remote(current_mm, Memory << PAGE_SHIFT,
+				       pageCount, writeFlag ? FOLL_WRITE : 0, pages, NULL, NULL);
+#endif
+
+	if (result > 0 && result >= pageCount) {
+		ret = result;
+	} else {
+		if (result > 0)
+			for (i = 0; i < result; i++)
+				put_page(pages[i]);
+
+		ret = rga_get_user_pages_from_vma(pages, Memory, pageCount, current_mm);
+		if (ret < 0 && result > 0) {
+			rga_err("Only get buffer %d byte from user pages, but current image required %d byte\n",
+				(int)(result * PAGE_SIZE), (int)(pageCount * PAGE_SIZE));
+		}
+	}
+
+	rga_current_mm_read_unlock(current_mm);
+
+	return ret;
+}
+
+static void rga_free_sgt(struct sg_table **sgt_ptr)
+{
+	if (sgt_ptr == NULL || *sgt_ptr == NULL)
+		return;
+
+	sg_free_table(*sgt_ptr);
+	kfree(*sgt_ptr);
+	*sgt_ptr = NULL;
+}
+
+static struct sg_table *rga_alloc_sgt(struct rga_virt_addr *virt_addr)
+{
+	int ret;
+	struct sg_table *sgt = NULL;
+
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (sgt == NULL) {
+		rga_err("%s alloc sgt error!\n", __func__);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* get sg form pages. */
+	/* iova requires minimum page alignment, so sgt cannot have offset */
+	ret = sg_alloc_table_from_pages(sgt,
+					virt_addr->pages,
+					virt_addr->page_count,
+					0,
+					virt_addr->size,
+					GFP_KERNEL);
+	if (ret) {
+		rga_err("sg_alloc_table_from_pages failed");
+		goto out_free_sgt;
+	}
+
+	return sgt;
+
+out_free_sgt:
+	kfree(sgt);
+
+	return ERR_PTR(ret);
+}
+
+static void rga_free_virt_addr(struct rga_virt_addr **virt_addr_p)
+{
+	int i;
+	struct rga_virt_addr *virt_addr = NULL;
+
+	if (virt_addr_p == NULL)
+		return;
+
+	virt_addr = *virt_addr_p;
+	if (virt_addr == NULL)
+		return;
+
+	for (i = 0; i < virt_addr->result; i++)
+		put_page(virt_addr->pages[i]);
+
+	free_pages((unsigned long)virt_addr->pages, virt_addr->pages_order);
+	kfree(virt_addr);
+	*virt_addr_p = NULL;
+}
+
+static int rga_alloc_virt_addr(struct rga_virt_addr **virt_addr_p,
+			       uint64_t viraddr,
+			       struct rga_memory_parm *memory_parm,
+			       int writeFlag,
+			       struct mm_struct *mm)
+{
+	int i;
+	int ret;
+	int result = 0;
+	int order;
+	unsigned int count;
+	int img_size;
+	size_t offset;
+	unsigned long size;
+	struct page **pages = NULL;
+	struct rga_virt_addr *virt_addr = NULL;
+
+	if (memory_parm->size)
+		img_size = memory_parm->size;
+	else
+		img_size = rga_image_size_cal(memory_parm->width,
+					      memory_parm->height,
+					      memory_parm->format,
+					      NULL, NULL, NULL);
+
+	offset = viraddr & (~PAGE_MASK);
+	count = RGA_GET_PAGE_COUNT(img_size + offset);
+	size = count * PAGE_SIZE;
+	if (!size) {
+		rga_err("failed to calculating buffer size! size = %ld, count = %d, offset = %ld\n",
+			size, count, (unsigned long)offset);
+		rga_dump_memory_parm(memory_parm);
+		return -EFAULT;
+	}
+
+	/* alloc pages and page_table */
+	order = get_order(count * sizeof(struct page *));
+	if (order >= MAX_ORDER) {
+		rga_err("Can not alloc pages with order[%d] for viraddr pages, max_order = %d\n",
+			order, MAX_ORDER);
+		return -ENOMEM;
+	}
+
+	pages = (struct page **)__get_free_pages(GFP_KERNEL, order);
+	if (pages == NULL) {
+		rga_err("%s can not alloc pages for viraddr pages\n", __func__);
+		return -ENOMEM;
+	}
+
+	/* get pages from virtual address. */
+	ret = rga_get_user_pages(pages, viraddr >> PAGE_SHIFT, count, writeFlag, mm);
+	if (ret < 0) {
+		rga_err("failed to get pages from virtual adrees: 0x%lx\n",
+		       (unsigned long)viraddr);
+		ret = -EINVAL;
+		goto out_free_pages;
+	} else if (ret > 0) {
+		/* For put pages */
+		result = ret;
+	}
+
+	*virt_addr_p = kzalloc(sizeof(struct rga_virt_addr), GFP_KERNEL);
+	if (*virt_addr_p == NULL) {
+		rga_err("%s alloc virt_addr error!\n", __func__);
+		ret = -ENOMEM;
+		goto out_put_and_free_pages;
+	}
+	virt_addr = *virt_addr_p;
+
+	virt_addr->addr = viraddr;
+	virt_addr->pages = pages;
+	virt_addr->pages_order = order;
+	virt_addr->page_count = count;
+	virt_addr->size = size;
+	virt_addr->offset = offset;
+	virt_addr->result = result;
+
+	return 0;
+
+out_put_and_free_pages:
+	for (i = 0; i < result; i++)
+		put_page(pages[i]);
+out_free_pages:
+	free_pages((unsigned long)pages, order);
+
+	return ret;
+}
+
+static inline bool rga_mm_check_memory_limit(struct rga_scheduler_t *scheduler, int mm_flag)
+{
+	if (!scheduler)
+		return false;
+
+	if (scheduler->data->mmu == RGA_MMU &&
+	    !(mm_flag & RGA_MEM_UNDER_4G)) {
+		rga_err("%s unsupported memory larger than 4G!\n",
+		       rga_get_mmu_type_str(scheduler->data->mmu));
+		return false;
+	}
+
+	return true;
+}
+
+/* If it is within 0~4G, return 1 (true). */
+static int rga_mm_check_range_sgt(struct sg_table *sgt)
+{
+	int i;
+	struct scatterlist *sg;
+	phys_addr_t s_phys = 0;
+
+	for_each_sg(sgt->sgl, sg, sgt->orig_nents, i) {
+		s_phys = sg_phys(sg);
+		if ((s_phys > 0xffffffff) || (s_phys + sg->length > 0xffffffff))
+			return 0;
+	}
+
+	return 1;
+}
+
+static inline int rga_mm_check_range_phys_addr(phys_addr_t paddr, size_t size)
+{
+	return ((paddr + size) <= 0xffffffff);
+}
+
+static inline bool rga_mm_check_contiguous_sgt(struct sg_table *sgt)
+{
+	if (sgt->orig_nents == 1)
+		return true;
+
+	return false;
+}
+
+static void rga_mm_unmap_dma_buffer(struct rga_internal_buffer *internal_buffer)
+{
+	if (rga_mm_is_invalid_dma_buffer(internal_buffer->dma_buffer))
+		return;
+
+	rga_dma_unmap_buf(internal_buffer->dma_buffer);
+
+	if (internal_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS &&
+	    internal_buffer->phys_addr > 0)
+		internal_buffer->phys_addr = 0;
+
+	kfree(internal_buffer->dma_buffer);
+	internal_buffer->dma_buffer = NULL;
+}
+
+static int rga_mm_map_dma_buffer(struct rga_external_buffer *external_buffer,
+				 struct rga_internal_buffer *internal_buffer,
+				 struct rga_job *job)
+{
+	int ret;
+	int ex_buffer_size;
+	uint32_t mm_flag = 0;
+	phys_addr_t phys_addr = 0;
+	struct rga_dma_buffer *buffer;
+	struct device *map_dev;
+	struct rga_scheduler_t *scheduler;
+
+	scheduler = job ? job->scheduler :
+		    rga_drvdata->scheduler[rga_drvdata->map_scheduler_index];
+	if (scheduler == NULL) {
+		rga_err("Invalid scheduler device!\n");
+		return -EINVAL;
+	}
+
+	if (external_buffer->memory_parm.size)
+		ex_buffer_size = external_buffer->memory_parm.size;
+	else
+		ex_buffer_size = rga_image_size_cal(external_buffer->memory_parm.width,
+						    external_buffer->memory_parm.height,
+						    external_buffer->memory_parm.format,
+						    NULL, NULL, NULL);
+	if (ex_buffer_size <= 0) {
+		rga_err("failed to calculating buffer size!\n");
+		rga_dump_memory_parm(&external_buffer->memory_parm);
+		return ex_buffer_size == 0 ? -EINVAL : ex_buffer_size;
+	}
+
+	/*
+	 * dma-buf api needs to use default_domain of main dev,
+	 * and not IOMMU for devices without iommu_info ptr.
+	 */
+	map_dev = scheduler->iommu_info ? scheduler->iommu_info->default_dev : scheduler->dev;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (buffer == NULL) {
+		rga_err("%s alloc internal_buffer error!\n", __func__);
+		return  -ENOMEM;
+	}
+
+	switch (external_buffer->type) {
+	case RGA_DMA_BUFFER:
+		ret = rga_dma_map_fd((int)external_buffer->memory,
+				     buffer, DMA_BIDIRECTIONAL,
+				     map_dev);
+		break;
+	case RGA_DMA_BUFFER_PTR:
+		ret = rga_dma_map_buf((struct dma_buf *)u64_to_user_ptr(external_buffer->memory),
+				      buffer, DMA_BIDIRECTIONAL,
+				      map_dev);
+		break;
+	default:
+		ret = -EFAULT;
+		break;
+	}
+	if (ret < 0) {
+		rga_err("%s core[%d] map dma buffer error!\n",
+			__func__, scheduler->core);
+		goto free_buffer;
+	}
+
+	if (buffer->size < ex_buffer_size) {
+		rga_err("Only get buffer %ld byte from %s = 0x%lx, but current image required %d byte\n",
+			buffer->size, rga_get_memory_type_str(external_buffer->type),
+			(unsigned long)external_buffer->memory, ex_buffer_size);
+		rga_dump_memory_parm(&external_buffer->memory_parm);
+		ret = -EINVAL;
+		goto unmap_buffer;
+	}
+
+	buffer->scheduler = scheduler;
+
+	if (scheduler->data->mmu == RGA_IOMMU)
+		buffer->iova = buffer->dma_addr;
+
+	if (rga_mm_check_range_sgt(buffer->sgt))
+		mm_flag |= RGA_MEM_UNDER_4G;
+
+	/*
+	 * If it's physically contiguous, then the RGA_MMU can
+	 * directly use the physical address.
+	 */
+	if (rga_mm_check_contiguous_sgt(buffer->sgt)) {
+		phys_addr = sg_phys(buffer->sgt->sgl);
+		if (phys_addr == 0) {
+			rga_err("%s get physical address error!", __func__);
+			ret = -EFAULT;
+			goto unmap_buffer;
+		}
+
+		mm_flag |= RGA_MEM_PHYSICAL_CONTIGUOUS;
+	}
+
+	if (!rga_mm_check_memory_limit(scheduler, mm_flag)) {
+		rga_err("scheduler core[%d] unsupported mm_flag[0x%x]!\n",
+			scheduler->core, mm_flag);
+		ret = -EINVAL;
+		goto unmap_buffer;
+	}
+
+	internal_buffer->dma_buffer = buffer;
+	internal_buffer->mm_flag = mm_flag;
+	internal_buffer->phys_addr = phys_addr ? phys_addr : 0;
+
+	return 0;
+
+unmap_buffer:
+	rga_dma_unmap_buf(buffer);
+
+free_buffer:
+	kfree(buffer);
+
+	return ret;
+}
+
+static void rga_mm_unmap_virt_addr(struct rga_internal_buffer *internal_buffer)
+{
+	WARN_ON(internal_buffer->dma_buffer == NULL || internal_buffer->virt_addr == NULL);
+
+	if (rga_mm_is_invalid_dma_buffer(internal_buffer->dma_buffer))
+		return;
+
+	switch (internal_buffer->dma_buffer->scheduler->data->mmu) {
+	case RGA_IOMMU:
+		rga_iommu_unmap(internal_buffer->dma_buffer);
+		break;
+	case RGA_MMU:
+		dma_unmap_sg(internal_buffer->dma_buffer->scheduler->dev,
+			     internal_buffer->dma_buffer->sgt->sgl,
+			     internal_buffer->dma_buffer->sgt->orig_nents,
+			     DMA_BIDIRECTIONAL);
+		break;
+	default:
+		break;
+	}
+
+	if (internal_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS &&
+	    internal_buffer->phys_addr > 0)
+		internal_buffer->phys_addr = 0;
+
+	rga_free_sgt(&internal_buffer->dma_buffer->sgt);
+
+	kfree(internal_buffer->dma_buffer);
+	internal_buffer->dma_buffer = NULL;
+
+	rga_free_virt_addr(&internal_buffer->virt_addr);
+
+	mmput(internal_buffer->current_mm);
+	mmdrop(internal_buffer->current_mm);
+	internal_buffer->current_mm = NULL;
+}
+
+static int rga_mm_map_virt_addr(struct rga_external_buffer *external_buffer,
+				struct rga_internal_buffer *internal_buffer,
+				struct rga_job *job, int write_flag)
+{
+	int ret;
+	uint32_t mm_flag = 0;
+	phys_addr_t phys_addr = 0;
+	struct sg_table *sgt;
+	struct rga_virt_addr *virt_addr;
+	struct rga_dma_buffer *buffer;
+	struct rga_scheduler_t *scheduler;
+
+	scheduler = job ? job->scheduler :
+		    rga_drvdata->scheduler[rga_drvdata->map_scheduler_index];
+	if (scheduler == NULL) {
+		rga_err("Invalid scheduler device!\n");
+		return -EINVAL;
+	}
+
+	internal_buffer->current_mm = job ? job->mm : current->mm;
+	if (internal_buffer->current_mm == NULL) {
+		rga_err("%s, cannot get current mm!\n", __func__);
+		return -EFAULT;
+	}
+	mmgrab(internal_buffer->current_mm);
+	mmget(internal_buffer->current_mm);
+
+	ret = rga_alloc_virt_addr(&virt_addr,
+				  external_buffer->memory,
+				  &internal_buffer->memory_parm,
+				  write_flag, internal_buffer->current_mm);
+	if (ret < 0) {
+		rga_err("Can not alloc rga_virt_addr from 0x%lx\n",
+		       (unsigned long)external_buffer->memory);
+		goto put_current_mm;
+	}
+
+	sgt = rga_alloc_sgt(virt_addr);
+	if (IS_ERR(sgt)) {
+		rga_err("alloc sgt error!\n");
+		ret = PTR_ERR(sgt);
+		goto free_virt_addr;
+	}
+
+	if (rga_mm_check_range_sgt(sgt))
+		mm_flag |= RGA_MEM_UNDER_4G;
+
+	if (rga_mm_check_contiguous_sgt(sgt)) {
+		phys_addr = sg_phys(sgt->sgl);
+		if (phys_addr == 0) {
+			rga_err("%s get physical address error!", __func__);
+			ret = -EFAULT;
+			goto free_sgt;
+		}
+
+		mm_flag |= RGA_MEM_PHYSICAL_CONTIGUOUS;
+	}
+
+	/*
+	 * Some userspace virtual addresses do not have an
+	 * interface for flushing the cache, so it is mandatory
+	 * to flush the cache when the virtual address is used.
+	 */
+	mm_flag |= RGA_MEM_FORCE_FLUSH_CACHE;
+
+	if (!rga_mm_check_memory_limit(scheduler, mm_flag)) {
+		rga_err("scheduler core[%d] unsupported mm_flag[0x%x]!\n",
+			scheduler->core, mm_flag);
+		ret = -EINVAL;
+		goto free_sgt;
+	}
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (buffer == NULL) {
+		rga_err("%s alloc internal dma_buffer error!\n", __func__);
+		ret =  -ENOMEM;
+		goto free_sgt;
+	}
+
+	switch (scheduler->data->mmu) {
+	case RGA_IOMMU:
+		ret = rga_iommu_map_sgt(sgt, virt_addr->size, buffer, scheduler->dev);
+		if (ret < 0) {
+			rga_err("%s core[%d] iommu_map virtual address error!\n",
+				__func__, scheduler->core);
+			goto free_dma_buffer;
+		}
+
+		buffer->dma_addr = buffer->iova;
+
+		break;
+	case RGA_MMU:
+		ret = dma_map_sg(scheduler->dev, sgt->sgl, sgt->orig_nents, DMA_BIDIRECTIONAL);
+		if (ret == 0) {
+			rga_err("%s core[%d] dma_map_sgt error! va = 0x%lx, nents = %d\n",
+				__func__, scheduler->core,
+				(unsigned long)virt_addr->addr, sgt->orig_nents);
+			ret = -EINVAL;
+			goto free_dma_buffer;
+		}
+		break;
+	default:
+		if (mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+			break;
+
+		rga_err("Current %s[%d] cannot support physically discontinuous virtual address!\n",
+			rga_get_mmu_type_str(scheduler->data->mmu), scheduler->data->mmu);
+		ret = -EOPNOTSUPP;
+		goto free_dma_buffer;
+	}
+
+	buffer->sgt = sgt;
+	buffer->offset = virt_addr->offset;
+	buffer->size = virt_addr->size;
+	buffer->scheduler = scheduler;
+
+	internal_buffer->virt_addr = virt_addr;
+	internal_buffer->dma_buffer = buffer;
+	internal_buffer->mm_flag = mm_flag;
+	internal_buffer->phys_addr = phys_addr ? phys_addr + virt_addr->offset : 0;
+
+	return 0;
+
+free_dma_buffer:
+	kfree(buffer);
+free_sgt:
+	rga_free_sgt(&sgt);
+free_virt_addr:
+	rga_free_virt_addr(&virt_addr);
+put_current_mm:
+	mmput(internal_buffer->current_mm);
+	mmdrop(internal_buffer->current_mm);
+	internal_buffer->current_mm = NULL;
+
+	return ret;
+}
+
+static void rga_mm_unmap_phys_addr(struct rga_internal_buffer *internal_buffer)
+{
+	WARN_ON(internal_buffer->dma_buffer == NULL);
+
+	if (rga_mm_is_invalid_dma_buffer(internal_buffer->dma_buffer))
+		return;
+
+	if (internal_buffer->dma_buffer->scheduler->data->mmu == RGA_IOMMU)
+		rga_iommu_unmap(internal_buffer->dma_buffer);
+
+	kfree(internal_buffer->dma_buffer);
+	internal_buffer->dma_buffer = NULL;
+	internal_buffer->phys_addr = 0;
+	internal_buffer->size = 0;
+}
+
+static int rga_mm_map_phys_addr(struct rga_external_buffer *external_buffer,
+				struct rga_internal_buffer *internal_buffer,
+				struct rga_job *job)
+{
+	int ret;
+	phys_addr_t phys_addr;
+	int buffer_size;
+	uint32_t mm_flag = 0;
+	struct rga_dma_buffer *buffer;
+	struct rga_scheduler_t *scheduler;
+
+	scheduler = job ? job->scheduler :
+		    rga_drvdata->scheduler[rga_drvdata->map_scheduler_index];
+	if (scheduler == NULL) {
+		rga_err("Invalid scheduler device!\n");
+		return -EINVAL;
+	}
+
+	if (internal_buffer->memory_parm.size)
+		buffer_size = internal_buffer->memory_parm.size;
+	else
+		buffer_size = rga_image_size_cal(internal_buffer->memory_parm.width,
+						 internal_buffer->memory_parm.height,
+						 internal_buffer->memory_parm.format,
+						 NULL, NULL, NULL);
+	if (buffer_size <= 0) {
+		rga_err("Failed to get phys addr size!\n");
+		rga_dump_memory_parm(&internal_buffer->memory_parm);
+		return buffer_size == 0 ? -EINVAL : buffer_size;
+	}
+
+	phys_addr = external_buffer->memory;
+	mm_flag |= RGA_MEM_PHYSICAL_CONTIGUOUS;
+	if (rga_mm_check_range_phys_addr(phys_addr, buffer_size))
+		mm_flag |= RGA_MEM_UNDER_4G;
+
+	if (!rga_mm_check_memory_limit(scheduler, mm_flag)) {
+		rga_err("scheduler core[%d] unsupported mm_flag[0x%x]!\n",
+			scheduler->core, mm_flag);
+		return -EINVAL;
+	}
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (buffer == NULL) {
+		rga_err("%s alloc internal dma buffer error!\n", __func__);
+		return  -ENOMEM;
+	}
+
+	if (scheduler->data->mmu == RGA_IOMMU) {
+		ret = rga_iommu_map(phys_addr, buffer_size, buffer, scheduler->dev);
+		if (ret < 0) {
+			rga_err("%s core[%d] map phys_addr error!\n", __func__, scheduler->core);
+			goto free_dma_buffer;
+		}
+	}
+
+	buffer->scheduler = scheduler;
+
+	internal_buffer->phys_addr = phys_addr;
+	internal_buffer->size = buffer_size;
+	internal_buffer->mm_flag = mm_flag;
+	internal_buffer->dma_buffer = buffer;
+
+	return 0;
+
+free_dma_buffer:
+	kfree(buffer);
+
+	return ret;
+}
+
+static int rga_mm_unmap_buffer(struct rga_internal_buffer *internal_buffer)
+{
+	switch (internal_buffer->type) {
+	case RGA_DMA_BUFFER:
+	case RGA_DMA_BUFFER_PTR:
+		rga_mm_unmap_dma_buffer(internal_buffer);
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		rga_mm_unmap_virt_addr(internal_buffer);
+		break;
+	case RGA_PHYSICAL_ADDRESS:
+		rga_mm_unmap_phys_addr(internal_buffer);
+		break;
+	default:
+		rga_err("Illegal external buffer!\n");
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int rga_mm_map_buffer(struct rga_external_buffer *external_buffer,
+			     struct rga_internal_buffer *internal_buffer,
+			     struct rga_job *job, int write_flag)
+{
+	int ret;
+
+	memcpy(&internal_buffer->memory_parm, &external_buffer->memory_parm,
+	       sizeof(internal_buffer->memory_parm));
+
+	switch (external_buffer->type) {
+	case RGA_DMA_BUFFER:
+	case RGA_DMA_BUFFER_PTR:
+		internal_buffer->type = external_buffer->type;
+
+		ret = rga_mm_map_dma_buffer(external_buffer, internal_buffer, job);
+		if (ret < 0)
+			return ret;
+
+		internal_buffer->size = internal_buffer->dma_buffer->size -
+					internal_buffer->dma_buffer->offset;
+		internal_buffer->mm_flag |= RGA_MEM_NEED_USE_IOMMU;
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		internal_buffer->type = RGA_VIRTUAL_ADDRESS;
+
+		ret = rga_mm_map_virt_addr(external_buffer, internal_buffer, job, write_flag);
+		if (ret < 0)
+			return ret;
+
+		internal_buffer->size = internal_buffer->virt_addr->size -
+					internal_buffer->virt_addr->offset;
+		internal_buffer->mm_flag |= RGA_MEM_NEED_USE_IOMMU;
+		break;
+	case RGA_PHYSICAL_ADDRESS:
+		internal_buffer->type = RGA_PHYSICAL_ADDRESS;
+
+		ret = rga_mm_map_phys_addr(external_buffer, internal_buffer, job);
+		if (ret < 0)
+			return ret;
+
+		internal_buffer->mm_flag |= RGA_MEM_NEED_USE_IOMMU;
+		break;
+	default:
+		if (job)
+			rga_job_err(job, "Illegal external buffer!\n");
+		else
+			rga_err("Illegal external buffer!\n");
+
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static void rga_mm_kref_release_buffer(struct kref *ref)
+{
+	struct rga_internal_buffer *internal_buffer;
+	struct rga_mm *mm = rga_drvdata->mm;
+
+	internal_buffer = container_of(ref, struct rga_internal_buffer, refcount);
+	idr_remove(&mm->memory_idr, internal_buffer->handle);
+	mm->buffer_count--;
+	mutex_unlock(&mm->lock);
+
+
+	rga_mm_unmap_buffer(internal_buffer);
+	kfree(internal_buffer);
+
+	mutex_lock(&mm->lock);
+}
+
+/* Force release the current internal_buffer from the IDR. */
+static void rga_mm_force_releaser_buffer(struct rga_internal_buffer *buffer)
+{
+	struct rga_mm *mm = rga_drvdata->mm;
+
+	WARN_ON(!mutex_is_locked(&mm->lock));
+
+	idr_remove(&mm->memory_idr, buffer->handle);
+	mm->buffer_count--;
+
+	rga_mm_unmap_buffer(buffer);
+	kfree(buffer);
+}
+
+/*
+ * Called at driver close to release the memory's handle references.
+ */
+static int rga_mm_buffer_destroy_for_idr(int id, void *ptr, void *data)
+{
+	struct rga_internal_buffer *internal_buffer = ptr;
+
+	rga_mm_force_releaser_buffer(internal_buffer);
+
+	return 0;
+}
+
+static struct rga_internal_buffer *
+rga_mm_lookup_external(struct rga_mm *mm_session,
+		       struct rga_external_buffer *external_buffer,
+		       struct mm_struct *current_mm)
+{
+	int id;
+	struct dma_buf *dma_buf = NULL;
+	struct rga_internal_buffer *temp_buffer = NULL;
+	struct rga_internal_buffer *output_buffer = NULL;
+
+	WARN_ON(!mutex_is_locked(&mm_session->lock));
+
+	switch (external_buffer->type) {
+	case RGA_DMA_BUFFER:
+		dma_buf = dma_buf_get((int)external_buffer->memory);
+		if (IS_ERR(dma_buf))
+			return (struct rga_internal_buffer *)dma_buf;
+
+		idr_for_each_entry(&mm_session->memory_idr, temp_buffer, id) {
+			if (temp_buffer->dma_buffer == NULL)
+				continue;
+
+			if (temp_buffer->dma_buffer[0].dma_buf == dma_buf) {
+				output_buffer = temp_buffer;
+				break;
+			}
+		}
+
+		dma_buf_put(dma_buf);
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		idr_for_each_entry(&mm_session->memory_idr, temp_buffer, id) {
+			if (temp_buffer->virt_addr == NULL)
+				continue;
+
+			if (temp_buffer->virt_addr->addr == external_buffer->memory) {
+				if (temp_buffer->current_mm == current_mm) {
+					output_buffer = temp_buffer;
+					break;
+				}
+
+				continue;
+			}
+		}
+
+		break;
+	case RGA_PHYSICAL_ADDRESS:
+		idr_for_each_entry(&mm_session->memory_idr, temp_buffer, id) {
+			if (temp_buffer->phys_addr == external_buffer->memory) {
+				output_buffer = temp_buffer;
+				break;
+			}
+		}
+
+		break;
+	case RGA_DMA_BUFFER_PTR:
+		idr_for_each_entry(&mm_session->memory_idr, temp_buffer, id) {
+			if (temp_buffer->dma_buffer == NULL)
+				continue;
+
+			if ((unsigned long)temp_buffer->dma_buffer[0].dma_buf ==
+			    external_buffer->memory) {
+				output_buffer = temp_buffer;
+				break;
+			}
+		}
+
+		break;
+
+	default:
+		rga_err("Illegal external buffer!\n");
+		return NULL;
+	}
+
+	return output_buffer;
+}
+
+struct rga_internal_buffer *rga_mm_lookup_handle(struct rga_mm *mm_session, uint32_t handle)
+{
+	struct rga_internal_buffer *output_buffer;
+
+	WARN_ON(!mutex_is_locked(&mm_session->lock));
+
+	output_buffer = idr_find(&mm_session->memory_idr, handle);
+
+	return output_buffer;
+}
+
+int rga_mm_lookup_flag(struct rga_mm *mm_session, uint64_t handle)
+{
+	struct rga_internal_buffer *output_buffer;
+
+	output_buffer = rga_mm_lookup_handle(mm_session, handle);
+	if (output_buffer == NULL) {
+		rga_err("This handle[%ld] is illegal.\n", (unsigned long)handle);
+		return -EINVAL;
+	}
+
+	return output_buffer->mm_flag;
+}
+
+dma_addr_t rga_mm_lookup_iova(struct rga_internal_buffer *buffer)
+{
+	if (rga_mm_is_invalid_dma_buffer(buffer->dma_buffer))
+		return 0;
+
+	return buffer->dma_buffer->iova + buffer->dma_buffer->offset;
+}
+
+struct sg_table *rga_mm_lookup_sgt(struct rga_internal_buffer *buffer)
+{
+	if (rga_mm_is_invalid_dma_buffer(buffer->dma_buffer))
+		return NULL;
+
+	return buffer->dma_buffer->sgt;
+}
+
+void rga_mm_dump_buffer(struct rga_internal_buffer *dump_buffer)
+{
+	rga_buf_log(dump_buffer, "type = %s, refcount = %d mm_flag = 0x%x\n",
+		rga_get_memory_type_str(dump_buffer->type),
+		kref_read(&dump_buffer->refcount),
+		dump_buffer->mm_flag);
+
+	switch (dump_buffer->type) {
+	case RGA_DMA_BUFFER:
+	case RGA_DMA_BUFFER_PTR:
+		if (rga_mm_is_invalid_dma_buffer(dump_buffer->dma_buffer))
+			break;
+
+		rga_buf_log(dump_buffer, "dma_buf = %p\n",
+			dump_buffer->dma_buffer->dma_buf);
+		rga_buf_log(dump_buffer, "iova = 0x%lx, dma_addr = 0x%lx, offset = 0x%lx, sgt = %p, size = %ld, map_core = 0x%x\n",
+			(unsigned long)dump_buffer->dma_buffer->iova,
+			(unsigned long)dump_buffer->dma_buffer->dma_addr,
+			(unsigned long)dump_buffer->dma_buffer->offset,
+			dump_buffer->dma_buffer->sgt,
+			dump_buffer->dma_buffer->size,
+			dump_buffer->dma_buffer->scheduler->core);
+
+		if (dump_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+			rga_log("is contiguous, pa = 0x%lx\n",
+				(unsigned long)dump_buffer->phys_addr);
+		break;
+	case RGA_VIRTUAL_ADDRESS:
+		if (dump_buffer->virt_addr == NULL)
+			break;
+
+		rga_buf_log(dump_buffer, "va = 0x%lx, pages = %p, size = %ld\n",
+			(unsigned long)dump_buffer->virt_addr->addr,
+			dump_buffer->virt_addr->pages,
+			dump_buffer->virt_addr->size);
+
+		if (rga_mm_is_invalid_dma_buffer(dump_buffer->dma_buffer))
+			break;
+
+		rga_buf_log(dump_buffer, "iova = 0x%lx, dma_addr = 0x%lx, offset = 0x%lx, sgt = %p, size = %ld, map_core = 0x%x\n",
+			(unsigned long)dump_buffer->dma_buffer->iova,
+			(unsigned long)dump_buffer->dma_buffer->dma_addr,
+			(unsigned long)dump_buffer->dma_buffer->offset,
+			dump_buffer->dma_buffer->sgt,
+			dump_buffer->dma_buffer->size,
+			dump_buffer->dma_buffer->scheduler->core);
+
+		if (dump_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+			rga_buf_log(dump_buffer, "is contiguous, pa = 0x%lx\n",
+				(unsigned long)dump_buffer->phys_addr);
+		break;
+	case RGA_PHYSICAL_ADDRESS:
+		rga_buf_log(dump_buffer, "pa = 0x%lx\n", (unsigned long)dump_buffer->phys_addr);
+		break;
+	default:
+		rga_buf_err(dump_buffer, "Illegal buffer! type= %d\n", dump_buffer->type);
+		break;
+	}
+}
+
+void rga_mm_dump_info(struct rga_mm *mm_session)
+{
+	int id;
+	struct rga_internal_buffer *dump_buffer;
+
+	WARN_ON(!mutex_is_locked(&mm_session->lock));
+
+	rga_log("rga mm info:\n");
+
+	rga_log("buffer count = %d\n", mm_session->buffer_count);
+	rga_log("===============================================================\n");
+
+	idr_for_each_entry(&mm_session->memory_idr, dump_buffer, id) {
+		rga_mm_dump_buffer(dump_buffer);
+
+		rga_log("---------------------------------------------------------------\n");
+	}
+}
+
+static bool rga_mm_is_need_mmu(struct rga_job *job, struct rga_internal_buffer *buffer)
+{
+	if (buffer == NULL || job == NULL || job->scheduler == NULL)
+		return false;
+
+	/* RK_IOMMU no need to configure enable or not in the driver. */
+	if (job->scheduler->data->mmu == RGA_IOMMU)
+		return false;
+
+	/* RK_MMU need to configure enable or not in the driver. */
+	if (buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS)
+		return false;
+	else if (buffer->mm_flag & RGA_MEM_NEED_USE_IOMMU)
+		return true;
+
+	return false;
+}
+
+static int rga_mm_set_mmu_flag(struct rga_job *job)
+{
+	struct rga_mmu_t *mmu_info;
+	int src_mmu_en;
+	int src1_mmu_en;
+	int dst_mmu_en;
+	int els_mmu_en;
+
+	src_mmu_en = rga_mm_is_need_mmu(job, job->src_buffer.addr);
+	src1_mmu_en = rga_mm_is_need_mmu(job, job->src1_buffer.addr);
+	dst_mmu_en = rga_mm_is_need_mmu(job, job->dst_buffer.addr);
+	els_mmu_en = rga_mm_is_need_mmu(job, job->els_buffer.addr);
+
+	mmu_info = &job->rga_command_base.mmu_info;
+	memset(mmu_info, 0x0, sizeof(*mmu_info));
+	if (src_mmu_en)
+		mmu_info->mmu_flag |= (0x1 << 8);
+	if (src1_mmu_en)
+		mmu_info->mmu_flag |= (0x1 << 9);
+	if (dst_mmu_en)
+		mmu_info->mmu_flag |= (0x1 << 10);
+	if (els_mmu_en)
+		mmu_info->mmu_flag |= (0x1 << 11);
+
+	if (mmu_info->mmu_flag & (0xf << 8)) {
+		mmu_info->mmu_flag |= 1;
+		mmu_info->mmu_flag |= 1 << 31;
+		mmu_info->mmu_en  = 1;
+	}
+
+	return 0;
+}
+
+static int rga_mm_sgt_to_page_table(struct sg_table *sg,
+				    uint32_t *page_table,
+				    int32_t pageCount,
+				    int32_t use_dma_address)
+{
+	uint32_t i;
+	unsigned long Address;
+	uint32_t mapped_size = 0;
+	uint32_t len;
+	struct scatterlist *sgl = sg->sgl;
+	uint32_t sg_num = 0;
+	uint32_t break_flag = 0;
+
+	do {
+		/*
+		 *   The length of each sgl is expected to be obtained here, not
+		 * the length of the entire dma_buf, so sg_dma_len() is not used.
+		 */
+		len = sgl->length >> PAGE_SHIFT;
+
+		if (use_dma_address)
+			/*
+			 *   The fd passed by user space gets sg through
+			 * dma_buf_map_attachment, so dma_address can
+			 * be use here.
+			 *   When the mapped device does not have iommu, it will
+			 * return the first address of the real physical page
+			 * when it meets the requirements of the current device,
+			 * and will trigger swiotlb when it does not meet the
+			 * requirements to obtain a software-mapped physical
+			 * address that is mapped to meet the device address
+			 * requirements.
+			 */
+			Address = sg_dma_address(sgl);
+		else
+			Address = sg_phys(sgl);
+
+		for (i = 0; i < len; i++) {
+			if (mapped_size + i >= pageCount) {
+				break_flag = 1;
+				break;
+			}
+			page_table[mapped_size + i] = (uint32_t)(Address + (i << PAGE_SHIFT));
+		}
+		if (break_flag)
+			break;
+		mapped_size += len;
+		sg_num += 1;
+	} while ((sgl = sg_next(sgl)) && (mapped_size < pageCount) && (sg_num < sg->orig_nents));
+
+	return 0;
+}
+
+static int rga_mm_set_mmu_base(struct rga_job *job,
+			       struct rga_img_info_t *img,
+			       struct rga_job_buffer *job_buf)
+{
+	int ret;
+	int yrgb_count = 0;
+	int uv_count = 0;
+	int v_count = 0;
+	int page_count = 0;
+	int order = 0;
+	uint32_t *page_table = NULL;
+	struct sg_table *sgt = NULL;
+
+	int img_size, yrgb_size, uv_size, v_size;
+	int img_offset = 0;
+	int yrgb_offset = 0;
+	int uv_offset = 0;
+	int v_offset = 0;
+
+	img_size = rga_image_size_cal(img->vir_w, img->vir_h, img->format,
+				      &yrgb_size, &uv_size, &v_size);
+	if (img_size <= 0) {
+		rga_job_err(job, "Image size cal error! width = %d, height = %d, format = %s\n",
+			img->vir_w, img->vir_h, rga_get_format_name(img->format));
+		return -EINVAL;
+	}
+
+	/* using third-address */
+	if (job_buf->uv_addr) {
+		if (job_buf->y_addr->virt_addr != NULL)
+			yrgb_offset = job_buf->y_addr->virt_addr->offset;
+		if (job_buf->uv_addr->virt_addr != NULL)
+			uv_offset = job_buf->uv_addr->virt_addr->offset;
+		if (job_buf->v_addr->virt_addr != NULL)
+			v_offset = job_buf->v_addr->virt_addr->offset;
+
+		yrgb_count = RGA_GET_PAGE_COUNT(yrgb_size + yrgb_offset);
+		uv_count = RGA_GET_PAGE_COUNT(uv_size + uv_offset);
+		v_count = RGA_GET_PAGE_COUNT(v_size + v_offset);
+		page_count = yrgb_count + uv_count + v_count;
+
+		if (page_count <= 0) {
+			rga_job_err(job, "page count cal error! yrba = %d, uv = %d, v = %d\n",
+				yrgb_count, uv_count, v_count);
+			return -EFAULT;
+		}
+
+		if (job->flags & RGA_JOB_USE_HANDLE) {
+			order = get_order(page_count * sizeof(uint32_t *));
+			if (order >= MAX_ORDER) {
+				rga_job_err(job, "Can not alloc pages with order[%d] for page_table, max_order = %d\n",
+					order, MAX_ORDER);
+				return -ENOMEM;
+			}
+
+			page_table = (uint32_t *)__get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+			if (page_table == NULL) {
+				rga_job_err(job, "%s can not alloc pages for page_table, order = %d\n",
+					__func__, order);
+				return -ENOMEM;
+			}
+		} else {
+			mutex_lock(&rga_drvdata->lock);
+
+			page_table = rga_mmu_buf_get(rga_drvdata->mmu_base, page_count);
+			if (page_table == NULL) {
+				rga_err("mmu_buf get error!\n");
+				mutex_unlock(&rga_drvdata->lock);
+				return -EFAULT;
+			}
+
+			mutex_unlock(&rga_drvdata->lock);
+		}
+
+		sgt = rga_mm_lookup_sgt(job_buf->y_addr);
+		if (sgt == NULL) {
+			rga_job_err(job, "rga2 cannot get sgt from internal buffer!\n");
+			ret = -EINVAL;
+			goto err_free_page_table;
+		}
+		rga_mm_sgt_to_page_table(sgt, page_table, yrgb_count, false);
+
+		sgt = rga_mm_lookup_sgt(job_buf->uv_addr);
+		if (sgt == NULL) {
+			rga_job_err(job, "rga2 cannot get sgt from internal buffer!\n");
+			ret = -EINVAL;
+			goto err_free_page_table;
+		}
+		rga_mm_sgt_to_page_table(sgt, page_table + yrgb_count, uv_count, false);
+
+		sgt = rga_mm_lookup_sgt(job_buf->v_addr);
+		if (sgt == NULL) {
+			rga_job_err(job, "rga2 cannot get sgt from internal buffer!\n");
+			ret = -EINVAL;
+			goto err_free_page_table;
+		}
+		rga_mm_sgt_to_page_table(sgt, page_table + yrgb_count + uv_count, v_count, false);
+
+		img->yrgb_addr = yrgb_offset;
+		img->uv_addr = (yrgb_count << PAGE_SHIFT) + uv_offset;
+		img->v_addr = ((yrgb_count + uv_count) << PAGE_SHIFT) + v_offset;
+	} else {
+		if (job_buf->addr->virt_addr != NULL)
+			img_offset = job_buf->addr->virt_addr->offset;
+
+		page_count = RGA_GET_PAGE_COUNT(img_size + img_offset);
+		if (page_count < 0) {
+			rga_job_err(job, "page count cal error! yrba = %d, uv = %d, v = %d\n",
+				yrgb_count, uv_count, v_count);
+			return -EFAULT;
+		}
+
+		if (job->flags & RGA_JOB_USE_HANDLE) {
+			order = get_order(page_count * sizeof(uint32_t *));
+			if (order >= MAX_ORDER) {
+				rga_job_err(job, "Can not alloc pages with order[%d] for page_table, max_order = %d\n",
+					order, MAX_ORDER);
+				return -ENOMEM;
+			}
+
+			page_table = (uint32_t *)__get_free_pages(GFP_KERNEL | GFP_DMA32, order);
+			if (page_table == NULL) {
+				rga_job_err(job, "%s can not alloc pages for page_table, order = %d\n",
+					__func__, order);
+				return -ENOMEM;
+			}
+		} else {
+			mutex_lock(&rga_drvdata->lock);
+
+			page_table = rga_mmu_buf_get(rga_drvdata->mmu_base, page_count);
+			if (page_table == NULL) {
+				rga_job_err(job, "mmu_buf get error!\n");
+				mutex_unlock(&rga_drvdata->lock);
+				return -EFAULT;
+			}
+
+			mutex_unlock(&rga_drvdata->lock);
+		}
+
+		sgt = rga_mm_lookup_sgt(job_buf->addr);
+		if (sgt == NULL) {
+			rga_job_err(job, "rga2 cannot get sgt from internal buffer!\n");
+			ret = -EINVAL;
+			goto err_free_page_table;
+		}
+		rga_mm_sgt_to_page_table(sgt, page_table, page_count, false);
+
+		img->yrgb_addr = img_offset;
+		rga_convert_addr(img, false);
+	}
+
+	job_buf->page_table = page_table;
+	job_buf->order = order;
+	job_buf->page_count = page_count;
+
+	return 0;
+
+err_free_page_table:
+	if (job->flags & RGA_JOB_USE_HANDLE)
+		free_pages((unsigned long)page_table, order);
+	return ret;
+}
+
+static int rga_mm_sync_dma_sg_for_device(struct rga_internal_buffer *buffer,
+					 struct rga_job *job,
+					 enum dma_data_direction dir)
+{
+	struct sg_table *sgt;
+	struct rga_scheduler_t *scheduler;
+	ktime_t timestamp = ktime_get();
+
+	scheduler = buffer->dma_buffer->scheduler;
+	if (scheduler == NULL) {
+		rga_job_err(job, "%s(%d), failed to get scheduler, core = 0x%x\n",
+			__func__, __LINE__, job->core);
+		return -EFAULT;
+	}
+
+	if (buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS &&
+	    scheduler->data->mmu != RGA_IOMMU) {
+		dma_sync_single_for_device(scheduler->dev, buffer->phys_addr, buffer->size, dir);
+	} else {
+		sgt = rga_mm_lookup_sgt(buffer);
+		if (sgt == NULL) {
+			rga_job_err(job, "%s(%d), failed to get sgt, core = 0x%x\n",
+				__func__, __LINE__, job->core);
+			return -EINVAL;
+		}
+
+		dma_sync_sg_for_device(scheduler->dev, sgt->sgl, sgt->orig_nents, dir);
+	}
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "handle[%d], %s, flush CPU cache for device cost %lld us\n",
+			buffer->handle, rga_get_dma_data_direction_str(dir),
+			ktime_us_delta(ktime_get(), timestamp));
+
+	return 0;
+}
+
+static int rga_mm_sync_dma_sg_for_cpu(struct rga_internal_buffer *buffer,
+				      struct rga_job *job,
+				      enum dma_data_direction dir)
+{
+	struct sg_table *sgt;
+	struct rga_scheduler_t *scheduler;
+	ktime_t timestamp = ktime_get();
+
+	scheduler = buffer->dma_buffer->scheduler;
+	if (scheduler == NULL) {
+		rga_job_err(job, "%s(%d), failed to get scheduler, core = 0x%x\n",
+			__func__, __LINE__, job->core);
+		return -EFAULT;
+	}
+
+	if (buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS &&
+	    scheduler->data->mmu != RGA_IOMMU) {
+		dma_sync_single_for_cpu(scheduler->dev, buffer->phys_addr, buffer->size, dir);
+	} else {
+		sgt = rga_mm_lookup_sgt(buffer);
+		if (sgt == NULL) {
+			rga_job_err(job, "%s(%d), failed to get sgt, core = 0x%x\n",
+				__func__, __LINE__, job->core);
+			return -EINVAL;
+		}
+
+		dma_sync_sg_for_cpu(scheduler->dev, sgt->sgl, sgt->orig_nents, dir);
+	}
+
+	if (DEBUGGER_EN(TIME))
+		rga_job_log(job, "handle[%d], %s, flush CPU cache for CPU cost %lld us\n",
+			buffer->handle, rga_get_dma_data_direction_str(dir),
+			ktime_us_delta(ktime_get(), timestamp));
+
+	return 0;
+}
+
+static int rga_mm_get_buffer_info(struct rga_job *job,
+				  struct rga_internal_buffer *internal_buffer,
+				  uint64_t *channel_addr)
+{
+	uint64_t addr;
+
+	switch (job->scheduler->data->mmu) {
+	case RGA_IOMMU:
+		addr = rga_mm_lookup_iova(internal_buffer);
+		if (addr == 0) {
+			rga_job_err(job, "core[%d] lookup buffer_type[0x%x] iova error!\n",
+			       job->core, internal_buffer->type);
+			return -EINVAL;
+		}
+		break;
+	case RGA_MMU:
+	default:
+		if (internal_buffer->mm_flag & RGA_MEM_PHYSICAL_CONTIGUOUS) {
+			addr = internal_buffer->phys_addr;
+			break;
+		}
+
+		switch (internal_buffer->type) {
+		case RGA_DMA_BUFFER:
+		case RGA_DMA_BUFFER_PTR:
+			addr = 0;
+			break;
+		case RGA_VIRTUAL_ADDRESS:
+			addr = internal_buffer->virt_addr->addr;
+			break;
+		case RGA_PHYSICAL_ADDRESS:
+			addr = internal_buffer->phys_addr;
+			break;
+		default:
+			rga_job_err(job, "Illegal external buffer!\n");
+			return -EFAULT;
+		}
+		break;
+	}
+
+	*channel_addr = addr;
+
+	return 0;
+}
+
+static int rga_mm_get_buffer(struct rga_mm *mm,
+			     struct rga_job *job,
+			     uint64_t handle,
+			     uint64_t *channel_addr,
+			     struct rga_internal_buffer **buf,
+			     int require_size,
+			     enum dma_data_direction dir)
+{
+	int ret = 0;
+	struct rga_internal_buffer *internal_buffer = NULL;
+
+	if (handle == 0) {
+		rga_job_err(job, "No buffer handle can be used!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&mm->lock);
+	*buf = rga_mm_lookup_handle(mm, handle);
+	if (*buf == NULL) {
+		rga_job_err(job, "This handle[%ld] is illegal.\n", (unsigned long)handle);
+
+		mutex_unlock(&mm->lock);
+		return -EFAULT;
+	}
+
+	internal_buffer = *buf;
+	kref_get(&internal_buffer->refcount);
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "handle[%d] get info:\n", (int)handle);
+		rga_mm_dump_buffer(internal_buffer);
+	}
+
+	mutex_unlock(&mm->lock);
+
+	ret = rga_mm_get_buffer_info(job, internal_buffer, channel_addr);
+	if (ret < 0) {
+		rga_job_err(job, "handle[%ld] failed to get internal buffer info!\n",
+			(unsigned long)handle);
+		return ret;
+	}
+
+	if (internal_buffer->size < require_size) {
+		ret = -EINVAL;
+		rga_job_err(job, "Only get buffer %ld byte from handle[%ld], but current required %d byte\n",
+		       internal_buffer->size, (unsigned long)handle, require_size);
+
+		goto put_internal_buffer;
+	}
+
+	if (internal_buffer->mm_flag & RGA_MEM_FORCE_FLUSH_CACHE) {
+		/*
+		 * Some userspace virtual addresses do not have an
+		 * interface for flushing the cache, so it is mandatory
+		 * to flush the cache when the virtual address is used.
+		 */
+		ret = rga_mm_sync_dma_sg_for_device(internal_buffer, job, dir);
+		if (ret < 0) {
+			rga_job_err(job, "sync sgt for device error!\n");
+			goto put_internal_buffer;
+		}
+	}
+
+	return 0;
+
+put_internal_buffer:
+	mutex_lock(&mm->lock);
+	kref_put(&internal_buffer->refcount, rga_mm_kref_release_buffer);
+	mutex_unlock(&mm->lock);
+
+	return ret;
+
+}
+
+static void rga_mm_put_buffer(struct rga_mm *mm,
+			      struct rga_job *job,
+			      struct rga_internal_buffer *internal_buffer,
+			      enum dma_data_direction dir)
+{
+	if (internal_buffer->mm_flag & RGA_MEM_FORCE_FLUSH_CACHE && dir != DMA_NONE)
+		if (rga_mm_sync_dma_sg_for_cpu(internal_buffer, job, dir))
+			rga_job_err(job, "sync sgt for cpu error!\n");
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "handle[%d] put info:\n", (int)internal_buffer->handle);
+		rga_mm_dump_buffer(internal_buffer);
+	}
+
+	mutex_lock(&mm->lock);
+	kref_put(&internal_buffer->refcount, rga_mm_kref_release_buffer);
+	mutex_unlock(&mm->lock);
+}
+
+static void rga_mm_put_channel_handle_info(struct rga_mm *mm,
+					   struct rga_job *job,
+					   struct rga_job_buffer *job_buf,
+					   enum dma_data_direction dir)
+{
+	if (job_buf->y_addr)
+		rga_mm_put_buffer(mm, job, job_buf->y_addr, dir);
+	if (job_buf->uv_addr)
+		rga_mm_put_buffer(mm, job, job_buf->uv_addr, dir);
+	if (job_buf->v_addr)
+		rga_mm_put_buffer(mm, job, job_buf->v_addr, dir);
+
+	if (job_buf->page_table)
+		free_pages((unsigned long)job_buf->page_table, job_buf->order);
+}
+
+static int rga_mm_get_channel_handle_info(struct rga_mm *mm,
+					  struct rga_job *job,
+					  struct rga_img_info_t *img,
+					  struct rga_job_buffer *job_buf,
+					  enum dma_data_direction dir)
+{
+	int ret = 0;
+	int handle = 0;
+	int img_size, yrgb_size, uv_size, v_size;
+
+	img_size = rga_image_size_cal(img->vir_w, img->vir_h, img->format,
+				      &yrgb_size, &uv_size, &v_size);
+	if (img_size <= 0) {
+		rga_job_err(job, "Image size cal error! width = %d, height = %d, format = %s\n",
+			img->vir_w, img->vir_h, rga_get_format_name(img->format));
+		return -EINVAL;
+	}
+
+	/* using third-address */
+	if (img->uv_addr > 0) {
+		handle = img->yrgb_addr;
+		if (handle > 0) {
+			ret = rga_mm_get_buffer(mm, job, handle, &img->yrgb_addr,
+						&job_buf->y_addr, yrgb_size, dir);
+			if (ret < 0) {
+				rga_job_err(job, "handle[%d] Can't get y/rgb address info!\n",
+					handle);
+				return ret;
+			}
+		}
+
+		handle = img->uv_addr;
+		if (handle > 0) {
+			ret = rga_mm_get_buffer(mm, job, handle, &img->uv_addr,
+						&job_buf->uv_addr, uv_size, dir);
+			if (ret < 0) {
+				rga_job_err(job, "handle[%d] Can't get uv address info!\n", handle);
+				return ret;
+			}
+		}
+
+		handle = img->v_addr;
+		if (handle > 0) {
+			ret = rga_mm_get_buffer(mm, job, handle, &img->v_addr,
+						&job_buf->v_addr, v_size, dir);
+			if (ret < 0) {
+				rga_job_err(job, "handle[%d] Can't get uv address info!\n", handle);
+				return ret;
+			}
+		}
+	} else {
+		handle = img->yrgb_addr;
+		if (handle > 0) {
+			ret = rga_mm_get_buffer(mm, job, handle, &img->yrgb_addr,
+						&job_buf->addr, img_size, dir);
+			if (ret < 0) {
+				rga_job_err(job, "handle[%d] Can't get y/rgb address info!\n",
+					handle);
+				return ret;
+			}
+		}
+
+		rga_convert_addr(img, false);
+	}
+
+	if (job->scheduler->data->mmu == RGA_MMU &&
+	    rga_mm_is_need_mmu(job, job_buf->addr)) {
+		ret = rga_mm_set_mmu_base(job, img, job_buf);
+		if (ret < 0) {
+			rga_job_err(job, "Can't set RGA2 MMU_BASE from handle!\n");
+
+			rga_mm_put_channel_handle_info(mm, job, job_buf, dir);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int rga_mm_get_handle_info(struct rga_job *job)
+{
+	int ret = 0;
+	struct rga_req *req = NULL;
+	struct rga_mm *mm = NULL;
+	enum dma_data_direction dir;
+
+	req = &job->rga_command_base;
+	mm = rga_drvdata->mm;
+
+	switch (req->render_mode) {
+	case BITBLT_MODE:
+	case COLOR_PALETTE_MODE:
+		if (unlikely(req->src.yrgb_addr <= 0)) {
+			rga_job_err(job, "render_mode[0x%x] src0 channel handle[%ld] must is valid!",
+				req->render_mode, (unsigned long)req->src.yrgb_addr);
+			return -EINVAL;
+		}
+
+		if (unlikely(req->dst.yrgb_addr <= 0)) {
+			rga_job_err(job, "render_mode[0x%x] dst channel handle[%ld] must is valid!",
+				req->render_mode, (unsigned long)req->dst.yrgb_addr);
+			return -EINVAL;
+		}
+
+		if (req->bsfilter_flag) {
+			if (unlikely(req->pat.yrgb_addr <= 0)) {
+				rga_job_err(job, "render_mode[0x%x] src1/pat channel handle[%ld] must is valid!",
+					req->render_mode, (unsigned long)req->pat.yrgb_addr);
+				return -EINVAL;
+			}
+		}
+
+		break;
+	case COLOR_FILL_MODE:
+		if (unlikely(req->dst.yrgb_addr <= 0)) {
+			rga_job_err(job, "render_mode[0x%x] dst channel handle[%ld] must is valid!",
+				req->render_mode, (unsigned long)req->dst.yrgb_addr);
+			return -EINVAL;
+		}
+
+		break;
+
+	case UPDATE_PALETTE_TABLE_MODE:
+	case UPDATE_PATTEN_BUF_MODE:
+		if (unlikely(req->pat.yrgb_addr <= 0)) {
+			rga_job_err(job, "render_mode[0x%x] lut/pat channel handle[%ld] must is valid!",
+				req->render_mode, (unsigned long)req->pat.yrgb_addr);
+			return -EINVAL;
+		}
+
+		break;
+	default:
+		rga_job_err(job, "%s, unknown render mode!\n", __func__);
+		break;
+	}
+
+	if (likely(req->src.yrgb_addr > 0)) {
+		ret = rga_mm_get_channel_handle_info(mm, job, &req->src,
+						     &job->src_buffer,
+						     DMA_TO_DEVICE);
+		if (ret < 0) {
+			rga_job_err(job, "Can't get src buffer info from handle!\n");
+			return ret;
+		}
+	}
+
+	if (likely(req->dst.yrgb_addr > 0)) {
+		ret = rga_mm_get_channel_handle_info(mm, job, &req->dst,
+						     &job->dst_buffer,
+						     DMA_TO_DEVICE);
+		if (ret < 0) {
+			rga_job_err(job, "Can't get dst buffer info from handle!\n");
+			return ret;
+		}
+	}
+
+	if (likely(req->pat.yrgb_addr > 0)) {
+
+		if (req->render_mode != UPDATE_PALETTE_TABLE_MODE) {
+			if (req->bsfilter_flag)
+				dir = DMA_BIDIRECTIONAL;
+			else
+				dir = DMA_TO_DEVICE;
+
+			ret = rga_mm_get_channel_handle_info(mm, job, &req->pat,
+							     &job->src1_buffer,
+							     dir);
+		} else {
+			ret = rga_mm_get_channel_handle_info(mm, job, &req->pat,
+							     &job->els_buffer,
+							     DMA_BIDIRECTIONAL);
+		}
+		if (ret < 0) {
+			rga_job_err(job, "Can't get pat buffer info from handle!\n");
+			return ret;
+		}
+	}
+
+	rga_mm_set_mmu_flag(job);
+
+	return 0;
+}
+
+static void rga_mm_put_handle_info(struct rga_job *job)
+{
+	struct rga_mm *mm = rga_drvdata->mm;
+
+	rga_mm_put_channel_handle_info(mm, job, &job->src_buffer, DMA_NONE);
+	rga_mm_put_channel_handle_info(mm, job, &job->dst_buffer, DMA_FROM_DEVICE);
+	rga_mm_put_channel_handle_info(mm, job, &job->src1_buffer, DMA_NONE);
+	rga_mm_put_channel_handle_info(mm, job, &job->els_buffer, DMA_NONE);
+}
+
+static void rga_mm_put_channel_external_buffer(struct rga_job_buffer *job_buffer)
+{
+	if (job_buffer->ex_addr->type == RGA_DMA_BUFFER_PTR)
+		dma_buf_put((struct dma_buf *)(unsigned long)job_buffer->ex_addr->memory);
+
+	kfree(job_buffer->ex_addr);
+	job_buffer->ex_addr = NULL;
+}
+
+static int rga_mm_get_channel_external_buffer(int mmu_flag,
+					      struct rga_img_info_t *img_info,
+					      struct rga_job_buffer *job_buffer)
+{
+	struct dma_buf *dma_buf = NULL;
+	struct rga_external_buffer *external_buffer = NULL;
+
+	/* Default unsupported multi-planar format */
+	external_buffer = kzalloc(sizeof(*external_buffer), GFP_KERNEL);
+	if (external_buffer == NULL) {
+		rga_err("Cannot alloc job_buffer!\n");
+		return -ENOMEM;
+	}
+
+	if (img_info->yrgb_addr) {
+		dma_buf = dma_buf_get(img_info->yrgb_addr);
+		if (IS_ERR(dma_buf)) {
+			rga_err("%s dma_buf_get fail fd[%lu]\n",
+				__func__, (unsigned long)img_info->yrgb_addr);
+			kfree(external_buffer);
+			return -EINVAL;
+		}
+
+		external_buffer->memory = (unsigned long)dma_buf;
+		external_buffer->type = RGA_DMA_BUFFER_PTR;
+	} else if (mmu_flag && img_info->uv_addr) {
+		external_buffer->memory = (uint64_t)img_info->uv_addr;
+		external_buffer->type = RGA_VIRTUAL_ADDRESS;
+	} else if (img_info->uv_addr) {
+		external_buffer->memory = (uint64_t)img_info->uv_addr;
+		external_buffer->type = RGA_PHYSICAL_ADDRESS;
+	} else {
+		kfree(external_buffer);
+		return -EINVAL;
+	}
+
+	external_buffer->memory_parm.width = img_info->vir_w;
+	external_buffer->memory_parm.height = img_info->vir_h;
+	external_buffer->memory_parm.format = img_info->format;
+
+	job_buffer->ex_addr = external_buffer;
+
+	return 0;
+}
+
+static void rga_mm_put_external_buffer(struct rga_job *job)
+{
+	if (job->src_buffer.ex_addr)
+		rga_mm_put_channel_external_buffer(&job->src_buffer);
+	if (job->src1_buffer.ex_addr)
+		rga_mm_put_channel_external_buffer(&job->src1_buffer);
+	if (job->dst_buffer.ex_addr)
+		rga_mm_put_channel_external_buffer(&job->dst_buffer);
+	if (job->els_buffer.ex_addr)
+		rga_mm_put_channel_external_buffer(&job->els_buffer);
+}
+
+static int rga_mm_get_external_buffer(struct rga_job *job)
+{
+	int ret = -EINVAL;
+	int mmu_flag;
+
+	struct rga_img_info_t *src0 = NULL;
+	struct rga_img_info_t *src1 = NULL;
+	struct rga_img_info_t *dst = NULL;
+	struct rga_img_info_t *els = NULL;
+
+	if (job->rga_command_base.render_mode != COLOR_FILL_MODE)
+		src0 = &job->rga_command_base.src;
+
+	if (job->rga_command_base.render_mode != UPDATE_PALETTE_TABLE_MODE)
+		src1 = job->rga_command_base.bsfilter_flag ?
+		       &job->rga_command_base.pat : NULL;
+	else
+		els = &job->rga_command_base.pat;
+
+	dst = &job->rga_command_base.dst;
+
+	if (likely(src0)) {
+		mmu_flag = ((job->rga_command_base.mmu_info.mmu_flag >> 8) & 1);
+		ret = rga_mm_get_channel_external_buffer(mmu_flag, src0, &job->src_buffer);
+		if (ret < 0) {
+			rga_job_err(job, "Cannot get src0 channel buffer!\n");
+			return ret;
+		}
+	}
+
+	if (likely(dst)) {
+		mmu_flag = ((job->rga_command_base.mmu_info.mmu_flag >> 10) & 1);
+		ret = rga_mm_get_channel_external_buffer(mmu_flag, dst, &job->dst_buffer);
+		if (ret < 0) {
+			rga_job_err(job, "Cannot get dst channel buffer!\n");
+			goto error_put_buffer;
+		}
+	}
+
+	if (src1) {
+		mmu_flag = ((job->rga_command_base.mmu_info.mmu_flag >> 9) & 1);
+		ret = rga_mm_get_channel_external_buffer(mmu_flag, src1, &job->src1_buffer);
+		if (ret < 0) {
+			rga_job_err(job, "Cannot get src1 channel buffer!\n");
+			goto error_put_buffer;
+		}
+	}
+
+	if (els) {
+		mmu_flag = ((job->rga_command_base.mmu_info.mmu_flag >> 11) & 1);
+		ret = rga_mm_get_channel_external_buffer(mmu_flag, els, &job->els_buffer);
+		if (ret < 0) {
+			rga_job_err(job, "Cannot get els channel buffer!\n");
+			goto error_put_buffer;
+		}
+	}
+
+	return 0;
+error_put_buffer:
+	rga_mm_put_external_buffer(job);
+	return ret;
+}
+
+static void rga_mm_unmap_channel_job_buffer(struct rga_job *job,
+					    struct rga_job_buffer *job_buffer,
+					    enum dma_data_direction dir)
+{
+	if (job_buffer->addr->mm_flag & RGA_MEM_FORCE_FLUSH_CACHE && dir != DMA_NONE)
+		if (rga_mm_sync_dma_sg_for_cpu(job_buffer->addr, job, dir))
+			rga_job_err(job, "sync sgt for cpu error!\n");
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "unmap buffer:\n");
+		rga_mm_dump_buffer(job_buffer->addr);
+	}
+
+	rga_mm_unmap_buffer(job_buffer->addr);
+	kfree(job_buffer->addr);
+
+	job_buffer->page_table = NULL;
+}
+
+static int rga_mm_map_channel_job_buffer(struct rga_job *job,
+					 struct rga_img_info_t *img,
+					 struct rga_job_buffer *job_buffer,
+					 enum dma_data_direction dir,
+					 int write_flag)
+{
+	int ret;
+	struct rga_internal_buffer *buffer = NULL;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (buffer == NULL) {
+		rga_job_err(job, "%s alloc internal_buffer error!\n", __func__);
+		return -ENOMEM;
+	}
+
+	ret = rga_mm_map_buffer(job_buffer->ex_addr, buffer, job, write_flag);
+	if (ret < 0) {
+		rga_job_err(job, "job buffer map failed!\n");
+		goto error_free_buffer;
+	}
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "map buffer:\n");
+		rga_mm_dump_buffer(buffer);
+	}
+
+	ret = rga_mm_get_buffer_info(job, buffer, &img->yrgb_addr);
+	if (ret < 0) {
+		rga_job_err(job, "Failed to get internal buffer info!\n");
+		goto error_unmap_buffer;
+	}
+
+	if (buffer->mm_flag & RGA_MEM_FORCE_FLUSH_CACHE) {
+		ret = rga_mm_sync_dma_sg_for_device(buffer, job, dir);
+		if (ret < 0) {
+			rga_job_err(job, "sync sgt for device error!\n");
+			goto error_unmap_buffer;
+		}
+	}
+
+	rga_convert_addr(img, false);
+
+	job_buffer->addr = buffer;
+
+	if (job->scheduler->data->mmu == RGA_MMU &&
+	    rga_mm_is_need_mmu(job, job_buffer->addr)) {
+		ret = rga_mm_set_mmu_base(job, img, job_buffer);
+		if (ret < 0) {
+			rga_job_err(job, "Can't set RGA2 MMU_BASE!\n");
+			job_buffer->addr = NULL;
+			goto error_unmap_buffer;
+		}
+	}
+
+	return 0;
+
+error_unmap_buffer:
+	rga_mm_unmap_buffer(buffer);
+error_free_buffer:
+	kfree(buffer);
+
+	return ret;
+}
+
+static void rga_mm_unmap_buffer_info(struct rga_job *job)
+{
+	if (job->src_buffer.addr)
+		rga_mm_unmap_channel_job_buffer(job, &job->src_buffer, DMA_NONE);
+	if (job->dst_buffer.addr)
+		rga_mm_unmap_channel_job_buffer(job, &job->dst_buffer, DMA_FROM_DEVICE);
+	if (job->src1_buffer.addr)
+		rga_mm_unmap_channel_job_buffer(job, &job->src1_buffer, DMA_NONE);
+	if (job->els_buffer.addr)
+		rga_mm_unmap_channel_job_buffer(job, &job->els_buffer, DMA_NONE);
+
+	rga_mm_put_external_buffer(job);
+}
+
+static int rga_mm_map_buffer_info(struct rga_job *job)
+{
+	int ret = 0;
+	struct rga_req *req = NULL;
+	enum dma_data_direction dir;
+
+	ret = rga_mm_get_external_buffer(job);
+	if (ret < 0) {
+		rga_job_err(job, "failed to get external buffer from job_cmd!\n");
+		return ret;
+	}
+
+	req = &job->rga_command_base;
+
+	if (likely(job->src_buffer.ex_addr)) {
+		ret = rga_mm_map_channel_job_buffer(job, &req->src,
+						    &job->src_buffer,
+						    DMA_TO_DEVICE, false);
+		if (ret < 0) {
+			rga_job_err(job, "src channel map job buffer failed!");
+			goto error_unmap_buffer;
+		}
+	}
+
+	if (likely(job->dst_buffer.ex_addr)) {
+		ret = rga_mm_map_channel_job_buffer(job, &req->dst,
+						    &job->dst_buffer,
+						    DMA_TO_DEVICE, true);
+		if (ret < 0) {
+			rga_job_err(job, "dst channel map job buffer failed!");
+			goto error_unmap_buffer;
+		}
+	}
+
+	if (job->src1_buffer.ex_addr) {
+		if (req->bsfilter_flag)
+			dir = DMA_BIDIRECTIONAL;
+		else
+			dir = DMA_TO_DEVICE;
+
+		ret = rga_mm_map_channel_job_buffer(job, &req->pat,
+						    &job->src1_buffer,
+						    dir, false);
+		if (ret < 0) {
+			rga_job_err(job, "src1 channel map job buffer failed!");
+			goto error_unmap_buffer;
+		}
+	}
+
+	if (job->els_buffer.ex_addr) {
+		ret = rga_mm_map_channel_job_buffer(job, &req->pat,
+						    &job->els_buffer,
+						    DMA_BIDIRECTIONAL, false);
+		if (ret < 0) {
+			rga_job_err(job, "els channel map job buffer failed!");
+			goto error_unmap_buffer;
+		}
+	}
+
+	rga_mm_set_mmu_flag(job);
+	return 0;
+
+error_unmap_buffer:
+	rga_mm_unmap_buffer_info(job);
+
+	return ret;
+}
+
+static void rga_mm_free_channel_fake_buffer(struct rga_job *job,
+					    struct rga_job_buffer *job_buffer,
+					    enum dma_data_direction dir)
+{
+	struct rga_internal_buffer *buffer = job_buffer->addr;
+
+	if (rga_mm_is_invalid_dma_buffer(buffer->dma_buffer))
+		return;
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "free fake-buffer dump info:\n");
+		rga_mm_dump_buffer(buffer);
+	}
+
+	rga_dma_free(buffer->dma_buffer);
+	kfree(buffer);
+	job_buffer->addr = NULL;
+}
+
+static int rga_mm_alloc_channel_fake_buffer(struct rga_job *job,
+					    struct rga_img_info_t *img,
+					    struct rga_job_buffer *job_buffer,
+					    enum dma_data_direction dir)
+{
+	int ret;
+	int size;
+	uint32_t mm_flag;
+	uint64_t phys_addr;
+	struct rga_internal_buffer *buffer;
+	struct rga_dma_buffer *dma_buf;
+
+	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
+	if (buffer == NULL) {
+		rga_job_err(job, "%s alloc internal_buffer error!\n", __func__);
+		return -ENOMEM;
+	}
+
+	size = rga_image_size_cal(img->vir_w, img->vir_h, img->format,
+				  NULL, NULL, NULL);
+	dma_buf = rga_dma_alloc_coherent(job->scheduler, size);
+	if (dma_buf == NULL) {
+		ret = -ENOMEM;
+		rga_job_err(job, "%s failed to alloc dma_buf.\n", __func__);
+		goto error_free_buffer;
+	}
+
+	mm_flag = RGA_MEM_PHYSICAL_CONTIGUOUS | RGA_MEM_UNDER_4G;
+	if (job->scheduler->data->mmu != RGA_IOMMU) {
+		mm_flag |= RGA_MEM_NEED_USE_IOMMU;
+		phys_addr = 0;
+	} else {
+		phys_addr = dma_buf->dma_addr;
+	}
+
+	if (!rga_mm_check_memory_limit(job->scheduler, mm_flag)) {
+		rga_job_err(job, "%s scheduler core[%d] unsupported mm_flag[0x%x]!\n",
+		       __func__, job->scheduler->core, mm_flag);
+		ret = -EINVAL;
+		goto error_free_dma_buf;
+	}
+
+	buffer->type = RGA_DMA_BUFFER_PTR;
+	buffer->size = dma_buf->size - dma_buf->offset;
+	buffer->mm_flag = mm_flag;
+	buffer->dma_buffer = dma_buf;
+	buffer->phys_addr = phys_addr;
+
+	buffer->memory_parm.width = img->vir_w;
+	buffer->memory_parm.height = img->vir_h;
+	buffer->memory_parm.format = img->format;
+	buffer->memory_parm.size = size;
+
+	ret = rga_mm_get_buffer_info(job, buffer, &img->yrgb_addr);
+	if (ret < 0)
+		goto error_free_dma_buf;
+
+	rga_convert_addr(img, false);
+
+	job_buffer->addr = buffer;
+
+	if (job->scheduler->data->mmu == RGA_MMU &&
+	    rga_mm_is_need_mmu(job, job_buffer->addr)) {
+		ret = rga_mm_set_mmu_base(job, img, job_buffer);
+		if (ret < 0) {
+			job_buffer->addr = NULL;
+			goto error_free_dma_buf;
+		}
+	}
+
+	if (DEBUGGER_EN(MM)) {
+		rga_job_log(job, "alloc fake-buffer dump info:\n");
+		rga_mm_dump_buffer(buffer);
+	}
+
+	return 0;
+
+error_free_dma_buf:
+	rga_dma_free(dma_buf);
+
+error_free_buffer:
+	kfree(buffer);
+
+	return ret;
+}
+
+static void rga_mm_free_fake_buffer(struct rga_job *job)
+{
+	if (job->src_buffer.addr)
+		rga_mm_free_channel_fake_buffer(job, &job->src_buffer, DMA_NONE);
+	if (job->dst_buffer.addr)
+		rga_mm_free_channel_fake_buffer(job, &job->dst_buffer, DMA_FROM_DEVICE);
+	if (job->src1_buffer.addr)
+		rga_mm_free_channel_fake_buffer(job, &job->src1_buffer, DMA_NONE);
+	if (job->els_buffer.addr)
+		rga_mm_free_channel_fake_buffer(job, &job->els_buffer, DMA_NONE);
+}
+
+static int rga_mm_alloc_fake_buffer(struct rga_job *job)
+{
+	int ret = 0;
+	struct rga_req *req = NULL;
+	enum dma_data_direction dir;
+
+	req = &job->rga_command_base;
+
+	if (req->src.yrgb_addr != 0 || req->src.uv_addr != 0) {
+		ret = rga_mm_alloc_channel_fake_buffer(job, &req->src,
+						       &job->src_buffer, DMA_TO_DEVICE);
+		if (ret < 0) {
+			rga_job_err(job, "%s src channel map job buffer failed!", __func__);
+			goto error_free_fake_buffer;
+		}
+	}
+
+	if (req->dst.yrgb_addr != 0 || req->dst.uv_addr != 0) {
+		ret = rga_mm_alloc_channel_fake_buffer(job, &req->dst,
+						       &job->dst_buffer, DMA_TO_DEVICE);
+		if (ret < 0) {
+			rga_job_err(job, "%s dst channel map job buffer failed!", __func__);
+			goto error_free_fake_buffer;
+		}
+	}
+
+	if (job->rga_command_base.render_mode != UPDATE_PALETTE_TABLE_MODE &&
+	    (req->pat.yrgb_addr != 0 || req->pat.uv_addr != 0)) {
+		if (req->bsfilter_flag)
+			dir = DMA_BIDIRECTIONAL;
+		else
+			dir = DMA_TO_DEVICE;
+
+		ret = rga_mm_alloc_channel_fake_buffer(job, &req->pat,
+						       &job->src1_buffer, dir);
+		if (ret < 0) {
+			rga_job_err(job, "%s src1 channel map job buffer failed!", __func__);
+			goto error_free_fake_buffer;
+		}
+	} else if (req->pat.yrgb_addr != 0 || req->pat.uv_addr != 0) {
+		ret = rga_mm_alloc_channel_fake_buffer(job, &req->pat,
+						       &job->els_buffer, DMA_TO_DEVICE);
+		if (ret < 0) {
+			rga_job_err(job, "%s els channel map job buffer failed!", __func__);
+			goto error_free_fake_buffer;
+		}
+	}
+
+	rga_mm_set_mmu_flag(job);
+
+	return 0;
+
+error_free_fake_buffer:
+	rga_mm_free_fake_buffer(job);
+
+	return ret;
+}
+
+int rga_mm_map_job_info(struct rga_job *job)
+{
+	int ret;
+	ktime_t timestamp = ktime_get();
+
+	if (job->flags & RGA_JOB_DEBUG_FAKE_BUFFER) {
+		ret = rga_mm_alloc_fake_buffer(job);
+		if (ret < 0)
+			return ret;
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "alloc fake buffer cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+
+		job->flags &= ~RGA_JOB_USE_HANDLE;
+		job->flags |= RGA_JOB_DEBUG_FAKE_BUFFER;
+
+		return 0;
+	}
+
+	if (job->flags & RGA_JOB_USE_HANDLE) {
+		ret = rga_mm_get_handle_info(job);
+		if (ret < 0) {
+			rga_job_err(job, "failed to get buffer from handle\n");
+			return ret;
+		}
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "get buffer_handle info cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+	} else {
+		ret = rga_mm_map_buffer_info(job);
+		if (ret < 0) {
+			rga_job_err(job, "failed to map buffer\n");
+			return ret;
+		}
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "map buffer cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+	}
+
+	return 0;
+}
+
+void rga_mm_unmap_job_info(struct rga_job *job)
+{
+	ktime_t timestamp = ktime_get();
+
+	if (job->flags & RGA_JOB_DEBUG_FAKE_BUFFER) {
+		rga_mm_free_fake_buffer(job);
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "free fake buffer cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+
+		return;
+	}
+
+	if (job->flags & RGA_JOB_USE_HANDLE) {
+		rga_mm_put_handle_info(job);
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "put buffer_handle info cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+	} else {
+		rga_mm_unmap_buffer_info(job);
+
+		if (DEBUGGER_EN(TIME))
+			rga_job_log(job, "unmap buffer cost %lld us\n",
+				ktime_us_delta(ktime_get(), timestamp));
+	}
+}
+
+/*
+ * rga_mm_import_buffer - Importing external buffer into the RGA driver
+ *
+ * @external_buffer: [in] Parameters of external buffer
+ * @session:         [in] Session of the current process
+ *
+ * returns:
+ * if return value > 0, the buffer import is successful and is the generated
+ * buffer-handle, negative error code on failure.
+ */
+int rga_mm_import_buffer(struct rga_external_buffer *external_buffer,
+			 struct rga_session *session)
+{
+	int ret = 0, new_id;
+	struct rga_mm *mm;
+	struct rga_internal_buffer *internal_buffer;
+	ktime_t timestamp = ktime_get();
+
+	mm = rga_drvdata->mm;
+	if (mm == NULL) {
+		rga_err("rga mm is null!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&mm->lock);
+
+	/* first, Check whether to rga_mm */
+	internal_buffer = rga_mm_lookup_external(mm, external_buffer, current->mm);
+	if (!IS_ERR_OR_NULL(internal_buffer)) {
+		kref_get(&internal_buffer->refcount);
+
+		mutex_unlock(&mm->lock);
+
+		if (DEBUGGER_EN(MM)) {
+			rga_buf_log(internal_buffer, "import existing buffer:\n");
+			rga_mm_dump_buffer(internal_buffer);
+		}
+
+		return internal_buffer->handle;
+	}
+
+	mutex_unlock(&mm->lock);
+
+	/* finally, map and cached external_buffer in rga_mm */
+	internal_buffer = kzalloc(sizeof(struct rga_internal_buffer), GFP_KERNEL);
+	if (internal_buffer == NULL) {
+		rga_err("%s alloc internal_buffer error!\n", __func__);
+
+		mutex_unlock(&mm->lock);
+		return -ENOMEM;
+	}
+
+	ret = rga_mm_map_buffer(external_buffer, internal_buffer, NULL, true);
+	if (ret < 0)
+		goto FREE_INTERNAL_BUFFER;
+
+	kref_init(&internal_buffer->refcount);
+	internal_buffer->session = session;
+
+	mutex_lock(&mm->lock);
+	/*
+	 * Get the user-visible handle using idr. Preload and perform
+	 * allocation under our spinlock.
+	 */
+	idr_preload(GFP_KERNEL);
+	new_id = idr_alloc_cyclic(&mm->memory_idr, internal_buffer, 1, 0, GFP_NOWAIT);
+	idr_preload_end();
+	if (new_id < 0) {
+		rga_err("internal_buffer alloc id failed!\n");
+		ret = new_id;
+
+		mutex_unlock(&mm->lock);
+		goto FREE_INTERNAL_BUFFER;
+	}
+
+	internal_buffer->handle = new_id;
+	mm->buffer_count++;
+
+	if (DEBUGGER_EN(MM)) {
+		rga_buf_log(internal_buffer, "import buffer:\n");
+		rga_mm_dump_buffer(internal_buffer);
+	}
+
+	mutex_unlock(&mm->lock);
+
+	if (DEBUGGER_EN(TIME))
+		rga_buf_log(internal_buffer, "import buffer cost %lld us\n",
+			ktime_us_delta(ktime_get(), timestamp));
+
+	return internal_buffer->handle;
+
+FREE_INTERNAL_BUFFER:
+	kfree(internal_buffer);
+
+	return ret;
+}
+
+int rga_mm_release_buffer(uint32_t handle)
+{
+	struct rga_mm *mm;
+	struct rga_internal_buffer *internal_buffer;
+	ktime_t timestamp = ktime_get();
+
+	mm = rga_drvdata->mm;
+	if (mm == NULL) {
+		rga_err("rga mm is null!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&mm->lock);
+
+	/* Find the buffer that has been imported */
+	internal_buffer = rga_mm_lookup_handle(mm, handle);
+	if (IS_ERR_OR_NULL(internal_buffer)) {
+		rga_err("This is not a buffer that has been imported, handle = %d\n", (int)handle);
+
+		mutex_unlock(&mm->lock);
+		return -ENOENT;
+	}
+
+	if (DEBUGGER_EN(MM)) {
+		rga_buf_log(internal_buffer, "release buffer:\n");
+		rga_mm_dump_buffer(internal_buffer);
+	}
+
+	kref_put(&internal_buffer->refcount, rga_mm_kref_release_buffer);
+
+	if (DEBUGGER_EN(TIME))
+		rga_log("handle[%d]: release buffer cost %lld us\n",
+			handle, ktime_us_delta(ktime_get(), timestamp));
+
+	mutex_unlock(&mm->lock);
+
+	return 0;
+}
+
+int rga_mm_session_release_buffer(struct rga_session *session)
+{
+	int i;
+	struct rga_mm *mm;
+	struct rga_internal_buffer *buffer;
+
+	mm = rga_drvdata->mm;
+	if (mm == NULL) {
+		rga_err("rga mm is null!\n");
+		return -EFAULT;
+	}
+
+	mutex_lock(&mm->lock);
+
+	idr_for_each_entry(&mm->memory_idr, buffer, i) {
+		if (session == buffer->session) {
+			rga_err("[tgid:%d] Destroy handle[%d] when the user exits\n",
+			       session->tgid, buffer->handle);
+			rga_mm_force_releaser_buffer(buffer);
+		}
+	}
+
+	mutex_unlock(&mm->lock);
+	return 0;
+}
+
+int rga_mm_init(struct rga_mm **mm_session)
+{
+	struct rga_mm *mm = NULL;
+
+	*mm_session = kzalloc(sizeof(struct rga_mm), GFP_KERNEL);
+	if (*mm_session == NULL) {
+		pr_err("can not kzalloc for rga buffer mm_session\n");
+		return -ENOMEM;
+	}
+
+	mm = *mm_session;
+
+	mutex_init(&mm->lock);
+	idr_init_base(&mm->memory_idr, 1);
+
+	return 0;
+}
+
+int rga_mm_remove(struct rga_mm **mm_session)
+{
+	struct rga_mm *mm = *mm_session;
+
+	mutex_lock(&mm->lock);
+
+	idr_for_each(&mm->memory_idr, &rga_mm_buffer_destroy_for_idr, mm);
+	idr_destroy(&mm->memory_idr);
+
+	mutex_unlock(&mm->lock);
+
+	kfree(*mm_session);
+	*mm_session = NULL;
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/rga3/rga_policy.c b/drivers/video/rockchip/rga3/rga_policy.c
new file mode 100644
index 0000000000000..b8fbfdd4a95ce
--- /dev/null
+++ b/drivers/video/rockchip/rga3/rga_policy.c
@@ -0,0 +1,493 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#include "rga_job.h"
+#include "rga_common.h"
+#include "rga_hw_config.h"
+#include "rga_debugger.h"
+
+#define GET_GCD(n1, n2) \
+	({ \
+		int i; \
+		int gcd = 1; \
+		for (i = 1; i <= (n1) && i <= (n2); i++) { \
+			if ((n1) % i == 0 && (n2) % i == 0) \
+				gcd = i; \
+		} \
+		gcd; \
+	})
+#define GET_LCM(n1, n2, gcd) (((n1) * (n2)) / gcd)
+
+static int rga_set_feature(struct rga_req *rga_base)
+{
+	int feature = 0;
+
+	if (rga_base->render_mode == COLOR_FILL_MODE)
+		feature |= RGA_COLOR_FILL;
+
+	if (rga_base->render_mode == COLOR_PALETTE_MODE)
+		feature |= RGA_COLOR_PALETTE;
+
+	if (rga_base->color_key_max > 0 || rga_base->color_key_min > 0)
+		feature |= RGA_COLOR_KEY;
+
+	if ((rga_base->alpha_rop_flag >> 1) & 1)
+		feature |= RGA_ROP_CALCULATE;
+
+	if ((rga_base->alpha_rop_flag >> 8) & 1)
+		feature |= RGA_NN_QUANTIZE;
+
+	return feature;
+}
+
+static bool rga_check_csc_constant(const struct rga_hw_data *data, struct rga_req *rga_base,
+				   uint32_t mode, uint32_t flag)
+{
+	if (mode & flag)
+		return true;
+
+	if ((rga_base->full_csc.flag & 0x1) && (data->feature & RGA_FULL_CSC))
+		return true;
+
+	return false;
+}
+
+static bool rga_check_csc(const struct rga_hw_data *data, struct rga_req *rga_base)
+{
+	switch (rga_base->yuv2rgb_mode) {
+	case 0x1:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_y2r_mode, RGA_MODE_CSC_BT601L);
+	case 0x2:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_y2r_mode, RGA_MODE_CSC_BT601F);
+	case 0x3:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_y2r_mode, RGA_MODE_CSC_BT709);
+	case 0x1 << 2:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_r2y_mode, RGA_MODE_CSC_BT601F);
+	case 0x2 << 2:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_r2y_mode, RGA_MODE_CSC_BT601L);
+	case 0x3 << 2:
+		return rga_check_csc_constant(data, rga_base,
+					      data->csc_r2y_mode, RGA_MODE_CSC_BT709);
+	default:
+		break;
+	}
+
+	if ((rga_base->full_csc.flag & 0x1)) {
+		if (data->feature & RGA_FULL_CSC)
+			return true;
+		else
+			return false;
+	}
+
+	return true;
+}
+
+static bool rga_check_resolution(const struct rga_rect_range *range, int width, int height)
+{
+	if (width > range->max.width || height > range->max.height)
+		return false;
+
+	if (width < range->min.width || height < range->min.height)
+		return false;
+
+	return true;
+}
+
+static bool rga_check_format(const struct rga_hw_data *data,
+		int rd_mode, int format, int win_num)
+{
+	int i;
+	const uint32_t *formats;
+	uint32_t format_count;
+
+	switch (rd_mode) {
+	case RGA_RASTER_MODE:
+		formats = data->win[win_num].formats[RGA_RASTER_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_RASTER_INDEX];
+		break;
+	case RGA_FBC_MODE:
+		formats = data->win[win_num].formats[RGA_AFBC16x16_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_AFBC16x16_INDEX];
+		break;
+	case RGA_TILE_MODE:
+		formats = data->win[win_num].formats[RGA_TILE8x8_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_TILE8x8_INDEX];
+		break;
+	case RGA_TILE4x4_MODE:
+		formats = data->win[win_num].formats[RGA_TILE4x4_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_TILE4x4_INDEX];
+		break;
+	case RGA_RKFBC_MODE:
+		formats = data->win[win_num].formats[RGA_RKFBC64x4_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_RKFBC64x4_INDEX];
+		break;
+	case RGA_AFBC32x8_MODE:
+		formats = data->win[win_num].formats[RGA_AFBC32x8_INDEX];
+		format_count = data->win[win_num].formats_count[RGA_AFBC32x8_INDEX];
+		break;
+	default:
+		return false;
+	}
+
+	if (formats == NULL || format_count == 0)
+		return false;
+
+	for (i = 0; i < format_count; i++)
+		if (format == formats[i])
+			return true;
+
+	return false;
+}
+
+static bool rga_check_align(struct rga_job *job,
+			    uint32_t byte_stride_align, uint32_t format, uint16_t w_stride)
+{
+	int bit_stride, pixel_stride, align, gcd;
+
+	pixel_stride = rga_get_pixel_stride_from_format(format);
+	if (pixel_stride <= 0)
+		return false;
+
+	bit_stride = pixel_stride * w_stride;
+
+	if (bit_stride % (byte_stride_align * 8) == 0)
+		return true;
+
+	if (DEBUGGER_EN(MSG)) {
+		gcd = GET_GCD(pixel_stride, byte_stride_align * 8);
+		align = GET_LCM(pixel_stride, byte_stride_align * 8, gcd) / pixel_stride;
+		rga_job_log(job, "unsupported width stride %d, 0x%x should be %d aligned!",
+			w_stride, format, align);
+	}
+
+	return false;
+}
+
+static bool rga_check_channel(struct rga_job *job, const struct rga_hw_data *data,
+			      struct rga_img_info_t *img,
+			      const char *name, int input, int win_num)
+{
+	const struct rga_rect_range *range;
+
+	if (input)
+		range = &data->input_range;
+	else
+		range = &data->output_range;
+
+	if (!rga_check_resolution(range, img->act_w, img->act_h)) {
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "%s resolution check error, input range[%dx%d ~ %dx%d], [w,h] = [%d, %d]\n",
+				name,
+				data->input_range.min.width, data->input_range.min.height,
+				data->input_range.max.width, data->input_range.max.height,
+				img->act_w, img->act_h);
+
+		return false;
+	}
+
+	if (data == &rga3_data &&
+	    !rga_check_resolution(&data->input_range,
+				  img->act_w + img->x_offset,
+				  img->act_h + img->y_offset)) {
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "%s RGA3 resolution check error, input range[%dx%d ~ %dx%d], [w+x,h+y] = [%d, %d]\n",
+				name,
+				data->input_range.min.width, data->input_range.min.height,
+				data->input_range.max.width, data->input_range.max.height,
+				img->act_w + img->x_offset,
+				img->act_h + img->y_offset);
+		return false;
+	}
+
+	if (!rga_check_format(data, img->rd_mode, img->format, win_num)) {
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "%s format check error, mode = %#x, format = %#x\n",
+				name, img->rd_mode, img->format);
+		return false;
+	}
+
+	if (!rga_check_align(job, data->byte_stride_align, img->format, img->vir_w)) {
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "%s align check error, byte_stride_align[%d], format[%#x], vir_w[%d]\n",
+				name, data->byte_stride_align, img->format, img->vir_w);
+		return false;
+	}
+
+	return true;
+}
+
+static bool rga_check_scale(struct rga_job *job, const struct rga_hw_data *data,
+			    struct rga_req *rga_base)
+{
+	struct rga_img_info_t *src0 = &rga_base->src;
+	struct rga_img_info_t *dst = &rga_base->dst;
+
+	int sw, sh;
+	int dw, dh;
+
+	sw = src0->act_w;
+	sh = src0->act_h;
+	dw = dst->act_w;
+	dh = dst->act_h;
+
+	if (sw > dw) {
+		if ((sw >> data->max_downscale_factor) > dw)
+			goto check_error;
+	} else if (sw < dw) {
+		if ((sw << data->max_upscale_factor) < dw)
+			goto check_error;
+	}
+
+	if (sh > dh) {
+		if ((sh >> data->max_downscale_factor) > dh)
+			goto check_error;
+	} else if (sh < dh) {
+		if ((sh << data->max_upscale_factor) < dh)
+			goto check_error;
+	}
+
+	return true;
+check_error:
+	if (DEBUGGER_EN(MSG))
+		rga_job_log(job, "scale check error, scale limit[1/%d ~ %d], src[%d, %d], dst[%d, %d]\n",
+			(1 << data->max_downscale_factor), (1 << data->max_upscale_factor),
+			sw, sh, dw, dh);
+
+	return false;
+}
+
+static bool rga_check_rotate(struct rga_job *job, const struct rga_hw_data *data,
+			    struct rga_req *rga_base)
+{
+	/* rot-90 and rot-270 */
+	if (((rga_base->rotate_mode & 0x0f) == 1) &&
+	    ((rga_base->sina == 65536 && rga_base->cosa == 0) ||
+	     (rga_base->sina == -65536 && rga_base->cosa == 0))) {
+		if (data == &rga3_data &&
+		    (rga_is_yuv422_packed_format(rga_base->src.format) ||
+		     rga_is_yuv422_semi_planar_format(rga_base->src.format))) {
+			if (DEBUGGER_EN(MSG))
+				rga_job_log(job, "rotate check error, RGA3 unsupported YUV422 rotate 90/270, format[%s(%#x)]\n",
+					    rga_get_format_name(rga_base->src.format),
+					    rga_base->src.format);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+int rga_job_assign(struct rga_job *job)
+{
+	struct rga_img_info_t *src0 = &job->rga_command_base.src;
+	struct rga_img_info_t *src1 = &job->rga_command_base.pat;
+	struct rga_img_info_t *dst = &job->rga_command_base.dst;
+
+	struct rga_req *rga_base = &job->rga_command_base;
+	const struct rga_hw_data *data;
+	struct rga_scheduler_t *scheduler = NULL;
+
+	int feature;
+	int core = RGA_NONE_CORE;
+	int optional_cores = RGA_NONE_CORE;
+	int specified_cores = RGA_NONE_CORE;
+	int i;
+	int min_of_job_count = -1;
+	unsigned long flags;
+
+	/* assigned by userspace */
+	if (rga_base->core > RGA_NONE_CORE) {
+		if (rga_base->core > RGA_CORE_MASK) {
+			rga_job_err(job, "invalid setting core by user\n");
+			return -1;
+		} else if (rga_base->core & RGA_CORE_MASK)
+			specified_cores = rga_base->core;
+	}
+
+	feature = rga_set_feature(rga_base);
+
+	/* function */
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		data = rga_drvdata->scheduler[i]->data;
+		scheduler = rga_drvdata->scheduler[i];
+
+		if ((specified_cores != RGA_NONE_CORE) &&
+			(!(scheduler->core & specified_cores)))
+			continue;
+
+		if (DEBUGGER_EN(MSG))
+			rga_job_log(job, "start policy on %s(%#x)",
+				rga_get_core_name(scheduler->core), scheduler->core);
+
+		if (scheduler->data->mmu == RGA_MMU &&
+		    job->flags & RGA_JOB_UNSUPPORT_RGA_MMU) {
+			if (DEBUGGER_EN(MSG))
+				rga_job_log(job, "RGA2 only support under 4G memory!\n");
+			continue;
+		}
+
+		if (feature > 0) {
+			if (!(feature & data->feature)) {
+				if (DEBUGGER_EN(MSG))
+					rga_job_log(job, "%s(%#x), break on feature\n",
+						rga_get_core_name(scheduler->core),
+						scheduler->core);
+				continue;
+			}
+		}
+
+		/* only colorfill need single win (colorpalette?) */
+		if (!(feature & 1)) {
+			if (src1->yrgb_addr > 0) {
+				if (!(src0->rd_mode & data->win[0].rd_mode)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), src0 break on %s(%#x)\n",
+							rga_get_core_name(scheduler->core),
+							scheduler->core,
+							rga_get_store_mode_str(src0->rd_mode),
+							src0->rd_mode);
+					continue;
+				}
+
+				if (!(src1->rd_mode & data->win[1].rd_mode)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), src1 break on %s(%#x)\n",
+							rga_get_core_name(scheduler->core),
+							scheduler->core,
+							rga_get_store_mode_str(src1->rd_mode),
+							src1->rd_mode);
+					continue;
+				}
+
+				if (!(dst->rd_mode & data->win[2].rd_mode)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), dst break on %s(%#x)\n",
+							rga_get_core_name(scheduler->core),
+							scheduler->core,
+							rga_get_store_mode_str(dst->rd_mode),
+							dst->rd_mode);
+					continue;
+				}
+			} else {
+				if (!(src0->rd_mode & data->win[0].rd_mode)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), src break on %s(%#x)\n",
+							rga_get_core_name(scheduler->core),
+							scheduler->core,
+							rga_get_store_mode_str(src0->rd_mode),
+							src0->rd_mode);
+					continue;
+				}
+
+				if (!(dst->rd_mode & data->win[2].rd_mode)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), dst break on %s(%#x)\n",
+							rga_get_core_name(scheduler->core),
+							scheduler->core,
+							rga_get_store_mode_str(dst->rd_mode),
+							dst->rd_mode);
+					continue;
+				}
+			}
+
+			if (!rga_check_scale(job, data, rga_base)) {
+				if (DEBUGGER_EN(MSG))
+					rga_job_log(job, "%s(%#x), break on rga_check_scale",
+						rga_get_core_name(scheduler->core),
+						scheduler->core);
+				continue;
+			}
+
+			if (!rga_check_channel(job, data, src0, "src0", true, 0)) {
+				if (DEBUGGER_EN(MSG))
+					rga_job_log(job, "%s(%#x), break on src0",
+						rga_get_core_name(scheduler->core),
+						scheduler->core);
+				continue;
+			}
+
+			if (src1->yrgb_addr > 0) {
+				if (!rga_check_channel(job, data, src1, "src1", true, 1)) {
+					if (DEBUGGER_EN(MSG))
+						rga_job_log(job, "%s(%#x), break on src1",
+							rga_get_core_name(scheduler->core),
+							scheduler->core);
+					continue;
+				}
+			}
+		}
+
+		if (!rga_check_channel(job, data, dst, "dst", false, 2)) {
+			if (DEBUGGER_EN(MSG))
+				rga_job_log(job, "%s(%#x), break on dst",
+					rga_get_core_name(scheduler->core),
+					scheduler->core);
+			continue;
+		}
+
+		if (!rga_check_csc(data, rga_base)) {
+			if (DEBUGGER_EN(MSG))
+				rga_job_log(job, "%s(%#x), break on rga_check_csc",
+					rga_get_core_name(scheduler->core),
+					scheduler->core);
+			continue;
+		}
+
+		if (!rga_check_rotate(job, data, rga_base)) {
+			if (DEBUGGER_EN(MSG))
+				rga_job_log(job, "%s(%#x), break on rga_check_rotate",
+					rga_get_core_name(scheduler->core),
+					scheduler->core);
+			continue;
+		}
+
+		optional_cores |= scheduler->core;
+	}
+
+	if (optional_cores == 0) {
+		rga_job_err(job, "no core match\n");
+		return -1;
+	}
+
+	for (i = 0; i < rga_drvdata->num_of_scheduler; i++) {
+		scheduler = rga_drvdata->scheduler[i];
+
+		if (optional_cores & scheduler->core) {
+			spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+			if (scheduler->running_job == NULL) {
+				core = scheduler->core;
+				job->scheduler = scheduler;
+				spin_unlock_irqrestore(&scheduler->irq_lock,
+							 flags);
+				break;
+			} else {
+				if ((min_of_job_count == -1) ||
+				    (min_of_job_count > scheduler->job_count)) {
+					min_of_job_count = scheduler->job_count;
+					core = scheduler->core;
+					job->scheduler = scheduler;
+				}
+			}
+
+			spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		}
+	}
+
+	/* TODO: need consider full load */
+	if (DEBUGGER_EN(MSG))
+		rga_job_log(job, "matched cores = %#x, assign core: %s(%#x)\n",
+			optional_cores,
+			rga_get_core_name(core), core);
+
+	return core;
+}
diff --git a/drivers/video/rockchip/rve/Kconfig b/drivers/video/rockchip/rve/Kconfig
new file mode 100644
index 0000000000000..d28b9d0599cf1
--- /dev/null
+++ b/drivers/video/rockchip/rve/Kconfig
@@ -0,0 +1,29 @@
+# SPDX-License-Identifier: GPL-2.0
+menuconfig ROCKCHIP_RVE
+	tristate "RVE"
+	depends on ARCH_ROCKCHIP
+	help
+	  RVE module.
+
+if ROCKCHIP_RVE
+
+config ROCKCHIP_RVE_PROC_FS
+	bool "Enable RVE procfs"
+	select ROCKCHIP_RVE_DEBUGGER
+	depends on PROC_FS
+	help
+	  Enable procfs to debug RVE driver.
+
+config ROCKCHIP_RVE_DEBUG_FS
+	bool "Enable RVE debugfs"
+	select ROCKCHIP_RVE_DEBUGGER
+	depends on DEBUG_FS
+	help
+	  Enable debugfs to debug RVE driver.
+
+config ROCKCHIP_RVE_DEBUGGER
+	bool
+	help
+	  Enabling the debugger of RVE, you can use procfs and debugfs for debugging.
+
+endif
diff --git a/drivers/video/rockchip/rve/Makefile b/drivers/video/rockchip/rve/Makefile
new file mode 100644
index 0000000000000..e475b77175100
--- /dev/null
+++ b/drivers/video/rockchip/rve/Makefile
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0
+
+ccflags-y += -I$(srctree)/$(src)/include
+
+rve-y	:= rve_drv.o rve_job.o rve_reg.o
+rve-$(CONFIG_ROCKCHIP_RVE_DEBUGGER) += rve_debugger.o
+rve-$(CONFIG_SYNC_FILE) += rve_fence.o
+
+obj-$(CONFIG_ROCKCHIP_RVE)	+= rve.o
diff --git a/drivers/video/rockchip/rve/include/rve.h b/drivers/video/rockchip/rve/include/rve.h
new file mode 100644
index 0000000000000..922e9e3bfb478
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve.h
@@ -0,0 +1,72 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+#ifndef _RVE_DRIVER_H_
+#define _RVE_DRIVER_H_
+
+#include <linux/mutex.h>
+#include <linux/scatterlist.h>
+
+/* Use 'r' as magic number */
+#define RVE_IOC_MAGIC		'r'
+#define RVE_IOW(nr, type)	_IOW(RVE_IOC_MAGIC, nr, type)
+#define RVE_IOR(nr, type)	_IOR(RVE_IOC_MAGIC, nr, type)
+#define RVE_IOWR(nr, type)	_IOWR(RVE_IOC_MAGIC, nr, type)
+
+#define RVE_IOC_GET_VER				RVE_IOR(0x1, struct rve_version_t)
+#define RVE_IOC_GET_HW_VER			RVE_IOR(0x2, struct rve_hw_versions_t)
+#define RVE_IOC_IMPORT_BUFFER		RVE_IOWR(0x3, struct rve_buffer_pool)
+#define RVE_IOC_RELEASE_BUFFER		RVE_IOW(0x4, struct rve_buffer_pool)
+
+#define RVE_IOC_START_CONFIG		RVE_IOR(0x5, uint32_t)
+#define RVE_IOC_END_CONFIG			RVE_IOWR(0x6, struct rve_user_ctx_t)
+#define RVE_IOC_CMD_CONFIG			RVE_IOWR(0x7, struct rve_user_ctx_t)
+#define RVE_IOC_CANCEL_CONFIG		RVE_IOWR(0x8, uint32_t)
+
+#define RVE_CMD_NUM_MAX 10
+
+#define RVE_BUFFER_POOL_SIZE_MAX 40
+
+enum rve_memory_type {
+	RVE_DMA_BUFFER = 0,
+	RVE_VIRTUAL_ADDRESS,
+	RVE_PHYSICAL_ADDRESS
+};
+
+#define RVE_SCHED_PRIORITY_DEFAULT 0
+#define RVE_SCHED_PRIORITY_MAX 6
+
+#define RVE_VERSION_SIZE	16
+#define RVE_HW_SIZE		5
+
+struct rve_version_t {
+	uint32_t major;
+	uint32_t minor;
+	uint32_t revision;
+	uint32_t prod_num;
+	uint8_t str[RVE_VERSION_SIZE];
+};
+
+struct rve_hw_versions_t {
+	struct rve_version_t version[RVE_HW_SIZE];
+	uint32_t size;
+};
+
+struct rve_user_ctx_t {
+	uint32_t header;
+	uint64_t regcmd_data;
+	int32_t in_fence_fd;
+	int32_t out_fence_fd;
+	int32_t cmd_num;
+	uint32_t id;
+	uint8_t priority;
+	uint32_t sync_mode;
+	uint32_t disable_auto_cancel;
+
+	uint32_t reserve[31];
+};
+
+#endif /*_RVE_DRIVER_H_*/
diff --git a/drivers/video/rockchip/rve/include/rve_debugger.h b/drivers/video/rockchip/rve/include/rve_debugger.h
new file mode 100644
index 0000000000000..8dbb46850532a
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve_debugger.h
@@ -0,0 +1,132 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *	Cerf Yu <cerf.yu@rock-chips.com>
+ *	Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef _RVE_DEBUGGER_H_
+#define _RVE_DEBUGGER_H_
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER
+
+extern int RVE_DEBUG_MONITOR;
+extern int RVE_DEBUG_REG;
+extern int RVE_DEBUG_MSG;
+extern int RVE_DEBUG_TIME;
+extern int RVE_DEBUG_CHECK_MODE;
+extern int RVE_DEBUG_NONUSE;
+extern int RVE_DEBUG_INT_FLAG;
+
+#define DEBUGGER_EN(name) (unlikely(RVE_DEBUG_##name ? true : false))
+
+/*
+ * struct rve_debugger - RVE debugger information
+ *
+ * This structure represents a debugger to be created by the rve driver
+ * or core.
+ */
+struct rve_debugger {
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS
+	/* Directory of debugfs file */
+	struct dentry *debugfs_dir;
+	struct list_head debugfs_entry_list;
+	struct mutex debugfs_lock;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RVE_PROC_FS
+	/* Directory of procfs file */
+	struct proc_dir_entry *procfs_dir;
+	struct list_head procfs_entry_list;
+	struct mutex procfs_lock;
+#endif
+};
+
+/*
+ * struct rve_debugger_list - debugfs/procfs info list entry
+ *
+ * This structure represents a debugfs/procfs file to be created by the rve
+ * driver or core.
+ */
+struct rve_debugger_list {
+	/* File name */
+	const char *name;
+	/*
+	 * Show callback. &seq_file->private will be set to the &struct
+	 * rve_debugger_node corresponding to the instance of this info
+	 * on a given &struct rve_debugger.
+	 */
+	int (*show)(struct seq_file *seq, void *data);
+	/*
+	 * Write callback. &seq_file->private will be set to the &struct
+	 * rve_debugger_node corresponding to the instance of this info
+	 * on a given &struct rve_debugger.
+	 */
+	ssize_t (*write)(struct file *file, const char __user *ubuf,
+		size_t len, loff_t *offp);
+	/* Procfs/Debugfs private data. */
+	void *data;
+};
+
+/*
+ * struct rve_debugger_node - Nodes for debugfs/procfs
+ *
+ * This structure represents each instance of procfs/debugfs created from the
+ * template.
+ */
+struct rve_debugger_node {
+	struct rve_debugger *debugger;
+
+	/* template for this node. */
+	const struct rve_debugger_list *info_ent;
+
+	/* Each Procfs/Debugfs file. */
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS
+	struct dentry *dent;
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RVE_PROC_FS
+	struct proc_dir_entry *pent;
+#endif
+
+	struct list_head list;
+};
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS
+int rve_debugfs_init(void);
+int rve_debugfs_remove(void);
+#else
+static inline int rve_debugfs_remove(void)
+{
+	return 0;
+}
+static inline int rve_debugfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RVE_PROC_FS
+int rve_procfs_remove(void);
+int rve_procfs_init(void);
+#else
+static inline int rve_procfs_remove(void)
+{
+	return 0;
+}
+static inline int rve_procfs_init(void)
+{
+	return 0;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RVE_PROC_FS */
+
+#else
+
+#define DEBUGGER_EN(name) (unlikely(false))
+
+#endif /* #ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER */
+
+#endif /* #ifndef _RVE_DEBUGGER_H_ */
+
diff --git a/drivers/video/rockchip/rve/include/rve_drv.h b/drivers/video/rockchip/rve/include/rve_drv.h
new file mode 100644
index 0000000000000..9220b97247727
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve_drv.h
@@ -0,0 +1,332 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RVE_DRV_H_
+#define __LINUX_RVE_DRV_H_
+
+#include <linux/clk.h>
+#include <linux/completion.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-buf-cache.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/fb.h>
+#include <linux/fdtable.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
+#include <linux/kref.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/regulator/consumer.h>
+#include <linux/scatterlist.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/syscalls.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/uaccess.h>
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/wakelock.h>
+#include <linux/pm_runtime.h>
+#include <linux/sched/mm.h>
+
+#include <asm/cacheflush.h>
+
+#include <linux/iommu.h>
+#include <linux/iova.h>
+#include <linux/dma-map-ops.h>
+#include <linux/hrtimer.h>
+
+#include "rve_debugger.h"
+#include "rve.h"
+
+/* sample interval: 1000ms */
+#define RVE_LOAD_INTERVAL 1000000000
+
+/* Driver information */
+#define DRIVER_DESC		"RVE Device Driver"
+#define DRIVER_NAME		"rve"
+
+#define STR_HELPER(x) #x
+#define STR(x) STR_HELPER(x)
+
+#define RVE_MAJOR_VERSION_MASK		(0x0000FF00)
+#define RVE_MINOR_VERSION_MASK		(0x000000FF)
+#define RVE_PROD_NUM_MASK				(0xFFFF0000)
+
+#define DRIVER_MAJOR_VERSION		1
+#define DRIVER_MINOR_VERSION		0
+#define DRIVER_REVISION_VERSION		5
+
+#define DRIVER_VERSION (STR(DRIVER_MAJOR_VERSION) "." STR(DRIVER_MINOR_VERSION) \
+			"." STR(DRIVER_REVISION_VERSION))
+
+/* time limit */
+#define RVE_ASYNC_TIMEOUT_DELAY		500
+#define RVE_SYNC_TIMEOUT_DELAY		HZ
+#define RVE_RESET_TIMEOUT			10000
+
+#define RVE_BUFFER_POOL_MAX_SIZE	64
+#define RVE_MAX_SCHEDULER 1
+
+#define RVE_MAX_BUS_CLK 10
+#define RVE_MAX_PID_INFO 10
+
+extern struct rve_drvdata_t *rve_drvdata;
+
+enum {
+	RVE_SCHEDULER_CORE0		= 1,
+	RVE_NONE_CORE			 = 0,
+};
+
+enum {
+	RVE_CMD_SLAVE		= 1,
+	RVE_CMD_MASTER		= 2,
+};
+
+struct rve_fence_context {
+	unsigned int context;
+	unsigned int seqno;
+	spinlock_t spinlock;
+};
+
+struct rve_fence_waiter {
+	/* Base sync driver waiter structure */
+	struct dma_fence_cb waiter;
+
+	struct rve_job *job;
+};
+
+struct rve_scheduler_t;
+struct rve_internal_ctx_t;
+
+struct rve_session {
+	int id;
+
+	pid_t tgid;
+};
+
+struct rve_job {
+	struct list_head head;
+	struct rve_scheduler_t *scheduler;
+	struct rve_session *session;
+
+	struct rve_cmd_reg_array_t *regcmd_data;
+
+	struct rve_internal_ctx_t *ctx;
+
+	/* for rve virtual_address */
+	struct mm_struct *mm;
+
+	struct dma_fence *out_fence;
+	struct dma_fence *in_fence;
+	spinlock_t fence_lock;
+	ktime_t timestamp;
+	ktime_t hw_running_time;
+	ktime_t hw_recoder_time;
+	unsigned int flags;
+
+	int priority;
+	int core;
+	int ret;
+	pid_t pid;
+};
+
+struct rve_backend_ops {
+	int (*get_version)(struct rve_scheduler_t *scheduler);
+	int (*set_reg)(struct rve_job *job, struct rve_scheduler_t *scheduler);
+	int (*init_reg)(struct rve_job *job);
+	void (*soft_reset)(struct rve_scheduler_t *scheduler);
+};
+
+struct rve_timer {
+	u32 busy_time;
+	u32 busy_time_record;
+};
+
+struct rve_sche_pid_info_t {
+	pid_t pid;
+	/* hw total use time, per hrtimer */
+	u32 hw_time_total;
+
+	uint32_t last_job_rd_bandwidth;
+	uint32_t last_job_wr_bandwidth;
+	uint32_t last_job_cycle_cnt;
+};
+
+struct rve_sche_session_info_t {
+	struct rve_sche_pid_info_t pid_info[RVE_MAX_PID_INFO];
+
+	int pd_refcount;
+
+	/* the bandwidth of total read bytes, per hrtimer */
+	uint32_t rd_bandwidth;
+	/* the bandwidth of total write bytes, per hrtimer */
+	uint32_t wr_bandwidth;
+	/* the total running cycle of current frame, per hrtimer */
+	uint32_t cycle_cnt;
+	/* total interrupt count */
+	uint64_t total_int_cnt;
+};
+
+struct rve_scheduler_t {
+	struct device *dev;
+	void __iomem *rve_base;
+
+	struct clk *clks[RVE_MAX_BUS_CLK];
+	int num_clks;
+
+	struct rve_job *running_job;
+	struct list_head todo_list;
+	spinlock_t irq_lock;
+	wait_queue_head_t job_done_wq;
+	const struct rve_backend_ops *ops;
+	const struct rve_hw_data *data;
+	int job_count;
+	int irq;
+	struct rve_version_t version;
+	int core;
+
+	struct rve_timer timer;
+
+	struct rve_sche_session_info_t session;
+};
+
+struct rve_cmd_reg_array_t {
+	uint32_t cmd_reg[58];
+};
+
+struct rve_ctx_debug_info_t {
+	pid_t pid;
+	u32 timestamp;
+	/* hw total use time, per hrtimer */
+	u32 hw_time_total;
+	/* last job use time, per hrtimer*/
+	u32 last_job_use_time;
+	/* last job hardware use time, per hrtimer*/
+	u32 last_job_hw_use_time;
+	/* the most time-consuming job, per hrtimer */
+	u32 max_cost_time_per_sec;
+};
+
+struct rve_internal_ctx_t {
+	struct rve_scheduler_t *scheduler;
+	struct rve_session *session;
+
+	struct rve_cmd_reg_array_t *regcmd_data;
+	uint32_t cmd_num;
+
+	uint32_t sync_mode;
+	int flags;
+	int id;
+
+	uint32_t running_job_count;
+	uint32_t finished_job_count;
+	bool is_running;
+
+	uint32_t disable_auto_cancel;
+
+	int priority;
+	int32_t out_fence_fd;
+	int32_t in_fence_fd;
+
+	struct dma_fence *out_fence;
+
+	spinlock_t lock;
+	struct kref refcount;
+
+	/* debug info */
+	struct rve_ctx_debug_info_t debug_info;
+
+	/* TODO: add some common work */
+};
+
+struct rve_pending_ctx_manager {
+	spinlock_t lock;
+
+	/*
+	 * @ctx_id_idr:
+	 *
+	 * Mapping of ctx id to object pointers. Used by the GEM
+	 * subsystem. Protected by @lock.
+	 */
+	struct idr ctx_id_idr;
+
+	int ctx_count;
+};
+
+struct rve_session_manager {
+	struct mutex lock;
+
+	struct idr ctx_id_idr;
+
+	int session_cnt;
+};
+
+struct rve_drvdata_t {
+	struct rve_fence_context *fence_ctx;
+
+	/* used by rve2's mmu lock */
+	struct mutex lock;
+
+	struct rve_scheduler_t *scheduler[RVE_MAX_SCHEDULER];
+	int num_of_scheduler;
+
+	struct delayed_work power_off_work;
+	struct wake_lock wake_lock;
+
+	struct rve_mm *mm;
+
+	/* rve_job pending manager, import by RVE_IOC_START_CONFIG */
+	struct rve_pending_ctx_manager *pend_ctx_manager;
+
+	struct rve_session_manager *session_manager;
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER
+	struct rve_debugger *debugger;
+#endif
+};
+
+struct rve_irqs_data_t {
+	const char *name;
+	irqreturn_t (*irq_hdl)(int irq, void *ctx);
+	irqreturn_t (*irq_thread)(int irq, void *ctx);
+};
+
+struct rve_match_data_t {
+	const char * const *clks;
+	int num_clks;
+	const struct rve_irqs_data_t *irqs;
+	int num_irqs;
+};
+
+static inline int rve_read(int offset, struct rve_scheduler_t *scheduler)
+{
+	return readl(scheduler->rve_base + offset);
+}
+
+static inline void rve_write(int value, int offset, struct rve_scheduler_t *scheduler)
+{
+	writel(value, scheduler->rve_base + offset);
+}
+
+int rve_power_enable(struct rve_scheduler_t *scheduler);
+int rve_power_disable(struct rve_scheduler_t *scheduler);
+
+#endif /* __LINUX_RVE_FENCE_H_ */
diff --git a/drivers/video/rockchip/rve/include/rve_fence.h b/drivers/video/rockchip/rve/include/rve_fence.h
new file mode 100644
index 0000000000000..e70b6ac0442b8
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve_fence.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RVE_FENCE_H_
+#define __LINUX_RVE_FENCE_H_
+
+#ifdef CONFIG_SYNC_FILE
+
+#include "rve_drv.h"
+
+struct rve_fence_context *rve_fence_context_alloc(void);
+
+void rve_fence_context_free(struct rve_fence_context *fence_ctx);
+
+int rve_out_fence_alloc(struct rve_job *job);
+
+int rve_out_fence_get_fd(struct rve_job *job);
+
+struct dma_fence *rve_get_input_fence(int in_fence_fd);
+
+int rve_wait_input_fence(struct dma_fence *in_fence);
+
+int rve_add_dma_fence_callback(struct rve_job *job,
+	struct dma_fence *in_fence, dma_fence_func_t func);
+
+#endif
+
+#endif /* __LINUX_RVE_FENCE_H_ */
diff --git a/drivers/video/rockchip/rve/include/rve_job.h b/drivers/video/rockchip/rve/include/rve_job.h
new file mode 100644
index 0000000000000..b2a86f317e5f8
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve_job.h
@@ -0,0 +1,53 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#ifndef __LINUX_RKRVE_JOB_H_
+#define __LINUX_RKRVE_JOB_H_
+
+#include <linux/spinlock.h>
+#include <linux/dma-fence.h>
+
+#include "rve_drv.h"
+
+enum job_flags {
+	RVE_JOB_DONE			= 1 << 0,
+	RVE_ASYNC			= 1 << 1,
+	RVE_SYNC			= 1 << 2,
+	RVE_JOB_USE_HANDLE		= 1 << 3,
+	RVE_JOB_UNSUPPORT_RVE2		= 1 << 4,
+};
+
+struct rve_scheduler_t *rve_job_get_scheduler(struct rve_job *job);
+struct rve_internal_ctx_t *rve_job_get_internal_ctx(struct rve_job *job);
+
+void rve_job_done(struct rve_scheduler_t *rve_scheduler, int ret);
+int rve_job_commit(struct rve_internal_ctx_t *ctx);
+
+int rve_job_config_by_user_ctx(struct rve_user_ctx_t *user_ctx);
+int rve_job_commit_by_user_ctx(struct rve_user_ctx_t *user_ctx);
+int rve_job_cancel_by_user_ctx(uint32_t ctx_id);
+
+void rve_job_session_destroy(struct rve_session *session);
+
+int rve_ctx_manager_init(struct rve_pending_ctx_manager **ctx_manager_session);
+int rve_ctx_manager_remove(struct rve_pending_ctx_manager **ctx_manager_session);
+
+int rve_internal_ctx_alloc_to_get_idr_id(struct rve_session *session);
+void rve_internal_ctx_kref_release(struct kref *ref);
+
+int rve_internal_ctx_signal(struct rve_job *job);
+
+struct rve_internal_ctx_t *
+rve_internal_ctx_lookup(struct rve_pending_ctx_manager *ctx_manager, uint32_t id);
+
+struct rve_job *
+rve_scheduler_get_pending_job_list(struct rve_scheduler_t *scheduler);
+
+struct rve_job *
+rve_scheduler_get_running_job(struct rve_scheduler_t *scheduler);
+
+#endif /* __LINUX_RKRVE_JOB_H_ */
diff --git a/drivers/video/rockchip/rve/include/rve_reg.h b/drivers/video/rockchip/rve/include/rve_reg.h
new file mode 100644
index 0000000000000..05df28037a83a
--- /dev/null
+++ b/drivers/video/rockchip/rve/include/rve_reg.h
@@ -0,0 +1,88 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __RVE_REG_H__
+#define __RVE_REG_H__
+
+#include "rve_drv.h"
+
+/* sys reg */
+#define RVE_SWREG0_IVE_VERSION            0x000
+#define RVE_SWREG1_IVE_IRQ                0x004
+#define RVE_SWREG2_IRQ_CTRL               0x008
+#define RVE_SWREG3_IVE_IDLE_PRC_STA       0x00c
+#define RVE_SWREG4_IVE_FORCE_IDLE_WBASE   0x010
+#define RVE_SWREG5_IVE_IDLE_CTRL          0x014
+#define RVE_SWREG6_IVE_WORK_STA           0x018
+#define RVE_SWREG7_IVE_SWAP               0x01c
+
+/* llp reg */
+#define RVE_SWLTB0_START_BASE             0x100
+#define RVE_SWLTB1_CTRL                   0x104
+#define RVE_SWLTB2_CFG_DONE               0x108
+#define RVE_SWLTB3_ENABLE                 0x10c
+#define RVE_SWLTB4_PAUSE_CTRL             0x110
+#define RVE_SWLTB5_DECODED_NUM            0x114
+#define RVE_SWLTB6_SKIP_NUM               0x118
+#define RVE_SWLTB7_TOTAL_NUM              0x11c
+#define RVE_SWLTB8_LAST_FRAME_BASE        0x120
+#define RVE_SWLTB9_LAST_IDX               0x124
+
+/* op reg */
+#define RVE_SWCFG0_EN                     0x200
+#define RVE_SWCFG4_OPERATOR               0x210
+#define RVE_SWCFG5_CTRL                   0x214
+#define RVE_SWCFG6_TIMEOUT_THRESH         0x218
+#define RVE_SWCFG7_DDR_CTRL               0x21c
+#define RVE_SWCFG9_PIC_INFO               0x224
+#define RVE_SWCFG10_HOR_STRIDE0           0x228
+#define RVE_SWCFG11_HOR_STRIDE1           0x22c
+#define RVE_SWCFG12_SRC0_BASE             0x230
+#define RVE_SWCFG13_SRC1_BASE             0x234
+#define RVE_SWCFG14_SRC2_BASE             0x238
+#define RVE_SWCFG15_SRC3_BASE             0x23c
+#define RVE_SWCFG16_DST0_BASE             0x240
+#define RVE_SWCFG17_DST1_BASE             0x244
+#define RVE_SWCFG18_DST2_BASE             0x248
+#define RVE_SWCFG20_OP_CTRL0              0x250
+#define RVE_SWCFG21_OP_CTRL1              0x254
+#define RVE_SWCFG22_OP_CTRL2              0x258
+#define RVE_SWCFG23_OP_CTRL3              0x25c
+#define RVE_SWCFG24_OP_CTRL4              0x260
+#define RVE_SWCFG25_OP_CTRL5              0x264
+#define RVE_SWCFG26_OP_CTRL6              0x268
+#define RVE_SWCFG27_OP_CTRL7              0x26c
+#define RVE_SWCFG28_OP_CTRL8              0x270
+#define RVE_SWCFG29_OP_CTRL9              0x274
+
+/* monitor reg */
+#define RVE_SWCFG32_MONITOR_CTRL0         0x280
+#define RVE_SWCFG33_MONITOR_CTRL1         0x284
+#define RVE_SWCFG34_MONITOR_INFO0         0x288
+#define RVE_SWCFG35_MONITOR_INFO1         0x28c
+#define RVE_SWCFG36_MONITOR_INFO2         0x290
+#define RVE_SWCFG37_MONITOR_INFO3         0x294
+#define RVE_SWCFG38_MONITOR_INFO4         0x298
+#define RVE_SWCFG39_MONITOR_INFO5         0x29c
+
+/* mmu reg */
+
+/* common reg */
+#define RVE_SYS_REG                       0x000
+#define RVE_LTB_REG                       0x100
+#define RVE_CFG_REG                       0x200
+#define RVE_MMU_REG                       0x300
+
+/* mode value */
+#define RVE_LLP_MODE                      0x8000
+#define RVE_LLP_DONE                      0x10
+#define RVE_CLEAR_UP_REG6_WROK_STA        0xff0000
+
+void rve_soft_reset(struct rve_scheduler_t *scheduler);
+int rve_set_reg(struct rve_job *job, struct rve_scheduler_t *scheduler);
+int rve_init_reg(struct rve_job *job);
+int rve_get_version(struct rve_scheduler_t *scheduler);
+
+void rve_dump_read_back_reg(struct rve_scheduler_t *scheduler);
+void rve_get_monitor_info(struct rve_job *job);
+
+#endif
+
diff --git a/drivers/video/rockchip/rve/rve_debugger.c b/drivers/video/rockchip/rve/rve_debugger.c
new file mode 100644
index 0000000000000..1e75dfb5e26a6
--- /dev/null
+++ b/drivers/video/rockchip/rve/rve_debugger.c
@@ -0,0 +1,566 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author:
+ *	Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#define pr_fmt(fmt) "rve_debugger: " fmt
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/syscalls.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+#include "rve.h"
+#include "rve_debugger.h"
+#include "rve_drv.h"
+
+#define RVE_DEBUGGER_ROOT_NAME "rve"
+
+#define STR_ENABLE(en) (en ? "EN" : "DIS")
+
+int RVE_DEBUG_REG;
+int RVE_DEBUG_MSG;
+int RVE_DEBUG_TIME;
+int RVE_DEBUG_CHECK_MODE;
+int RVE_DEBUG_NONUSE;
+int RVE_DEBUG_INT_FLAG;
+int RVE_DEBUG_MONITOR;
+
+static int rve_debug_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "REG [%s]\n"
+		 "MSG [%s]\n"
+		 "TIME [%s]\n"
+		 "INT [%s]\n"
+		 "CHECK [%s]\n"
+		 "STOP [%s]\n"
+		 "MONITOR [%s]",
+		 STR_ENABLE(RVE_DEBUG_REG),
+		 STR_ENABLE(RVE_DEBUG_MSG),
+		 STR_ENABLE(RVE_DEBUG_TIME),
+		 STR_ENABLE(RVE_DEBUG_INT_FLAG),
+		 STR_ENABLE(RVE_DEBUG_CHECK_MODE),
+		 STR_ENABLE(RVE_DEBUG_NONUSE),
+		 STR_ENABLE(RVE_DEBUG_MONITOR));
+
+	seq_puts(m, "\nhelp:\n");
+	seq_puts(m,
+		 " 'echo reg > debug' to enable/disable register log printing.\n");
+	seq_puts(m,
+		 " 'echo msg > debug' to enable/disable message log printing.\n");
+	seq_puts(m,
+		 " 'echo time > debug' to enable/disable time log printing.\n");
+	seq_puts(m,
+		 " 'echo int > debug' to enable/disable interruppt log printing.\n");
+	seq_puts(m, " 'echo check > debug' to enable/disable check mode.\n");
+	seq_puts(m,
+		 " 'echo stop > debug' to enable/disable stop using hardware\n");
+	seq_puts(m, " 'echo mon > debug' to enable/disable monitor");
+
+	return 0;
+}
+
+static ssize_t rve_debug_write(struct file *file, const char __user *ubuf,
+				 size_t len, loff_t *offp)
+{
+	char buf[14];
+
+	if (len > sizeof(buf) - 1)
+		return -EINVAL;
+	if (copy_from_user(buf, ubuf, len))
+		return -EFAULT;
+	buf[len - 1] = '\0';
+
+	if (strncmp(buf, "reg", 4) == 0) {
+		if (RVE_DEBUG_REG) {
+			RVE_DEBUG_REG = 0;
+			pr_err("close rve reg!\n");
+		} else {
+			RVE_DEBUG_REG = 1;
+			pr_err("open rve reg!\n");
+		}
+	} else if (strncmp(buf, "msg", 3) == 0) {
+		if (RVE_DEBUG_MSG) {
+			RVE_DEBUG_MSG = 0;
+			pr_err("close rve test MSG!\n");
+		} else {
+			RVE_DEBUG_MSG = 1;
+			pr_err("open rve test MSG!\n");
+		}
+	} else if (strncmp(buf, "time", 4) == 0) {
+		if (RVE_DEBUG_TIME) {
+			RVE_DEBUG_TIME = 0;
+			pr_err("close rve test time!\n");
+		} else {
+			RVE_DEBUG_TIME = 1;
+			pr_err("open rve test time!\n");
+		}
+	} else if (strncmp(buf, "check", 5) == 0) {
+		if (RVE_DEBUG_CHECK_MODE) {
+			RVE_DEBUG_CHECK_MODE = 0;
+			pr_err("close rve check flag!\n");
+		} else {
+			RVE_DEBUG_CHECK_MODE = 1;
+			pr_err("open rve check flag!\n");
+		}
+	} else if (strncmp(buf, "stop", 4) == 0) {
+		if (RVE_DEBUG_NONUSE) {
+			RVE_DEBUG_NONUSE = 0;
+			pr_err("using rve hardware!\n");
+		} else {
+			RVE_DEBUG_NONUSE = 1;
+			pr_err("stop using rve hardware!\n");
+		}
+	} else if (strncmp(buf, "int", 3) == 0) {
+		if (RVE_DEBUG_INT_FLAG) {
+			RVE_DEBUG_INT_FLAG = 0;
+			pr_err("close inturrupt MSG!\n");
+		} else {
+			RVE_DEBUG_INT_FLAG = 1;
+			pr_err("open inturrupt MSG!\n");
+		}
+	} else if (strncmp(buf, "mon", 3) == 0) {
+		if (RVE_DEBUG_MONITOR) {
+			RVE_DEBUG_MONITOR = 0;
+			pr_err("close monitor!\n");
+		} else {
+			RVE_DEBUG_MONITOR = 1;
+			pr_err("open monitor!\n");
+		}
+	} else if (strncmp(buf, "slt", 3) == 0) {
+		pr_err("Null");
+	}
+
+	return len;
+}
+
+static int rve_version_show(struct seq_file *m, void *data)
+{
+	seq_printf(m, "%s: v%s\n", DRIVER_DESC, DRIVER_VERSION);
+
+	return 0;
+}
+
+static int rve_load_show(struct seq_file *m, void *data)
+{
+	struct rve_scheduler_t *scheduler = NULL;
+	struct rve_sche_pid_info_t *pid_info = NULL;
+	unsigned long flags;
+	int i;
+	int load;
+	u32 busy_time_total;
+
+	seq_printf(m, "num of scheduler = %d\n", rve_drvdata->num_of_scheduler);
+	seq_printf(m, "================= load ==================\n");
+
+	scheduler = rve_drvdata->scheduler[0];
+
+	seq_printf(m, "scheduler[0]: %s\n", dev_driver_string(scheduler->dev));
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	busy_time_total = scheduler->timer.busy_time_record;
+	pid_info = scheduler->session.pid_info;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	load = (busy_time_total * 100000 / RVE_LOAD_INTERVAL);
+	seq_printf(m, "\t load = %d\n", load);
+
+	seq_printf(m, "---------------- PID INFO ---------------\n");
+
+	for (i = 0; i < RVE_MAX_PID_INFO; i++) {
+		seq_printf(m, "\t [pid: %d] hw_time_total = %llu us\n",
+			pid_info[i].pid, ktime_to_us(pid_info[i].hw_time_total));
+		seq_printf(m, "\t\t last_job_rd_bandwidth: %u bytes/s\n",
+			pid_info[i].last_job_rd_bandwidth);
+		seq_printf(m, "\t\t last_job_wr_bandwidth: %u bytes/s\n",
+			pid_info[i].last_job_wr_bandwidth);
+		seq_printf(m, "\t\t last_job_cycle_cnt/s: %u\n",
+			pid_info[i].last_job_cycle_cnt);
+	}
+	return 0;
+}
+
+static int rve_scheduler_show(struct seq_file *m, void *data)
+{
+	struct rve_scheduler_t *scheduler = NULL;
+	int i;
+	unsigned long flags;
+
+	int pd_refcount;
+	uint64_t total_int_cnt;
+	uint32_t rd_bandwidth, wr_bandwidth, cycle_cnt;
+
+	seq_printf(m, "num of scheduler = %d\n", rve_drvdata->num_of_scheduler);
+	seq_printf(m, "===================================\n");
+
+	for (i = 0; i < rve_drvdata->num_of_scheduler; i++) {
+		scheduler = rve_drvdata->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		pd_refcount = scheduler->session.pd_refcount;
+		total_int_cnt = scheduler->session.total_int_cnt;
+		rd_bandwidth = scheduler->session.rd_bandwidth;
+		wr_bandwidth = scheduler->session.wr_bandwidth;
+		cycle_cnt = scheduler->session.cycle_cnt;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		seq_printf(m, "scheduler[%d]: %s\n", i, dev_driver_string(scheduler->dev));
+		seq_printf(m, "-----------------------------------\n");
+		seq_printf(m, "pd_ref = %d\n", pd_refcount);
+		seq_printf(m, "total_int_cnt = %llu\n", total_int_cnt);
+		seq_printf(m, "rd_bandwidth: %u bytes/s\t wr_bandwidth: %u bytes/s\n",
+				rd_bandwidth, wr_bandwidth);
+		seq_printf(m, "cycle_cnt/s: %u\n", cycle_cnt);
+	}
+
+	return 0;
+}
+
+static int rve_ctx_manager_show(struct seq_file *m, void *data)
+{
+	int id;
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	unsigned long flags;
+	int cmd_num = 0;
+	int finished_job_count = 0;
+	bool status = false;
+	pid_t pid;
+
+	u32 last_job_hw_use_time;
+	u32 last_job_use_time;
+	u32 hw_time_total;
+	u32 max_cost_time_per_sec;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	seq_puts(m, "rve internal ctx dump:\n");
+	seq_printf(m, "ctx count = %d\n", ctx_manager->ctx_count);
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	idr_for_each_entry(&ctx_manager->ctx_id_idr, ctx, id) {
+		seq_printf(m, "================= ctx id: %d =================\n", ctx->id);
+
+		spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+		spin_lock_irqsave(&ctx->lock, flags);
+
+		cmd_num = ctx->cmd_num;
+		finished_job_count = ctx->finished_job_count;
+		status = ctx->is_running;
+		pid = ctx->debug_info.pid;
+		last_job_hw_use_time = ctx->debug_info.last_job_hw_use_time;
+		last_job_use_time = ctx->debug_info.last_job_use_time;
+		hw_time_total = ctx->debug_info.hw_time_total;
+		max_cost_time_per_sec = ctx->debug_info.max_cost_time_per_sec;
+
+		spin_unlock_irqrestore(&ctx->lock, flags);
+
+		seq_printf(m, "----------------- RVE CTX INFO -----------------\n");
+		seq_printf(m, "\t [pid: %d] status: %s\n", pid, status ? "active" : "pending");
+		seq_printf(m, "\t set cmd num: %d\t finish job sum: %d\n",
+				cmd_num, finished_job_count);
+		seq_printf(m, "\t last_job_use_time: %u us\t last_job_hw_use_time: %u us",
+				last_job_use_time, last_job_hw_use_time);
+		seq_printf(m, "\t hw_time_total: %u us\t max_cost_time_per_sec: %u us",
+				hw_time_total, max_cost_time_per_sec);
+
+		seq_printf(m, "----------------- RVE INVOKE INFO -----------------\n");
+		/* TODO: */
+
+		spin_lock_irqsave(&ctx_manager->lock, flags);
+	}
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	return 0;
+}
+
+
+struct rve_debugger_list rve_debugger_root_list[] = {
+	{"debug", rve_debug_show, rve_debug_write, NULL},
+	{"driver_version", rve_version_show, NULL, NULL},
+	{"load", rve_load_show, NULL, NULL},
+	{"scheduler_status", rve_scheduler_show, NULL, NULL},
+	{"ctx_manager", rve_ctx_manager_show, NULL, NULL},
+};
+
+static ssize_t rve_debugger_write(struct file *file, const char __user *ubuf,
+				 size_t len, loff_t *offp)
+{
+	struct seq_file *priv = file->private_data;
+	struct rve_debugger_node *node = priv->private;
+
+	if (node->info_ent->write)
+		return node->info_ent->write(file, ubuf, len, offp);
+	else
+		return len;
+}
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS
+
+static int rve_debugfs_open(struct inode *inode, struct file *file)
+{
+	struct rve_debugger_node *node = inode->i_private;
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct file_operations rve_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = rve_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.write = rve_debugger_write,
+};
+
+static int rve_debugfs_remove_files(struct rve_debugger *debugger)
+{
+	struct rve_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->debugfs_lock);
+
+	/* Delete debugfs entry list */
+	entry_list = &debugger->debugfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->dent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all debugfs node in this directory */
+	debugfs_remove_recursive(debugger->debugfs_dir);
+	debugger->debugfs_dir = NULL;
+
+	mutex_unlock(&debugger->debugfs_lock);
+
+	return 0;
+}
+
+static int rve_debugfs_create_files(const struct rve_debugger_list *files,
+					int count, struct dentry *root,
+					struct rve_debugger *debugger)
+{
+	int i;
+	struct dentry *ent;
+	struct rve_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rve_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			pr_err("Cannot alloc node path /sys/kernel/debug/%pd/%s\n",
+				 root, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = debugfs_create_file(files[i].name, S_IFREG | S_IRUGO,
+					 root, tmp, &rve_debugfs_fops);
+		if (!ent) {
+			pr_err("Cannot create /sys/kernel/debug/%pd/%s\n", root,
+				 files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->dent = ent;
+
+		mutex_lock(&debugger->debugfs_lock);
+		list_add_tail(&tmp->list, &debugger->debugfs_entry_list);
+		mutex_unlock(&debugger->debugfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rve_debugfs_remove_files(debugger);
+
+	return -1;
+}
+
+int rve_debugfs_remove(void)
+{
+	struct rve_debugger *debugger;
+
+	debugger = rve_drvdata->debugger;
+
+	rve_debugfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rve_debugfs_init(void)
+{
+	int ret;
+	struct rve_debugger *debugger;
+
+	debugger = rve_drvdata->debugger;
+
+	debugger->debugfs_dir =
+		debugfs_create_dir(RVE_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->debugfs_dir)) {
+		pr_err("failed on mkdir /sys/kernel/debug/%s\n",
+			 RVE_DEBUGGER_ROOT_NAME);
+		debugger->debugfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rve_debugfs_create_files(rve_debugger_root_list, ARRAY_SIZE(rve_debugger_root_list),
+					 debugger->debugfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rve_debugger_root_list debugfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rve_debugfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS */
+
+#ifdef CONFIG_ROCKCHIP_RVE_PROC_FS
+static int rve_procfs_open(struct inode *inode, struct file *file)
+{
+	struct rve_debugger_node *node = pde_data(inode);
+
+	return single_open(file, node->info_ent->show, node);
+}
+
+static const struct proc_ops rve_procfs_fops = {
+	.proc_open = rve_procfs_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = rve_debugger_write,
+};
+
+static int rve_procfs_remove_files(struct rve_debugger *debugger)
+{
+	struct rve_debugger_node *pos, *q;
+	struct list_head *entry_list;
+
+	mutex_lock(&debugger->procfs_lock);
+
+	/* Delete procfs entry list */
+	entry_list = &debugger->procfs_entry_list;
+	list_for_each_entry_safe(pos, q, entry_list, list) {
+		if (pos->pent == NULL)
+			continue;
+		list_del(&pos->list);
+		kfree(pos);
+		pos = NULL;
+	}
+
+	/* Delete all procfs node in this directory */
+	proc_remove(debugger->procfs_dir);
+	debugger->procfs_dir = NULL;
+
+	mutex_unlock(&debugger->procfs_lock);
+
+	return 0;
+}
+
+static int rve_procfs_create_files(const struct rve_debugger_list *files,
+				 int count, struct proc_dir_entry *root,
+				 struct rve_debugger *debugger)
+{
+	int i;
+	struct proc_dir_entry *ent;
+	struct rve_debugger_node *tmp;
+
+	for (i = 0; i < count; i++) {
+		tmp = kmalloc(sizeof(struct rve_debugger_node), GFP_KERNEL);
+		if (tmp == NULL) {
+			pr_err("Cannot alloc node path for /proc/%s/%s\n",
+				 RVE_DEBUGGER_ROOT_NAME, files[i].name);
+			goto MALLOC_FAIL;
+		}
+
+		tmp->info_ent = &files[i];
+		tmp->debugger = debugger;
+
+		ent = proc_create_data(files[i].name, S_IFREG | S_IRUGO,
+					 root, &rve_procfs_fops, tmp);
+		if (!ent) {
+			pr_err("Cannot create /proc/%s/%s\n",
+				 RVE_DEBUGGER_ROOT_NAME, files[i].name);
+			goto CREATE_FAIL;
+		}
+
+		tmp->pent = ent;
+
+		mutex_lock(&debugger->procfs_lock);
+		list_add_tail(&tmp->list, &debugger->procfs_entry_list);
+		mutex_unlock(&debugger->procfs_lock);
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	kfree(tmp);
+MALLOC_FAIL:
+	rve_procfs_remove_files(debugger);
+	return -1;
+}
+
+int rve_procfs_remove(void)
+{
+	struct rve_debugger *debugger;
+
+	debugger = rve_drvdata->debugger;
+
+	rve_procfs_remove_files(debugger);
+
+	return 0;
+}
+
+int rve_procfs_init(void)
+{
+	int ret;
+	struct rve_debugger *debugger;
+
+	debugger = rve_drvdata->debugger;
+
+	debugger->procfs_dir = proc_mkdir(RVE_DEBUGGER_ROOT_NAME, NULL);
+	if (IS_ERR_OR_NULL(debugger->procfs_dir)) {
+		pr_err("failed on mkdir /proc/%s\n", RVE_DEBUGGER_ROOT_NAME);
+		debugger->procfs_dir = NULL;
+		return -EIO;
+	}
+
+	ret = rve_procfs_create_files(rve_debugger_root_list, ARRAY_SIZE(rve_debugger_root_list),
+					 debugger->procfs_dir, debugger);
+	if (ret) {
+		pr_err("Could not install rve_debugger_root_list procfs\n");
+		goto CREATE_FAIL;
+	}
+
+	return 0;
+
+CREATE_FAIL:
+	rve_procfs_remove();
+
+	return ret;
+}
+#endif /* #ifdef CONFIG_ROCKCHIP_RVE_PROC_FS */
+
diff --git a/drivers/video/rockchip/rve/rve_drv.c b/drivers/video/rockchip/rve/rve_drv.c
new file mode 100644
index 0000000000000..b4b460437b655
--- /dev/null
+++ b/drivers/video/rockchip/rve/rve_drv.c
@@ -0,0 +1,897 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#define pr_fmt(fmt) "rve: " fmt
+
+#include "rve_job.h"
+#include "rve_fence.h"
+#include "rve_debugger.h"
+#include "rve_reg.h"
+
+struct rve_drvdata_t *rve_drvdata;
+
+/* set hrtimer */
+static struct hrtimer timer;
+static ktime_t kt;
+
+static const struct rve_backend_ops rve_ops = {
+	.get_version = rve_get_version,
+	.set_reg = rve_set_reg,
+	.init_reg = rve_init_reg,
+	.soft_reset = rve_soft_reset
+};
+
+static int rve_ctx_set_debuf_info_cb(int id, void *ptr, void *data)
+{
+	struct rve_internal_ctx_t *ctx = ptr;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ctx->debug_info.max_cost_time_per_sec = 0;
+	ctx->debug_info.hw_time_total = 0;
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	return 0;
+}
+
+static enum hrtimer_restart hrtimer_handler(struct hrtimer *timer)
+{
+	struct rve_drvdata_t *rve = rve_drvdata;
+	struct rve_scheduler_t *scheduler = NULL;
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_job *job = NULL;
+	unsigned long flags;
+	int i;
+
+	ktime_t now = ktime_get();
+
+	for (i = 0; i < rve->num_of_scheduler; i++) {
+		scheduler = rve->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		/* if timer action on job running */
+		job = scheduler->running_job;
+		if (job) {
+			scheduler->timer.busy_time += ktime_us_delta(now, job->hw_recoder_time);
+			job->hw_recoder_time = now;
+		}
+
+		scheduler->timer.busy_time_record = scheduler->timer.busy_time;
+		scheduler->timer.busy_time = 0;
+
+		/* monitor */
+		scheduler->session.rd_bandwidth = 0;
+		scheduler->session.wr_bandwidth = 0;
+		scheduler->session.cycle_cnt = 0;
+
+		for (i = 0; i < RVE_MAX_PID_INFO; i++) {
+			if (scheduler->session.pid_info[i].pid > 0)
+				scheduler->session.pid_info[i].hw_time_total = 0;
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		ctx_manager = rve_drvdata->pend_ctx_manager;
+
+		spin_lock_irqsave(&ctx_manager->lock, flags);
+
+		idr_for_each(&ctx_manager->ctx_id_idr, &rve_ctx_set_debuf_info_cb, ctx_manager);
+
+		spin_unlock_irqrestore(&ctx_manager->lock, flags);
+	}
+
+	hrtimer_forward_now(timer, kt);
+	return HRTIMER_RESTART;
+}
+
+static void rve_init_timer(void)
+{
+	kt = ktime_set(0, RVE_LOAD_INTERVAL);
+
+	hrtimer_init(&timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+
+	timer.function = hrtimer_handler;
+
+	hrtimer_start(&timer, kt, HRTIMER_MODE_REL);
+}
+
+static void rve_cancel_timer(void)
+{
+	hrtimer_cancel(&timer);
+}
+
+#ifndef RVE_PD_AWAYS_ON
+int rve_power_enable(struct rve_scheduler_t *scheduler)
+{
+	int ret = -EINVAL;
+	int i;
+
+	pm_runtime_get_sync(scheduler->dev);
+	pm_stay_awake(scheduler->dev);
+
+	for (i = 0; i < scheduler->num_clks; i++) {
+		if (!IS_ERR(scheduler->clks[i])) {
+			ret = clk_prepare_enable(scheduler->clks[i]);
+			if (ret < 0)
+				goto err_enable_clk;
+		}
+	}
+
+	scheduler->session.pd_refcount++;
+
+	return 0;
+
+err_enable_clk:
+	for (--i; i >= 0; --i)
+		if (!IS_ERR(scheduler->clks[i]))
+			clk_disable_unprepare(scheduler->clks[i]);
+
+	pm_relax(scheduler->dev);
+	pm_runtime_put_sync_suspend(scheduler->dev);
+
+	return ret;
+}
+
+int rve_power_disable(struct rve_scheduler_t *scheduler)
+{
+	int i;
+
+	for (i = scheduler->num_clks - 1; i >= 0; i--)
+		if (!IS_ERR(scheduler->clks[i]))
+			clk_disable_unprepare(scheduler->clks[i]);
+
+	pm_relax(scheduler->dev);
+	pm_runtime_put_sync_suspend(scheduler->dev);
+
+	scheduler->session.pd_refcount--;
+
+	return 0;
+}
+
+#endif //RVE_PD_AWAYS_ON
+
+static int rve_session_manager_init(struct rve_session_manager **session_manager_ptr)
+{
+	struct rve_session_manager *session_manager = NULL;
+
+	*session_manager_ptr = kzalloc(sizeof(struct rve_session_manager), GFP_KERNEL);
+	if (*session_manager_ptr == NULL) {
+		pr_err("can not kzalloc for rve_session_manager\n");
+		return -ENOMEM;
+	}
+
+	session_manager = *session_manager_ptr;
+
+	mutex_init(&session_manager->lock);
+
+	idr_init_base(&session_manager->ctx_id_idr, 1);
+
+	return 0;
+}
+
+/*
+ * Called at driver close to release the rve session's id references.
+ */
+static int rve_session_free_remove_idr_cb(int id, void *ptr, void *data)
+{
+	struct rve_session *session = ptr;
+
+	idr_remove(&rve_drvdata->session_manager->ctx_id_idr, session->id);
+	kfree(session);
+
+	return 0;
+}
+
+static int rve_session_free_remove_idr(struct rve_session *session)
+{
+	struct rve_session_manager *session_manager;
+
+	session_manager = rve_drvdata->session_manager;
+
+	mutex_lock(&session_manager->lock);
+
+	session_manager->session_cnt--;
+	idr_remove(&session_manager->ctx_id_idr, session->id);
+
+	mutex_unlock(&session_manager->lock);
+
+	return 0;
+}
+
+static int rve_session_manager_remove(struct rve_session_manager **session_manager_ptr)
+{
+	struct rve_session_manager *session_manager = *session_manager_ptr;
+
+	mutex_lock(&session_manager->lock);
+
+	idr_for_each(&session_manager->ctx_id_idr, &rve_session_free_remove_idr_cb, session_manager);
+	idr_destroy(&session_manager->ctx_id_idr);
+
+	mutex_unlock(&session_manager->lock);
+
+	kfree(*session_manager_ptr);
+
+	*session_manager_ptr = NULL;
+
+	return 0;
+}
+
+static struct rve_session *rve_session_init(void)
+{
+	struct rve_session_manager *session_manager = NULL;
+	struct rve_session *session = kzalloc(sizeof(*session), GFP_KERNEL);
+
+	session_manager = rve_drvdata->session_manager;
+	if (session_manager == NULL) {
+		pr_err("rve_session_manager is null!\n");
+		kfree(session);
+		return NULL;
+	}
+
+	mutex_lock(&session_manager->lock);
+
+	idr_preload(GFP_KERNEL);
+	session->id = idr_alloc(&session_manager->ctx_id_idr, session, 1, 0, GFP_ATOMIC);
+	session_manager->session_cnt++;
+	idr_preload_end();
+
+	mutex_unlock(&session_manager->lock);
+
+	session->tgid = current->tgid;
+
+	return session;
+}
+
+static int rve_session_deinit(struct rve_session *session)
+{
+	int ctx_id;
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	unsigned long flags;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	idr_for_each_entry(&ctx_manager->ctx_id_idr, ctx, ctx_id) {
+
+		spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+		if (session == ctx->session)
+			kref_put(&ctx->refcount, rve_internal_ctx_kref_release);
+
+		spin_lock_irqsave(&ctx_manager->lock, flags);
+	}
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	rve_job_session_destroy(session);
+
+	rve_session_free_remove_idr(session);
+	kfree(session);
+
+	return 0;
+}
+
+static long rve_ioctl_cmd_start(unsigned long arg, struct rve_session *session)
+{
+	int rve_user_ctx_id;
+	int ret = 0;
+
+	rve_user_ctx_id = rve_internal_ctx_alloc_to_get_idr_id(session);
+
+	if (copy_to_user((void *)arg, &rve_user_ctx_id, sizeof(int)))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+static long rve_ioctl_cmd_config(unsigned long arg)
+{
+	struct rve_user_ctx_t user_ctx;
+	int ret = 0;
+
+	if (unlikely(copy_from_user(&user_ctx, (struct rve_user_ctx_t *)arg,
+			sizeof(user_ctx)))) {
+		pr_err("rve_user_ctx copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+/* TODO:
+ *	if (rve_user_ctx.cmd_num > RVE_CMD_NUM_MAX) {
+ *		pr_err("Cannot import more than %d buffers at a time!\n",
+ *			RVE_CMD_NUM_MAX);
+ *		return -EFBIG;
+ *	}
+ */
+
+	if (user_ctx.id <= 0) {
+		pr_err("ctx id[%d] is invalid", user_ctx.id);
+		return -EINVAL;
+	}
+
+	if (DEBUGGER_EN(MSG))
+		pr_info("config cmd id = %d", user_ctx.id);
+
+	/* find internal_ctx to set cmd by user ctx (internal ctx id) */
+	ret = rve_job_config_by_user_ctx(&user_ctx);
+	if (ret < 0) {
+		pr_err("config ctx id[%d] failed!\n", user_ctx.id);
+		return -EFAULT;
+	}
+
+	return ret;
+}
+
+static long rve_ioctl_cmd_end(unsigned long arg)
+{
+	struct rve_user_ctx_t rve_user_ctx;
+	int ret = 0;
+
+	if (unlikely(copy_from_user(&rve_user_ctx, (uint32_t *)arg,
+			sizeof(rve_user_ctx)))) {
+		pr_err("rve_user_ctx copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+	if (DEBUGGER_EN(MSG))
+		pr_info("config end id = %d", rve_user_ctx.id);
+
+	/* find internal_ctx to set cmd by user ctx (internal ctx id) */
+	ret = rve_job_commit_by_user_ctx(&rve_user_ctx);
+	if (ret < 0) {
+		pr_err("commit ctx id[%d] failed!\n", rve_user_ctx.id);
+		return -EFAULT;
+	}
+
+	if (copy_to_user((struct rve_user_ctx_t *)arg,
+			&rve_user_ctx, sizeof(struct rve_user_ctx_t))) {
+		pr_err("rve_user_ctx copy_to_user failed\n");
+		return -EFAULT;
+	}
+
+	return ret;
+}
+
+static long rve_ioctl_cmd_cancel(unsigned long arg)
+{
+	uint32_t rve_user_ctx_id;
+	int ret = 0;
+
+	if (unlikely(copy_from_user(&rve_user_ctx_id, (uint32_t *)arg,
+			sizeof(uint32_t)))) {
+		pr_err("rve_user_ctx copy_from_user failed!\n");
+		return -EFAULT;
+	}
+
+	if (DEBUGGER_EN(MSG))
+		pr_info("config cancel id = %d", rve_user_ctx_id);
+
+	/* find internal_ctx to set cmd by user ctx (internal ctx id) */
+	ret = rve_job_cancel_by_user_ctx(rve_user_ctx_id);
+	if (ret < 0) {
+		pr_err("cancel ctx id[%d] failed!\n", rve_user_ctx_id);
+		return -EFAULT;
+	}
+
+	return ret;
+}
+
+static long rve_ioctl(struct file *file, uint32_t cmd, unsigned long arg)
+{
+	struct rve_drvdata_t *rve = rve_drvdata;
+
+	int ret = 0;
+	int i = 0;
+	struct rve_version_t driver_version;
+	struct rve_hw_versions_t hw_versions;
+	struct rve_session *session = file->private_data;
+
+	if (!rve) {
+		pr_err("rve_drvdata is null, rve is not init\n");
+		return -ENODEV;
+	}
+
+	//if (DEBUGGER_EN(NONUSE))
+	//	return 0;
+
+	switch (cmd) {
+	case RVE_IOC_GET_HW_VER:
+		/* RVE hardware version */
+		hw_versions.size = rve->num_of_scheduler > RVE_HW_SIZE ?
+			RVE_HW_SIZE : rve->num_of_scheduler;
+
+		for (i = 0; i < hw_versions.size; i++) {
+			memcpy(&hw_versions.version[i], &rve->scheduler[i]->version,
+				sizeof(rve->scheduler[i]->version));
+		}
+
+		if (copy_to_user((void *)arg, &hw_versions, sizeof(hw_versions)))
+			ret = -EFAULT;
+		else
+			ret = true;
+
+		break;
+
+	case RVE_IOC_GET_VER:
+		/* Driver version */
+		driver_version.major = DRIVER_MAJOR_VERSION;
+		driver_version.minor = DRIVER_MINOR_VERSION;
+		driver_version.revision = DRIVER_REVISION_VERSION;
+		driver_version.prod_num = 0;
+		strncpy((char *)driver_version.str, DRIVER_VERSION, sizeof(driver_version.str));
+
+		if (copy_to_user((void *)arg, &driver_version, sizeof(driver_version)))
+			ret = -EFAULT;
+		else
+			ret = true;
+
+		break;
+
+	case RVE_IOC_START_CONFIG:
+		ret = rve_ioctl_cmd_start(arg, session);
+
+		break;
+
+	case RVE_IOC_END_CONFIG:
+		ret = rve_ioctl_cmd_end(arg);
+
+		break;
+
+	case RVE_IOC_CMD_CONFIG:
+		ret = rve_ioctl_cmd_config(arg);
+
+		break;
+
+	case RVE_IOC_CANCEL_CONFIG:
+		ret = rve_ioctl_cmd_cancel(arg);
+
+		break;
+
+	default:
+		pr_err("unknown ioctl cmd!\n");
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER
+static int rve_debugger_init(struct rve_debugger **debugger_p)
+{
+	struct rve_debugger *debugger;
+
+	*debugger_p = kzalloc(sizeof(struct rve_debugger), GFP_KERNEL);
+	if (*debugger_p == NULL) {
+		pr_err("can not alloc for rve debugger\n");
+		return -ENOMEM;
+	}
+
+	debugger = *debugger_p;
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUG_FS
+	mutex_init(&debugger->debugfs_lock);
+	INIT_LIST_HEAD(&debugger->debugfs_entry_list);
+#endif
+
+#ifdef CONFIG_ROCKCHIP_RVE_PROC_FS
+	mutex_init(&debugger->procfs_lock);
+	INIT_LIST_HEAD(&debugger->procfs_entry_list);
+#endif
+
+	rve_debugfs_init();
+	rve_procfs_init();
+
+	return 0;
+}
+
+static int rve_debugger_remove(struct rve_debugger **debugger_p)
+{
+	rve_debugfs_remove();
+	rve_procfs_remove();
+
+	kfree(*debugger_p);
+	*debugger_p = NULL;
+
+	return 0;
+}
+#endif
+
+static int rve_open(struct inode *inode, struct file *file)
+{
+	struct rve_session *session = NULL;
+
+	session = rve_session_init();
+	if (!session)
+		return -ENOMEM;
+
+	file->private_data = (void *)session;
+
+	return nonseekable_open(inode, file);
+}
+
+static int rve_release(struct inode *inode, struct file *file)
+{
+	struct rve_session *session = file->private_data;
+
+	rve_session_deinit(session);
+
+	return 0;
+}
+
+static irqreturn_t rve_irq_handler(int irq, void *data)
+{
+	struct rve_scheduler_t *scheduler = data;
+	u32 error_flag;
+
+	error_flag = rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+	if (error_flag & 0x6) {
+		pr_err("irq thread work_status[%x]\n", error_flag);
+
+		if (error_flag & 0x2)
+			pr_err("irq: bus error");
+		else if (error_flag & 0x4)
+			pr_err("irq: timeout error");
+
+		scheduler->ops->soft_reset(scheduler);
+	}
+
+	/* clear INT */
+	rve_write(0x30000, RVE_SWREG1_IVE_IRQ, scheduler);
+
+	return IRQ_WAKE_THREAD;
+}
+
+static irqreturn_t rve_irq_thread(int irq, void *data)
+{
+	struct rve_scheduler_t *scheduler = data;
+	struct rve_job *job;
+	u32 error_flag;
+
+	job = scheduler->running_job;
+	scheduler->session.total_int_cnt++;
+
+	if (!job) {
+		pr_err("running job is invalid on irq thread\n");
+		return IRQ_HANDLED;
+	}
+
+	error_flag = rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+	if (DEBUGGER_EN(INT_FLAG)) {
+		pr_err("irq thread work_status[%x]\n", error_flag);
+		if (error_flag & 0x6) {
+			if (error_flag & 0x2)
+				pr_err("irq: bus error");
+			else if (error_flag & 0x4)
+				pr_err("irq: timeout error");
+		}
+	}
+
+	/* if llp mode*/
+	if ((error_flag & RVE_LLP_MODE) &&
+	    (!(error_flag & RVE_LLP_DONE))) {
+		if (DEBUGGER_EN(INT_FLAG))
+			pr_err("irq: llp mode need to skip rve_job_done");
+		goto skip_job_done;
+	}
+
+	rve_job_done(scheduler, 0);
+
+skip_job_done:
+	return IRQ_HANDLED;
+}
+
+const struct file_operations rve_fops = {
+	.owner = THIS_MODULE,
+	.open = rve_open,
+	.release = rve_release,
+	.unlocked_ioctl = rve_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = rve_ioctl,
+#endif
+};
+
+static struct miscdevice rve_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "rve",
+	.fops = &rve_fops,
+};
+
+static const char *const rve_clks[] = {
+	"aclk_rve",
+	"hclk_rve",
+};
+
+static const struct rve_irqs_data_t rve_irqs[] = {
+	{"rve_irq", rve_irq_handler, rve_irq_thread}
+};
+
+static const struct rve_match_data_t rve_match_data = {
+	.clks = rve_clks,
+	.num_clks = ARRAY_SIZE(rve_clks),
+	.irqs = rve_irqs,
+	.num_irqs = ARRAY_SIZE(rve_irqs)
+};
+
+static const struct of_device_id rve_dt_ids[] = {
+	{
+	 .compatible = "rockchip,rve",
+	 .data = &rve_match_data,
+	},
+	{},
+};
+
+static void init_scheduler(struct rve_scheduler_t *scheduler,
+			 const char *name)
+{
+	spin_lock_init(&scheduler->irq_lock);
+	INIT_LIST_HEAD(&scheduler->todo_list);
+	init_waitqueue_head(&scheduler->job_done_wq);
+
+	if (!strcmp(name, "rve")) {
+		scheduler->ops = &rve_ops;
+		scheduler->core = RVE_SCHEDULER_CORE0;
+	}
+}
+
+static int rve_drv_probe(struct platform_device *pdev)
+{
+	struct rve_drvdata_t *data = rve_drvdata;
+	struct resource *res;
+	int ret = 0;
+	const struct of_device_id *match = NULL;
+	struct device *dev = &pdev->dev;
+	const struct rve_match_data_t *match_data;
+	int i = 0, irq;
+	struct rve_scheduler_t *scheduler = NULL;
+
+	if (!pdev->dev.of_node)
+		return -EINVAL;
+
+	if (!strcmp(dev_driver_string(dev), "rve"))
+		match = of_match_device(rve_dt_ids, dev);
+
+	if (!match) {
+		dev_err(dev, "%s missing DT entry!\n", dev_driver_string(dev));
+		return -EINVAL;
+	}
+
+	scheduler =
+		devm_kzalloc(&pdev->dev, sizeof(struct rve_scheduler_t),
+			GFP_KERNEL);
+	if (scheduler == NULL) {
+		pr_err("failed to allocate scheduler. dev name = %s\n",
+			dev_driver_string(dev));
+		return -ENOMEM;
+	}
+
+	init_scheduler(scheduler, dev_driver_string(dev));
+
+	scheduler->dev = &pdev->dev;
+
+	/* map the registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		pr_err("get memory resource failed.\n");
+		return -ENXIO;
+	}
+
+	scheduler->rve_base =
+		devm_ioremap(&pdev->dev, res->start, resource_size(res));
+	if (!scheduler->rve_base) {
+		pr_err("ioremap failed\n");
+		ret = -ENOENT;
+		return ret;
+	}
+
+	/* get the IRQ */
+	match_data = match->data;
+
+	/* there are irq names in dts */
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(dev, "no irq %s in dts\n", match_data->irqs[0].name);
+		return irq;
+	}
+
+	scheduler->irq = irq;
+
+	pr_info("%s, irq = %d, match scheduler\n",
+			match_data->irqs[0].name, irq);
+
+	ret = devm_request_threaded_irq(dev, irq,
+			match_data->irqs[0].irq_hdl,
+			match_data->irqs[0].irq_thread, IRQF_SHARED,
+			dev_driver_string(dev), scheduler);
+	if (ret < 0) {
+		pr_err("request irq name: %s failed: %d\n",
+				match_data->irqs[0].name, ret);
+		return ret;
+	}
+
+#ifndef RVE_PD_AWAYS_ON
+	for (i = 0; i < match_data->num_clks; i++) {
+		struct clk *clk = devm_clk_get(dev, match_data->clks[i]);
+
+		if (IS_ERR(clk))
+			pr_err("failed to get %s\n", match_data->clks[i]);
+
+		scheduler->clks[i] = clk;
+	}
+	scheduler->num_clks = match_data->num_clks;
+#endif
+
+	platform_set_drvdata(pdev, scheduler);
+
+	device_init_wakeup(dev, true);
+
+	/* PM init */
+#ifndef RVE_PD_AWAYS_ON
+	pm_runtime_enable(&pdev->dev);
+
+	ret = pm_runtime_get_sync(scheduler->dev);
+	if (ret < 0) {
+		pr_err("failed to get pm runtime, ret = %d\n",
+			 ret);
+		goto failed;
+	}
+
+	for (i = 0; i < scheduler->num_clks; i++) {
+		if (!IS_ERR(scheduler->clks[i])) {
+			ret = clk_prepare_enable(scheduler->clks[i]);
+			if (ret < 0) {
+				pr_err("failed to enable clk\n");
+				goto failed;
+			}
+		}
+	}
+#endif //RVE_PD_AWAYS_ON
+
+	scheduler->ops->get_version(scheduler);
+	pr_info("Driver loaded successfully rve[%d] ver:%s\n", i,
+		scheduler->version.str);
+
+	data->scheduler[data->num_of_scheduler] = scheduler;
+
+	data->num_of_scheduler++;
+
+#ifndef RVE_PD_AWAYS_ON
+	for (i = scheduler->num_clks - 1; i >= 0; i--)
+		if (!IS_ERR(scheduler->clks[i]))
+			clk_disable_unprepare(scheduler->clks[i]);
+
+	pm_runtime_put_sync(&pdev->dev);
+#endif //RVE_PD_AWAYS_ON
+
+	pr_info("probe successfully\n");
+
+	return 0;
+
+#ifndef RVE_PD_AWAYS_ON
+failed:
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+
+	return ret;
+#endif //RVE_PD_AWAYS_ON
+}
+
+static int rve_drv_remove(struct platform_device *pdev)
+{
+	device_init_wakeup(&pdev->dev, false);
+#ifndef RVE_PD_AWAYS_ON
+	pm_runtime_disable(&pdev->dev);
+#endif //RVE_PD_AWAYS_ON
+
+	return 0;
+}
+
+static struct platform_driver rve_driver = {
+	.probe = rve_drv_probe,
+	.remove = rve_drv_remove,
+	.driver = {
+		 .name = "rve",
+		 .of_match_table = of_match_ptr(rve_dt_ids),
+		 },
+};
+
+static int __init rve_init(void)
+{
+	int ret;
+
+	rve_drvdata = kzalloc(sizeof(struct rve_drvdata_t), GFP_KERNEL);
+	if (rve_drvdata == NULL) {
+		pr_err("failed to allocate driver data.\n");
+		return -ENOMEM;
+	}
+
+	mutex_init(&rve_drvdata->lock);
+
+	wake_lock_init(&rve_drvdata->wake_lock, WAKE_LOCK_SUSPEND, "rve");
+
+	ret = platform_driver_register(&rve_driver);
+	if (ret != 0) {
+		pr_err("Platform device rve register failed (%d).\n", ret);
+		return ret;
+	}
+
+#ifdef CONFIG_SYNC_FILE
+	rve_drvdata->fence_ctx = rve_fence_context_alloc();
+	if (IS_ERR(rve_drvdata->fence_ctx)) {
+		pr_err("failed to allocate fence context for RVE\n");
+		ret = PTR_ERR(rve_drvdata->fence_ctx);
+		return ret;
+	}
+#endif
+
+	ret = misc_register(&rve_dev);
+	if (ret) {
+		pr_err("cannot register miscdev (%d)\n", ret);
+		return ret;
+	}
+
+	rve_ctx_manager_init(&rve_drvdata->pend_ctx_manager);
+
+	rve_session_manager_init(&rve_drvdata->session_manager);
+
+	rve_init_timer();
+
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER
+	rve_debugger_init(&rve_drvdata->debugger);
+#endif
+
+	pr_info("Module initialized. v%s\n", DRIVER_VERSION);
+
+	return 0;
+}
+
+static void __exit rve_exit(void)
+{
+#ifdef CONFIG_ROCKCHIP_RVE_DEBUGGER
+	rve_debugger_remove(&rve_drvdata->debugger);
+#endif
+
+	rve_ctx_manager_remove(&rve_drvdata->pend_ctx_manager);
+
+	rve_session_manager_remove(&rve_drvdata->session_manager);
+
+	wake_lock_destroy(&rve_drvdata->wake_lock);
+
+#ifdef CONFIG_SYNC_FILE
+	rve_fence_context_free(rve_drvdata->fence_ctx);
+#endif
+
+	rve_cancel_timer();
+
+	platform_driver_unregister(&rve_driver);
+
+	misc_deregister(&rve_dev);
+
+	kfree(rve_drvdata);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+#ifdef CONFIG_ROCKCHIP_THUNDER_BOOT
+module_init(rve_init);
+#else
+late_initcall(rve_init);
+#endif
+#else
+fs_initcall(rve_init);
+#endif
+module_exit(rve_exit);
+
+/* Module information */
+MODULE_AUTHOR("putin.li@rock-chips.com");
+MODULE_DESCRIPTION("Driver for rve device");
+MODULE_LICENSE("GPL");
diff --git a/drivers/video/rockchip/rve/rve_fence.c b/drivers/video/rockchip/rve/rve_fence.c
new file mode 100644
index 0000000000000..8d48f8033fa76
--- /dev/null
+++ b/drivers/video/rockchip/rve/rve_fence.c
@@ -0,0 +1,136 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#define pr_fmt(fmt) "rve_fence: " fmt
+
+#include <linux/dma-fence.h>
+#include <linux/sync_file.h>
+#include <linux/slab.h>
+
+#include "rve_fence.h"
+
+static const char *rve_fence_get_name(struct dma_fence *fence)
+{
+	return DRIVER_NAME;
+}
+
+static const struct dma_fence_ops rve_fence_ops = {
+	.get_driver_name = rve_fence_get_name,
+	.get_timeline_name = rve_fence_get_name,
+};
+
+struct rve_fence_context *rve_fence_context_alloc(void)
+{
+	struct rve_fence_context *fence_ctx = NULL;
+
+	fence_ctx = kzalloc(sizeof(*fence_ctx), GFP_KERNEL);
+	if (!fence_ctx)
+		return ERR_PTR(-ENOMEM);
+
+	fence_ctx->context = dma_fence_context_alloc(1);
+	spin_lock_init(&fence_ctx->spinlock);
+
+	return fence_ctx;
+}
+
+void rve_fence_context_free(struct rve_fence_context *fence_ctx)
+{
+	kfree(fence_ctx);
+}
+
+int rve_out_fence_alloc(struct rve_job *job)
+{
+	struct rve_fence_context *fence_ctx = rve_drvdata->fence_ctx;
+	struct dma_fence *fence = NULL;
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return -ENOMEM;
+
+	dma_fence_init(fence, &rve_fence_ops, &job->fence_lock,
+			 fence_ctx->context, ++fence_ctx->seqno);
+
+	job->out_fence = fence;
+
+	return 0;
+}
+
+int rve_out_fence_get_fd(struct rve_job *job)
+{
+	struct sync_file *sync_file = NULL;
+	int fence_fd = -1;
+
+	if (!job->out_fence)
+		return -EINVAL;
+
+	fence_fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fence_fd < 0)
+		return fence_fd;
+
+	sync_file = sync_file_create(job->out_fence);
+	if (!sync_file)
+		return -ENOMEM;
+
+	fd_install(fence_fd, sync_file->file);
+
+	return fence_fd;
+}
+
+struct dma_fence *rve_get_input_fence(int in_fence_fd)
+{
+	struct dma_fence *in_fence;
+
+	in_fence = sync_file_get_fence(in_fence_fd);
+
+	if (!in_fence)
+		pr_err("can not get in-fence from fd\n");
+
+	return in_fence;
+}
+
+int rve_wait_input_fence(struct dma_fence *in_fence)
+{
+	int ret = 0;
+
+	ret = dma_fence_wait(in_fence, true);
+
+	dma_fence_put(in_fence);
+
+	return ret;
+}
+
+int rve_add_dma_fence_callback(struct rve_job *job, struct dma_fence *in_fence,
+				 dma_fence_func_t func)
+{
+	struct rve_fence_waiter *waiter;
+	int ret;
+
+	waiter = kmalloc(sizeof(*waiter), GFP_KERNEL);
+	if (!waiter) {
+		pr_err("%s: Failed to allocate waiter\n", __func__);
+		return -ENOMEM;
+	}
+
+	waiter->job = job;
+
+	ret = dma_fence_add_callback(in_fence, &waiter->waiter, func);
+	if (ret == -ENOENT) {
+		pr_err("'input fence' has been already signaled.");
+		goto err_free_waiter;
+	} else if (ret == -EINVAL) {
+		pr_err
+			("%s: failed to add callback to dma_fence, err: %d\n",
+			 __func__, ret);
+		goto err_free_waiter;
+	}
+
+	return ret;
+
+err_free_waiter:
+	kfree(waiter);
+	return ret;
+}
diff --git a/drivers/video/rockchip/rve/rve_job.c b/drivers/video/rockchip/rve/rve_job.c
new file mode 100644
index 0000000000000..f31bba66175a4
--- /dev/null
+++ b/drivers/video/rockchip/rve/rve_job.c
@@ -0,0 +1,1028 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#define pr_fmt(fmt) "rve_job: " fmt
+
+#include "rve_job.h"
+#include "rve_fence.h"
+#include "rve_reg.h"
+
+struct rve_job *
+rve_scheduler_get_pending_job_list(struct rve_scheduler_t *scheduler)
+{
+	unsigned long flags;
+	struct rve_job *job;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = list_first_entry_or_null(&scheduler->todo_list,
+		struct rve_job, head);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	return job;
+}
+
+struct rve_job *
+rve_scheduler_get_running_job(struct rve_scheduler_t *scheduler)
+{
+	unsigned long flags;
+	struct rve_job *job;
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	return job;
+}
+
+static void rve_scheduler_set_pid_info(struct rve_job *job, ktime_t now)
+{
+	struct rve_scheduler_t *scheduler;
+	bool pid_match_flag = false;
+	ktime_t tmp = 0;
+	int pid_mark = 0, i;
+
+	scheduler = rve_job_get_scheduler(job);
+
+	for (i = 0; i < RVE_MAX_PID_INFO; i++) {
+		if (scheduler->session.pid_info[i].pid == 0)
+			scheduler->session.pid_info[i].pid = job->pid;
+
+		if (scheduler->session.pid_info[i].pid == job->pid) {
+			pid_match_flag = true;
+			scheduler->session.pid_info[i].hw_time_total +=
+				(job->hw_running_time - now);
+			break;
+		}
+	}
+
+	if (!pid_match_flag) {
+		for (i = 0; i < RVE_MAX_PID_INFO; i++) {
+			if (i == 0) {
+				tmp = scheduler->session.pid_info[i].hw_time_total;
+				continue;
+			}
+
+			if (tmp > scheduler->session.pid_info[i].hw_time_total)
+				pid_mark = i;
+		}
+
+		scheduler->session.pid_info[pid_mark].pid = job->pid;
+		scheduler->session.pid_info[pid_mark].hw_time_total +=
+					ktime_us_delta(now, job->hw_running_time);
+	}
+}
+
+struct rve_scheduler_t *rve_job_get_scheduler(struct rve_job *job)
+{
+	return job->scheduler;
+}
+
+struct rve_internal_ctx_t *rve_job_get_internal_ctx(struct rve_job *job)
+{
+	return job->ctx;
+}
+
+static void rve_job_free(struct rve_job *job)
+{
+#ifdef CONFIG_SYNC_FILE
+	if (job->out_fence)
+		dma_fence_put(job->out_fence);
+#endif
+
+	free_page((unsigned long)job);
+}
+
+static int rve_job_cleanup(struct rve_job *job)
+{
+	ktime_t now = ktime_get();
+
+	if (DEBUGGER_EN(TIME)) {
+		pr_info("(pid:%d) job clean use time = %lld\n", job->pid,
+			ktime_us_delta(now, job->timestamp));
+	}
+	rve_job_free(job);
+
+	return 0;
+}
+
+void rve_job_session_destroy(struct rve_session *session)
+{
+	struct rve_scheduler_t *scheduler = NULL;
+	struct rve_job *job_pos, *job_q;
+	int i;
+
+	unsigned long flags;
+
+	for (i = 0; i < rve_drvdata->num_of_scheduler; i++) {
+		scheduler = rve_drvdata->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		list_for_each_entry_safe(job_pos, job_q, &scheduler->todo_list, head) {
+			if (session == job_pos->session) {
+				list_del(&job_pos->head);
+
+				spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+				rve_job_free(job_pos);
+
+				spin_lock_irqsave(&scheduler->irq_lock, flags);
+			}
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+}
+
+static struct rve_job *rve_job_alloc(struct rve_internal_ctx_t *ctx)
+{
+	struct rve_job *job = NULL;
+
+	job = (struct rve_job *)get_zeroed_page(GFP_KERNEL | GFP_DMA32);
+	if (!job)
+		return NULL;
+
+#ifdef CONFIG_SYNC_FILE
+	spin_lock_init(&job->fence_lock);
+#endif
+	INIT_LIST_HEAD(&job->head);
+
+	job->timestamp = ktime_get();
+	job->pid = current->pid;
+	job->regcmd_data = &ctx->regcmd_data[ctx->running_job_count];
+
+	job->scheduler = rve_drvdata->scheduler[0];
+	job->core = rve_drvdata->scheduler[0]->core;
+	job->ctx = ctx;
+	ctx->scheduler = job->scheduler;
+	job->session = ctx->session;
+
+	if (ctx->priority > 0) {
+		if (ctx->priority > RVE_SCHED_PRIORITY_MAX)
+			job->priority = RVE_SCHED_PRIORITY_MAX;
+		else
+			job->priority = ctx->priority;
+	}
+
+	return job;
+}
+
+static void rve_job_dump_info(struct rve_job *job)
+{
+	pr_info("job: priority = %d, core = %d\n",
+		job->priority, job->core);
+}
+
+static int rve_job_run(struct rve_job *job)
+{
+	struct rve_scheduler_t *scheduler;
+	int ret = 0;
+
+	scheduler = rve_job_get_scheduler(job);
+
+#ifndef RVE_PD_AWAYS_ON
+	/* enable power */
+	ret = rve_power_enable(scheduler);
+	if (ret < 0) {
+		pr_err("power enable failed");
+		return ret;
+	}
+#endif
+
+	ret = scheduler->ops->init_reg(job);
+	if (ret < 0) {
+		pr_err("init reg failed");
+		goto failed;
+	}
+
+	ret = scheduler->ops->set_reg(job, scheduler);
+	if (ret < 0) {
+		pr_err("set reg failed");
+		goto failed;
+	}
+
+	/* for debug */
+	if (DEBUGGER_EN(MSG))
+		rve_job_dump_info(job);
+
+	return ret;
+
+failed:
+#ifndef RVE_PD_AWAYS_ON
+	rve_power_disable(scheduler);
+#endif
+
+	return ret;
+}
+
+static void rve_job_next(struct rve_scheduler_t *scheduler)
+{
+	struct rve_job *job = NULL;
+	unsigned long flags;
+
+next_job:
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	if (scheduler->running_job ||
+		list_empty(&scheduler->todo_list)) {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+		return;
+	}
+
+	job = list_first_entry(&scheduler->todo_list, struct rve_job, head);
+
+	list_del_init(&job->head);
+
+	scheduler->job_count--;
+
+	scheduler->running_job = job;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	job->ret = rve_job_run(job);
+
+	/* If some error before hw run */
+	if (job->ret < 0) {
+		pr_err("some error on rve_job_run before hw start, %s(%d)\n",
+			__func__, __LINE__);
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		scheduler->running_job = NULL;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		rve_internal_ctx_signal(job);
+
+		goto next_job;
+	}
+}
+
+static void rve_job_finish_and_next(struct rve_job *job, int ret)
+{
+	ktime_t now = ktime_get();
+	struct rve_scheduler_t *scheduler;
+
+	job->ret = ret;
+
+	scheduler = rve_job_get_scheduler(job);
+
+	if (DEBUGGER_EN(TIME)) {
+		pr_info("hw use time = %lld\n", ktime_us_delta(now, job->hw_running_time));
+		pr_info("(pid:%d) job done use time = %lld\n", job->pid,
+			ktime_us_delta(now, job->timestamp));
+	}
+
+	rve_internal_ctx_signal(job);
+
+	rve_job_next(scheduler);
+
+#ifndef RVE_PD_AWAYS_ON
+	rve_power_disable(scheduler);
+#endif
+}
+
+void rve_job_done(struct rve_scheduler_t *scheduler, int ret)
+{
+	struct rve_job *job;
+	unsigned long flags;
+	u32 error_flag;
+	uint32_t *cmd_reg;
+	int i;
+
+	ktime_t now = ktime_get();
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+	scheduler->running_job = NULL;
+
+	scheduler->timer.busy_time += ktime_us_delta(now, job->hw_recoder_time);
+
+	rve_scheduler_set_pid_info(job, now);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	spin_lock_irqsave(&job->ctx->lock, flags);
+
+	job->ctx->debug_info.max_cost_time_per_sec =
+		max(job->ctx->debug_info.last_job_hw_use_time,
+			job->ctx->debug_info.max_cost_time_per_sec);
+	job->ctx->debug_info.last_job_hw_use_time = ktime_us_delta(now, job->hw_running_time);
+	job->ctx->debug_info.hw_time_total += job->ctx->debug_info.last_job_hw_use_time;
+	job->ctx->debug_info.last_job_use_time = ktime_us_delta(now, job->timestamp);
+
+	spin_unlock_irqrestore(&job->ctx->lock, flags);
+
+	/* record CFG REG copy to user */
+	cmd_reg = job->regcmd_data->cmd_reg;
+	for (i = 0; i < 40; i++)
+		cmd_reg[18 + i] = rve_read(RVE_CFG_REG + i * 4, scheduler);
+
+	error_flag = rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+	rve_get_monitor_info(job);
+
+	if (DEBUGGER_EN(MSG))
+		pr_info("irq thread work_status[%.8x]\n", error_flag);
+
+	/* disable llp enable, TODO: support pause mode */
+	rve_write(0, RVE_SWLTB3_ENABLE, scheduler);
+
+	rve_job_finish_and_next(job, ret);
+}
+
+static void rve_job_timeout_clean(struct rve_scheduler_t *scheduler)
+{
+	unsigned long flags;
+	struct rve_job *job = NULL;
+	ktime_t now = ktime_get();
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	job = scheduler->running_job;
+	if (job && (job->flags & RVE_ASYNC) &&
+	   (ktime_to_ms(ktime_sub(now, job->hw_running_time)) >= RVE_ASYNC_TIMEOUT_DELAY)) {
+		scheduler->running_job = NULL;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		scheduler->ops->soft_reset(scheduler);
+
+		rve_internal_ctx_signal(job);
+
+#ifndef RVE_PD_AWAYS_ON
+		rve_power_disable(scheduler);
+#endif
+	} else {
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+}
+
+static struct rve_scheduler_t *rve_job_schedule(struct rve_job *job)
+{
+	unsigned long flags;
+	struct rve_scheduler_t *scheduler = NULL;
+	struct rve_job *job_pos;
+	bool first_match = 0;
+
+	scheduler = rve_job_get_scheduler(job);
+	if (scheduler == NULL) {
+		pr_err("failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+		return NULL;
+	}
+
+	/* Only async will timeout clean */
+	rve_job_timeout_clean(scheduler);
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	/* priority policy set by userspace */
+	if (list_empty(&scheduler->todo_list)
+		|| (job->priority == RVE_SCHED_PRIORITY_DEFAULT)) {
+		list_add_tail(&job->head, &scheduler->todo_list);
+	} else {
+		list_for_each_entry(job_pos, &scheduler->todo_list, head) {
+			if (job->priority > job_pos->priority &&
+					(!first_match)) {
+				list_add(&job->head, &job_pos->head);
+				first_match = true;
+			}
+
+			/*
+			 * Increase the priority of subsequent tasks
+			 * after inserting into the list
+			 */
+			if (first_match)
+				job_pos->priority++;
+		}
+
+		if (!first_match)
+			list_add_tail(&job->head, &scheduler->todo_list);
+	}
+
+	scheduler->job_count++;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rve_job_next(scheduler);
+
+	return scheduler;
+}
+
+static void rve_job_abort_running(struct rve_job *job)
+{
+	unsigned long flags;
+	struct rve_scheduler_t *scheduler;
+
+	scheduler = rve_job_get_scheduler(job);
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	/* invalid job */
+	if (job == scheduler->running_job)
+		scheduler->running_job = NULL;
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	rve_job_cleanup(job);
+}
+
+static void rve_job_abort_invalid(struct rve_job *job)
+{
+	rve_job_cleanup(job);
+}
+
+static inline int rve_job_wait(struct rve_job *job)
+{
+	struct rve_scheduler_t *scheduler;
+
+	int left_time;
+	ktime_t now;
+	int ret;
+
+	scheduler = rve_job_get_scheduler(job);
+
+	left_time = wait_event_timeout(scheduler->job_done_wq,
+		job->ctx->finished_job_count == job->ctx->cmd_num,
+		RVE_SYNC_TIMEOUT_DELAY * job->ctx->cmd_num);
+
+	switch (left_time) {
+	case 0:
+		pr_err("%s timeout", __func__);
+		scheduler->ops->soft_reset(scheduler);
+		ret = -EBUSY;
+		break;
+	case -ERESTARTSYS:
+		ret = -ERESTARTSYS;
+		break;
+	default:
+		ret = 0;
+		break;
+	}
+
+	now = ktime_get();
+
+	if (DEBUGGER_EN(TIME))
+		pr_info("%s use time = %lld\n", __func__,
+			 ktime_to_us(ktime_sub(now, job->hw_running_time)));
+
+	return ret;
+}
+
+#ifdef CONFIG_SYNC_FILE
+static void rve_job_input_fence_signaled(struct dma_fence *fence,
+					 struct dma_fence_cb *_waiter)
+{
+	struct rve_fence_waiter *waiter = (struct rve_fence_waiter *)_waiter;
+	struct rve_scheduler_t *scheduler = NULL;
+
+	ktime_t now;
+
+	now = ktime_get();
+
+	if (DEBUGGER_EN(TIME))
+		pr_err("rve job wait in_fence signal use time = %lld\n",
+			ktime_to_us(ktime_sub(now, waiter->job->timestamp)));
+
+	scheduler = rve_job_schedule(waiter->job);
+
+	if (scheduler == NULL)
+		pr_err("failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+
+	kfree(waiter);
+}
+#endif
+
+int rve_job_config_by_user_ctx(struct rve_user_ctx_t *user_ctx)
+{
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	int ret = 0;
+	unsigned long flags;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	ctx = rve_internal_ctx_lookup(ctx_manager, user_ctx->id);
+	if (IS_ERR_OR_NULL(ctx)) {
+		pr_err("can not find internal ctx from id[%d]", user_ctx->id);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (ctx->is_running) {
+		pr_err("can not re-config when ctx is running");
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		return -EFAULT;
+	}
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/* TODO: user cmd_num */
+	user_ctx->cmd_num = 1;
+
+	if (ctx->regcmd_data == NULL) {
+		ctx->regcmd_data = kmalloc_array(user_ctx->cmd_num,
+			sizeof(struct rve_cmd_reg_array_t), GFP_KERNEL);
+		if (ctx->regcmd_data == NULL) {
+			pr_err("regcmd_data alloc error!\n");
+			return -ENOMEM;
+		}
+	}
+
+	if (unlikely(copy_from_user(ctx->regcmd_data,
+					u64_to_user_ptr(user_ctx->regcmd_data),
+				    sizeof(struct rve_cmd_reg_array_t) * user_ctx->cmd_num))) {
+		pr_err("regcmd_data copy_from_user failed\n");
+		ret = -EFAULT;
+
+		goto err_free_regcmd_data;
+	}
+
+	ctx->sync_mode = user_ctx->sync_mode;
+	ctx->cmd_num = user_ctx->cmd_num;
+	ctx->priority = user_ctx->priority;
+	ctx->in_fence_fd = user_ctx->in_fence_fd;
+
+	/* TODO: cmd addr */
+
+	return ret;
+
+err_free_regcmd_data:
+	kfree(ctx->regcmd_data);
+	return ret;
+}
+
+int rve_job_commit_by_user_ctx(struct rve_user_ctx_t *user_ctx)
+{
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	int ret = 0;
+	unsigned long flags;
+	int i;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	ctx = rve_internal_ctx_lookup(ctx_manager, user_ctx->id);
+	if (IS_ERR_OR_NULL(ctx)) {
+		pr_err("can not find internal ctx from id[%d]", user_ctx->id);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (ctx->is_running) {
+		pr_err("can not re-config when ctx is running");
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		return -EFAULT;
+	}
+
+	/* Reset */
+	ctx->finished_job_count = 0;
+	ctx->running_job_count = 0;
+	ctx->is_running = true;
+	ctx->disable_auto_cancel = user_ctx->disable_auto_cancel;
+
+	ctx->sync_mode = user_ctx->sync_mode;
+	if (ctx->sync_mode == 0)
+		ctx->sync_mode = RVE_SYNC;
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	for (i = 0; i < ctx->cmd_num; i++) {
+		ret = rve_job_commit(ctx);
+		if (ret < 0) {
+			pr_err("rve_job_commit failed, i = %d\n", i);
+			return -EFAULT;
+		}
+
+		ctx->running_job_count++;
+	}
+
+	user_ctx->out_fence_fd = ctx->out_fence_fd;
+
+	if (unlikely(copy_to_user(u64_to_user_ptr(user_ctx->regcmd_data),
+				  ctx->regcmd_data,
+				  sizeof(struct rve_cmd_reg_array_t) * ctx->cmd_num))) {
+		pr_err("ctx->regcmd_data copy_to_user failed\n");
+		return -EFAULT;
+	}
+
+	if (!ctx->disable_auto_cancel && ctx->sync_mode == RVE_SYNC)
+		kref_put(&ctx->refcount, rve_internal_ctx_kref_release);
+
+	return ret;
+}
+
+int rve_job_cancel_by_user_ctx(uint32_t ctx_id)
+{
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	int ret = 0;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	ctx = rve_internal_ctx_lookup(ctx_manager, ctx_id);
+	if (IS_ERR_OR_NULL(ctx)) {
+		pr_err("can not find internal ctx from id[%d]", ctx_id);
+		return -EINVAL;
+	}
+
+	kref_put(&ctx->refcount, rve_internal_ctx_kref_release);
+
+	return ret;
+}
+
+int rve_job_commit(struct rve_internal_ctx_t *ctx)
+{
+	struct rve_job *job = NULL;
+	struct rve_scheduler_t *scheduler = NULL;
+#ifdef CONFIG_SYNC_FILE
+	struct dma_fence *in_fence;
+#endif
+	int ret = 0;
+
+	job = rve_job_alloc(ctx);
+	if (!job) {
+		pr_err("failed to alloc rve job!\n");
+		return -ENOMEM;
+	}
+
+	if (ctx->sync_mode == RVE_ASYNC) {
+#ifdef CONFIG_SYNC_FILE
+		job->flags |= RVE_ASYNC;
+
+		if (!ctx->out_fence) {
+			ret = rve_out_fence_alloc(job);
+			if (ret) {
+				rve_job_free(job);
+				return ret;
+			}
+		}
+
+		ctx->out_fence = job->out_fence;
+
+		ctx->out_fence_fd = rve_out_fence_get_fd(job);
+
+		if (ctx->out_fence_fd < 0)
+			pr_err("out fence get fd failed");
+
+		if (DEBUGGER_EN(MSG))
+			pr_info("in_fence_fd = %d", ctx->in_fence_fd);
+
+		/* if input fence is valiable */
+		if (ctx->in_fence_fd > 0) {
+			in_fence = rve_get_input_fence(
+				ctx->in_fence_fd);
+			if (!in_fence) {
+				pr_err("%s: failed to get input dma_fence\n",
+					 __func__);
+				rve_job_free(job);
+				return ret;
+			}
+
+			/* close input fence fd */
+			close_fd(ctx->in_fence_fd);
+
+			ret = dma_fence_get_status(in_fence);
+			/* ret = 1: fence has been signaled */
+			if (ret == 1) {
+				scheduler = rve_job_schedule(job);
+
+				if (scheduler == NULL) {
+					pr_err("failed to get scheduler, %s(%d)\n",
+						 __func__, __LINE__);
+					goto invalid_job;
+				}
+				/* if input fence is valid */
+			} else if (ret == 0) {
+				ret = rve_add_dma_fence_callback(job,
+					in_fence, rve_job_input_fence_signaled);
+				if (ret < 0) {
+					pr_err("%s: failed to add fence callback\n",
+						 __func__);
+					rve_job_free(job);
+					return ret;
+				}
+			} else {
+				pr_err("%s: fence status error\n", __func__);
+				rve_job_free(job);
+				return ret;
+			}
+		} else {
+			scheduler = rve_job_schedule(job);
+
+			if (scheduler == NULL) {
+				pr_err("failed to get scheduler, %s(%d)\n",
+					 __func__, __LINE__);
+				goto invalid_job;
+			}
+		}
+
+		return ret;
+#else
+		pr_err("can not support ASYNC mode, please enable CONFIG_SYNC_FILE");
+		return -EFAULT;
+#endif
+
+	/* RVE_SYNC: wait until job finish */
+	} else if (ctx->sync_mode == RVE_SYNC) {
+		scheduler = rve_job_schedule(job);
+
+		if (scheduler == NULL) {
+			pr_err("failed to get scheduler, %s(%d)\n", __func__,
+				 __LINE__);
+			goto invalid_job;
+		}
+
+		ret = job->ret;
+		if (ret < 0) {
+			pr_err("some error on job, %s(%d)\n", __func__,
+				 __LINE__);
+			goto running_job_abort;
+		}
+
+		ret = rve_job_wait(job);
+		if (ret < 0)
+			goto running_job_abort;
+
+		rve_job_cleanup(job);
+	}
+	return ret;
+
+invalid_job:
+	rve_job_abort_invalid(job);
+	return ret;
+
+/* only used by SYNC mode */
+running_job_abort:
+	rve_job_abort_running(job);
+	return ret;
+}
+
+struct rve_internal_ctx_t *
+rve_internal_ctx_lookup(struct rve_pending_ctx_manager *ctx_manager, uint32_t id)
+{
+	struct rve_internal_ctx_t *ctx = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	ctx = idr_find(&ctx_manager->ctx_id_idr, id);
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	if (ctx == NULL)
+		pr_err("can not find internal ctx from id[%d]", id);
+
+	return ctx;
+}
+
+/*
+ * Called at driver close to release the internal ctx's id references.
+ */
+static int rve_internal_ctx_free_remove_idr_cb(int id, void *ptr, void *data)
+{
+	struct rve_internal_ctx_t *ctx = ptr;
+
+	idr_remove(&rve_drvdata->pend_ctx_manager->ctx_id_idr, ctx->id);
+	kfree(ctx);
+
+	return 0;
+}
+
+static int rve_internal_ctx_free_remove_idr(struct rve_internal_ctx_t *ctx)
+{
+	struct rve_pending_ctx_manager *ctx_manager;
+	unsigned long flags;
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	ctx_manager->ctx_count--;
+	idr_remove(&ctx_manager->ctx_id_idr, ctx->id);
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	kfree(ctx);
+
+	return 0;
+}
+
+int rve_internal_ctx_signal(struct rve_job *job)
+{
+	struct rve_internal_ctx_t *ctx;
+	struct rve_scheduler_t *scheduler;
+	int finished_job_count;
+	unsigned long flags;
+
+	scheduler = rve_job_get_scheduler(job);
+	if (scheduler == NULL) {
+		pr_err("failed to get scheduler, %s(%d)\n", __func__, __LINE__);
+		return -EFAULT;
+	}
+
+	ctx = rve_job_get_internal_ctx(job);
+	if (IS_ERR_OR_NULL(ctx)) {
+		pr_err("can not find internal ctx");
+		return -EINVAL;
+	}
+
+	ctx->regcmd_data = job->regcmd_data;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	finished_job_count = ++ctx->finished_job_count;
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (finished_job_count >= ctx->cmd_num) {
+#ifdef CONFIG_SYNC_FILE
+		if (ctx->out_fence)
+			dma_fence_signal(ctx->out_fence);
+#endif
+
+		job->flags |= RVE_JOB_DONE;
+
+		wake_up(&scheduler->job_done_wq);
+
+		spin_lock_irqsave(&ctx->lock, flags);
+
+		ctx->is_running = false;
+		ctx->out_fence = NULL;
+
+		spin_unlock_irqrestore(&ctx->lock, flags);
+
+		if (job->flags & RVE_ASYNC) {
+			rve_job_cleanup(job);
+			if (!ctx->disable_auto_cancel)
+				kref_put(&ctx->refcount, rve_internal_ctx_kref_release);
+		}
+	}
+
+	return 0;
+}
+
+int rve_internal_ctx_alloc_to_get_idr_id(struct rve_session *session)
+{
+	struct rve_pending_ctx_manager *ctx_manager;
+	struct rve_internal_ctx_t *ctx;
+	unsigned long flags;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (ctx == NULL) {
+		pr_err("can not kzalloc for rve_pending_ctx_manager\n");
+		return -ENOMEM;
+	}
+
+	ctx_manager = rve_drvdata->pend_ctx_manager;
+	if (ctx_manager == NULL) {
+		pr_err("rve_pending_ctx_manager is null!\n");
+		goto failed;
+	}
+
+	spin_lock_init(&ctx->lock);
+
+	/*
+	 * Get the user-visible handle using idr. Preload and perform
+	 * allocation under our spinlock.
+	 */
+
+	idr_preload(GFP_KERNEL);
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	ctx->id = idr_alloc(&ctx_manager->ctx_id_idr, ctx, 1, 0, GFP_ATOMIC);
+	if (ctx->id < 0) {
+		pr_err("idr_alloc failed");
+		spin_unlock_irqrestore(&ctx_manager->lock, flags);
+		goto failed;
+	}
+
+	ctx_manager->ctx_count++;
+
+	ctx->debug_info.pid = current->pid;
+	ctx->debug_info.timestamp = ktime_get();
+	ctx->session = session;
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	idr_preload_end();
+
+	ctx->regcmd_data = NULL;
+
+	kref_init(&ctx->refcount);
+
+	return ctx->id;
+
+failed:
+	kfree(ctx);
+	return -EFAULT;
+}
+
+void rve_internal_ctx_kref_release(struct kref *ref)
+{
+	struct rve_internal_ctx_t *ctx;
+	struct rve_scheduler_t *scheduler = NULL;
+	struct rve_job *job_pos, *job_q, *job;
+	int i;
+	bool need_reset = false;
+	unsigned long flags;
+	ktime_t now = ktime_get();
+
+	ctx = container_of(ref, struct rve_internal_ctx_t, refcount);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+	if (!ctx->is_running || ctx->finished_job_count >= ctx->cmd_num) {
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		goto free_ctx;
+	}
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	for (i = 0; i < rve_drvdata->num_of_scheduler; i++) {
+		scheduler = rve_drvdata->scheduler[i];
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		list_for_each_entry_safe(job_pos, job_q, &scheduler->todo_list, head) {
+			if (ctx->id == job_pos->ctx->id) {
+				job = job_pos;
+				list_del_init(&job_pos->head);
+
+				scheduler->job_count--;
+			}
+		}
+
+		/* for load */
+		if (scheduler->running_job) {
+			job = scheduler->running_job;
+
+			if (job->ctx->id == ctx->id) {
+				scheduler->running_job = NULL;
+				scheduler->timer.busy_time += ktime_us_delta(now, job->hw_recoder_time);
+				need_reset = true;
+			}
+		}
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+		if (need_reset) {
+			pr_err("reset core[%d] by user cancel", scheduler->core);
+			scheduler->ops->soft_reset(scheduler);
+
+			rve_job_finish_and_next(job, 0);
+		}
+	}
+
+free_ctx:
+	kfree(ctx->regcmd_data);
+	rve_internal_ctx_free_remove_idr(ctx);
+}
+
+int rve_ctx_manager_init(struct rve_pending_ctx_manager **ctx_manager_session)
+{
+	struct rve_pending_ctx_manager *ctx_manager = NULL;
+
+	*ctx_manager_session = kzalloc(sizeof(struct rve_pending_ctx_manager), GFP_KERNEL);
+	if (*ctx_manager_session == NULL) {
+		pr_err("can not kzalloc for rve_pending_ctx_manager\n");
+		return -ENOMEM;
+	}
+
+	ctx_manager = *ctx_manager_session;
+
+	spin_lock_init(&ctx_manager->lock);
+
+	idr_init_base(&ctx_manager->ctx_id_idr, 1);
+
+	return 0;
+}
+
+int rve_ctx_manager_remove(struct rve_pending_ctx_manager **ctx_manager_session)
+{
+	struct rve_pending_ctx_manager *ctx_manager = *ctx_manager_session;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ctx_manager->lock, flags);
+
+	idr_for_each(&ctx_manager->ctx_id_idr, &rve_internal_ctx_free_remove_idr_cb, ctx_manager);
+	idr_destroy(&ctx_manager->ctx_id_idr);
+
+	spin_unlock_irqrestore(&ctx_manager->lock, flags);
+
+	kfree(*ctx_manager_session);
+
+	*ctx_manager_session = NULL;
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/rve/rve_reg.c b/drivers/video/rockchip/rve/rve_reg.c
new file mode 100644
index 0000000000000..44b305cce0db3
--- /dev/null
+++ b/drivers/video/rockchip/rve/rve_reg.c
@@ -0,0 +1,277 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ *
+ * Author: Huang Lee <Putin.li@rock-chips.com>
+ */
+
+#define pr_fmt(fmt) "rve_reg: " fmt
+
+#include "rve_reg.h"
+#include "rve_job.h"
+
+void rve_soft_reset(struct rve_scheduler_t *scheduler)
+{
+	u32 i;
+	u32 reg;
+
+	rve_write(1, RVE_SWREG5_IVE_IDLE_CTRL, scheduler);
+
+	if (DEBUGGER_EN(REG)) {
+		pr_err("dump reg info on soft reset");
+		rve_dump_read_back_reg(scheduler);
+	}
+
+	if (DEBUGGER_EN(MSG)) {
+		pr_err("soft reset idle_ctrl = %.8x, idle_prc_sta = %.8x",
+			rve_read(RVE_SWREG5_IVE_IDLE_CTRL, scheduler),
+			rve_read(RVE_SWREG3_IVE_IDLE_PRC_STA, scheduler));
+
+		pr_err("work status = %.8x", rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler));
+	}
+
+	mdelay(20);
+
+	for (i = 0; i < RVE_RESET_TIMEOUT; i++) {
+		reg = rve_read(RVE_SWREG3_IVE_IDLE_PRC_STA, scheduler);
+		if (reg & 0x2) {
+			pr_info("soft reset successfully");
+
+			/* reset sw_softrst_rdy_sta reg */
+			rve_write(0x30000, RVE_SWREG3_IVE_IDLE_PRC_STA, scheduler);
+
+			/* reset RVE_SWREG6_IVE_WORK_STA */
+			rve_write(0xff0000, RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+			/* clean up int */
+			rve_write(0x30000, RVE_SWREG1_IVE_IRQ, scheduler);
+
+			break;
+		}
+
+		udelay(1);
+	}
+
+	if (i == RVE_RESET_TIMEOUT)
+		pr_err("soft reset timeout.\n");
+
+	if (DEBUGGER_EN(MSG)) {
+		pr_err("after soft reset idle_ctrl = %.8x, idle_prc_sta = %.8x",
+			rve_read(RVE_SWREG5_IVE_IDLE_CTRL, scheduler),
+			rve_read(RVE_SWREG3_IVE_IDLE_PRC_STA, scheduler));
+
+		pr_err("work status = %x", rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler));
+	}
+}
+
+int rve_init_reg(struct rve_job *job)
+{
+	int ret = 0;
+
+	if (DEBUGGER_EN(MSG))
+		pr_err("TODO: debug info");
+
+	return ret;
+}
+
+void rve_dump_read_back_reg(struct rve_scheduler_t *scheduler)
+{
+	int i;
+	unsigned long flags;
+	uint32_t sys_reg[8] = {0};
+	uint32_t ltb_reg[12] = {0};
+	uint32_t cfg_reg[40] = {0};
+	uint32_t mmu_reg[12] = {0};
+
+	spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+	for (i = 0; i < 8; i++)
+		sys_reg[i] = rve_read(RVE_SYS_REG + i * 4, scheduler);
+
+	for (i = 0; i < 12; i++)
+		ltb_reg[i] = rve_read(RVE_LTB_REG + i * 4, scheduler);
+
+	for (i = 0; i < 40; i++)
+		cfg_reg[i] = rve_read(RVE_CFG_REG + i * 4, scheduler);
+
+	for (i = 0; i < 12; i++)
+		mmu_reg[i] = rve_read(RVE_MMU_REG + i * 4, scheduler);
+
+	spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+
+	pr_info("sys_reg:");
+	for (i = 0; i < 2; i++)
+		pr_info("i = %x : %.8x %.8x %.8x %.8x\n", RVE_SYS_REG + i * 16,
+			sys_reg[0 + i * 4], sys_reg[1 + i * 4],
+			sys_reg[2 + i * 4], sys_reg[3 + i * 4]);
+
+	pr_info("ltb_reg:");
+	for (i = 0; i < 3; i++)
+		pr_info("i = %x : %.8x %.8x %.8x %.8x\n", RVE_LTB_REG + i * 16,
+			ltb_reg[0 + i * 4], ltb_reg[1 + i * 4],
+			ltb_reg[2 + i * 4], ltb_reg[3 + i * 4]);
+
+	pr_info("cfg_reg:");
+	for (i = 0; i < 10; i++)
+		pr_info("i = %x : %.8x %.8x %.8x %.8x\n", RVE_CFG_REG + i * 16,
+			cfg_reg[0 + i * 4], cfg_reg[1 + i * 4],
+			cfg_reg[2 + i * 4], cfg_reg[3 + i * 4]);
+
+	pr_info("mmu_reg:");
+	for (i = 0; i < 3; i++)
+		pr_info("i = %x : %.8x %.8x %.8x %.8x\n", RVE_MMU_REG + i * 16,
+			mmu_reg[0 + i * 4], mmu_reg[1 + i * 4],
+			mmu_reg[2 + i * 4], mmu_reg[3 + i * 4]);
+}
+
+int rve_set_reg(struct rve_job *job, struct rve_scheduler_t *scheduler)
+{
+	ktime_t now = ktime_get();
+	//uint32_t cmd_reg[58];
+	uint32_t *cmd_reg;
+	int i;
+
+	cmd_reg = job->regcmd_data->cmd_reg;
+
+	if (DEBUGGER_EN(REG)) {
+		pr_info("user readback:");
+		for (i = 0; i < 14; i++)
+			pr_info("%.8x %.8x %.8x %.8x\n",
+				cmd_reg[0 + i * 4], cmd_reg[1 + i * 4],
+				cmd_reg[2 + i * 4], cmd_reg[3 + i * 4]);
+		pr_info("%.8x %.8x", cmd_reg[56], cmd_reg[57]);
+	}
+
+	/* clean up irq status reg */
+	rve_write(0x00000, RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+	if (DEBUGGER_EN(MSG)) {
+		pr_info("idle_ctrl = %x, idle_prc_sta = %x",
+			rve_read(RVE_SWREG5_IVE_IDLE_CTRL, scheduler),
+			rve_read(RVE_SWREG3_IVE_IDLE_PRC_STA, scheduler));
+
+		pr_info("work status = %x", rve_read(RVE_SWREG6_IVE_WORK_STA, scheduler));
+	}
+
+	if (DEBUGGER_EN(TIME))
+		pr_info("set cmd use time = %lld\n", ktime_to_us(ktime_sub(now, job->timestamp)));
+
+	job->hw_running_time = now;
+	job->hw_recoder_time = now;
+
+	/* start hw, CMD buff */
+	for (i = 0; i < 8; i++)
+		rve_write(cmd_reg[i], RVE_SYS_REG + i * 4, scheduler);
+
+	for (i = 0; i < 10; i++) {
+		/* skip start reg */
+		if (i == 2)
+			continue;
+
+		rve_write(cmd_reg[8 + i], RVE_LTB_REG + i * 4, scheduler);
+	}
+
+	/* 0x200(start)(40 - 1 = 39) need config after reg ready */
+	for (i = 0; i < 39; i++)
+		rve_write(cmd_reg[19 + i], RVE_CFG_REG + (i + 1) * 4, scheduler);
+
+	//TODO: ddr config
+	rve_write(0x30000, RVE_SWCFG5_CTRL, scheduler);
+	rve_write(0xf4240, RVE_SWCFG6_TIMEOUT_THRESH, scheduler);
+	rve_write(0x1f0001, RVE_SWCFG7_DDR_CTRL, scheduler);
+
+	/* reset RVE_SWREG6_IVE_WORK_STA */
+	rve_write(RVE_CLEAR_UP_REG6_WROK_STA, RVE_SWREG6_IVE_WORK_STA, scheduler);
+
+	/* enable monitor */
+	if (DEBUGGER_EN(MONITOR))
+		rve_write(1, RVE_SWCFG32_MONITOR_CTRL0, scheduler);
+
+	if (DEBUGGER_EN(REG)) {
+		pr_err("before config:");
+		rve_dump_read_back_reg(scheduler);
+	}
+
+	/* if llp mode enable, skip to enable slave mode */
+	if (cmd_reg[11] != 1)
+		rve_write(1, RVE_SWCFG0_EN, scheduler);
+	else
+		/* llp config done, to start hw */
+		rve_write(cmd_reg[10], RVE_SWLTB2_CFG_DONE, scheduler);
+
+	if (DEBUGGER_EN(REG)) {
+		pr_err("after config:");
+		rve_dump_read_back_reg(scheduler);
+	}
+
+	return 0;
+}
+
+int rve_get_version(struct rve_scheduler_t *scheduler)
+{
+	u32 major_version, minor_version, prod_num;
+	u32 reg_version;
+
+	if (!scheduler) {
+		pr_err("scheduler is null\n");
+		return -EINVAL;
+	}
+
+	reg_version = rve_read(RVE_SWREG0_IVE_VERSION, scheduler);
+
+	major_version = (reg_version & RVE_MAJOR_VERSION_MASK) >> 8;
+	minor_version = (reg_version & RVE_MINOR_VERSION_MASK);
+	prod_num = (reg_version & RVE_PROD_NUM_MASK) >> 16;
+
+	snprintf(scheduler->version.str, sizeof(scheduler->version.str), "[%x]%x.%x",
+		prod_num, major_version, minor_version);
+
+	scheduler->version.major = major_version;
+	scheduler->version.minor = minor_version;
+	scheduler->version.prod_num = prod_num;
+
+	return 0;
+}
+
+void rve_get_monitor_info(struct rve_job *job)
+{
+	struct rve_sche_pid_info_t *pid_info = NULL;
+	struct rve_scheduler_t *scheduler = NULL;
+	unsigned long flags;
+	uint32_t rd_bandwidth, wr_bandwidth, cycle_cnt;
+	int i;
+
+	scheduler = rve_job_get_scheduler(job);
+	pid_info = scheduler->session.pid_info;
+
+	/* monitor */
+	if (DEBUGGER_EN(MONITOR)) {
+		rd_bandwidth = rve_read(RVE_SWCFG37_MONITOR_INFO3, scheduler);
+		wr_bandwidth = rve_read(RVE_SWCFG38_MONITOR_INFO4, scheduler);
+		cycle_cnt = rve_read(RVE_SWCFG39_MONITOR_INFO5, scheduler);
+
+		/* reset per htimer occur */
+		rve_write(2, RVE_SWCFG32_MONITOR_CTRL0, scheduler);
+
+		spin_lock_irqsave(&scheduler->irq_lock, flags);
+
+		for (i = 0; i < RVE_MAX_PID_INFO; i++) {
+			if (pid_info[i].pid == job->pid) {
+				pid_info[i].last_job_rd_bandwidth = rd_bandwidth;
+				pid_info[i].last_job_wr_bandwidth = wr_bandwidth;
+				pid_info[i].last_job_cycle_cnt = cycle_cnt;
+				break;
+			}
+		}
+
+		if (DEBUGGER_EN(MSG))
+			pr_info("rd_bandwidth = %d, wd_bandwidth = %d, cycle_cnt = %d\n",
+				rd_bandwidth, wr_bandwidth, cycle_cnt);
+
+		scheduler->session.rd_bandwidth += rd_bandwidth;
+		scheduler->session.wr_bandwidth += wr_bandwidth;
+		scheduler->session.cycle_cnt += cycle_cnt;
+
+		spin_unlock_irqrestore(&scheduler->irq_lock, flags);
+	}
+}
diff --git a/drivers/video/rockchip/vehicle/Kconfig b/drivers/video/rockchip/vehicle/Kconfig
new file mode 100644
index 0000000000000..e772c06255375
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/Kconfig
@@ -0,0 +1,46 @@
+# SPDX-License-Identifier: GPL-2.0
+config VIDEO_REVERSE_IMAGE
+	bool "Rockchip Fast Reverse Image driver"
+	depends on ARCH_ROCKCHIP && ROCKCHIP_DRM_DIRECT_SHOW
+	depends on VIDEO_ROCKCHIP_CIF && PHY_ROCKCHIP_CSI2_DPHY
+	help
+	  fast reverse Image module.
+
+if VIDEO_REVERSE_IMAGE
+
+config VIDEO_REVERSE_TP2815
+	bool "tp2815 for reverse sensor"
+	help
+	  Say y if use tp2815.
+
+config VIDEO_REVERSE_TP2855
+	bool "tp2855 for reverse sensor"
+	help
+	  Say y if use tp2855.
+
+config VIDEO_REVERSE_NVP6324
+	bool "nvp6324 for reverse sensor"
+	help
+	  Say y if use nvp6324.
+
+config VIDEO_REVERSE_NVP6188
+	bool "nvp6188 for reverse sensor"
+	help
+	  Say y if use nvp6188.
+
+config VIDEO_REVERSE_MAX96714
+	bool "max96714 for reverse sensor"
+	help
+	  Say y if use max96714.
+
+config VIDEO_REVERSE_GC2145
+	bool "gc2145 for reverse sensor"
+	help
+	  Say y if use gc2145.
+
+config VIDEO_REVERSE_AD7181
+	bool "ad7181 for reverse sensor"
+	help
+	  Say y if use ad7181.
+
+endif
diff --git a/drivers/video/rockchip/vehicle/Makefile b/drivers/video/rockchip/vehicle/Makefile
new file mode 100644
index 0000000000000..b88f826080a29
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/Makefile
@@ -0,0 +1,29 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-$(CONFIG_VIDEO_REVERSE_IMAGE) += video_rkvehicle.o
+video_rkvehicle-objs += vehicle_flinger.o \
+			vehicle_dev.o \
+			vehicle_main.o \
+			vehicle_cif.o \
+			vehicle_generic_sensor.o \
+			vehicle_gpio.o \
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_NVP6324) += \
+			vehicle_ad_nvp6324.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_NVP6188) += \
+			vehicle_ad_nvp6188.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_MAX96714) += \
+			vehicle_ad_max96714.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_GC2145) += \
+			vehicle_ad_gc2145.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_TP2825) += \
+			vehicle_ad_tp2825.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_TP2855) += \
+			vehicle_ad_tp2855.o
+
+video_rkvehicle-$(CONFIG_VIDEO_REVERSE_AD7181) += \
+			vehicle_ad_7181.o
diff --git a/drivers/video/rockchip/vehicle/vehicle-csi2-dphy-common.h b/drivers/video/rockchip/vehicle/vehicle-csi2-dphy-common.h
new file mode 100644
index 0000000000000..9a5810ae13b9e
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle-csi2-dphy-common.h
@@ -0,0 +1,385 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+
+#ifndef _VEHICLE_CSI2_DPHY_COMMON_H_
+#define _VEHICLE_CSI2_DPHY_COMMON_H_
+
+#include <linux/kernel.h>
+#include <linux/rk-camera-module.h>
+#include <media/v4l2-subdev.h>
+#include "vehicle_samsung_dcphy_common.h"
+#include "../../../media/platform/rockchip/cif/mipi-csi2.h"
+
+/* RK3562 DPHY GRF REG OFFSET */
+#define RK3562_GRF_VI_CON0	(0x0520)
+#define RK3562_GRF_VI_CON1	(0x0524)
+
+/* GRF REG OFFSET */
+#define GRF_VI_CON0	(0x0340)
+#define GRF_VI_CON1	(0x0344)
+
+/*RK3588 DPHY GRF REG OFFSET */
+#define GRF_DPHY_CON0	(0x0)
+#define GRF_SOC_CON2	(0x0308)
+
+/*RK3576 DPHY GRF REG OFFSET */
+#define GRF_SOC_CON5_RK3576	(0x14)
+
+/*GRF REG BIT DEFINE */
+#define GRF_CSI2PHY_LANE_SEL_SPLIT	(0x1)
+#define GRF_CSI2PHY_SEL_SPLIT_0_1	(0x0)
+#define GRF_CSI2PHY_SEL_SPLIT_2_3	BIT(0)
+
+/*RK3588 DCPHY GRF REG OFFSET */
+#define GRF_DCPHY_CON0			(0x0)
+
+/* PHY REG OFFSET */
+#define CSI2_DPHY_CTRL_INVALID_OFFSET	(0xffff)
+#define CSI2_DPHY_CTRL_PWRCTL	\
+				CSI2_DPHY_CTRL_INVALID_OFFSET
+#define CSI2_DPHY_CTRL_LANE_ENABLE	(0x00)
+#define CSI2_DPHY_CLK1_LANE_EN		(0x2C)
+#define CSI2_DPHY_DUAL_CAL_EN		(0x80)
+#define CSI2_DPHY_CLK_WR_THS_SETTLE	(0x160)
+#define CSI2_DPHY_CLK_CALIB_EN		(0x168)
+#define CSI2_DPHY_LANE0_WR_THS_SETTLE	(0x1e0)
+#define CSI2_DPHY_LANE0_CALIB_EN	(0x1e8)
+#define CSI2_DPHY_LANE1_WR_THS_SETTLE	(0x260)
+#define CSI2_DPHY_LANE1_CALIB_EN	(0x268)
+#define CSI2_DPHY_LANE2_WR_THS_SETTLE	(0x2e0)
+#define CSI2_DPHY_LANE2_CALIB_EN	(0x2e8)
+#define CSI2_DPHY_LANE3_WR_THS_SETTLE	(0x360)
+#define CSI2_DPHY_LANE3_CALIB_EN	(0x368)
+#define CSI2_DPHY_CLK1_WR_THS_SETTLE	(0x3e0)
+#define CSI2_DPHY_CLK1_CALIB_EN		(0x3e8)
+
+//DCPHY
+#define CSI2_DCPHY_CLK_WR_THS_SETTLE		(0x030)
+#define CSI2_DCPHY_LANE0_WR_THS_SETTLE		(0x130)
+#define CSI2_DCPHY_LANE0_WR_ERR_SOT_SYNC	(0x134)
+#define CSI2_DCPHY_LANE1_WR_THS_SETTLE		(0x230)
+#define CSI2_DCPHY_LANE1_WR_ERR_SOT_SYNC	(0x234)
+#define CSI2_DCPHY_LANE2_WR_THS_SETTLE		(0x330)
+#define CSI2_DCPHY_LANE2_WR_ERR_SOT_SYNC	(0x334)
+#define CSI2_DCPHY_LANE3_WR_THS_SETTLE		(0x430)
+#define CSI2_DCPHY_LANE3_WR_ERR_SOT_SYNC	(0x434)
+#define CSI2_DCPHY_CLK_LANE_ENABLE		(0x000)
+#define CSI2_DCPHY_DATA_LANE0_ENABLE		(0x100)
+#define CSI2_DCPHY_DATA_LANE1_ENABLE		(0x200)
+#define CSI2_DCPHY_DATA_LANE2_ENABLE		(0x300)
+#define CSI2_DCPHY_DATA_LANE3_ENABLE		(0x400)
+
+#define CSI2_DCPHY_S0C_GNR_CON1                 (0x004)
+#define CSI2_DCPHY_S0C_ANA_CON1			(0x00c)
+#define CSI2_DCPHY_S0C_ANA_CON2			(0x010)
+#define CSI2_DCPHY_S0C_ANA_CON3			(0x014)
+#define CSI2_DCPHY_COMBO_S0D0_GNR_CON1          (0x104)
+#define CSI2_DCPHY_COMBO_S0D0_ANA_CON1		(0x10c)
+#define CSI2_DCPHY_COMBO_S0D0_ANA_CON2		(0x110)
+#define CSI2_DCPHY_COMBO_S0D0_ANA_CON3		(0x114)
+#define CSI2_DCPHY_COMBO_S0D0_ANA_CON6		(0x120)
+#define CSI2_DCPHY_COMBO_S0D0_ANA_CON7		(0x124)
+#define CSI2_DCPHY_COMBO_S0D0_DESKEW_CON0	(0x140)
+#define CSI2_DCPHY_COMBO_S0D0_DESKEW_CON2	(0x148)
+#define CSI2_DCPHY_COMBO_S0D0_DESKEW_CON4	(0x150)
+#define CSI2_DCPHY_COMBO_S0D0_CRC_CON1		(0x164)
+#define CSI2_DCPHY_COMBO_S0D0_CRC_CON2		(0x168)
+#define CSI2_DCPHY_COMBO_S0D1_GNR_CON1          (0x204)
+#define CSI2_DCPHY_COMBO_S0D1_ANA_CON1		(0x20c)
+#define CSI2_DCPHY_COMBO_S0D1_ANA_CON2		(0x210)
+#define CSI2_DCPHY_COMBO_S0D1_ANA_CON3		(0x214)
+#define CSI2_DCPHY_COMBO_S0D1_ANA_CON6		(0x220)
+#define CSI2_DCPHY_COMBO_S0D1_ANA_CON7		(0x224)
+#define CSI2_DCPHY_COMBO_S0D1_DESKEW_CON0	(0x240)
+#define CSI2_DCPHY_COMBO_S0D1_DESKEW_CON2	(0x248)
+#define CSI2_DCPHY_COMBO_S0D1_DESKEW_CON4	(0x250)
+#define CSI2_DCPHY_COMBO_S0D1_CRC_CON1		(0x264)
+#define CSI2_DCPHY_COMBO_S0D1_CRC_CON2		(0x268)
+#define CSI2_DCPHY_COMBO_S0D2_GNR_CON1          (0x304)
+#define CSI2_DCPHY_COMBO_S0D2_ANA_CON1		(0x30c)
+#define CSI2_DCPHY_COMBO_S0D2_ANA_CON2		(0x310)
+#define CSI2_DCPHY_COMBO_S0D2_ANA_CON3		(0x314)
+#define CSI2_DCPHY_COMBO_S0D2_ANA_CON6		(0x320)
+#define CSI2_DCPHY_COMBO_S0D2_ANA_CON7		(0x324)
+#define CSI2_DCPHY_COMBO_S0D2_DESKEW_CON0	(0x340)
+#define CSI2_DCPHY_COMBO_S0D2_DESKEW_CON2	(0x348)
+#define CSI2_DCPHY_COMBO_S0D2_DESKEW_CON4	(0x350)
+#define CSI2_DCPHY_COMBO_S0D2_CRC_CON1		(0x364)
+#define CSI2_DCPHY_COMBO_S0D2_CRC_CON2		(0x368)
+#define CSI2_DCPHY_S0D3_GNR_CON1                (0x404)
+#define CSI2_DCPHY_S0D3_ANA_CON1		(0x40c)
+#define CSI2_DCPHY_S0D3_ANA_CON2		(0x410)
+#define CSI2_DCPHY_S0D3_ANA_CON3		(0x414)
+#define CSI2_DCPHY_S0D3_DESKEW_CON0		(0x440)
+#define CSI2_DCPHY_S0D3_DESKEW_CON2		(0x448)
+#define CSI2_DCPHY_S0D3_DESKEW_CON4		(0x450)
+
+/* PHY REG BIT DEFINE */
+#define CSI2_DPHY_LANE_MODE_FULL	(0x4)
+#define CSI2_DPHY_LANE_MODE_SPLIT	(0x2)
+#define CSI2_DPHY_LANE_SPLIT_TOP	(0x1)
+#define CSI2_DPHY_LANE_SPLIT_BOT	(0x2)
+#define CSI2_DPHY_LANE_SPLIT_LANE0_1	(0x3 << 2)
+#define CSI2_DPHY_LANE_SPLIT_LANE2_3	(0x3 << 4)
+#define CSI2_DPHY_LANE_DUAL_MODE_EN	BIT(6)
+#define CSI2_DPHY_LANE_PARA_ARR_NUM	(0x2)
+
+#define CSI2_DPHY_CTRL_DATALANE_ENABLE_OFFSET_BIT	2
+#define CSI2_DPHY_CTRL_DATALANE_SPLIT_LANE2_3_OFFSET_BIT	4
+#define CSI2_DPHY_CTRL_CLKLANE_ENABLE_OFFSET_BIT	6
+
+enum csi2_dphy_index {
+	DPHY0 = 0x0,
+	DPHY1,
+	DPHY2,
+};
+
+enum csi2_dphy_lane {
+	CSI2_DPHY_LANE_CLOCK = 0,
+	CSI2_DPHY_LANE_CLOCK1,
+	CSI2_DPHY_LANE_DATA0,
+	CSI2_DPHY_LANE_DATA1,
+	CSI2_DPHY_LANE_DATA2,
+	CSI2_DPHY_LANE_DATA3
+};
+
+enum grf_reg_id {
+	GRF_DPHY_RX0_TURNDISABLE = 0,
+	GRF_DPHY_RX0_FORCERXMODE,
+	GRF_DPHY_RX0_FORCETXSTOPMODE,
+	GRF_DPHY_RX0_ENABLE,
+	GRF_DPHY_RX0_TESTCLR,
+	GRF_DPHY_RX0_TESTCLK,
+	GRF_DPHY_RX0_TESTEN,
+	GRF_DPHY_RX0_TESTDIN,
+	GRF_DPHY_RX0_TURNREQUEST,
+	GRF_DPHY_RX0_TESTDOUT,
+	GRF_DPHY_TX0_TURNDISABLE,
+	GRF_DPHY_TX0_FORCERXMODE,
+	GRF_DPHY_TX0_FORCETXSTOPMODE,
+	GRF_DPHY_TX0_TURNREQUEST,
+	GRF_DPHY_TX1RX1_TURNDISABLE,
+	GRF_DPHY_TX1RX1_FORCERXMODE,
+	GRF_DPHY_TX1RX1_FORCETXSTOPMODE,
+	GRF_DPHY_TX1RX1_ENABLE,
+	GRF_DPHY_TX1RX1_MASTERSLAVEZ,
+	GRF_DPHY_TX1RX1_BASEDIR,
+	GRF_DPHY_TX1RX1_ENABLECLK,
+	GRF_DPHY_TX1RX1_TURNREQUEST,
+	GRF_DPHY_RX1_SRC_SEL,
+	/* rk3288 only */
+	GRF_CON_DISABLE_ISP,
+	GRF_CON_ISP_DPHY_SEL,
+	GRF_DSI_CSI_TESTBUS_SEL,
+	GRF_DVP_V18SEL,
+	/* rk1808 & rk3326 & rv1126 */
+	GRF_DPHY_CSI2PHY_FORCERXMODE,
+	GRF_DPHY_CSI2PHY_CLKLANE_EN,
+	GRF_DPHY_CSI2PHY_DATALANE_EN,
+	/* rv1126 only */
+	GRF_DPHY_CLK_INV_SEL,
+	GRF_DPHY_SEL,
+	/* rk3368 only */
+	GRF_ISP_MIPI_CSI_HOST_SEL,
+	/* below is for rk3399 only */
+	GRF_DPHY_RX0_CLK_INV_SEL,
+	GRF_DPHY_RX1_CLK_INV_SEL,
+	GRF_DPHY_TX1RX1_SRC_SEL,
+	/* below is for rk3568 only */
+	GRF_DPHY_CSI2PHY_CLKLANE1_EN,
+	GRF_DPHY_CLK1_INV_SEL,
+	GRF_DPHY_ISP_CSI2PHY_SEL,
+	GRF_DPHY_CIF_CSI2PHY_SEL,
+	GRF_DPHY_CSI2PHY_LANE_SEL,
+	GRF_DPHY_CSI2PHY1_LANE_SEL,
+	GRF_DPHY_CSI2PHY_DATALANE_EN0,
+	GRF_DPHY_CSI2PHY_DATALANE_EN1,
+	GRF_CPHY_MODE,
+	GRF_DPHY_CSIHOST2_SEL,
+	GRF_DPHY_CSIHOST3_SEL,
+	GRF_DPHY_CSIHOST4_SEL,
+	GRF_DPHY_CSIHOST5_SEL,
+	/* below is for rv1106 only */
+	GRF_MIPI_HOST0_SEL,
+	GRF_LVDS_HOST0_SEL,
+	/* below is for rk3562 */
+	GRF_DPHY1_CLK_INV_SEL,
+	GRF_DPHY1_CLK1_INV_SEL,
+	GRF_DPHY1_CSI2PHY_CLKLANE1_EN,
+	GRF_DPHY1_CSI2PHY_FORCERXMODE,
+	GRF_DPHY1_CSI2PHY_CLKLANE_EN,
+	GRF_DPHY1_CSI2PHY_DATALANE_EN,
+	GRF_DPHY1_CSI2PHY_DATALANE_EN0,
+	GRF_DPHY1_CSI2PHY_DATALANE_EN1,
+};
+
+enum csi2dphy_reg_id {
+	CSI2PHY_REG_CTRL_LANE_ENABLE = 0,
+	CSI2PHY_CTRL_PWRCTL,
+	CSI2PHY_CTRL_DIG_RST,
+	CSI2PHY_CLK_THS_SETTLE,
+	CSI2PHY_LANE0_THS_SETTLE,
+	CSI2PHY_LANE1_THS_SETTLE,
+	CSI2PHY_LANE2_THS_SETTLE,
+	CSI2PHY_LANE3_THS_SETTLE,
+	CSI2PHY_CLK_CALIB_ENABLE,
+	CSI2PHY_LANE0_CALIB_ENABLE,
+	CSI2PHY_LANE1_CALIB_ENABLE,
+	CSI2PHY_LANE2_CALIB_ENABLE,
+	CSI2PHY_LANE3_CALIB_ENABLE,
+	//rv1126 only
+	CSI2PHY_MIPI_LVDS_MODEL,
+	CSI2PHY_LVDS_MODE,
+	//rk3568 only
+	CSI2PHY_DUAL_CLK_EN,
+	CSI2PHY_CLK1_THS_SETTLE,
+	CSI2PHY_CLK1_CALIB_ENABLE,
+	//rk3588
+	CSI2PHY_CLK_LANE_ENABLE,
+	CSI2PHY_CLK1_LANE_ENABLE,
+	CSI2PHY_DATA_LANE0_ENABLE,
+	CSI2PHY_DATA_LANE1_ENABLE,
+	CSI2PHY_DATA_LANE2_ENABLE,
+	CSI2PHY_DATA_LANE3_ENABLE,
+	CSI2PHY_LANE0_ERR_SOT_SYNC,
+	CSI2PHY_LANE1_ERR_SOT_SYNC,
+	CSI2PHY_LANE2_ERR_SOT_SYNC,
+	CSI2PHY_LANE3_ERR_SOT_SYNC,
+	CSI2PHY_S0C_GNR_CON1,
+	CSI2PHY_S0C_ANA_CON1,
+	CSI2PHY_S0C_ANA_CON2,
+	CSI2PHY_S0C_ANA_CON3,
+	CSI2PHY_COMBO_S0D0_GNR_CON1,
+	CSI2PHY_COMBO_S0D0_ANA_CON1,
+	CSI2PHY_COMBO_S0D0_ANA_CON2,
+	CSI2PHY_COMBO_S0D0_ANA_CON3,
+	CSI2PHY_COMBO_S0D0_ANA_CON6,
+	CSI2PHY_COMBO_S0D0_ANA_CON7,
+	CSI2PHY_COMBO_S0D0_DESKEW_CON0,
+	CSI2PHY_COMBO_S0D0_DESKEW_CON2,
+	CSI2PHY_COMBO_S0D0_DESKEW_CON4,
+	CSI2PHY_COMBO_S0D0_CRC_CON1,
+	CSI2PHY_COMBO_S0D0_CRC_CON2,
+	CSI2PHY_COMBO_S0D1_GNR_CON1,
+	CSI2PHY_COMBO_S0D1_ANA_CON1,
+	CSI2PHY_COMBO_S0D1_ANA_CON2,
+	CSI2PHY_COMBO_S0D1_ANA_CON3,
+	CSI2PHY_COMBO_S0D1_ANA_CON6,
+	CSI2PHY_COMBO_S0D1_ANA_CON7,
+	CSI2PHY_COMBO_S0D1_DESKEW_CON0,
+	CSI2PHY_COMBO_S0D1_DESKEW_CON2,
+	CSI2PHY_COMBO_S0D1_DESKEW_CON4,
+	CSI2PHY_COMBO_S0D1_CRC_CON1,
+	CSI2PHY_COMBO_S0D1_CRC_CON2,
+	CSI2PHY_COMBO_S0D2_GNR_CON1,
+	CSI2PHY_COMBO_S0D2_ANA_CON1,
+	CSI2PHY_COMBO_S0D2_ANA_CON2,
+	CSI2PHY_COMBO_S0D2_ANA_CON3,
+	CSI2PHY_COMBO_S0D2_ANA_CON6,
+	CSI2PHY_COMBO_S0D2_ANA_CON7,
+	CSI2PHY_COMBO_S0D2_DESKEW_CON0,
+	CSI2PHY_COMBO_S0D2_DESKEW_CON2,
+	CSI2PHY_COMBO_S0D2_DESKEW_CON4,
+	CSI2PHY_COMBO_S0D2_CRC_CON1,
+	CSI2PHY_COMBO_S0D2_CRC_CON2,
+	CSI2PHY_S0D3_GNR_CON1,
+	CSI2PHY_S0D3_ANA_CON1,
+	CSI2PHY_S0D3_ANA_CON2,
+	CSI2PHY_S0D3_ANA_CON3,
+	CSI2PHY_S0D3_DESKEW_CON0,
+	CSI2PHY_S0D3_DESKEW_CON2,
+	CSI2PHY_S0D3_DESKEW_CON4,
+};
+
+#define HIWORD_UPDATE(val, mask, shift) \
+		((val) << (shift) | (mask) << ((shift) + 16))
+
+#define GRF_REG(_offset, _width, _shift) \
+	{ .offset = _offset, .mask = BIT(_width) - 1, .shift = _shift, }
+
+#define CSI2PHY_REG(_offset) \
+	{ .offset = _offset, }
+
+/* add new chip id in tail by time order */
+enum csi2_dphy_chip_id {
+	CHIP_ID_RK3568 = 0x0,
+	CHIP_ID_RK3588 = 0x1,
+	CHIP_ID_RK3588_DCPHY = 0x2,
+	CHIP_ID_RV1106 = 0x3,
+	CHIP_ID_RK3562 = 0x4,
+	CHIP_ID_RK3576 = 0x5,
+};
+
+enum csi2_dphy_rx_pads {
+	CSI2_DPHY_RX_PAD_SINK = 0,
+	CSI2_DPHY_RX_PAD_SOURCE,
+	CSI2_DPHY_RX_PADS_NUM,
+};
+
+enum csi2_dphy_lane_mode {
+	LANE_MODE_UNDEF = 0x0,
+	LANE_MODE_FULL,
+	LANE_MODE_SPLIT,
+};
+
+struct grf_reg {
+	u32 offset;
+	u32 mask;
+	u32 shift;
+};
+
+struct csi2dphy_reg {
+	u32 offset;
+};
+
+struct hsfreq_range {
+	u32 range_h;
+	u16 cfg_bit;
+};
+
+#define MAX_DPHY_SENSORS	(2)
+#define MAX_NUM_CSI2_DPHY	(0x2)
+
+#define RKCSI2_MAX_RESET 8
+#define RKDPHY_MAX_RESET 8
+/* csi2 head */
+
+struct csi2_dphy_hw {
+	struct	clk_bulk_data *dphy_clks;
+	int num_dphy_clks;
+	struct	clk_bulk_data *csi2_clks;
+	int num_csi2_clks;
+	const char * const *csi2_rsts;
+	struct reset_control *csi2_rst[RKCSI2_MAX_RESET];
+	int num_csi2_rsts;
+	const char * const *dphy_rsts;
+	struct reset_control *dphy_rst[RKDPHY_MAX_RESET];
+	int num_dphy_rsts;
+	// struct reset_control	*rsts_bulk;
+	/*  spinlock_t lock; */
+	bool on;
+	const struct hsfreq_range *hsfreq_ranges;
+	int num_hsfreq_ranges;
+	const struct grf_reg *grf_regs;
+	const struct txrx_reg *txrx_regs;
+	const struct csi2dphy_reg *csi2dphy_regs;
+	enum csi2_dphy_chip_id chip_id;
+	struct device *dev;
+	struct regmap *regmap_grf;
+	struct regmap *regmap_sys_grf;
+	void __iomem	*csi2_dphy_base; /*csi2_dphy base addr*/
+	void __iomem	*csi2_base; /*csi2 base addr*/
+	struct mutex mutex; /* lock for updating protection */
+	atomic_t stream_cnt;
+	struct csi2_err_stats err_list[RK_CSI2_ERR_MAX];
+	u64 data_rate_mbps;
+	struct rkmodule_csi_dphy_param *dphy_param;
+	struct samsung_mipi_dcphy *samsung_phy;
+	int phy_index;
+};
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad.h b/drivers/video/rockchip/vehicle/vehicle_ad.h
new file mode 100644
index 0000000000000..dd189459145d9
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad.h
@@ -0,0 +1,84 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_COMMON_H
+#define __VEHICLE_AD_COMMON_H
+#include <linux/i2c.h>
+#include "vehicle_cfg.h"
+#include <linux/rk-camera-module.h>
+
+enum vehicle_ad_fix_format {
+	AD_FIX_FORMAT_AUTO_DETECT = 0,
+	AD_FIX_FORMAT_PAL = 1,
+	AD_FIX_FORMAT_NTSC = 2,
+	AD_FIX_FORMAT_720P_50FPS = 3,
+	AD_FIX_FORMAT_720P_30FPS = 4,
+	AD_FIX_FORMAT_720P_25FPS = 5,
+	AD_FIX_FORMAT_1080P_30FPS = 6,
+	AD_FIX_FORMAT_1080P_25FPS = 7,
+};
+
+struct vehicle_camera_device_defrect {
+	unsigned int width;
+	unsigned int height;
+	unsigned int crop_x;
+	unsigned int crop_y;
+	unsigned int crop_width;
+	unsigned int crop_height;
+	const char *interface;
+};
+
+struct vehicle_state_check_work {
+	struct workqueue_struct *state_check_wq;
+	struct delayed_work work;
+};
+
+struct vehicle_ad_dev {
+	struct device *dev;
+	struct i2c_adapter *adapter;
+	const char *ad_name;
+	int resolution;
+	int mclk_rate;
+	int ad_chl;
+	int i2c_chl;
+	int i2c_add;
+//	int i2c_rate;
+	int powerdown;
+	int pwdn_active;
+	int power;
+	int pwr_active;
+	int reset;
+	int rst_active;
+	int cvstd;
+	int cvstd_irq_flag;
+	int irq;
+	int fix_format;
+	struct vehicle_camera_device_defrect defrects[4];
+	struct vehicle_state_check_work	state_check_work;
+	struct vehicle_cfg cfg;
+	int cif_error_last_line;
+	u32 channel_reso[PAD_MAX];
+	u8 detect_status;
+	u8 last_detect_status;
+	int drop_frames;
+	struct clk	*xvclk;
+};
+
+int vehicle_generic_sensor_write(struct vehicle_ad_dev *ad, char reg, char *pval);
+int vehicle_sensor_write(struct vehicle_ad_dev *ad, u8 reg, u8 val);
+int vehicle_generic_sensor_read(struct vehicle_ad_dev *ad, char reg);
+int vehicle_sensor_read(struct vehicle_ad_dev *ad, u8 reg, u8 *val);
+int vehicle_parse_sensor(struct vehicle_ad_dev *ad);
+void vehicle_ad_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+int vehicle_ad_init(struct vehicle_ad_dev *ad);
+int vehicle_ad_deinit(struct vehicle_ad_dev *ad);
+int vehicle_ad_stream(struct vehicle_ad_dev *ad, int val);
+struct vehicle_cfg *vehicle_ad_get_vehicle_cfg(void);
+void vehicle_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int vehicle_to_v4l2_drv_init(void);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_7181.c b/drivers/video/rockchip/vehicle/vehicle_ad_7181.c
new file mode 100644
index 0000000000000..d42b33f148807
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_7181.c
@@ -0,0 +1,608 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor adv7181
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      Zhiqin Wei <wzq@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include <linux/videodev2.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_7181.h"
+
+enum {
+	FORCE_PAL_WIDTH = 720,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 720,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+static struct vehicle_ad_dev *ad7181_g_addev;
+static v4l2_std_id std_old = V4L2_STD_NTSC;
+
+#define SENSOR_REGISTER_LEN	1	/* sensor register address bytes*/
+#define SENSOR_VALUE_LEN	1	/* sensor register value bytes*/
+
+struct rk_sensor_reg {
+	unsigned int reg;
+	unsigned int val;
+};
+
+#define ADV7181_STATUS1_REG		0x10
+#define ADV7181_STATUS1_IN_LOCK		0x01
+#define ADV7181_STATUS1_AUTOD_MASK	0x70
+#define ADV7181_STATUS1_AUTOD_NTSM_M_J	0x00
+#define ADV7181_STATUS1_AUTOD_NTSC_4_43 0x10
+#define ADV7181_STATUS1_AUTOD_PAL_M	0x20
+#define ADV7181_STATUS1_AUTOD_PAL_60	0x30
+#define ADV7181_STATUS1_AUTOD_PAL_B_G	0x40
+#define ADV7181_STATUS1_AUTOD_SECAM	0x50
+#define ADV7181_STATUS1_AUTOD_PAL_COMB	0x60
+#define ADV7181_STATUS1_AUTOD_SECAM_525	0x70
+
+#define ADV7181_INPUT_CONTROL		0x00
+#define ADV7181_INPUT_DEFAULT		0x00
+#define ADV7181_INPUT_CVBS_AIN2		0x00
+#define ADV7181_INPUT_CVBS_AIN3		0x01
+#define ADV7181_INPUT_CVBS_AIN5		0x02
+#define ADV7181_INPUT_CVBS_AIN6		0x03
+#define ADV7181_INPUT_CVBS_AIN8		0x04
+#define ADV7181_INPUT_CVBS_AIN10	0x05
+#define ADV7181_INPUT_CVBS_AIN1		0x0B
+#define ADV7181_INPUT_CVBS_AIN4		0x0D
+#define ADV7181_INPUT_CVBS_AIN7		0x0F
+#define ADV7181_INPUT_YPRPB_AIN6_8_10	0x00
+
+#define SEQCMD_END  0xFF000000
+#define SensorEnd   {SEQCMD_END, 0x00}
+
+#define SENSOR_DG VEHICLE_DG
+
+/* Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data[] = {
+	/* autodetect cvbs in ntsc/pal/secam 8-bit 422 encode */
+	{0x00, 0x0B}, /*cvbs in AIN1*/
+	{0x04, 0x77},
+	{0x17, 0x41},
+	{0x1D, 0x47},
+	{0x31, 0x02},
+	{0x3A, 0x17},
+	{0x3B, 0x81},
+	{0x3D, 0xA2},
+	{0x3E, 0x6A},
+	{0x3F, 0xA0},
+	{0x86, 0x0B},
+	{0xF3, 0x01},
+	{0xF9, 0x03},
+	{0x0E, 0x80},
+	{0x52, 0x46},
+	{0x54, 0x80},
+	{0x7F, 0xFF},
+	{0x81, 0x30},
+	{0x90, 0xC9},
+	{0x91, 0x40},
+	{0x92, 0x3C},
+	{0x93, 0xCA},
+	{0x94, 0xD5},
+	{0xB1, 0xFF},
+	{0xB6, 0x08},
+	{0xC0, 0x9A},
+	{0xCF, 0x50},
+	{0xD0, 0x4E},
+	{0xD1, 0xB9},
+	{0xD6, 0xDD},
+	{0xD7, 0xE2},
+	{0xE5, 0x51},
+	{0xF6, 0x3B},
+	{0x0E, 0x00},
+	{0x03, 0x4C}, //stream off
+	{0xDF, 0X46},
+	{0xC9, 0x04},
+	{0xC5, 0x81},
+	{0xC4, 0x34},
+	{0xBf, 0x02},
+	{0xB5, 0x83},
+	{0xB6, 0x00},
+	{0xaf, 0x03},
+	{0xae, 0x00},
+	{0xac, 0x00},
+	{0xAB, 0x00},
+	{0xa1, 0xFF},
+	{0xA2, 0x00},
+	{0xA3, 0x00},
+	{0xA4, 0x00},
+	{0xa5, 0x01},
+	{0xA6, 0x00},
+	{0xA6, 0x00},
+	{0xA7, 0x00},
+	{0xA8, 0x00},
+	{0xa0, 0x03},
+	{0x98, 0X00},
+	{0x97, 0X00},
+	{0X90, 0X00},
+	{0X85, 0X02},
+	{0x7B, 0x1E},
+	{0x74, 0x04},
+	{0x75, 0x01},
+	{0x76, 0x00},
+	{0x6B, 0xC0},
+	{0x67, 0x03},
+	{0x3C, 0x58},
+	{0x30, 0x4C},
+	{0x2E, 0X9F},
+	{0x12, 0XC0},
+	{0x10, 0X0D},
+	{0x05, 0X00},
+	{0x06, 0X02},
+	{0x60, 0x01},
+	SensorEnd
+};
+
+static struct rk_sensor_reg sensor_preview_data_yprpb_p[] = {
+	{0x05, 0x01},
+	{0x06, 0x06},
+	{0xc3, 0x56},
+	{0xc4, 0xb4},
+	{0x1d, 0x47},
+	{0x3a, 0x11},
+	{0x3b, 0x81},
+	{0x3c, 0x3b},
+	{0x6b, 0x83},
+	{0xc9, 0x00},
+	{0x73, 0x10},
+	{0x74, 0xa3},
+	{0x75, 0xe8},
+	{0x76, 0xfa},
+	{0x7b, 0x1c},
+	{0x85, 0x19},
+	{0x86, 0x0b},
+	{0xbf, 0x06},
+	{0xc0, 0x40},
+	{0xc1, 0xf0},
+	{0xc2, 0x80},
+	{0xc5, 0x01},
+	{0xc9, 0x08},
+	{0x0e, 0x80},
+	{0x52, 0x46},
+	{0x54, 0x80},
+	{0x57, 0x01},
+	{0xf6, 0x3b},
+	{0x0e, 0x00},
+	{0x67, 0x2f},
+	{0x03, 0x4C}, //disable out put
+	SensorEnd
+};
+
+static v4l2_std_id adv7181_std_to_v4l2(u8 status1)
+{
+	/* in case V4L2_IN_ST_NO_SIGNAL */
+	if (!(status1 & ADV7181_STATUS1_IN_LOCK))
+		return V4L2_STD_UNKNOWN;
+
+	switch (status1 & ADV7181_STATUS1_AUTOD_MASK) {
+	case ADV7181_STATUS1_AUTOD_PAL_M:
+	case ADV7181_STATUS1_AUTOD_NTSM_M_J:
+		return V4L2_STD_NTSC;
+	case ADV7181_STATUS1_AUTOD_NTSC_4_43:
+		return V4L2_STD_NTSC_443;
+	case ADV7181_STATUS1_AUTOD_PAL_60:
+		return V4L2_STD_PAL_60;
+	case ADV7181_STATUS1_AUTOD_PAL_B_G:
+		return V4L2_STD_PAL;
+	case ADV7181_STATUS1_AUTOD_SECAM:
+		return V4L2_STD_SECAM;
+	case ADV7181_STATUS1_AUTOD_PAL_COMB:
+		return V4L2_STD_PAL_Nc | V4L2_STD_PAL_N;
+	case ADV7181_STATUS1_AUTOD_SECAM_525:
+		return V4L2_STD_SECAM;
+	default:
+		return V4L2_STD_UNKNOWN;
+	}
+}
+
+static u32 adv7181_status_to_v4l2(u8 status1)
+{
+	if (!(status1 & ADV7181_STATUS1_IN_LOCK))
+		return V4L2_IN_ST_NO_SIGNAL;
+
+	return 0;
+}
+
+static int adv7181_vehicle_status(struct vehicle_ad_dev *ad,
+				  u32 *status,
+				  v4l2_std_id *std)
+{
+	unsigned char status1 = 0;
+
+	status1 = vehicle_generic_sensor_read(ad, ADV7181_STATUS1_REG);
+	if (status1)
+		return status1;
+
+	if (status)
+		*status = adv7181_status_to_v4l2(status1);
+
+	if (std)
+		*std = adv7181_std_to_v4l2(status1);
+
+	return 0;
+}
+
+static void adv7181_reinit_parameter(struct vehicle_ad_dev *ad, v4l2_std_id std)
+{
+	int i;
+
+	if (ad7181_g_addev->ad_chl == 0) {
+		ad->cfg.width = 1024;
+		ad->cfg.height = 500;
+		ad->cfg.start_x = 56;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 1;
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 60;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+	} else if (std == V4L2_STD_PAL) {
+		ad->cfg.width = FORCE_PAL_WIDTH;
+		ad->cfg.height = FORCE_PAL_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_PAL;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+	} else {
+		ad->cfg.width = FORCE_NTSC_WIDTH;
+		ad->cfg.height = FORCE_NTSC_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_NTSC;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 2;
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+
+	SENSOR_DG("size %dx%d, crop(%d,%d)\n",
+	    ad->cfg.width, ad->cfg.height,
+	    ad->cfg.start_x, ad->cfg.start_y);
+}
+
+static void adv7181_reg_init(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	struct rk_sensor_reg *sensor;
+	int i = 0;
+	unsigned char val[2];
+
+	switch (ad->ad_chl) {
+	case 0:
+		ad->ad_chl = ADV7181_INPUT_CVBS_AIN1;
+		break;
+	case 1:
+		ad->ad_chl = ADV7181_INPUT_CVBS_AIN6;
+		break;
+	case 2:
+		ad->ad_chl = ADV7181_INPUT_CVBS_AIN8;
+		break;
+	case 3:
+		ad->ad_chl = ADV7181_INPUT_CVBS_AIN10;
+		break;
+	case 4:
+		ad->ad_chl = ADV7181_INPUT_YPRPB_AIN6_8_10;
+		break;
+	default:
+		ad->ad_chl = ADV7181_INPUT_CVBS_AIN1;
+	}
+	val[0] = ad->ad_chl;
+	vehicle_generic_sensor_write(ad, ADV7181_INPUT_CONTROL, val);
+
+	if (ad->ad_chl == ADV7181_INPUT_YPRPB_AIN6_8_10) {
+		SENSOR_DG("%s %d set sensor_preview_data_yprpb_p/p", __func__, __LINE__);
+		sensor = sensor_preview_data_yprpb_p;
+	} else {
+		SENSOR_DG("%s %d set n/p", __func__, __LINE__);
+		sensor = sensor_preview_data;
+	}
+	while ((sensor[i].reg != SEQCMD_END) && (sensor[i].reg != 0xFC000000)) {
+		if (sensor[i].reg == ADV7181_INPUT_CONTROL) {
+			SENSOR_DG("%s %d lkg test ad channel = %d\n",
+					__func__, __LINE__, ad->ad_chl);
+		} else {
+			val[0] = sensor[i].val;
+			vehicle_generic_sensor_write(ad, sensor[i].reg, val);
+		}
+		i++;
+	}
+
+	val[0] = ad->ad_chl;
+	vehicle_generic_sensor_write(ad, ADV7181_INPUT_CONTROL, val);
+}
+
+int adv7181_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	u32 status;
+
+	if (!ad7181_g_addev)
+		return -1;
+
+	adv7181_vehicle_status(ad7181_g_addev, &status, NULL);
+
+	ad7181_g_addev->cfg.ad_ready = true;
+
+	*cfg = &ad7181_g_addev->cfg;
+
+	return 0;
+}
+
+void adv7181_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	SENSOR_DG("%s, last_line %d\n", __func__, last_line);
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (std_old == V4L2_STD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (std_old == V4L2_STD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int adv7181_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	int val;
+
+	val = vehicle_generic_sensor_read(ad, 0x11);
+	SENSOR_DG("%s vehicle read 0x11 --> 0x%02x\n", ad->ad_name, val);
+	if (val != 0x20) {
+		SENSOR_DG("%s vehicle wrong camera ID, expected 0x20, detected 0x%02x\n",
+		    ad->ad_name, val);
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int adv7181_check_std(struct vehicle_ad_dev *ad, v4l2_std_id *std)
+{
+	u32 status = 0;
+
+	adv7181_vehicle_status(ad, &status, std);
+
+	if (status != 0) { /* No signal */
+		mdelay(30);
+		adv7181_vehicle_status(ad, &status, std);
+		SENSOR_DG("status 0x%x\n", status);
+	}
+
+	return 0;
+}
+void adv7181_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	static int channel_change = 11;
+	v4l2_std_id std = 0;
+
+	ad->ad_chl = channel;
+	adv7181_reg_init(ad, std);
+	adv7181_check_std(ad, &std);
+	adv7181_reinit_parameter(ad, std);
+	if (channel_change != ad->ad_chl) {
+		SENSOR_DG("%s %d channel changed now channel = %d old_channel = %d\n",
+						__func__, __LINE__, ad->ad_chl, channel);
+		channel_change = ad->ad_chl;
+		vehicle_ad_stat_change_notify();
+	}
+}
+
+int adv7181_stream(struct vehicle_ad_dev *ad, int value)
+{
+	char val;
+
+	if (value)
+		val = 0x0c;	//on
+	else
+		val = 0x4c;
+
+	SENSOR_DG("stream write 0x%x to reg 0x03\n", val);
+	vehicle_generic_sensor_write(ad, 0x03, &val);
+	if (value)
+		val = 0x47;	//on
+	else
+		val = 0x87;
+
+	SENSOR_DG("stream write 0x%x to reg 0x01d\n", val);
+	vehicle_generic_sensor_write(ad, 0x1d, &val);
+
+	return 0;
+}
+
+static void power_on(struct vehicle_ad_dev *ad)
+{
+	/* gpio_direction_output(ad->power, ad->pwr_active); */
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "ad_powerdown");
+		gpio_direction_output(ad->powerdown, !ad->pwdn_active);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "ad_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->reset)) {
+		gpio_request(ad->reset, "ad_reset");
+		gpio_direction_output(ad->reset, 0);
+		usleep_range(10000, 12000);
+		gpio_set_value(ad->reset, 1);
+		usleep_range(10000, 12000);
+	}
+}
+
+static void power_off(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+
+	if (gpio_is_valid(ad->reset))
+		gpio_free(ad->reset);
+}
+
+static void adv7181_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+	v4l2_std_id std;
+
+	ad = ad7181_g_addev;
+
+	if (ad->cif_error_last_line > 0)
+		ad->cif_error_last_line = 0;
+
+	adv7181_check_std(ad, &std);
+	SENSOR_DG("%s:new std(%llx), std_old(%llx)\n", __func__, std, std_old);
+	if (std != std_old) {
+		std_old = std;
+		adv7181_reinit_parameter(ad, std);
+		SENSOR_DG("%s:ad signal change notify\n", __func__);
+		vehicle_ad_stat_change_notify();
+	}
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(3000));
+}
+
+int adv7181_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = ad7181_g_addev;
+
+	if (!ad)
+		return -ENODEV;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+	if (ad->irq)
+		free_irq(ad->irq, ad);
+	power_off(ad);
+
+	return 0;
+}
+
+int adv7181_ad_init(struct vehicle_ad_dev *ad)
+{
+	v4l2_std_id std = V4L2_STD_NTSC;
+
+	if (!ad)
+		return -1;
+
+	ad7181_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	/*  2. ad power on sequence */
+	power_on(ad);
+
+	/* fix mode */
+	adv7181_check_std(ad, &std);
+	std_old = std;
+	SENSOR_DG("std: %s\n", (std == V4L2_STD_NTSC) ? "ntsc" : "pal");
+	SENSOR_DG("std_old: %s\n", (std_old == V4L2_STD_NTSC) ? "ntsc" : "pal");
+
+	/*  3 .init default format params */
+	adv7181_reg_init(ad, std);
+	adv7181_reinit_parameter(ad, std);
+	vehicle_ad_stat_change_notify();
+
+	/*  5. create workqueue to detect signal change */
+	INIT_DELAYED_WORK(&ad->state_check_work.work, adv7181_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-adv7181");
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
+
+
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_7181.h b/drivers/video/rockchip/vehicle/vehicle_ad_7181.h
new file mode 100644
index 0000000000000..b4e572ed0ef86
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_7181.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_7181_H__
+#define __VEHICLE_AD_7181_H__
+
+int adv7181_ad_init(struct vehicle_ad_dev *ad);
+int adv7181_ad_deinit(void);
+int adv7181_ad_get_cfg(struct vehicle_cfg **cfg);
+int adv7181_stream(struct vehicle_ad_dev *ad, int value);
+void adv7181_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int adv7181_check_id(struct vehicle_ad_dev *ad);
+void adv7181_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
+
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.c b/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.c
new file mode 100644
index 0000000000000..4a102baa16b97
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.c
@@ -0,0 +1,1149 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor gc2145
+ *
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      Zhiqin Wei <wzq@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_gc2145.h"
+#include <media/v4l2-mediabus.h>
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SVGAP30,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_SVGA_WIDTH = 800,
+	FORCE_SVGA_HEIGHT = 600,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_422,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+#define FLAG_LOSS			(0x1 << 7)
+#define FLAG_V_LOCKED			(0x1 << 6)
+#define FLAG_H_LOCKED			(0x1 << 5)
+#define FLAG_CARRIER_PLL_LOCKED		(0x1 << 4)
+#define FLAG_VIDEO_DETECTED		(0x1 << 3)
+#define FLAG_EQ_SD_DETECTED		(0x1 << 2)
+#define FLAG_PROGRESSIVE		(0x1 << 1)
+#define FLAG_NO_CARRIER			(0x1 << 0)
+#define FLAG_LOCKED			(FLAG_V_LOCKED | FLAG_H_LOCKED)
+
+static struct vehicle_ad_dev *gc2145_g_addev;
+static int cvstd_mode = CVSTD_SVGAP30;
+static int cvstd_old = CVSTD_NTSC;
+//static int cvstd_sd = CVSTD_NTSC;
+static int cvstd_state = VIDEO_UNPLUG;
+static int cvstd_old_state = VIDEO_UNLOCK;
+
+#define SENSOR_REGISTER_LEN	1	/* sensor register address bytes*/
+#define SENSOR_VALUE_LEN	1	/* sensor register value bytes*/
+
+struct rk_sensor_reg {
+	unsigned int reg;
+	unsigned int val;
+};
+
+#define SENSOR_CHANNEL_REG		0x41
+
+#define SEQCMD_END  0xFF000000
+#define SensorEnd   {SEQCMD_END, 0x00}
+
+#define SENSOR_DG VEHICLE_DG
+#define SENSOR_ID(_msb, _lsb)		((_msb) << 8 | (_lsb))
+
+/* Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_svga_30hz[] = {
+	{0xfe, 0xf0},
+	{0xfe, 0xf0},
+	{0xfe, 0xf0},
+	{0xfc, 0x06},
+	{0xf6, 0x00},
+	{0xf7, 0x1d},
+	{0xf8, 0x84},
+	{0xfa, 0x00},
+	{0xf9, 0xfe},
+	{0xf2, 0x00},
+	/*ISP reg*/
+	{0xfe, 0x00},
+	{0x03, 0x04},
+	{0x04, 0xe2},
+	{0x09, 0x00},
+	{0x0a, 0x00},
+	{0x0b, 0x00},
+	{0x0c, 0x00},
+	{0x0d, 0x04},
+	{0x0e, 0xc0},
+	{0x0f, 0x06},
+	{0x10, 0x52},
+	{0x12, 0x2e},
+	{0x17, 0x14},
+	{0x18, 0x22},
+	{0x19, 0x0e},
+	{0x1a, 0x01},
+	{0x1b, 0x4b},
+	{0x1c, 0x07},
+	{0x1d, 0x10},
+	{0x1e, 0x88},
+	{0x1f, 0x78},
+	{0x20, 0x03},
+	{0x21, 0x40},
+	{0x22, 0xa0},
+	{0x24, 0x3f},
+	{0x25, 0x01},
+	{0x26, 0x10},
+	{0x2d, 0x60},
+	{0x30, 0x01},
+	{0x31, 0x90},
+	{0x33, 0x06},
+	{0x34, 0x01},
+	{0xfe, 0x00},
+	{0x80, 0x7f},
+	{0x81, 0x26},
+	{0x82, 0xfa},
+	{0x83, 0x00},
+	{0x84, 0x00},
+	{0x86, 0x02},
+	{0x88, 0x03},
+	{0x89, 0x03},
+	{0x85, 0x08},
+	{0x8a, 0x00},
+	{0x8b, 0x00},
+	{0xb0, 0x55},
+	{0xc3, 0x00},
+	{0xc4, 0x80},
+	{0xc5, 0x90},
+	{0xc6, 0x3b},
+	{0xc7, 0x46},
+	{0xec, 0x06},
+	{0xed, 0x04},
+	{0xee, 0x60},
+	{0xef, 0x90},
+	{0xb6, 0x01},
+	{0x90, 0x01},
+	{0x91, 0x00},
+	{0x92, 0x00},
+	{0x93, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x04},
+	{0x96, 0xb0},
+	{0x97, 0x06},
+	{0x98, 0x40},
+	/*BLK*/
+	{0xfe, 0x00},
+	{0x40, 0x42},
+	{0x41, 0x00},
+	{0x43, 0x5b},
+	{0x5e, 0x00},
+	{0x5f, 0x00},
+	{0x60, 0x00},
+	{0x61, 0x00},
+	{0x62, 0x00},
+	{0x63, 0x00},
+	{0x64, 0x00},
+	{0x65, 0x00},
+	{0x66, 0x20},
+	{0x67, 0x20},
+	{0x68, 0x20},
+	{0x69, 0x20},
+	{0x76, 0x00},
+	{0x6a, 0x08},
+	{0x6b, 0x08},
+	{0x6c, 0x08},
+	{0x6d, 0x08},
+	{0x6e, 0x08},
+	{0x6f, 0x08},
+	{0x70, 0x08},
+	{0x71, 0x08},
+	{0x76, 0x00},
+	{0x72, 0xf0},
+	{0x7e, 0x3c},
+	{0x7f, 0x00},
+	{0xfe, 0x02},
+	{0x48, 0x15},
+	{0x49, 0x00},
+	{0x4b, 0x0b},
+	{0xfe, 0x00},
+	/*AEC*/
+	{0xfe, 0x01},
+	{0x01, 0x04},
+	{0x02, 0xc0},
+	{0x03, 0x04},
+	{0x04, 0x90},
+	{0x05, 0x30},
+	{0x06, 0x90},
+	{0x07, 0x30},
+	{0x08, 0x80},
+	{0x09, 0x00},
+	{0x0a, 0x82},
+	{0x0b, 0x11},
+	{0x0c, 0x10},
+	{0x11, 0x10},
+	{0x13, 0x7b},
+	{0x17, 0x00},
+	{0x1c, 0x11},
+	{0x1e, 0x61},
+	{0x1f, 0x35},
+	{0x20, 0x40},
+	{0x22, 0x40},
+	{0x23, 0x20},
+	{0xfe, 0x02},
+	{0x0f, 0x04},
+	{0xfe, 0x01},
+	{0x12, 0x35},
+	{0x15, 0xb0},
+	{0x10, 0x31},
+	{0x3e, 0x28},
+	{0x3f, 0xb0},
+	{0x40, 0x90},
+	{0x41, 0x0f},
+
+	/*INTPEE*/
+	{0xfe, 0x02},
+	{0x90, 0x6c},
+	{0x91, 0x03},
+	{0x92, 0xcb},
+	{0x94, 0x33},
+	{0x95, 0x84},
+	{0x97, 0x45},
+	{0xa2, 0x11},
+	{0xfe, 0x00},
+	/*DNDD*/
+	{0xfe, 0x02},
+	{0x80, 0xc1},
+	{0x81, 0x08},
+	{0x82, 0x1f},
+	{0x83, 0x10},
+	{0x84, 0x0a},
+	{0x86, 0xf0},
+	{0x87, 0x50},
+	{0x88, 0x15},
+	{0x89, 0xb0},
+	{0x8a, 0x30},
+	{0x8b, 0x10},
+	/*ASDE*/
+	{0xfe, 0x01},
+	{0x21, 0x04},
+	{0xfe, 0x02},
+	{0xa3, 0x50},
+	{0xa4, 0x20},
+	{0xa5, 0x40},
+	{0xa6, 0x80},
+	{0xab, 0x40},
+	{0xae, 0x0c},
+	{0xb3, 0x46},
+	{0xb4, 0x64},
+	{0xb6, 0x38},
+	{0xb7, 0x01},
+	{0xb9, 0x2b},
+	{0x3c, 0x04},
+	{0x3d, 0x15},
+	{0x4b, 0x06},
+	{0x4c, 0x20},
+	{0xfe, 0x00},
+	/*GAMMA*/
+	/*gamma1*/
+	{0xfe, 0x02},
+	{0x10, 0x09},
+	{0x11, 0x0d},
+	{0x12, 0x13},
+	{0x13, 0x19},
+	{0x14, 0x27},
+	{0x15, 0x37},
+	{0x16, 0x45},
+	{0x17, 0x53},
+	{0x18, 0x69},
+	{0x19, 0x7d},
+	{0x1a, 0x8f},
+	{0x1b, 0x9d},
+	{0x1c, 0xa9},
+	{0x1d, 0xbd},
+	{0x1e, 0xcd},
+	{0x1f, 0xd9},
+	{0x20, 0xe3},
+	{0x21, 0xea},
+	{0x22, 0xef},
+	{0x23, 0xf5},
+	{0x24, 0xf9},
+	{0x25, 0xff},
+	{0xfe, 0x00},
+	{0xc6, 0x20},
+	{0xc7, 0x2b},
+	/*gamma2*/
+	{0xfe, 0x02},
+	{0x26, 0x0f},
+	{0x27, 0x14},
+	{0x28, 0x19},
+	{0x29, 0x1e},
+	{0x2a, 0x27},
+	{0x2b, 0x33},
+	{0x2c, 0x3b},
+	{0x2d, 0x45},
+	{0x2e, 0x59},
+	{0x2f, 0x69},
+	{0x30, 0x7c},
+	{0x31, 0x89},
+	{0x32, 0x98},
+	{0x33, 0xae},
+	{0x34, 0xc0},
+	{0x35, 0xcf},
+	{0x36, 0xda},
+	{0x37, 0xe2},
+	{0x38, 0xe9},
+	{0x39, 0xf3},
+	{0x3a, 0xf9},
+	{0x3b, 0xff},
+	/*YCP*/
+	{0xfe, 0x02},
+	{0xd1, 0x40},
+	{0xd2, 0x40},
+	{0xd3, 0x48},
+	{0xd6, 0xf0},
+	{0xd7, 0x10},
+	{0xd8, 0xda},
+	{0xdd, 0x14},
+	{0xde, 0x86},
+	{0xed, 0x80},
+	{0xee, 0x00},
+	{0xef, 0x3f},
+	{0xd8, 0xd8},
+	/*abs*/
+	{0xfe, 0x01},
+	{0x9f, 0x40},
+	/*LSC*/
+	{0xfe, 0x01},
+	{0xc2, 0x14},
+	{0xc3, 0x0d},
+	{0xc4, 0x0c},
+	{0xc8, 0x15},
+	{0xc9, 0x0d},
+	{0xca, 0x0a},
+	{0xbc, 0x24},
+	{0xbd, 0x10},
+	{0xbe, 0x0b},
+	{0xb6, 0x25},
+	{0xb7, 0x16},
+	{0xb8, 0x15},
+	{0xc5, 0x00},
+	{0xc6, 0x00},
+	{0xc7, 0x00},
+	{0xcb, 0x00},
+	{0xcc, 0x00},
+	{0xcd, 0x00},
+	{0xbf, 0x07},
+	{0xc0, 0x00},
+	{0xc1, 0x00},
+	{0xb9, 0x00},
+	{0xba, 0x00},
+	{0xbb, 0x00},
+	{0xaa, 0x01},
+	{0xab, 0x01},
+	{0xac, 0x00},
+	{0xad, 0x05},
+	{0xae, 0x06},
+	{0xaf, 0x0e},
+	{0xb0, 0x0b},
+	{0xb1, 0x07},
+	{0xb2, 0x06},
+	{0xb3, 0x17},
+	{0xb4, 0x0e},
+	{0xb5, 0x0e},
+	{0xd0, 0x09},
+	{0xd1, 0x00},
+	{0xd2, 0x00},
+	{0xd6, 0x08},
+	{0xd7, 0x00},
+	{0xd8, 0x00},
+	{0xd9, 0x00},
+	{0xda, 0x00},
+	{0xdb, 0x00},
+	{0xd3, 0x0a},
+	{0xd4, 0x00},
+	{0xd5, 0x00},
+	{0xa4, 0x00},
+	{0xa5, 0x00},
+	{0xa6, 0x77},
+	{0xa7, 0x77},
+	{0xa8, 0x77},
+	{0xa9, 0x77},
+	{0xa1, 0x80},
+	{0xa2, 0x80},
+
+	{0xfe, 0x01},
+	{0xdf, 0x0d},
+	{0xdc, 0x25},
+	{0xdd, 0x30},
+	{0xe0, 0x77},
+	{0xe1, 0x80},
+	{0xe2, 0x77},
+	{0xe3, 0x90},
+	{0xe6, 0x90},
+	{0xe7, 0xa0},
+	{0xe8, 0x90},
+	{0xe9, 0xa0},
+	{0xfe, 0x00},
+	/*AWB*/
+	{0xfe, 0x01},
+	{0x4f, 0x00},
+	{0x4f, 0x00},
+	{0x4b, 0x01},
+	{0x4f, 0x00},
+
+	{0x4c, 0x01},
+	{0x4d, 0x71},
+	{0x4e, 0x01},
+	{0x4c, 0x01},
+	{0x4d, 0x91},
+	{0x4e, 0x01},
+	{0x4c, 0x01},
+	{0x4d, 0x70},
+	{0x4e, 0x01},
+	{0x4c, 0x01},
+	{0x4d, 0x90},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xb0},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0x8f},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0x6f},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xaf},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xd0},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xf0},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xcf},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0xef},
+	{0x4e, 0x02},
+	{0x4c, 0x01},
+	{0x4d, 0x6e},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x8e},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xae},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xce},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x4d},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x6d},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x8d},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xad},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xcd},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x4c},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x6c},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x8c},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xac},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xcc},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xcb},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x4b},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x6b},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x8b},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0xab},
+	{0x4e, 0x03},
+	{0x4c, 0x01},
+	{0x4d, 0x8a},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0xaa},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0xca},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0xca},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0xc9},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0x8a},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0x89},
+	{0x4e, 0x04},
+	{0x4c, 0x01},
+	{0x4d, 0xa9},
+	{0x4e, 0x04},
+	{0x4c, 0x02},
+	{0x4d, 0x0b},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x0a},
+	{0x4e, 0x05},
+	{0x4c, 0x01},
+	{0x4d, 0xeb},
+	{0x4e, 0x05},
+	{0x4c, 0x01},
+	{0x4d, 0xea},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x09},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x29},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x2a},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x4a},
+	{0x4e, 0x05},
+	{0x4c, 0x02},
+	{0x4d, 0x8a},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x49},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x69},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x89},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0xa9},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x48},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x68},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0x69},
+	{0x4e, 0x06},
+	{0x4c, 0x02},
+	{0x4d, 0xca},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xc9},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xe9},
+	{0x4e, 0x07},
+	{0x4c, 0x03},
+	{0x4d, 0x09},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xc8},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xe8},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xa7},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xc7},
+	{0x4e, 0x07},
+	{0x4c, 0x02},
+	{0x4d, 0xe7},
+	{0x4e, 0x07},
+	{0x4c, 0x03},
+	{0x4d, 0x07},
+	{0x4e, 0x07},
+
+	{0x4f, 0x01},
+	{0x50, 0x80},
+	{0x51, 0xa8},
+	{0x52, 0x47},
+	{0x53, 0x38},
+	{0x54, 0xc7},
+	{0x56, 0x0e},
+	{0x58, 0x08},
+	{0x5b, 0x00},
+	{0x5c, 0x74},
+	{0x5d, 0x8b},
+	{0x61, 0xdb},
+	{0x62, 0xb8},
+	{0x63, 0x86},
+	{0x64, 0xc0},
+	{0x65, 0x04},
+	{0x67, 0xa8},
+	{0x68, 0xb0},
+	{0x69, 0x00},
+	{0x6a, 0xa8},
+	{0x6b, 0xb0},
+	{0x6c, 0xaf},
+	{0x6d, 0x8b},
+	{0x6e, 0x50},
+	{0x6f, 0x18},
+	{0x73, 0xf0},
+	{0x70, 0x0d},
+	{0x71, 0x60},
+	{0x72, 0x80},
+	{0x74, 0x01},
+	{0x75, 0x01},
+	{0x7f, 0x0c},
+	{0x76, 0x70},
+	{0x77, 0x58},
+	{0x78, 0xa0},
+	{0x79, 0x5e},
+	{0x7a, 0x54},
+	{0x7b, 0x58},
+	{0xfe, 0x00},
+	/*CC*/
+	{0xfe, 0x02},
+	{0xc0, 0x01},
+	{0xc1, 0x44},
+	{0xc2, 0xfd},
+	{0xc3, 0x04},
+	{0xc4, 0xF0},
+	{0xc5, 0x48},
+	{0xc6, 0xfd},
+	{0xc7, 0x46},
+	{0xc8, 0xfd},
+	{0xc9, 0x02},
+	{0xca, 0xe0},
+	{0xcb, 0x45},
+	{0xcc, 0xec},
+	{0xcd, 0x48},
+	{0xce, 0xf0},
+	{0xcf, 0xf0},
+	{0xe3, 0x0c},
+	{0xe4, 0x4b},
+	{0xe5, 0xe0},
+	/*ABS*/
+	{0xfe, 0x01},
+	{0x9f, 0x40},
+	{0xfe, 0x00},
+	/*OUTPUT*/
+	{0xfe, 0x00},
+	{0xf2, 0x0f},
+	/*dark sun*/
+	{0xfe, 0x02},
+	{0x40, 0xbf},
+	{0x46, 0xcf},
+	{0xfe, 0x00},
+
+	/*frame rate 50Hz*/
+	{0xfe, 0x00},
+	{0x05, 0x02},
+	{0x06, 0x20},
+	{0x07, 0x00},
+	{0x08, 0x32},
+	{0xfe, 0x01},
+	{0x25, 0x00},
+	{0x26, 0xfa},
+
+	{0x27, 0x04},
+	{0x28, 0xe2},
+	{0x29, 0x04},
+	{0x2a, 0xe2},
+	{0x2b, 0x04},
+	{0x2c, 0xe2},
+	{0x2d, 0x04},
+	{0x2e, 0xe2},
+	{0xfe, 0x00},
+
+	{0xfe, 0x00},
+	{0xfd, 0x01},
+	{0xfa, 0x00},
+	/*crop window*/
+	{0xfe, 0x00},
+	{0x90, 0x01},
+	{0x91, 0x00},
+	{0x92, 0x00},
+	{0x93, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x02},
+	{0x96, 0x58},
+	{0x97, 0x03},
+	{0x98, 0x20},
+	{0x99, 0x11},
+	{0x9a, 0x06},
+	/*AWB*/
+	{0xfe, 0x00},
+	{0xec, 0x02},
+	{0xed, 0x02},
+	{0xee, 0x30},
+	{0xef, 0x48},
+	{0xfe, 0x02},
+	{0x9d, 0x08},
+	{0xfe, 0x01},
+	{0x74, 0x00},
+	/*AEC*/
+	{0xfe, 0x01},
+	{0x01, 0x04},
+	{0x02, 0x60},
+	{0x03, 0x02},
+	{0x04, 0x48},
+	{0x05, 0x18},
+	{0x06, 0x50},
+	{0x07, 0x10},
+	{0x08, 0x38},
+	{0x0a, 0x80},
+	{0x21, 0x04},
+	{0xfe, 0x00},
+	{0x20, 0x03},
+	{0xfe, 0x00},
+
+	{0xfe, 0x00},
+	{0x05, 0x02},
+	{0x06, 0x20},
+	{0x07, 0x00},
+	{0x08, 0x50},
+	{0xfe, 0x01},
+	{0x25, 0x00},
+	{0x26, 0xfa},
+
+	{0x27, 0x04},
+	{0x28, 0xe2},
+	{0x29, 0x04},
+	{0x2a, 0xe2},
+	{0x2b, 0x04},
+	{0x2c, 0xe2},
+	{0x2d, 0x04},
+	{0x2e, 0xe2},
+
+	{0xfe, 0x00},
+	{0xb6, 0x01},
+	{0xfd, 0x01},
+	{0xfa, 0x00},
+	{0x18, 0x22},
+	/*crop window*/
+	{0xfe, 0x00},
+	{0x90, 0x01},
+	{0x91, 0x00},
+	{0x92, 0x00},
+	{0x93, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x02},
+	{0x96, 0x58},
+	{0x97, 0x03},
+	{0x98, 0x20},
+	{0x99, 0x11},
+	{0x9a, 0x06},
+	/*AWB*/
+	{0xfe, 0x00},
+	{0xec, 0x02},
+	{0xed, 0x02},
+	{0xee, 0x30},
+	{0xef, 0x48},
+	{0xfe, 0x02},
+	{0x9d, 0x08},
+	{0xfe, 0x01},
+	{0x74, 0x00},
+	/*AEC*/
+	{0xfe, 0x01},
+	{0x01, 0x04},
+	{0x02, 0x60},
+	{0x03, 0x02},
+	{0x04, 0x48},
+	{0x05, 0x18},
+	{0x06, 0x50},
+	{0x07, 0x10},
+	{0x08, 0x38},
+	{0x0a, 0x80},
+	{0x21, 0x04},
+	{0xfe, 0x00},
+	{0x20, 0x03},
+	{0xfe, 0x00},
+	SensorEnd
+};
+
+static void gc2145_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0;
+
+	switch (cvstd) {
+	case CVSTD_PAL:
+	case CVSTD_NTSC:
+	case CVSTD_SVGAP30:
+	default:
+		ad->cfg.width = 800;
+		ad->cfg.height = 600;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_HIGH |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+
+#ifdef CVBS_DOUBLE_FPS_MODE
+#endif
+	SENSOR_DG("%s,crop(%d,%d)", __func__, ad->cfg.start_x, ad->cfg.start_y);
+}
+
+static void gc2145_reg_init(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	struct rk_sensor_reg *sensor;
+	int i;
+	unsigned char val[2];
+
+	switch (cvstd) {
+	case CVSTD_SVGAP30:
+		sensor = sensor_preview_data_svga_30hz;
+		break;
+	default:
+		sensor = sensor_preview_data_svga_30hz;
+		break;
+	}
+	i = 0;
+	while ((sensor[i].reg != SEQCMD_END) && (sensor[i].reg != 0xFC000000)) {
+		if (sensor[i].reg == SENSOR_CHANNEL_REG)
+			sensor[i].val = ad->ad_chl;
+
+		val[0] = sensor[i].val;
+		vehicle_generic_sensor_write(ad, sensor[i].reg, val);
+		i++;
+	}
+}
+
+void gc2145_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	unsigned int reg = 0x41;
+	unsigned char val[0];
+
+	val[0] = channel;
+	ad->ad_chl = channel;
+
+	vehicle_generic_sensor_write(ad, reg, val);
+}
+
+int gc2145_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	if (!gc2145_g_addev)
+		return -1;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		gc2145_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		gc2145_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		gc2145_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	gc2145_g_addev->cfg.ad_ready = true;
+	*cfg = &gc2145_g_addev->cfg;
+
+	return 0;
+}
+
+void gc2145_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	SENSOR_DG("%s, last_line %d\n", __func__, last_line);
+
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_SVGAP30) {
+		if (last_line == FORCE_SVGA_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int gc2145_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	int pidh, pidl;
+	unsigned short id;
+
+	pidh = vehicle_generic_sensor_read(ad, 0xf0);
+	pidl = vehicle_generic_sensor_read(ad, 0xf1);
+	if (pidh != 0x21 || pidl != 0x45) {
+		SENSOR_DG("%s: expected 0x2145, detected 0x%02x 0x%02x\n",
+		    ad->ad_name, pidh, pidl);
+		ret = -EINVAL;
+	} else {
+		id = SENSOR_ID(pidh, pidl);
+		SENSOR_DG("%s Found GC%04X sensor OK!\n", __func__, id);
+	}
+
+	return ret;
+}
+
+static int gc2145_check_cvstd(struct vehicle_ad_dev *ad, bool activate_check)
+{
+	return 0;
+}
+
+int gc2145_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	char val;
+
+	SENSOR_DG("%s on(%d)\n", __func__, enable);
+
+	if (enable)
+		val = 0x0f; //stream on
+	else
+		val = 0x00; //stream off
+	vehicle_generic_sensor_write(ad, 0xf2, &val);
+
+	return 0;
+}
+
+static void power_on(struct vehicle_ad_dev *ad)
+{
+	/* gpio_direction_output(ad->power, ad->pwr_active); */
+	SENSOR_DG("gpio: power(%d), powerdown(%d)", ad->power, ad->powerdown);
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "ad_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "ad_powerdown");
+		gpio_direction_output(ad->powerdown, !ad->pwdn_active);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+}
+
+static void power_off(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static void gc2145_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+	static bool is_first = true;
+
+	ad = gc2145_g_addev;
+
+	if (ad->cif_error_last_line > 0) {
+		gc2145_check_cvstd(ad, true);
+		ad->cif_error_last_line = 0;
+	} else {
+		gc2145_check_cvstd(ad, false);
+	}
+
+	if (is_first) {
+		SENSOR_DG("%s:cvstd_old(%d), cvstd_mode(%d)\n", __func__, cvstd_old, cvstd_mode);
+		is_first = false;
+	}
+
+	if (cvstd_old != cvstd_mode || cvstd_old_state != cvstd_state) {
+		SENSOR_DG("%s:ad sensor std mode change, cvstd_old(%d), cvstd_mode(%d)\n",
+				 __func__, cvstd_old, cvstd_mode);
+		cvstd_old = cvstd_mode;
+		cvstd_old_state = cvstd_state;
+		SENSOR_DG("ad signal change notify\n");
+		vehicle_ad_stat_change_notify();
+	}
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+}
+
+int gc2145_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = gc2145_g_addev;
+
+	if (!ad)
+		return -1;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+	if (ad->irq)
+		free_irq(ad->irq, ad);
+	power_off(ad);
+
+	return 0;
+}
+
+static __maybe_unused int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+	case AD_FIX_FORMAT_NTSC:
+	case AD_FIX_FORMAT_720P_50FPS:
+	case AD_FIX_FORMAT_720P_30FPS:
+	case AD_FIX_FORMAT_720P_25FPS:
+	default:
+		mode = CVSTD_SVGAP30;
+		break;
+	}
+
+	return mode;
+}
+
+int gc2145_ad_init(struct vehicle_ad_dev *ad)
+{
+	int val;
+	int i = 0;
+
+	gc2145_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	/*  2. ad power on sequence */
+	power_on(ad);
+
+	while (++i < 5) {
+		usleep_range(1000, 1200);
+		val = vehicle_generic_sensor_read(ad, 0xf0);
+		if (val != 0xff)
+			break;
+		SENSOR_DG("gc2145_init i2c_reg_read fail\n");
+	}
+
+	/*  3 .init default format params */
+	gc2145_reg_init(ad, cvstd_mode);
+	gc2145_reinit_parameter(ad, cvstd_mode);
+	SENSOR_DG("%s after reinit init\n", __func__);
+
+	/*  5. create workqueue to detect signal change */
+	INIT_DELAYED_WORK(&ad->state_check_work.work, gc2145_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-gc2145");
+
+	/* gc2145_check_cvstd(ad, true); */
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
+
+
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.h b/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.h
new file mode 100644
index 0000000000000..1934bc2172ada
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_gc2145.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_GC2145_H__
+#define __VEHICLE_AD_GC2145_H__
+
+int gc2145_ad_init(struct vehicle_ad_dev *ad);
+int gc2145_ad_deinit(void);
+int gc2145_ad_get_cfg(struct vehicle_cfg **cfg);
+void gc2145_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int gc2145_check_id(struct vehicle_ad_dev *ad);
+int gc2145_stream(struct vehicle_ad_dev *ad, int enable);
+void gc2145_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_max96714.c b/drivers/video/rockchip/vehicle/vehicle_ad_max96714.c
new file mode 100644
index 0000000000000..e3926a5139ae7
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_max96714.c
@@ -0,0 +1,539 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor max96714
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *	Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_max96714.h"
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SVGAP30,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_SVGA_WIDTH = 800,
+	FORCE_SVGA_HEIGHT = 600,
+	FORCE_720P_WIDTH = 1280,
+	FORCE_720P_HEIGHT = 720,
+	FORCE_1080P_WIDTH = 1920,
+	FORCE_1080P_HEIGHT = 1080,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+
+#define FLAG_LOCKED			(0x1 << 3)
+#define MAX96714_LINK_FREQ_150M		150000000UL
+
+static struct vehicle_ad_dev *max96714_g_addev;
+static int cvstd_mode = CVSTD_1080P30;
+//static int cvstd_old = CVSTD_720P25;
+static int cvstd_old = CVSTD_NTSC;
+
+//static int cvstd_sd = CVSTD_NTSC;
+static int cvstd_state = VIDEO_UNPLUG;
+static int cvstd_old_state = VIDEO_UNLOCK;
+
+static bool g_max96714_streaming;
+
+#define SENSOR_VALUE_LEN	1	/* sensor register value bytes*/
+#define MAX96714_CHIP_ID	0xC9
+#define MAX96714_CHIP_ID_REG	0x0D
+#define MAX96714_GMSL_STATE	0x0013
+#define MAX96714_STREAM_CTL	0x0313
+#define MAX96714_MODE_SW_STANDBY	0x0
+#define MAX96714_MODE_STREAMING		BIT(1)
+
+struct regval {
+	u16 reg;
+	u8 val;
+};
+#define REG_NULL  0xFFFF
+
+/* 1080p Preview resolution setting*/
+static struct regval sensor_preview_data_1080p_30hz[] = {
+	{0x0313, 0x00},
+	{0x0001, 0x01},
+	{0x0010, 0x21},
+	{0x0320, 0x23},
+	{0x0325, 0x80},
+	{0x0313, 0x00},
+	{REG_NULL, 0x00},
+};
+
+static struct rkmodule_csi_dphy_param max96714_dcphy_param = {
+	.vendor = PHY_VENDOR_SAMSUNG,
+	.lp_vol_ref = 3,
+	.lp_hys_sw = {3, 0, 0, 0},
+	.lp_escclk_pol_sel = {1, 0, 0, 0},
+	.skew_data_cal_clk = {0, 3, 3, 3},
+	.clk_hs_term_sel = 2,
+	.data_hs_term_sel = {2, 2, 2, 2},
+	.reserved = {0},
+};
+
+static int max96714_read_reg(struct vehicle_ad_dev *ad, u16 reg,
+			    unsigned int len, u32 *val)
+{
+	struct i2c_msg msgs[2];
+	u8 *data_be_p;
+	__be32 data_be = 0;
+	__be16 reg_addr_be = cpu_to_be16(reg);
+	int ret;
+
+	if (len > 4 || !len)
+		return -EINVAL;
+
+	data_be_p = (u8 *)&data_be;
+	/* Write register address */
+	msgs[0].addr = ad->i2c_add;
+	msgs[0].flags = 0;
+	msgs[0].len = 2;
+	msgs[0].buf = (u8 *)&reg_addr_be;
+
+	/* Read data from register */
+	msgs[1].addr = ad->i2c_add;
+	msgs[1].flags = I2C_M_RD;
+	msgs[1].len = len;
+	msgs[1].buf = &data_be_p[4 - len];
+
+	ret = i2c_transfer(ad->adapter, msgs, ARRAY_SIZE(msgs));
+	if (ret != ARRAY_SIZE(msgs))
+		return -EIO;
+
+	*val = be32_to_cpu(data_be);
+
+	return 0;
+}
+
+static int max96714_write_reg(struct vehicle_ad_dev *ad, u16 reg, u8 val)
+{
+	struct i2c_msg msg;
+	u8 buf[3];
+	int ret;
+
+	buf[0] = reg >> 8;
+	buf[1] = reg & 0xff;
+	buf[2] = val;
+
+	msg.addr = ad->i2c_add;
+	msg.flags = 0;
+	msg.buf = buf;
+	msg.len = sizeof(buf);
+
+	ret = i2c_transfer(ad->adapter, &msg, 1);
+	if (ret >= 0)
+		return 0;
+
+	VEHICLE_DGERR(
+		"max96714 write reg(0x%x val:0x%x) failed !\n", reg, val);
+
+	return ret;
+}
+
+static int max96714_write_array(struct vehicle_ad_dev *ad,
+				const struct regval *regs)
+{
+	u32 i = 0;
+	int ret = 0;
+
+	for (i = 0; ret == 0 && regs[i].reg != REG_NULL; i++)
+		ret = max96714_write_reg(ad, regs[i].reg, regs[i].val);
+
+	return ret;
+}
+
+static void max96714_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0;
+
+	switch (cvstd) {
+	case CVSTD_1080P30:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;
+		ad->cfg.mipi_freq = MAX96714_LINK_FREQ_150M;
+		break;
+
+	default:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;
+		ad->cfg.mipi_freq = MAX96714_LINK_FREQ_150M;
+		break;
+	}
+	ad->cfg.type = V4L2_MBUS_CSI2_DPHY;
+	ad->cfg.mbus_flags = V4L2_MBUS_CSI2_4_LANE | V4L2_MBUS_CSI2_CONTINUOUS_CLOCK |
+			 V4L2_MBUS_CSI2_CHANNEL_0;
+	ad->cfg.mbus_code = MEDIA_BUS_FMT_UYVY8_2X8;
+	ad->cfg.dphy_param = &max96714_dcphy_param;
+
+	switch (ad->cfg.mbus_flags & V4L2_MBUS_CSI2_LANES) {
+	case V4L2_MBUS_CSI2_1_LANE:
+		ad->cfg.lanes = 1;
+		break;
+	case V4L2_MBUS_CSI2_2_LANE:
+		ad->cfg.lanes = 2;
+		break;
+	case V4L2_MBUS_CSI2_3_LANE:
+		ad->cfg.lanes = 3;
+		break;
+	case V4L2_MBUS_CSI2_4_LANE:
+		ad->cfg.lanes = 4;
+		break;
+	default:
+		ad->cfg.lanes = 1;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+
+	VEHICLE_DG("crop(%d,%d)", ad->cfg.start_x, ad->cfg.start_y);
+}
+
+static void max96714_reg_init(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	struct regval *sensor;
+	int ret = 0;
+
+	switch (cvstd) {
+	case CVSTD_1080P30:
+		VEHICLE_INFO("%s, init CVSTD_1080P30 mode", __func__);
+		sensor = sensor_preview_data_1080p_30hz;
+		break;
+	default:
+		VEHICLE_INFO("%s, init CVSTD_1080P30 mode", __func__);
+		sensor = sensor_preview_data_1080p_30hz;
+		break;
+	}
+
+	ret = max96714_write_array(ad, sensor);
+	if (ret)
+		VEHICLE_DGERR("%s, init sensor fail", __func__);
+}
+
+void max96714_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+}
+
+int max96714_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	if (!max96714_g_addev)
+		return -1;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		max96714_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		max96714_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		max96714_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	max96714_g_addev->cfg.ad_ready = true;
+
+	*cfg = &max96714_g_addev->cfg;
+
+	return 0;
+}
+
+void max96714_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	VEHICLE_DG("last_line %d\n", last_line);
+
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_1080P30) {
+		if (last_line == FORCE_1080P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int max96714_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	u32 pid = 0;
+
+	ret = max96714_read_reg(ad, MAX96714_CHIP_ID_REG, SENSOR_VALUE_LEN, &pid);
+	if (pid != MAX96714_CHIP_ID) {
+		VEHICLE_DGERR("%s: expected 0xC9, detected: 0x%02x !",
+		    ad->ad_name, pid);
+		ret = -EINVAL;
+	} else {
+		VEHICLE_INFO("Found MAX96714 sensor: id(0x%2x) !\n", pid);
+	}
+
+	return ret;
+}
+
+static int max96714_check_cvstd(struct vehicle_ad_dev *ad, bool activate_check)
+{
+	static int state = VIDEO_UNPLUG;
+	int ret = 0;
+
+	ret = max96714_read_reg(ad, MAX96714_GMSL_STATE, SENSOR_VALUE_LEN, &state);
+	if (ret)
+		VEHICLE_DGERR("read GMSL2 link lock failed!\n");
+
+	if (state & FLAG_LOCKED) {
+		state = VIDEO_LOCKED;
+		VEHICLE_DG("GMSL2 link locked!\n");
+		cvstd_mode = CVSTD_1080P30;
+	} else {
+		state = VIDEO_UNPLUG;
+		VEHICLE_DG("GMSL2 link not locked!\n");
+		cvstd_mode = cvstd_old;
+	}
+
+	return 0;
+}
+
+int max96714_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	VEHICLE_INFO("%s on(%d)\n", __func__, enable);
+
+	g_max96714_streaming = (enable != 0);
+	if (g_max96714_streaming) {
+		max96714_write_reg(ad, MAX96714_STREAM_CTL, MAX96714_MODE_STREAMING);
+		if (ad->state_check_work.state_check_wq)
+			queue_delayed_work(ad->state_check_work.state_check_wq,
+				&ad->state_check_work.work, msecs_to_jiffies(200));
+	} else {
+		max96714_write_reg(ad, MAX96714_STREAM_CTL, MAX96714_MODE_SW_STANDBY);
+		if (ad->state_check_work.state_check_wq)
+			cancel_delayed_work_sync(&ad->state_check_work.work);
+	}
+
+	return 0;
+}
+
+static void max96714_power_on(struct vehicle_ad_dev *ad)
+{
+	/* gpio_direction_output(ad->power, ad->pwr_active); */
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "max96714_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "max96714_pwd");
+		gpio_direction_output(ad->powerdown, 1);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->reset)) {
+		gpio_request(ad->reset, "max96714_rst");
+		gpio_direction_output(ad->reset, 0);
+		usleep_range(1500, 2000);
+		gpio_direction_output(ad->reset, 1);
+	}
+}
+
+static void max96714_power_deinit(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->reset))
+		gpio_free(ad->reset);
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static void max96714_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = max96714_g_addev;
+
+	if (ad->cif_error_last_line > 0) {
+		max96714_check_cvstd(ad, true);
+		ad->cif_error_last_line = 0;
+	} else {
+		max96714_check_cvstd(ad, false);
+	}
+
+	VEHICLE_DG("%s:cvstd_old(%d), cvstd_mode(%d)\n", __func__, cvstd_old, cvstd_mode);
+	if (cvstd_old != cvstd_mode || cvstd_old_state != cvstd_state) {
+		VEHICLE_INFO("%s:ad sensor std mode change, cvstd_old(%d), cvstd_mode(%d)\n",
+				 __func__, cvstd_old, cvstd_mode);
+		cvstd_old = cvstd_mode;
+		cvstd_old_state = cvstd_state;
+		max96714_reinit_parameter(ad, cvstd_mode);
+		max96714_reg_init(ad, cvstd_mode);
+		vehicle_ad_stat_change_notify();
+	}
+	if (g_max96714_streaming) {
+		queue_delayed_work(ad->state_check_work.state_check_wq,
+			&ad->state_check_work.work, msecs_to_jiffies(100));
+	}
+}
+
+int max96714_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = max96714_g_addev;
+
+	if (!ad)
+		return -ENODEV;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+	if (ad->irq)
+		free_irq(ad->irq, ad);
+	max96714_power_deinit(ad);
+
+	return 0;
+}
+
+static __maybe_unused int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+	case AD_FIX_FORMAT_NTSC:
+	case AD_FIX_FORMAT_720P_50FPS:
+	case AD_FIX_FORMAT_720P_30FPS:
+	case AD_FIX_FORMAT_720P_25FPS:
+		mode = CVSTD_720P25;
+		break;
+	case AD_FIX_FORMAT_1080P_30FPS:
+	case AD_FIX_FORMAT_1080P_25FPS:
+
+	default:
+		mode = CVSTD_1080P30;
+		break;
+	}
+
+	return mode;
+}
+
+int max96714_ad_init(struct vehicle_ad_dev *ad)
+{
+	max96714_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	max96714_power_on(ad);
+
+	max96714_reg_init(ad, cvstd_mode);
+
+	max96714_reinit_parameter(ad, cvstd_mode);
+
+	INIT_DELAYED_WORK(&ad->state_check_work.work, max96714_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-max96714");
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_max96714.h b/drivers/video/rockchip/vehicle/vehicle_ad_max96714.h
new file mode 100644
index 0000000000000..5f1ce6fcda6a3
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_max96714.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_MAX96714_H__
+#define __VEHICLE_AD_MAX96714_H__
+
+int max96714_ad_init(struct vehicle_ad_dev *ad);
+int max96714_ad_deinit(void);
+int max96714_ad_get_cfg(struct vehicle_cfg **cfg);
+void max96714_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int max96714_check_id(struct vehicle_ad_dev *ad);
+int max96714_stream(struct vehicle_ad_dev *ad, int enable);
+void max96714_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.c b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.c
new file mode 100644
index 0000000000000..4c3d963af67d9
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.c
@@ -0,0 +1,1206 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor nvp6188
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      wpzz <randy.wang@rock-chips.com>
+ *      Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_nvp6188.h"
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SVGAP30,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_SVGA_WIDTH = 800,
+	FORCE_SVGA_HEIGHT = 600,
+	FORCE_720P_WIDTH = 1280,
+	FORCE_720P_HEIGHT = 720,
+	FORCE_1080P_WIDTH = 1920,
+	FORCE_1080P_HEIGHT = 1080,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+
+#define NVP6188_LINK_FREQ_1458M			(1458000000UL >> 1)
+
+static struct vehicle_ad_dev *nvp6188_g_addev;
+static int cvstd_mode = CVSTD_1080P25;
+//static int cvstd_old = CVSTD_720P25;
+static int cvstd_state = VIDEO_UNPLUG;
+// static int cvstd_old_state = VIDEO_UNLOCK;
+
+static bool g_nvp6188_streaming;
+
+#define NVP6188_CHIP_ID		0xD3
+#define NVP6188_CHIP_ID2	0xD0
+
+#define _MIPI_PORT0_
+#ifdef _MIPI_PORT0_
+#define _MAR_BANK_ 0x20
+#define _MTX_BANK_ 0x23
+#else
+#define _MAR_BANK_ 0x30
+#define _MTX_BANK_ 0x33
+#endif
+
+#define NVP_RESO_960H_NSTC_VALUE	0x00
+#define NVP_RESO_960H_PAL_VALUE		0x10
+#define NVP_RESO_720P_NSTC_VALUE	0x20
+#define NVP_RESO_720P_PAL_VALUE		0x21
+#define NVP_RESO_1080P_NSTC_VALUE	0x30
+#define NVP_RESO_1080P_PAL_VALUE	0x31
+#define NVP_RESO_960P_NSTC_VALUE	0xa0
+#define NVP_RESO_960P_PAL_VALUE		0xa1
+
+enum nvp6188_support_reso {
+	NVP_RESO_UNKNOWN = 0,
+	NVP_RESO_960H_PAL,
+	NVP_RESO_720P_PAL,
+	NVP_RESO_960P_PAL,
+	NVP_RESO_1080P_PAL,
+	NVP_RESO_960H_NSTC,
+	NVP_RESO_720P_NSTC,
+	NVP_RESO_960P_NSTC,
+	NVP_RESO_1080P_NSTC,
+};
+
+struct regval {
+	u8 addr;
+	u8 val;
+};
+
+static __maybe_unused const struct regval common_setting_1458M_regs[] = {
+	{0xff, 0x00},
+	{0x80, 0x0f},
+	{0x00, 0x10},
+	{0x01, 0x10},
+	{0x02, 0x10},
+	{0x03, 0x10},
+	{0x22, 0x0b},
+	{0x23, 0x41},
+	{0x26, 0x0b},
+	{0x27, 0x41},
+	{0x2a, 0x0b},
+	{0x2b, 0x41},
+	{0x2e, 0x0b},
+	{0x2f, 0x41},
+
+	{0xff, 0x01},
+	{0x98, 0x30},
+	{0xed, 0x00},
+
+	{0xff, 0x05+0},
+	{0x00, 0xd0},
+	{0x01, 0x22},
+	{0x47, 0xee},
+	{0x50, 0xc6},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x5b, 0x41},
+	{0x5c, 0x78},
+	{0xB8, 0xB8},
+
+	{0xff, 0x05+1},
+	{0x00, 0xd0},
+	{0x01, 0x22},
+	{0x47, 0xee},
+	{0x50, 0xc6},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x5b, 0x41},
+	{0x5c, 0x78},
+	{0xB8, 0xB8},
+
+	{0xff, 0x05+2},
+	{0x00, 0xd0},
+	{0x01, 0x22},
+	{0x47, 0xee},
+	{0x50, 0xc6},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x5b, 0x41},
+	{0x5c, 0x78},
+	{0xB8, 0xB8},
+
+	{0xff, 0x05+3},
+	{0x00, 0xd0},
+	{0x01, 0x22},
+	{0x47, 0xee},
+	{0x50, 0xc6},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x5b, 0x41},
+	{0x5c, 0x78},
+	{0xB8, 0xB8},
+
+	{0xff, 0x09},
+	{0x50, 0x30},
+	{0x51, 0x6f},
+	{0x52, 0x67},
+	{0x53, 0x48},
+	{0x54, 0x30},
+	{0x55, 0x6f},
+	{0x56, 0x67},
+	{0x57, 0x48},
+	{0x58, 0x30},
+	{0x59, 0x6f},
+	{0x5a, 0x67},
+	{0x5b, 0x48},
+	{0x5c, 0x30},
+	{0x5d, 0x6f},
+	{0x5e, 0x67},
+	{0x5f, 0x48},
+
+	{0xff, 0x0a},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xff, 0x0b},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+
+	{0xff, 0x13},
+	{0x05, 0xa0},
+	{0x31, 0xff},
+	{0x07, 0x47},
+	{0x12, 0x04},
+	{0x1e, 0x1f},
+	{0x1f, 0x27},
+	{0x2e, 0x10},
+	{0x2f, 0xc8},
+	{0x31, 0xff},
+	{0x32, 0x00},
+	{0x33, 0x00},
+	{0x72, 0x05},
+	{0x7a, 0xf0},
+	{0xff, _MAR_BANK_},
+	{0x10, 0xff},
+	{0x11, 0xff},
+
+	{0x30, 0x0f},
+	{0x32, 0xff},
+	{0x34, 0xcd},
+	{0x36, 0x04},
+	{0x38, 0xff},
+	{0x3c, 0x01},
+	{0x3d, 0x11},
+	{0x3e, 0x11},
+	{0x45, 0x60},
+	{0x46, 0x49},
+
+	{0xff, _MTX_BANK_},
+	{0xe9, 0x03},
+	{0x03, 0x02},
+	{0x01, 0xe4},
+	{0x00, 0x7d},
+	{0x01, 0xe0},
+	{0x02, 0xa0},
+	{0x20, 0x1e},
+	{0x20, 0x1f},
+	{0x04, 0x6c},
+	{0x45, 0xcd},
+	{0x46, 0x42},
+	{0x47, 0x36},
+	{0x48, 0x0f},
+	{0x65, 0xcd},
+	{0x66, 0x42},
+	{0x67, 0x0e},
+	{0x68, 0x0f},
+	{0x85, 0xcd},
+	{0x86, 0x42},
+	{0x87, 0x0e},
+	{0x88, 0x0f},
+	{0xa5, 0xcd},
+	{0xa6, 0x42},
+	{0xa7, 0x0e},
+	{0xa8, 0x0f},
+	{0xc5, 0xcd},
+	{0xc6, 0x42},
+	{0xc7, 0x0e},
+	{0xc8, 0x0f},
+	{0xeb, 0x8d},
+
+	{0xff, _MAR_BANK_},
+	{0x00, 0xff},
+	{0x40, 0x01},
+	{0x40, 0x00},
+	{0xff, 0x01},
+	{0x97, 0x00},
+	{0x97, 0x0f},
+
+	{0xff, 0x00},  //test pattern
+	{0x78, 0xba},
+	{0x79, 0xac},
+	{0xff, 0x05},
+	{0x2c, 0x08},
+	{0x6a, 0x80},
+	{0xff, 0x06},
+	{0x2c, 0x08},
+	{0x6a, 0x80},
+	{0xff, 0x07},
+	{0x2c, 0x08},
+	{0x6a, 0x80},
+	{0xff, 0x08},
+	{0x2c, 0x08},
+	{0x6a, 0x80},
+};
+
+static __maybe_unused const struct regval auto_detect_regs[] = {
+	{0xFF, 0x13},
+	{0x30, 0x7f},
+	{0x70, 0xf0},
+
+	{0xFF, 0x00},
+	{0x00, 0x18},
+	{0x01, 0x18},
+	{0x02, 0x18},
+	{0x03, 0x18},
+
+	{0x00, 0x10},
+	{0x01, 0x10},
+	{0x02, 0x10},
+	{0x03, 0x10},
+};
+
+static void nvp6188_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0;
+
+	switch (cvstd) {
+	case CVSTD_720P25:
+		ad->cfg.width = 1280;
+		ad->cfg.height = 720;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = NVP6188_LINK_FREQ_1458M;
+		break;
+
+	case CVSTD_1080P25:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = NVP6188_LINK_FREQ_1458M;
+		break;
+
+	case CVSTD_NTSC:
+		ad->cfg.width = 960;
+		ad->cfg.height = 480;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = NVP6188_LINK_FREQ_1458M;
+		break;
+
+	default:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = NVP6188_LINK_FREQ_1458M;
+		break;
+	}
+	ad->cfg.type = V4L2_MBUS_CSI2_DPHY;
+	ad->cfg.mbus_flags = V4L2_MBUS_CSI2_4_LANE | V4L2_MBUS_CSI2_NONCONTINUOUS_CLOCK |
+			 V4L2_MBUS_CSI2_CHANNELS;
+	ad->cfg.mbus_code = MEDIA_BUS_FMT_UYVY8_2X8;
+
+	switch (ad->cfg.mbus_flags & V4L2_MBUS_CSI2_LANES) {
+	case V4L2_MBUS_CSI2_1_LANE:
+		ad->cfg.lanes = 1;
+		break;
+	case V4L2_MBUS_CSI2_2_LANE:
+		ad->cfg.lanes = 2;
+		break;
+	case V4L2_MBUS_CSI2_3_LANE:
+		ad->cfg.lanes = 3;
+		break;
+	case V4L2_MBUS_CSI2_4_LANE:
+		ad->cfg.lanes = 4;
+		break;
+	default:
+		ad->cfg.lanes = 1;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+}
+
+/* sensor register write */
+static int nvp6188_write_reg(struct vehicle_ad_dev *ad, u8 reg, u8 val)
+{
+	struct i2c_msg msg;
+	u8 buf[2];
+	int ret;
+
+	buf[0] = reg & 0xFF;
+	buf[1] = val;
+
+	msg.addr = ad->i2c_add;
+	msg.flags = 0;
+	msg.buf = buf;
+	msg.len = sizeof(buf);
+
+	ret = i2c_transfer(ad->adapter, &msg, 1);
+	if (ret >= 0) {
+		usleep_range(300, 400);
+		return 0;
+	}
+
+	VEHICLE_DGERR("nvp6188 write reg(0x%x val:0x%x) failed !\n", reg, val);
+
+	return ret;
+}
+
+static int nvp6188_write_array(struct vehicle_ad_dev *ad,
+			       const struct regval *regs, int size)
+{
+	int i, ret = 0;
+
+	i = 0;
+	while (i < size) {
+		ret = nvp6188_write_reg(ad, regs[i].addr, regs[i].val);
+		if (ret) {
+			VEHICLE_DGERR("%s failed !\n", __func__);
+			break;
+		}
+		i++;
+	}
+
+	return ret;
+}
+
+/* sensor register read */
+static int nvp6188_read_reg(struct vehicle_ad_dev *ad, u8 reg, u8 *val)
+{
+	struct i2c_msg msg[2];
+	u8 buf[1];
+	int ret;
+
+	buf[0] = reg & 0xFF;
+
+	msg[0].addr = ad->i2c_add;
+	msg[0].flags = 0;
+	msg[0].buf = buf;
+	msg[0].len = sizeof(buf);
+
+	msg[1].addr = ad->i2c_add;
+	msg[1].flags = 0 | I2C_M_RD;
+	msg[1].buf = buf;
+	msg[1].len = 1;
+
+	ret = i2c_transfer(ad->adapter, msg, 2);
+	if (ret >= 0) {
+		*val = buf[0];
+		return 0;
+	}
+
+	VEHICLE_DGERR("nvp6188 read reg(0x%x) failed !\n", reg);
+
+	return ret;
+}
+
+static unsigned char nv6188_read_vfc(struct vehicle_ad_dev *ad, unsigned char ch)
+{
+	unsigned char ch_vfc = 0xff;
+
+	nvp6188_write_reg(ad, 0xff, 0x05 + ch);
+	nvp6188_read_reg(ad, 0xf0, &ch_vfc);
+	return ch_vfc;
+}
+
+static __maybe_unused int nvp6188_read_all_vfc(struct vehicle_ad_dev *ad,
+					       u8 *ch_vfc)
+{
+	int ret = 0;
+	int check_cnt = 0, ch = 0;
+
+	ret = nvp6188_write_array(ad,
+		auto_detect_regs, ARRAY_SIZE(auto_detect_regs));
+	if (ret)
+		VEHICLE_DGERR("write auto_detect_regs failed %d", ret);
+
+	ret = -1;
+	while ((check_cnt++) < 50) {
+		for (ch = 0; ch < 4; ch++)
+			ch_vfc[ch] = nv6188_read_vfc(ad, ch);
+
+		if (ch_vfc[0] != 0xff || ch_vfc[1] != 0xff ||
+		    ch_vfc[2] != 0xff || ch_vfc[3] != 0xff) {
+			ret = 0;
+			if (ch == 3) {
+				VEHICLE_DGERR("try check cnt %d", check_cnt);
+				break;
+			}
+		} else {
+			usleep_range(20 * 1000, 40 * 1000);
+		}
+	}
+
+	if (ret)
+		VEHICLE_DGERR("read vfc failed %d", ret);
+	else
+		VEHICLE_INFO("read vfc 0x%2x 0x%2x 0x%2x 0x%2x",
+				ch_vfc[0], ch_vfc[1], ch_vfc[2], ch_vfc[3]);
+
+	return ret;
+}
+
+static __maybe_unused int nvp6188_auto_detect_fmt(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	int ch = 0;
+	unsigned char ch_vfc[4] = { 0xff, 0xff, 0xff, 0xff };
+	unsigned char val_13x70 = 0, val_13x71 = 0;
+
+	if (nvp6188_read_all_vfc(ad, ch_vfc))
+		return -1;
+	ch = ad->ad_chl;
+	// for (ch = 0; ch < 4; ch++) {
+		nvp6188_write_reg(ad, 0xFF, 0x13);
+		nvp6188_read_reg(ad, 0x70, &val_13x70);
+		val_13x70 |= (0x01 << ch);
+		nvp6188_write_reg(ad, 0x70, val_13x70);
+		nvp6188_read_reg(ad, 0x71, &val_13x71);
+		val_13x71 |= (0x01 << ch);
+		nvp6188_write_reg(ad, 0x71, val_13x71);
+		switch (ch_vfc[ch]) {
+		case NVP_RESO_960H_NSTC_VALUE:
+			VEHICLE_INFO("channel %d det 960h nstc", ch);
+			ad->channel_reso[ch] = NVP_RESO_960H_NSTC;
+		break;
+		case NVP_RESO_960H_PAL_VALUE:
+			VEHICLE_INFO("channel %d det 960h pal", ch);
+			ad->channel_reso[ch] = NVP_RESO_960H_PAL;
+		break;
+		case NVP_RESO_720P_NSTC_VALUE:
+			VEHICLE_INFO("channel %d det 720p nstc", ch);
+			ad->channel_reso[ch] = NVP_RESO_720P_NSTC;
+		break;
+		case NVP_RESO_720P_PAL_VALUE:
+			VEHICLE_INFO("channel %d det 720p pal", ch);
+			ad->channel_reso[ch] = NVP_RESO_720P_PAL;
+		break;
+		case NVP_RESO_1080P_NSTC_VALUE:
+			VEHICLE_INFO("channel %d det 1080p nstc", ch);
+			ad->channel_reso[ch] = NVP_RESO_1080P_NSTC;
+		break;
+		case NVP_RESO_1080P_PAL_VALUE:
+			VEHICLE_INFO("channel %d det 1080p pal", ch);
+			ad->channel_reso[ch] = NVP_RESO_1080P_PAL;
+		break;
+		case NVP_RESO_960P_NSTC_VALUE:
+			VEHICLE_INFO("channel %d det 960p nstc", ch);
+			ad->channel_reso[ch] = NVP_RESO_960P_NSTC;
+		break;
+		case NVP_RESO_960P_PAL_VALUE:
+			VEHICLE_INFO("channel %d det 960p pal", ch);
+			ad->channel_reso[ch] = NVP_RESO_960P_PAL;
+		break;
+		default:
+			VEHICLE_INFO("channel %d not detect, def 1080p pal\n", ch);
+			ad->channel_reso[ch] = NVP_RESO_1080P_PAL;
+		break;
+		}
+	// }
+	return ret;
+}
+
+//each channel setting
+/*
+ * 960x480i
+ * ch : 0 ~ 3
+ * ntpal: 1:25p, 0:30p
+ */
+static __maybe_unused void nv6188_set_chn_960h(struct vehicle_ad_dev *ad, u8 ch,
+					       u8 ntpal)
+{
+	unsigned char val_0x54 = 0, val_20x01 = 0;
+
+	VEHICLE_INFO("%s ch %d ntpal %d", __func__, ch, ntpal);
+	nvp6188_write_reg(ad, 0xff, 0x00);
+	nvp6188_write_reg(ad, 0x08 + ch, ntpal ? 0xdd : 0xa0);
+	nvp6188_write_reg(ad, 0x18 + ch, 0x08);
+	nvp6188_write_reg(ad, 0x22 + ch * 4, 0x0b);
+	nvp6188_write_reg(ad, 0x23 + ch * 4, 0x41);
+	nvp6188_write_reg(ad, 0x30 + ch, 0x12);
+	nvp6188_write_reg(ad, 0x34 + ch, 0x01);
+	nvp6188_read_reg(ad, 0x54, &val_0x54);
+	if (ntpal)
+		val_0x54 &= ~(0x10 << ch);
+	else
+		val_0x54 |= (0x10 << ch);
+	nvp6188_write_reg(ad, 0x54, val_0x54);
+	nvp6188_write_reg(ad, 0x58 + ch, ntpal ? 0x80 : 0x90);
+	nvp6188_write_reg(ad, 0x5c + ch, ntpal ? 0xbe : 0xbc);
+	nvp6188_write_reg(ad, 0x64 + ch, ntpal ? 0xa0 : 0x81);
+	nvp6188_write_reg(ad, 0x81 + ch, ntpal ? 0xf0 : 0xe0);
+	nvp6188_write_reg(ad, 0x85 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x89 + ch, 0x00);
+	nvp6188_write_reg(ad, ch + 0x8e, 0x00);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x05);
+
+	nvp6188_write_reg(ad, 0xff, 0x01);
+	nvp6188_write_reg(ad, 0x84 + ch, 0x02);
+	nvp6188_write_reg(ad, 0x88 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x8c + ch, 0x40);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x20);
+	nvp6188_write_reg(ad, 0xed, 0x00);
+
+	nvp6188_write_reg(ad, 0xff, 0x05 + ch);
+	nvp6188_write_reg(ad, 0x01, 0x22);
+	nvp6188_write_reg(ad, 0x05, 0x00);
+	nvp6188_write_reg(ad, 0x08, 0x55);
+	nvp6188_write_reg(ad, 0x25, 0xdc);
+	nvp6188_write_reg(ad, 0x28, 0x80);
+	nvp6188_write_reg(ad, 0x2f, 0x00);
+	nvp6188_write_reg(ad, 0x30, 0xe0);
+	nvp6188_write_reg(ad, 0x31, 0x43);
+	nvp6188_write_reg(ad, 0x32, 0xa2);
+	nvp6188_write_reg(ad, 0x47, 0x04);
+	nvp6188_write_reg(ad, 0x50, 0x84);
+	nvp6188_write_reg(ad, 0x57, 0x00);
+	nvp6188_write_reg(ad, 0x58, 0x77);
+	nvp6188_write_reg(ad, 0x5b, 0x43);
+	nvp6188_write_reg(ad, 0x5c, 0x78);
+	nvp6188_write_reg(ad, 0x5f, 0x00);
+	nvp6188_write_reg(ad, 0x62, 0x20);
+	nvp6188_write_reg(ad, 0x7b, 0x00);
+	nvp6188_write_reg(ad, 0x7c, 0x01);
+	nvp6188_write_reg(ad, 0x7d, 0x80);
+	nvp6188_write_reg(ad, 0x80, 0x00);
+	nvp6188_write_reg(ad, 0x90, 0x01);
+	nvp6188_write_reg(ad, 0xa9, 0x00);
+	nvp6188_write_reg(ad, 0xb5, 0x00);
+	nvp6188_write_reg(ad, 0xb8, 0xb9);
+	nvp6188_write_reg(ad, 0xb9, 0x72);
+	nvp6188_write_reg(ad, 0xd1, 0x00);
+	nvp6188_write_reg(ad, 0xd5, 0x80);
+
+	nvp6188_write_reg(ad, 0xff, 0x09);
+	nvp6188_write_reg(ad, 0x96 + ch * 0x20, 0x10);
+	nvp6188_write_reg(ad, 0x98 + ch * 0x20, ntpal ? 0xc0 : 0xe0);
+	nvp6188_write_reg(ad, ch * 0x20 + 0x9e, 0x00);
+
+	nvp6188_write_reg(ad, 0xff, _MAR_BANK_);
+	nvp6188_read_reg(ad, 0x01, &val_20x01);
+	val_20x01 &= (~(0x03 << (ch * 2)));
+	val_20x01 |= (0x02 << (ch * 2));
+	nvp6188_write_reg(ad, 0x01, val_20x01);
+	nvp6188_write_reg(ad, 0x12 + ch * 2, 0xe0);
+	nvp6188_write_reg(ad, 0x13 + ch * 2, 0x01);
+}
+
+//each channel setting
+/*
+ * 1280x720p
+ * ch : 0 ~ 3
+ * ntpal: 1:25p, 0:30p
+ */
+static __maybe_unused void nv6188_set_chn_720p(struct vehicle_ad_dev *ad, u8 ch,
+					       u8 ntpal)
+{
+	unsigned char val_0x54 = 0, val_20x01 = 0;
+
+	VEHICLE_INFO("%s ch %d ntpal %d", __func__, ch, ntpal);
+	nvp6188_write_reg(ad, 0xff, 0x00);
+	nvp6188_write_reg(ad, 0x08 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x18 + ch, 0x3f);
+	nvp6188_write_reg(ad, 0x30 + ch, 0x12);
+	nvp6188_write_reg(ad, 0x34 + ch, 0x00);
+	nvp6188_read_reg(ad, 0x54, &val_0x54);
+	val_0x54 &= ~(0x10 << ch);
+	nvp6188_write_reg(ad, 0x54, val_0x54);
+	nvp6188_write_reg(ad, 0x58 + ch, ntpal ? 0x80 : 0x80);
+	nvp6188_write_reg(ad, 0x5c + ch, ntpal ? 0x00 : 0x00);
+	nvp6188_write_reg(ad, 0x64 + ch, ntpal ? 0x01 : 0x01);
+	nvp6188_write_reg(ad, 0x81 + ch, ntpal ? 0x0d : 0x0c);
+	nvp6188_write_reg(ad, 0x85 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x89 + ch, 0x00);
+	nvp6188_write_reg(ad, ch + 0x8e, 0x00);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x05);
+
+	nvp6188_write_reg(ad, 0xff, 0x01);
+	nvp6188_write_reg(ad, 0x84 + ch, 0x02);
+	nvp6188_write_reg(ad, 0x88 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x8c + ch, 0x40);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x20);
+
+	nvp6188_write_reg(ad, 0xff, 0x05 + ch);
+	nvp6188_write_reg(ad, 0x01, 0x22);
+	nvp6188_write_reg(ad, 0x05, 0x04);
+	nvp6188_write_reg(ad, 0x08, 0x55);
+	nvp6188_write_reg(ad, 0x25, 0xdc);
+	nvp6188_write_reg(ad, 0x28, 0x80);
+	nvp6188_write_reg(ad, 0x2f, 0x00);
+	nvp6188_write_reg(ad, 0x30, 0xe0);
+	nvp6188_write_reg(ad, 0x31, 0x43);
+	nvp6188_write_reg(ad, 0x32, 0xa2);
+	nvp6188_write_reg(ad, 0x47, 0xee);
+	nvp6188_write_reg(ad, 0x50, 0xc6);
+	nvp6188_write_reg(ad, 0x57, 0x00);
+	nvp6188_write_reg(ad, 0x58, 0x77);
+	nvp6188_write_reg(ad, 0x5b, 0x41);
+	nvp6188_write_reg(ad, 0x5c, 0x7C);
+	nvp6188_write_reg(ad, 0x5f, 0x00);
+	nvp6188_write_reg(ad, 0x62, 0x20);
+	nvp6188_write_reg(ad, 0x7b, 0x11);
+	nvp6188_write_reg(ad, 0x7c, 0x01);
+	nvp6188_write_reg(ad, 0x7d, 0x80);
+	nvp6188_write_reg(ad, 0x80, 0x00);
+	nvp6188_write_reg(ad, 0x90, 0x01);
+	nvp6188_write_reg(ad, 0xa9, 0x00);
+	nvp6188_write_reg(ad, 0xb5, 0x40);
+	nvp6188_write_reg(ad, 0xb8, 0x39);
+	nvp6188_write_reg(ad, 0xb9, 0x72);
+	nvp6188_write_reg(ad, 0xd1, 0x00);
+	nvp6188_write_reg(ad, 0xd5, 0x80);
+
+	nvp6188_write_reg(ad, 0xff, 0x09);
+	nvp6188_write_reg(ad, 0x96 + ch * 0x20, 0x00);
+	nvp6188_write_reg(ad, 0x98 + ch * 0x20, 0x00);
+	nvp6188_write_reg(ad, ch * 0x20 + 0x9e, 0x00);
+
+	nvp6188_write_reg(ad, 0xff, _MAR_BANK_);
+	nvp6188_read_reg(ad, 0x01, &val_20x01);
+	val_20x01 &= (~(0x03 << (ch * 2)));
+	val_20x01 |= (0x01 << (ch * 2));
+	nvp6188_write_reg(ad, 0x01, val_20x01);
+	nvp6188_write_reg(ad, 0x12 + ch * 2, 0x80);
+	nvp6188_write_reg(ad, 0x13 + ch * 2, 0x02);
+}
+
+//each channel setting
+/*
+ * 1920x1080p
+ * ch : 0 ~ 3
+ * ntpal: 1:25p, 0:30p
+ */
+static __maybe_unused void nv6188_set_chn_1080p(struct vehicle_ad_dev *ad, u8 ch,
+						u8 ntpal)
+{
+	unsigned char val_0x54 = 0, val_20x01 = 0;
+
+	VEHICLE_INFO("%s ch %d ntpal %d", __func__, ch, ntpal);
+	nvp6188_write_reg(ad, 0xff, 0x00);
+	nvp6188_write_reg(ad, 0x08 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x18 + ch, 0x3f);
+	nvp6188_write_reg(ad, 0x30 + ch, 0x12);
+	nvp6188_write_reg(ad, 0x34 + ch, 0x00);
+	nvp6188_read_reg(ad, 0x54, &val_0x54);
+	val_0x54 &= ~(0x10 << ch);
+	nvp6188_write_reg(ad, 0x54, val_0x54);
+	nvp6188_write_reg(ad, 0x58 + ch, ntpal ? 0x80 : 0x80);
+	nvp6188_write_reg(ad, 0x5c + ch, ntpal ? 0x00 : 0x00);
+	nvp6188_write_reg(ad, 0x64 + ch, ntpal ? 0x01 : 0x01);
+	nvp6188_write_reg(ad, 0x81 + ch, ntpal ? 0x03 : 0x02);
+	nvp6188_write_reg(ad, 0x85 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x89 + ch, 0x10);
+	nvp6188_write_reg(ad, ch + 0x8e, 0x00);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x05);
+
+	nvp6188_write_reg(ad, 0xff, 0x01);
+	nvp6188_write_reg(ad, 0x84 + ch, 0x02);
+	nvp6188_write_reg(ad, 0x88 + ch, 0x00);
+	nvp6188_write_reg(ad, 0x8c + ch, 0x40);
+	nvp6188_write_reg(ad, 0xa0 + ch, 0x20);
+
+	nvp6188_write_reg(ad, 0xff, 0x05 + ch);
+	nvp6188_write_reg(ad, 0x01, 0x22);
+	nvp6188_write_reg(ad, 0x05, 0x04);
+	nvp6188_write_reg(ad, 0x08, 0x55);
+	nvp6188_write_reg(ad, 0x25, 0xdc);
+	nvp6188_write_reg(ad, 0x28, 0x80);
+	nvp6188_write_reg(ad, 0x2f, 0x00);
+	nvp6188_write_reg(ad, 0x30, 0xe0);
+	nvp6188_write_reg(ad, 0x31, 0x41);
+	nvp6188_write_reg(ad, 0x32, 0xa2);
+	nvp6188_write_reg(ad, 0x47, 0xee);
+	nvp6188_write_reg(ad, 0x50, 0xc6);
+	nvp6188_write_reg(ad, 0x57, 0x00);
+	nvp6188_write_reg(ad, 0x58, 0x77);
+	nvp6188_write_reg(ad, 0x5b, 0x41);
+	nvp6188_write_reg(ad, 0x5c, 0x7C);
+	nvp6188_write_reg(ad, 0x5f, 0x00);
+	nvp6188_write_reg(ad, 0x62, 0x20);
+	nvp6188_write_reg(ad, 0x7b, 0x11);
+	nvp6188_write_reg(ad, 0x7c, 0x01);
+	nvp6188_write_reg(ad, 0x7d, 0x80);
+	nvp6188_write_reg(ad, 0x80, 0x00);
+	nvp6188_write_reg(ad, 0x90, 0x01);
+	nvp6188_write_reg(ad, 0xa9, 0x00);
+	nvp6188_write_reg(ad, 0xb5, 0x40);
+	nvp6188_write_reg(ad, 0xb8, 0x39);
+	nvp6188_write_reg(ad, 0xb9, 0x72);
+	nvp6188_write_reg(ad, 0xd1, 0x00);
+	nvp6188_write_reg(ad, 0xd5, 0x80);
+
+	nvp6188_write_reg(ad, 0xff, 0x09);
+	nvp6188_write_reg(ad, 0x96 + ch * 0x20, 0x00);
+	nvp6188_write_reg(ad, 0x98 + ch * 0x20, 0x00);
+	nvp6188_write_reg(ad, ch * 0x20 + 0x9e, 0x00);
+
+	nvp6188_write_reg(ad, 0xff, _MAR_BANK_);
+	nvp6188_read_reg(ad, 0x01, &val_20x01);
+	val_20x01 &= (~(0x03 << (ch * 2)));
+	nvp6188_write_reg(ad, 0x01, val_20x01);
+	nvp6188_write_reg(ad, 0x12 + ch * 2, 0xc0);
+	nvp6188_write_reg(ad, 0x13 + ch * 2, 0x03);
+}
+
+static __maybe_unused void nvp6188_manual_mode(struct vehicle_ad_dev *ad)
+{
+	int i, reso;
+
+	for (i = 3; i >= 0; i--) {
+		reso = ad->channel_reso[i];
+		switch (reso) {
+		case NVP_RESO_960H_PAL:
+			nv6188_set_chn_960h(ad, i, 1);
+			break;
+		case NVP_RESO_720P_PAL:
+			nv6188_set_chn_720p(ad, i, 1);
+			break;
+		case NVP_RESO_1080P_PAL:
+			nv6188_set_chn_1080p(ad, i, 1);
+			break;
+		case NVP_RESO_960H_NSTC:
+			nv6188_set_chn_960h(ad, i, 0);
+			break;
+		case NVP_RESO_720P_NSTC:
+			nv6188_set_chn_720p(ad, i, 0);
+			break;
+		case NVP_RESO_1080P_NSTC:
+			nv6188_set_chn_1080p(ad, i, 0);
+			break;
+		default:
+			nv6188_set_chn_1080p(ad, i, 1);
+			break;
+		}
+	}
+}
+
+void nvp6188_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	ad->ad_chl = channel;
+	VEHICLE_DG("%s, channel set(%d)", __func__, ad->ad_chl);
+}
+
+int nvp6188_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	if (!nvp6188_g_addev)
+		return -1;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		nvp6188_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		nvp6188_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		nvp6188_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	nvp6188_g_addev->cfg.ad_ready = true;
+
+	*cfg = &nvp6188_g_addev->cfg;
+
+	return 0;
+}
+
+void nvp6188_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	VEHICLE_INFO("%s, last_line %d\n", __func__, last_line);
+
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_1080P25) {
+		if (last_line == FORCE_1080P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_720P25) {
+		if (last_line == FORCE_720P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int nvp6188_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	u8 pid = 0;
+
+	ret = vehicle_sensor_write(ad, 0xFF, 0x00);
+	ret |= vehicle_sensor_read(ad, 0xf4, &pid);
+	if (ret)
+		return ret;
+
+	if (pid != NVP6188_CHIP_ID && pid != NVP6188_CHIP_ID2) {
+		VEHICLE_DGERR("%s: expected 0xd0/d3, detected: 0x%02x !",
+			ad->ad_name, pid);
+		ret = -EINVAL;
+	} else {
+		VEHICLE_INFO("%s Found NVP6188 sensor: id(0x%2x) !\n", __func__, pid);
+	}
+
+	return ret;
+}
+
+static int __nvp6188_start_stream(struct vehicle_ad_dev *ad)
+{
+	int ret;
+	int array_size = 0;
+
+	array_size = ARRAY_SIZE(common_setting_1458M_regs);
+
+	ret = nvp6188_write_array(ad,
+		common_setting_1458M_regs, array_size);
+	if (ret) {
+		VEHICLE_INFO(" nvp6188 start stream: wrote global reg failed");
+		return ret;
+	}
+
+	nvp6188_auto_detect_fmt(ad);
+	nvp6188_manual_mode(ad);
+	nvp6188_write_reg(ad, 0xff, 0x20);
+	nvp6188_write_reg(ad, 0xff, 0xff);
+	msleep(50);
+
+	return 0;
+}
+
+static int __nvp6188_stop_stream(struct vehicle_ad_dev *ad)
+{
+	nvp6188_write_reg(ad, 0xff, 0x20);
+	nvp6188_write_reg(ad, 0x00, 0x00);
+	nvp6188_write_reg(ad, 0x40, 0x01);
+	nvp6188_write_reg(ad, 0x40, 0x00);
+
+	return 0;
+}
+
+int nvp6188_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	VEHICLE_INFO("%s on(%d)\n", __func__, enable);
+
+	g_nvp6188_streaming = (enable != 0);
+	if (g_nvp6188_streaming) {
+		__nvp6188_start_stream(ad);
+		if (ad->state_check_work.state_check_wq)
+			queue_delayed_work(ad->state_check_work.state_check_wq,
+				&ad->state_check_work.work, msecs_to_jiffies(200));
+	} else {
+		__nvp6188_stop_stream(ad);
+		if (ad->state_check_work.state_check_wq)
+			cancel_delayed_work_sync(&ad->state_check_work.work);
+		VEHICLE_DG("%s(%d): cancel_queue_delayed_work!\n", __func__, __LINE__);
+	}
+
+	return 0;
+}
+
+static void nvp6188_power_on(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "nvp6188_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "nvp6188_pwd");
+		gpio_direction_output(ad->powerdown, 1);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->reset)) {
+		gpio_request(ad->reset, "nvp6188_rst");
+		gpio_direction_output(ad->reset, 0);
+		usleep_range(1500, 2000);
+		gpio_direction_output(ad->reset, 1);
+	}
+}
+
+static void nvp6188_power_off(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->reset))
+		gpio_free(ad->reset);
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static __maybe_unused int nvp6188_auto_detect_hotplug(struct vehicle_ad_dev *ad)
+{
+	nvp6188_write_reg(ad, 0xff, 0x00);
+	nvp6188_read_reg(ad, 0xa8, &ad->detect_status);
+
+	ad->detect_status = ~ad->detect_status;
+
+	return 0;
+}
+
+static void nvp6188_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = nvp6188_g_addev;
+	nvp6188_auto_detect_hotplug(ad);
+
+	if (ad->detect_status != ad->last_detect_status) {
+		ad->last_detect_status = ad->detect_status;
+		vehicle_ad_stat_change_notify();
+	}
+
+	if (g_nvp6188_streaming) {
+		queue_delayed_work(ad->state_check_work.state_check_wq,
+				   &ad->state_check_work.work, msecs_to_jiffies(100));
+	}
+}
+
+int nvp6188_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = nvp6188_g_addev;
+
+	if (!ad)
+		return -1;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+
+	nvp6188_power_off(ad);
+
+	return 0;
+}
+
+static __maybe_unused int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+	case AD_FIX_FORMAT_NTSC:
+	case AD_FIX_FORMAT_720P_50FPS:
+	case AD_FIX_FORMAT_720P_30FPS:
+	case AD_FIX_FORMAT_720P_25FPS:
+		mode = CVSTD_720P25;
+		break;
+	case AD_FIX_FORMAT_1080P_30FPS:
+	case AD_FIX_FORMAT_1080P_25FPS:
+
+	default:
+		mode = CVSTD_720P25;
+		break;
+	}
+
+	return mode;
+}
+
+int nvp6188_ad_init(struct vehicle_ad_dev *ad)
+{
+	int val;
+	int i = 0;
+
+	nvp6188_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	/*  2. ad power on sequence */
+	nvp6188_power_on(ad);
+
+	while (++i < 5) {
+		usleep_range(1000, 1200);
+		val = vehicle_generic_sensor_read(ad, 0xf0);
+		if (val != 0xff)
+			break;
+		VEHICLE_INFO("nvp6188_init i2c_reg_read fail\n");
+	}
+
+	nvp6188_reinit_parameter(ad, cvstd_mode);
+	ad->last_detect_status = true;
+
+	/*  create workqueue to detect signal change */
+	INIT_DELAYED_WORK(&ad->state_check_work.work, nvp6188_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-nvp6188");
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.h b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.h
new file mode 100644
index 0000000000000..4e9c6a61ed61c
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6188.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_NVP6188_H__
+#define __VEHICLE_AD_NVP6188_H__
+
+int nvp6188_ad_init(struct vehicle_ad_dev *ad);
+int nvp6188_ad_deinit(void);
+int nvp6188_ad_get_cfg(struct vehicle_cfg **cfg);
+void nvp6188_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int nvp6188_check_id(struct vehicle_ad_dev *ad);
+int nvp6188_stream(struct vehicle_ad_dev *ad, int enable);
+void nvp6188_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.c b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.c
new file mode 100644
index 0000000000000..858acf268970c
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.c
@@ -0,0 +1,2238 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor nvp6324
+ *
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      wpzz <randy.wang@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_nvp6324.h"
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SVGAP30,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_SVGA_WIDTH = 800,
+	FORCE_SVGA_HEIGHT = 600,
+	FORCE_720P_WIDTH = 1280,
+	FORCE_720P_HEIGHT = 720,
+	FORCE_1080P_WIDTH = 1920,
+	FORCE_1080P_HEIGHT = 1080,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+
+#define JAGUAR1_LINK_FREQ_320M			320000000UL
+#define JAGUAR1_LINK_FREQ_640M			640000000UL
+
+static struct vehicle_ad_dev *nvp6324_g_addev;
+static int cvstd_mode = CVSTD_1080P25;
+//static int cvstd_old = CVSTD_720P25;
+static int cvstd_old = CVSTD_NTSC;
+
+//static int cvstd_sd = CVSTD_NTSC;
+static int cvstd_state = VIDEO_UNPLUG;
+static int cvstd_old_state = VIDEO_UNLOCK;
+static int video_mode;
+static int video_old;
+
+static bool g_nvp6324_streaming;
+
+#define SENSOR_REGISTER_LEN	1	/* sensor register address bytes*/
+#define SENSOR_VALUE_LEN	1	/* sensor register value bytes*/
+#define JAGUAR1_CHIP_ID		0xB0
+
+struct rk_sensor_reg {
+	unsigned int reg;
+	unsigned int val;
+};
+
+#define SENSOR_CHANNEL_REG		0x41
+
+#define SEQCMD_END  0xFF000000
+#define SensorEnd   {SEQCMD_END, 0x00}
+
+#define SENSOR_ID(_msb, _lsb)		((_msb) << 8 | (_lsb))
+
+/* NTSC Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_ntsc_30hz[] = {
+	{0xff, 0x04},
+	{0xa0, 0x24},
+	{0xa1, 0x24},
+	{0xa2, 0x24},
+	{0xa3, 0x24},
+	{0xa4, 0x24},
+	{0xa5, 0x24},
+	{0xa6, 0x24},
+	{0xa7, 0x24},
+	{0xa8, 0x24},
+	{0xa9, 0x24},
+	{0xaa, 0x24},
+	{0xab, 0x24},
+	{0xac, 0x24},
+	{0xad, 0x24},
+	{0xae, 0x24},
+	{0xaf, 0x24},
+	{0xb0, 0x24},
+	{0xb1, 0x24},
+	{0xb2, 0x24},
+	{0xb3, 0x24},
+	{0xb4, 0x24},
+	{0xb5, 0x24},
+	{0xb6, 0x24},
+	{0xb7, 0x24},
+	{0xb8, 0x24},
+	{0xb9, 0x24},
+	{0xba, 0x24},
+	{0xbb, 0x24},
+	{0xbc, 0x24},
+	{0xbd, 0x24},
+	{0xbe, 0x24},
+	{0xbf, 0x24},
+	{0xc0, 0x24},
+	{0xc1, 0x24},
+	{0xc2, 0x24},
+	{0xc3, 0x24},
+	{0xff, 0x21},
+	{0x07, 0x80},
+	{0x07, 0x00},
+	{0xff, 0x0A},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xff, 0x0B},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+
+	{0xFF, 0x21},
+	{0x40, 0xAC},
+	{0x41, 0x10},
+	{0x42, 0x03},
+	{0x43, 0x43},
+	{0x11, 0x04},
+	{0x10, 0x0A},
+	{0x12, 0x06},
+	{0x13, 0x09},
+	{0x17, 0x01},
+	{0x18, 0x0D},
+	{0x15, 0x04},
+	{0x14, 0x16},
+	{0x16, 0x05},
+	{0x19, 0x05},
+	{0x1A, 0x0A},
+	{0x1B, 0x08},
+	{0x1C, 0x07},
+	{0x44, 0x00},
+	{0x49, 0xF3},
+	{0x49, 0xF0},
+	{0x44, 0x02},
+	{0x08, 0x40}, //0x40:non-continue;0x48:continuous
+	{0x0F, 0x01},
+	{0x38, 0x1E},
+	{0x39, 0x1E},
+	{0x3A, 0x1E},
+	{0x3B, 0x1E},
+	{0x07, 0x0f}, //0x07:2lane;0x0f:4lane
+	{0x2D, 0x01}, //0x00:2lane;0x01:4lane
+	{0x45, 0x02},
+	{0xFF, 0x13},
+	{0x30, 0x00},
+	{0x31, 0x00},
+	{0x32, 0x00},
+
+	{0xFF, 0x00},
+	{0x00, 0x00},
+	{0x01, 0x00},
+	{0x02, 0x00},
+	{0x03, 0x00},
+	{0x04, 0x0e}, //sd_mode
+	{0x05, 0x0e},
+	{0x06, 0x0e},
+	{0x07, 0x0e},
+	{0x08, 0x00}, //ahd_mode
+	{0x09, 0x00},
+	{0x0a, 0x00},
+	{0x0b, 0x00},
+	{0x0c, 0x00},
+	{0x0d, 0x00},
+	{0x0e, 0x00},
+	{0x0f, 0x00},
+	{0x10, 0xa0}, //video_format
+	{0x11, 0xa0},
+	{0x12, 0xa0},
+	{0x13, 0xa0},
+	{0x14, 0x00},
+	{0x15, 0x00},
+	{0x16, 0x00},
+	{0x17, 0x00},
+	{0x18, 0x13},
+	{0x19, 0x13},
+	{0x1a, 0x13},
+	{0x1b, 0x13},
+	{0x1c, 0x1a},
+	{0x1d, 0x1a},
+	{0x1e, 0x1a},
+	{0x1f, 0x1a},
+	{0x20, 0x00},
+	{0x21, 0x00},
+	{0x22, 0x00},
+	{0x23, 0x00},
+	{0x24, 0x90}, //contrast
+	{0x25, 0x90},
+	{0x26, 0x90},
+	{0x27, 0x90},
+	{0x28, 0x90}, //black_level
+	{0x29, 0x90},
+	{0x2a, 0x90},
+	{0x2b, 0x90},
+	{0x30, 0x00}, //y_peaking_mode
+	{0x31, 0x00},
+	{0x32, 0x00},
+	{0x33, 0x00},
+	{0x34, 0x08}, //y_fir_mode
+	{0x35, 0x08},
+	{0x36, 0x08},
+	{0x37, 0x08},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4a, 0x00},
+	{0x4b, 0x00},
+	{0x4c, 0xfe},
+	{0x4d, 0xfe},
+	{0x4e, 0xfe},
+	{0x4f, 0xfe},
+	{0x50, 0xfb},
+	{0x51, 0xfb},
+	{0x52, 0xfb},
+	{0x53, 0xfb},
+	{0x58, 0x80},
+	{0x59, 0x80},
+	{0x5a, 0x80},
+	{0x5b, 0x80},
+	{0x5c, 0x82}, //pal_cm_off
+	{0x5d, 0x82},
+	{0x5e, 0x82},
+	{0x5f, 0x82},
+	{0x60, 0x10},
+	{0x61, 0x10},
+	{0x62, 0x10},
+	{0x63, 0x10},
+	{0x64, 0x18}, //y_delay
+	{0x65, 0x18},
+	{0x66, 0x18},
+	{0x67, 0x18},
+	{0x68, 0x70}, //h_delay_a //h_delay_lsb
+	{0x69, 0x70},
+	{0x6a, 0x70},
+	{0x6b, 0x70},
+	{0x6c, 0x00},
+	{0x6d, 0x00},
+	{0x6e, 0x00},
+	{0x6f, 0x00},
+	{0x70, 0x9e}, //v_crop_start
+	{0x71, 0x9e},
+	{0x72, 0x9e},
+	{0x73, 0x9e},
+	{0x78, 0xc0},
+	{0x79, 0xc0},
+	{0x7a, 0xc0},
+	{0x7b, 0xc0},
+
+	{0xFF, 0x01},
+	{0x7C, 0x00},
+	{0x84, 0x04},
+	{0x85, 0x04},
+	{0x86, 0x04},
+	{0x87, 0x04},
+	{0x88, 0x01},
+	{0x89, 0x01},
+	{0x8a, 0x01},
+	{0x8b, 0x01},
+	{0x8c, 0x02},
+	{0x8d, 0x02},
+	{0x8e, 0x02},
+	{0x8f, 0x02},
+	{0xEC, 0x00},
+	{0xED, 0x00},
+	{0xEE, 0x00},
+	{0xEF, 0x00},
+
+	{0xFF, 0x05},
+	{0x00, 0xd0},
+	{0x01, 0x2c},
+	{0x05, 0x20}, //d_agc_option
+	{0x1d, 0x0c},
+	{0x21, 0x20}, //sub contrast
+	{0x24, 0x2a},
+	{0x25, 0xdc}, //fsc_lock_mode
+	{0x26, 0x40},
+	{0x27, 0x57},
+	{0x28, 0x80}, //s_point
+	{0x2b, 0xc0}, //saturation_b
+	{0x31, 0x82},
+	{0x32, 0x10},
+	{0x38, 0x00},
+	{0x47, 0x04},
+	{0x50, 0x84},
+	{0x53, 0x04},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x59, 0x00},
+	{0x5C, 0x78},
+	{0x5F, 0x00},
+	{0x62, 0x20},
+	{0x64, 0x01},
+	{0x65, 0x00},
+	{0x69, 0x00},
+	{0x6E, 0x00}, //VBLK_EXT_EN
+	{0x6F, 0x00}, //VBLK_EXT_[7:0]
+	{0x90, 0x01}, //comb_mode
+	{0x92, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x00},
+	{0xa9, 0x00},
+	{0xb5, 0x00},
+	{0xb7, 0xfc},
+	{0xb8, 0xb8},
+	{0xb9, 0x72},
+	{0xbb, 0x0f},
+	{0xd1, 0x30}, //burst_dec_c
+	{0xd5, 0x80},
+
+	{0xFF, 0x09},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x50, 0x30},
+	{0x51, 0x6f},
+	{0x52, 0x67},
+	{0x53, 0x48},
+	{0x54, 0x30},
+	{0x55, 0x6f},
+	{0x56, 0x67},
+	{0x57, 0x48},
+	{0x58, 0x30},
+	{0x59, 0x6f},
+	{0x5a, 0x67},
+	{0x5b, 0x48},
+	{0x5c, 0x30},
+	{0x5d, 0x6f},
+	{0x5e, 0x67},
+	{0x5f, 0x48},
+	{0x96, 0x10},
+	{0x97, 0x10},
+	{0x98, 0x00},
+	{0x99, 0x00},
+	{0x9a, 0x00},
+	{0x9b, 0x00},
+	{0x9c, 0x00},
+	{0x9d, 0x00},
+	{0x9e, 0x00},
+	{0xb6, 0x10},
+	{0xb7, 0x10},
+	{0xb8, 0x00},
+	{0xb9, 0x00},
+	{0xba, 0x00},
+	{0xbb, 0x00},
+	{0xbc, 0x00},
+	{0xbd, 0x00},
+	{0xbe, 0x00},
+	{0xd6, 0x10},
+	{0xd7, 0x10},
+	{0xd8, 0x00},
+	{0xd9, 0x00},
+	{0xda, 0x00},
+	{0xdb, 0x00},
+	{0xdc, 0x00},
+	{0xdd, 0x00},
+	{0xde, 0x00},
+	{0xf6, 0x10},
+	{0xf7, 0x10},
+	{0xf8, 0x00},
+	{0xf9, 0x00},
+	{0xfa, 0x00},
+	{0xfb, 0x00},
+	{0xfc, 0x00},
+	{0xfd, 0x00},
+	{0xfe, 0x00},
+
+	{0xff, 0x0a},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+
+	{0xff, 0x0b},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+
+	{0xFF, 0x21},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0xFF, 0x20},
+	{0x01, 0xaa}, //0x00:1/1;0x55:1/2;0xaa:1/4
+	{0x00, 0x00},
+	{0x40, 0x01},
+	{0x0F, 0x00},
+	{0x0D, 0x01}, //0x01:4lane;0x00:2lane
+	{0x40, 0x00},
+	{0x00, 0xff}, //0xff:ch1/2/3/4 0x33:ch1/2 0x11:ch1
+
+	{0xFF, 0x01},
+	{0xC8, 0x00},
+	{0xC9, 0x00},
+	{0xCA, 0x00},
+	{0xCB, 0x00},
+
+	//pattern enabled
+	{0xFF, 0x00},
+	{0x1C, 0x1A},
+	{0x1D, 0x1A},
+	{0x1E, 0x1A},
+	{0x1F, 0x1A},
+
+	{0xFF, 0x05},
+	{0x6A, 0x80},
+	{0xFF, 0x06},
+	{0x6A, 0x80},
+	{0xFF, 0x07},
+	{0x6A, 0x80},
+	{0xFF, 0x08},
+	{0x6A, 0x80},
+	{0xFF, 0x21}, //add frame num
+	{0x3E, 0x11}, //1 : Fix to 1 for Odd Field, 2 for Even Field
+	{0x3F, 0x11}, //1 : Fix to 1 for Odd Field, 2 for Even Field
+	SensorEnd
+};
+
+/* Pal Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_pal_25hz[] = {
+	{0xff, 0x04},
+	{0xa0, 0x24},
+	{0xa1, 0x24},
+	{0xa2, 0x24},
+	{0xa3, 0x24},
+	{0xa4, 0x24},
+	{0xa5, 0x24},
+	{0xa6, 0x24},
+	{0xa7, 0x24},
+	{0xa8, 0x24},
+	{0xa9, 0x24},
+	{0xaa, 0x24},
+	{0xab, 0x24},
+	{0xac, 0x24},
+	{0xad, 0x24},
+	{0xae, 0x24},
+	{0xaf, 0x24},
+	{0xb0, 0x24},
+	{0xb1, 0x24},
+	{0xb2, 0x24},
+	{0xb3, 0x24},
+	{0xb4, 0x24},
+	{0xb5, 0x24},
+	{0xb6, 0x24},
+	{0xb7, 0x24},
+	{0xb8, 0x24},
+	{0xb9, 0x24},
+	{0xba, 0x24},
+	{0xbb, 0x24},
+	{0xbc, 0x24},
+	{0xbd, 0x24},
+	{0xbe, 0x24},
+	{0xbf, 0x24},
+	{0xc0, 0x24},
+	{0xc1, 0x24},
+	{0xc2, 0x24},
+	{0xc3, 0x24},
+	{0xff, 0x21},
+	{0x07, 0x80},
+	{0x07, 0x00},
+	{0xff, 0x0A},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xff, 0x0B},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+
+	{0xFF, 0x21},
+	{0x40, 0xAC},
+	{0x41, 0x10},
+	{0x42, 0x03},
+	{0x43, 0x43},
+	{0x11, 0x04},
+	{0x10, 0x0A},
+	{0x12, 0x06},
+	{0x13, 0x09},
+	{0x17, 0x01},
+	{0x18, 0x0D},
+	{0x15, 0x04},
+	{0x14, 0x16},
+	{0x16, 0x05},
+	{0x19, 0x05},
+	{0x1A, 0x0A},
+	{0x1B, 0x08},
+	{0x1C, 0x07},
+	{0x44, 0x00},
+	{0x49, 0xF3},
+	{0x49, 0xF0},
+	{0x44, 0x02},
+	{0x08, 0x40}, //0x40:non-continue;0x48:continuous
+	{0x0F, 0x01},
+	{0x38, 0x1E},
+	{0x39, 0x1E},
+	{0x3A, 0x1E},
+	{0x3B, 0x1E},
+	{0x07, 0x0f}, //0x07:2lane;0x0f:4lane
+	{0x2D, 0x01}, //0x00:2lane;0x01:4lane
+	{0x45, 0x02},
+	{0xFF, 0x13},
+	{0x30, 0x00},
+	{0x31, 0x00},
+	{0x32, 0x00},
+
+	{0xFF, 0x00},
+	{0x00, 0x00},
+	{0x01, 0x00},
+	{0x02, 0x00},
+	{0x03, 0x00},
+	{0x04, 0x0f}, //sd_mode
+	{0x05, 0x0f},
+	{0x06, 0x0f},
+	{0x07, 0x0f},
+	{0x08, 0x00}, //ahd_mode
+	{0x09, 0x00},
+	{0x0a, 0x00},
+	{0x0b, 0x00},
+	{0x0c, 0x00},
+	{0x0d, 0x00},
+	{0x0e, 0x00},
+	{0x0f, 0x00},
+	{0x10, 0xdd}, //video_format
+	{0x11, 0xdd},
+	{0x12, 0xdd},
+	{0x13, 0xdd},
+	{0x14, 0x00},
+	{0x15, 0x00},
+	{0x16, 0x00},
+	{0x17, 0x00},
+	{0x18, 0x13},
+	{0x19, 0x13},
+	{0x1a, 0x13},
+	{0x1b, 0x13},
+	{0x1c, 0x1a},
+	{0x1d, 0x1a},
+	{0x1e, 0x1a},
+	{0x1f, 0x1a},
+	{0x20, 0x00},
+	{0x21, 0x00},
+	{0x22, 0x00},
+	{0x23, 0x00},
+	{0x24, 0x90}, //contrast
+	{0x25, 0x90},
+	{0x26, 0x90},
+	{0x27, 0x90},
+	{0x28, 0x90}, //black_level
+	{0x29, 0x90},
+	{0x2a, 0x90},
+	{0x2b, 0x90},
+	{0x30, 0x00}, //y_peaking_mode
+	{0x31, 0x00},
+	{0x32, 0x00},
+	{0x33, 0x00},
+	{0x34, 0x08}, //y_fir_mode
+	{0x35, 0x08},
+	{0x36, 0x08},
+	{0x37, 0x08},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4a, 0x00},
+	{0x4b, 0x00},
+	{0x4c, 0xfe},
+	{0x4d, 0xfe},
+	{0x4e, 0xfe},
+	{0x4f, 0xfe},
+	{0x50, 0xfb},
+	{0x51, 0xfb},
+	{0x52, 0xfb},
+	{0x53, 0xfb},
+	{0x58, 0x80},
+	{0x59, 0x80},
+	{0x5a, 0x80},
+	{0x5b, 0x80},
+	{0x5c, 0x82}, //pal_cm_off
+	{0x5d, 0x82},
+	{0x5e, 0x82},
+	{0x5f, 0x82},
+	{0x60, 0x10},
+	{0x61, 0x10},
+	{0x62, 0x10},
+	{0x63, 0x10},
+	{0x64, 0x07}, //y_delay
+	{0x65, 0x07},
+	{0x66, 0x07},
+	{0x67, 0x07},
+	{0x68, 0x68}, //h_delay_a //h_delay_lsb
+	{0x69, 0x68},
+	{0x6a, 0x68},
+	{0x6b, 0x68},
+	{0x6c, 0x00},
+	{0x6d, 0x00},
+	{0x6e, 0x00},
+	{0x6f, 0x00},
+	{0x70, 0x3f}, //v_crop_start
+	{0x71, 0x3f},
+	{0x72, 0x3f},
+	{0x73, 0x3f},
+	{0x78, 0x21},
+	{0x79, 0x21},
+	{0x7a, 0x21},
+	{0x7b, 0x21},
+
+	{0xFF, 0x01},
+	{0x7C, 0x00},
+	{0x84, 0x04},
+	{0x85, 0x04},
+	{0x86, 0x04},
+	{0x87, 0x04},
+	{0x88, 0x01},
+	{0x89, 0x01},
+	{0x8a, 0x01},
+	{0x8b, 0x01},
+	{0x8c, 0x02},
+	{0x8d, 0x02},
+	{0x8e, 0x02},
+	{0x8f, 0x02},
+	{0xEC, 0x00},
+	{0xED, 0x00},
+	{0xEE, 0x00},
+	{0xEF, 0x00},
+
+	{0xFF, 0x05},
+	{0x00, 0xd0},
+	{0x01, 0x2c},
+	{0x05, 0x20}, //d_agc_option
+	{0x1d, 0x0c},
+	{0x21, 0x20}, //sub contrast
+	{0x24, 0x2a},
+	{0x25, 0xcc}, //fsc_lock_mode
+	{0x26, 0x40},
+	{0x27, 0x57},
+	{0x28, 0x80}, //s_point
+	{0x2b, 0xc0}, //saturation_b
+	{0x31, 0x02},
+	{0x32, 0x10},
+	{0x38, 0x00},
+	{0x47, 0xEE},
+	{0x50, 0xc6},
+	{0x53, 0x04},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x59, 0x00},
+	{0x5C, 0x78},
+	{0x5F, 0x00},
+	{0x62, 0x20},
+	{0x64, 0x01},
+	{0x65, 0x00},
+	{0x69, 0x00},
+	{0x6E, 0x00}, //VBLK_EXT_EN
+	{0x6F, 0x00}, //VBLK_EXT_[7:0]
+	{0x90, 0x0d}, //comb_mode
+	{0x92, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x00},
+	{0xa9, 0x00},
+	{0xb5, 0x00},
+	{0xb7, 0xfc},
+	{0xb8, 0xb8},
+	{0xb9, 0x72},
+	{0xbb, 0x0f},
+	{0xd1, 0x30}, //burst_dec_c
+	{0xd5, 0x80},
+
+	{0xFF, 0x09},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x50, 0x30},
+	{0x51, 0x6f},
+	{0x52, 0x67},
+	{0x53, 0x48},
+	{0x54, 0x30},
+	{0x55, 0x6f},
+	{0x56, 0x67},
+	{0x57, 0x48},
+	{0x58, 0x30},
+	{0x59, 0x6f},
+	{0x5a, 0x67},
+	{0x5b, 0x48},
+	{0x5c, 0x30},
+	{0x5d, 0x6f},
+	{0x5e, 0x67},
+	{0x5f, 0x48},
+	{0x96, 0x10},
+	{0x97, 0x10},
+	{0x98, 0x00},
+	{0x99, 0x00},
+	{0x9a, 0x00},
+	{0x9b, 0x00},
+	{0x9c, 0x00},
+	{0x9d, 0x00},
+	{0x9e, 0x00},
+	{0xb6, 0x10},
+	{0xb7, 0x10},
+	{0xb8, 0x00},
+	{0xb9, 0x00},
+	{0xba, 0x00},
+	{0xbb, 0x00},
+	{0xbc, 0x00},
+	{0xbd, 0x00},
+	{0xbe, 0x00},
+	{0xd6, 0x10},
+	{0xd7, 0x10},
+	{0xd8, 0x00},
+	{0xd9, 0x00},
+	{0xda, 0x00},
+	{0xdb, 0x00},
+	{0xdc, 0x00},
+	{0xdd, 0x00},
+	{0xde, 0x00},
+	{0xf6, 0x10},
+	{0xf7, 0x10},
+	{0xf8, 0x00},
+	{0xf9, 0x00},
+	{0xfa, 0x00},
+	{0xfb, 0x00},
+	{0xfc, 0x00},
+	{0xfd, 0x00},
+	{0xfe, 0x00},
+
+	{0xff, 0x0a},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+
+	{0xff, 0x0b},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+
+	{0xFF, 0x21},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0xFF, 0x20},
+	{0x01, 0xaa}, //0x00:1/1;0x55:1/2;0xaa:1/4
+	{0x00, 0x00},
+	{0x40, 0x01},
+	{0x0F, 0x00},
+	{0x0D, 0x01}, //0x01:4lane;0x00:2lane
+	{0x40, 0x00},
+	{0x00, 0xff}, //0xff:ch1/2/3/4 0x33:ch1/2 0x11:ch1
+
+	{0xFF, 0x01},
+	{0xC8, 0x00},
+	{0xC9, 0x00},
+	{0xCA, 0x00},
+	{0xCB, 0x00},
+
+	//pattern enabled
+	{0xFF, 0x00},
+	{0x1C, 0x1A},
+	{0x1D, 0x1A},
+	{0x1E, 0x1A},
+	{0x1F, 0x1A},
+
+	{0xFF, 0x05},
+	{0x6A, 0x80},
+	{0xFF, 0x06},
+	{0x6A, 0x80},
+	{0xFF, 0x07},
+	{0x6A, 0x80},
+	{0xFF, 0x08},
+	{0x6A, 0x80},
+	{0xFF, 0x21}, //add frame num
+	{0x3E, 0x11}, //1 : Fix to 1 for Odd Field, 2 for Even Field
+	{0x3F, 0x11}, //1 : Fix to 1 for Odd Field, 2 for Even Field
+	SensorEnd
+};
+
+/* 720p Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_720p_25hz[] = {
+	{0xff, 0x04},
+	{0xa0, 0x24},
+	{0xa1, 0x24},
+	{0xa2, 0x24},
+	{0xa3, 0x24},
+	{0xa4, 0x24},
+	{0xa5, 0x24},
+	{0xa6, 0x24},
+	{0xa7, 0x24},
+	{0xa8, 0x24},
+	{0xa9, 0x24},
+	{0xaa, 0x24},
+	{0xab, 0x24},
+	{0xac, 0x24},
+	{0xad, 0x24},
+	{0xae, 0x24},
+	{0xaf, 0x24},
+	{0xb0, 0x24},
+	{0xb1, 0x24},
+	{0xb2, 0x24},
+	{0xb3, 0x24},
+	{0xb4, 0x24},
+	{0xb5, 0x24},
+	{0xb6, 0x24},
+	{0xb7, 0x24},
+	{0xb8, 0x24},
+	{0xb9, 0x24},
+	{0xba, 0x24},
+	{0xbb, 0x24},
+	{0xbc, 0x24},
+	{0xbd, 0x24},
+	{0xbe, 0x24},
+	{0xbf, 0x24},
+	{0xc0, 0x24},
+	{0xc1, 0x24},
+	{0xc2, 0x24},
+	{0xc3, 0x24},
+	{0xff, 0x21},
+	{0x07, 0x80},
+	{0x07, 0x00},
+	{0xff, 0x0A},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xff, 0x0B},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xFF, 0x21},
+	{0x40, 0xAC},
+	{0x41, 0x10},
+	{0x42, 0x03},
+	{0x43, 0x43},
+	{0x11, 0x04},
+	{0x10, 0x0A},
+	{0x12, 0x06},
+	{0x13, 0x09},
+	{0x17, 0x01},
+	{0x18, 0x0D},
+	{0x15, 0x04},
+	{0x14, 0x16},
+	{0x16, 0x05},
+	{0x19, 0x05},
+	{0x1A, 0x0A},
+	{0x1B, 0x08},
+	{0x1C, 0x07},
+	{0x44, 0x00},
+	{0x49, 0xF3},
+	{0x49, 0xF0},
+	{0x44, 0x02},
+	{0x08, 0x40}, //0x40:non-continue;0x48:continuous
+	{0x0F, 0x01},
+	{0x38, 0x1E},
+	{0x39, 0x1E},
+	{0x3A, 0x1E},
+	{0x3B, 0x1E},
+	{0x07, 0x0f}, //0x07:2lane;0x0f:4lane
+	{0x2D, 0x01}, //0x00:2lane;0x01:4lane
+	{0x45, 0x02},
+	{0xFF, 0x13},
+	{0x30, 0x00},
+	{0x31, 0x00},
+	{0x32, 0x00},
+	{0xFF, 0x00},
+	{0x00, 0x00},
+	{0x01, 0x00},
+	{0x02, 0x00},
+	{0x03, 0x00},
+	{0x04, 0x00}, //sd_mode
+	{0x05, 0x00},
+	{0x06, 0x00},
+	{0x07, 0x00},
+	{0x08, 0x0d}, //ahd_mode
+	{0x09, 0x0d},
+	{0x0a, 0x0d},
+	{0x0b, 0x0d},
+	{0x0c, 0x00},
+	{0x0d, 0x00},
+	{0x0e, 0x00},
+	{0x0f, 0x00},
+	{0x10, 0x20}, //video_format
+	{0x11, 0x20},
+	{0x12, 0x20},
+	{0x13, 0x20},
+	{0x14, 0x00},
+	{0x15, 0x00},
+	{0x16, 0x00},
+	{0x17, 0x00},
+	{0x18, 0x13},
+	{0x19, 0x13},
+	{0x1a, 0x13},
+	{0x1b, 0x13},
+	{0x1c, 0x1a},
+	{0x1d, 0x1a},
+	{0x1e, 0x1a},
+	{0x1f, 0x1a},
+	{0x20, 0x00},
+	{0x21, 0x00},
+	{0x22, 0x00},
+	{0x23, 0x00},
+	{0x24, 0x88}, //contrast
+	{0x25, 0x88},
+	{0x26, 0x88},
+	{0x27, 0x88},
+	{0x28, 0x84}, //black_level
+	{0x29, 0x84},
+	{0x2a, 0x84},
+	{0x2b, 0x84},
+	{0x30, 0x03}, //y_peaking_mode
+	{0x31, 0x03},
+	{0x32, 0x03},
+	{0x33, 0x03},
+	{0x34, 0x0f}, //y_fir_mode
+	{0x35, 0x0f},
+	{0x36, 0x0f},
+	{0x37, 0x0f},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4a, 0x00},
+	{0x4b, 0x00},
+	{0x4c, 0x00},
+	{0x4d, 0x00},
+	{0x4e, 0x00},
+	{0x4f, 0x00},
+	{0x50, 0x00},
+	{0x51, 0x00},
+	{0x52, 0x00},
+	{0x53, 0x00},
+	{0x58, 0x80},
+	{0x59, 0x80},
+	{0x5a, 0x80},
+	{0x5b, 0x80},
+	{0x5c, 0x82}, //pal_cm_off
+	{0x5d, 0x82},
+	{0x5e, 0x82},
+	{0x5f, 0x82},
+	{0x60, 0x10},
+	{0x61, 0x10},
+	{0x62, 0x10},
+	{0x63, 0x10},
+	{0x64, 0x05}, //y_delay
+	{0x65, 0x05},
+	{0x66, 0x05},
+	{0x67, 0x05},
+	{0x68, 0x43}, //h_delay_a //h_delay_lsb
+	{0x69, 0x43},
+	{0x6a, 0x43},
+	{0x6b, 0x43},
+	{0x6c, 0x00},
+	{0x6d, 0x00},
+	{0x6e, 0x00},
+	{0x6f, 0x00},
+	{0x78, 0x21},
+	{0x79, 0x21},
+	{0x7a, 0x21},
+	{0x7b, 0x21},
+	{0xFF, 0x01},
+	{0x7C, 0x00},
+	{0x84, 0x04},
+	{0x85, 0x04},
+	{0x86, 0x04},
+	{0x87, 0x04},
+	{0x88, 0x01},
+	{0x89, 0x01},
+	{0x8a, 0x01},
+	{0x8b, 0x01},
+	{0x8c, 0x02},
+	{0x8d, 0x02},
+	{0x8e, 0x02},
+	{0x8f, 0x02},
+	{0xEC, 0x00},
+	{0xED, 0x00},
+	{0xEE, 0x00},
+	{0xEF, 0x00},
+	{0xFF, 0x05},
+	{0x00, 0xd0},
+	{0x01, 0x2c},
+	{0x05, 0x24}, //d_agc_option
+	{0x1d, 0x0c},
+	{0x24, 0x2a},
+	{0x25, 0xdc}, //fsc_lock_mode
+	{0x26, 0x40},
+	{0x27, 0x57},
+	{0x28, 0x80}, //s_point
+	{0x2b, 0xa8}, //saturation_b
+	{0x31, 0x82},
+	{0x32, 0x10},
+	{0x38, 0x00}, //burst_dec_b
+	{0x47, 0xEE},
+	{0x50, 0xc6},
+	{0x53, 0x00},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x59, 0x00},
+	{0x5C, 0x78},
+	{0x5F, 0x00},
+	{0x62, 0x20},
+	{0x64, 0x00},
+	{0x65, 0x00},
+	{0x69, 0x00},
+	{0x6E, 0x00}, //VBLK_EXT_EN
+	{0x6F, 0x00}, //VBLK_EXT_[7:0]
+	{0x90, 0x01}, //comb_mode
+	{0x92, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x00},
+	{0xa9, 0x00},
+	{0xb5, 0x80},
+	{0xb7, 0xfc},
+	{0xb8, 0x39},
+	{0xb9, 0x72},
+	{0xbb, 0x0f},
+	{0xd1, 0x30}, //burst_dec_c
+	{0xd5, 0x80},
+	{0xFF, 0x09},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x50, 0x30},
+	{0x51, 0x6f},
+	{0x52, 0x67},
+	{0x53, 0x48},
+	{0x54, 0x30},
+	{0x55, 0x6f},
+	{0x56, 0x67},
+	{0x57, 0x48},
+	{0x58, 0x30},
+	{0x59, 0x6f},
+	{0x5a, 0x67},
+	{0x5b, 0x48},
+	{0x5c, 0x30},
+	{0x5d, 0x6f},
+	{0x5e, 0x67},
+	{0x5f, 0x48},
+	{0x96, 0x00},
+	{0x97, 0x00},
+	{0x98, 0x00},
+	{0x99, 0x00},
+	{0x9a, 0x00},
+	{0x9b, 0x00},
+	{0x9c, 0x00},
+	{0x9d, 0x00},
+	{0x9e, 0x00},
+	{0xb6, 0x00},
+	{0xb7, 0x00},
+	{0xb8, 0x00},
+	{0xb9, 0x00},
+	{0xba, 0x00},
+	{0xbb, 0x00},
+	{0xbc, 0x00},
+	{0xbd, 0x00},
+	{0xbe, 0x00},
+	{0xd6, 0x00},
+	{0xd7, 0x00},
+	{0xd8, 0x00},
+	{0xd9, 0x00},
+	{0xda, 0x00},
+	{0xdb, 0x00},
+	{0xdc, 0x00},
+	{0xdd, 0x00},
+	{0xde, 0x00},
+	{0xf6, 0x00},
+	{0xf7, 0x00},
+	{0xf8, 0x00},
+	{0xf9, 0x00},
+	{0xfa, 0x00},
+	{0xfb, 0x00},
+	{0xfc, 0x00},
+	{0xfd, 0x00},
+	{0xfe, 0x00},
+	{0xff, 0x0a},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xff, 0x0b},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xFF, 0x21},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0xFF, 0x20},
+	{0x01, 0x55},
+	{0x00, 0x00},
+	{0x40, 0x01},
+	{0x0F, 0x00},
+	{0x0D, 0x01},  //0x01:4lane;0x00:2lane
+	{0x40, 0x00},
+	{0x00, 0xFF},  //ch1/2/3/4 enabled
+	//{0x00, 0x33},  //ch1/2 enabled
+	//{0x00, 0x11},  //ch1 enabled
+	{0xFF, 0x01},
+	{0xC8, 0x00},
+	{0xC9, 0x00},
+	{0xCA, 0x00},
+	{0xCB, 0x00},
+	//pattern enabled
+	{0xFF, 0x00},
+	{0x1C, 0x1A},
+	{0x1D, 0x1A},
+	{0x1E, 0x1A},
+	{0x1F, 0x1A},
+	{0xFF, 0x05},
+	{0x6A, 0x80},
+	{0xFF, 0x06},
+	{0x6A, 0x80},
+	{0xFF, 0x07},
+	{0x6A, 0x80},
+	{0xFF, 0x08},
+	{0x6A, 0x80},
+	SensorEnd
+};
+
+/* 1080p Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_1080p_25hz[] = {
+	{0xff, 0x04},
+	{0xa0, 0x24},
+	{0xa1, 0x24},
+	{0xa2, 0x24},
+	{0xa3, 0x24},
+	{0xa4, 0x24},
+	{0xa5, 0x24},
+	{0xa6, 0x24},
+	{0xa7, 0x24},
+	{0xa8, 0x24},
+	{0xa9, 0x24},
+	{0xaa, 0x24},
+	{0xab, 0x24},
+	{0xac, 0x24},
+	{0xad, 0x24},
+	{0xae, 0x24},
+	{0xaf, 0x24},
+	{0xb0, 0x24},
+	{0xb1, 0x24},
+	{0xb2, 0x24},
+	{0xb3, 0x24},
+	{0xb4, 0x24},
+	{0xb5, 0x24},
+	{0xb6, 0x24},
+	{0xb7, 0x24},
+	{0xb8, 0x24},
+	{0xb9, 0x24},
+	{0xba, 0x24},
+	{0xbb, 0x24},
+	{0xbc, 0x24},
+	{0xbd, 0x24},
+	{0xbe, 0x24},
+	{0xbf, 0x24},
+	{0xc0, 0x24},
+	{0xc1, 0x24},
+	{0xc2, 0x24},
+	{0xc3, 0x24},
+	{0xff, 0x21},
+	{0x07, 0x80},
+	{0x07, 0x00},
+	{0xff, 0x0A},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xff, 0x0B},
+	{0x77, 0x8F},
+	{0xF7, 0x8F},
+	{0xFF, 0x21},
+	{0x40, 0xB4},
+	{0x41, 0x00},
+	{0x42, 0x03},
+	{0x43, 0x43},
+	{0x11, 0x08},
+	{0x10, 0x13},
+	{0x12, 0x0B},
+	{0x13, 0x12},
+	{0x17, 0x02},
+	{0x18, 0x12},
+	{0x15, 0x07},
+	{0x14, 0x2D},
+	{0x16, 0x0B},
+	{0x19, 0x09},
+	{0x1A, 0x15},
+	{0x1B, 0x11},
+	{0x1C, 0x0E},
+	{0x44, 0x00},
+	{0x49, 0xF3},
+	{0x49, 0xF0},
+	{0x44, 0x02},
+	{0x08, 0x40}, //0x40:non-continue;0x48:continuous
+	{0x0F, 0x01},
+	{0x38, 0x1E},
+	{0x39, 0x1E},
+	{0x3A, 0x1E},
+	{0x3B, 0x1E},
+	{0x07, 0x0f}, //0x07:2lane;0x0f:4lane
+	{0x2D, 0x01}, //0x00:2lane;0x01:4lane
+	{0x45, 0x02},
+	{0xFF, 0x13},
+	{0x30, 0x00},
+	{0x31, 0x00},
+	{0x32, 0x00},
+	{0xFF, 0x00},
+	{0x00, 0x00},
+	{0x01, 0x00},
+	{0x02, 0x00},
+	{0x03, 0x00},
+	{0x04, 0x00}, //sd_mode
+	{0x05, 0x00},
+	{0x06, 0x00},
+	{0x07, 0x00},
+	{0x08, 0x03}, //ahd_mode
+	{0x09, 0x03},
+	{0x0a, 0x03},
+	{0x0b, 0x03},
+	{0x0c, 0x00},
+	{0x0d, 0x00},
+	{0x0e, 0x00},
+	{0x0f, 0x00},
+	{0x10, 0x20}, //video_format
+	{0x11, 0x20},
+	{0x12, 0x20},
+	{0x13, 0x20},
+	{0x14, 0x00},
+	{0x15, 0x00},
+	{0x16, 0x00},
+	{0x17, 0x00},
+	{0x18, 0x13},
+	{0x19, 0x13},
+	{0x1a, 0x13},
+	{0x1b, 0x13},
+	{0x1c, 0x1a},
+	{0x1d, 0x1a},
+	{0x1e, 0x1a},
+	{0x1f, 0x1a},
+	{0x20, 0x00},
+	{0x21, 0x00},
+	{0x22, 0x00},
+	{0x23, 0x00},
+	{0x24, 0x86}, //contrast
+	{0x25, 0x86},
+	{0x26, 0x86},
+	{0x27, 0x86},
+	{0x28, 0x80}, //black_level
+	{0x29, 0x80},
+	{0x2a, 0x80},
+	{0x2b, 0x80},
+	{0x30, 0x00}, //y_peaking_mode
+	{0x31, 0x00},
+	{0x32, 0x00},
+	{0x33, 0x00},
+	{0x34, 0x00}, //y_fir_mode
+	{0x35, 0x00},
+	{0x36, 0x00},
+	{0x37, 0x00},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4a, 0x00},
+	{0x4b, 0x00},
+	{0x4c, 0xfe},
+	{0x4d, 0xfe},
+	{0x4e, 0xfe},
+	{0x4f, 0xfe},
+	{0x50, 0xfb},
+	{0x51, 0xfb},
+	{0x52, 0xfb},
+	{0x53, 0xfb},
+	{0x58, 0x80},
+	{0x59, 0x80},
+	{0x5a, 0x80},
+	{0x5b, 0x80},
+	{0x5c, 0x82}, //pal_cm_off
+	{0x5d, 0x82},
+	{0x5e, 0x82},
+	{0x5f, 0x82},
+	{0x60, 0x10},
+	{0x61, 0x10},
+	{0x62, 0x10},
+	{0x63, 0x10},
+	{0x64, 0x05}, //y_delay
+	{0x65, 0x05},
+	{0x66, 0x05},
+	{0x67, 0x05},
+	{0x68, 0x48}, //h_delay_a //h_delay_lsb
+	{0x69, 0x48},
+	{0x6a, 0x48},
+	{0x6b, 0x48},
+	{0x6c, 0x00},
+	{0x6d, 0x00},
+	{0x6e, 0x00},
+	{0x6f, 0x00},
+//	{0x78, 0x21},
+//	{0x79, 0x21},
+//	{0x7a, 0x21},
+//	{0x7b, 0x21},
+	{0x78, 0x22},
+	{0x79, 0x22},
+	{0x7a, 0x22},
+	{0x7b, 0x22},
+	{0xFF, 0x01},
+	{0x7C, 0x00},
+	{0x84, 0x04},
+	{0x85, 0x04},
+	{0x86, 0x04},
+	{0x87, 0x04},
+	{0x88, 0x01},
+	{0x89, 0x01},
+	{0x8a, 0x01},
+	{0x8b, 0x01},
+	{0x8c, 0x02},
+	{0x8d, 0x02},
+	{0x8e, 0x02},
+	{0x8f, 0x02},
+	{0xEC, 0x00},
+	{0xED, 0x00},
+	{0xEE, 0x00},
+	{0xEF, 0x00},
+	{0xFF, 0x05},
+	{0x00, 0xd0},
+	{0x01, 0x2c},
+	{0x05, 0x24}, //d_agc_option
+	{0x1d, 0x0c},
+	{0x24, 0x2a},
+	{0x25, 0xdc}, //fsc_lock_mode
+	{0x26, 0x40},
+	{0x27, 0x57},
+	{0x28, 0x80}, //s_point
+	{0x2b, 0xa8}, //saturation_b
+	{0x31, 0x82},
+	{0x32, 0x10},
+	{0x38, 0x13},
+	{0x47, 0xEE},
+	{0x50, 0xc6},
+	{0x53, 0x00},
+	{0x57, 0x00},
+	{0x58, 0x77},
+	{0x59, 0x00},
+	{0x5C, 0x78},
+	{0x5F, 0x00},
+	{0x62, 0x20},
+	{0x64, 0x00},
+	{0x65, 0x00},
+	{0x69, 0x00},
+	{0x6E, 0x00}, //VBLK_EXT_EN
+	{0x6F, 0x00}, //VBLK_EXT_[7:0]
+	{0x90, 0x01}, //comb_mode
+	{0x92, 0x00},
+	{0x94, 0x00},
+	{0x95, 0x00},
+	{0xa9, 0x00},
+	{0xb5, 0x80},
+	{0xb7, 0xfc},
+	{0xb8, 0x39},
+	{0xb9, 0x72},
+	{0xbb, 0x0f},
+	{0xd1, 0x30}, //burst_dec_c
+	{0xd5, 0x80},
+	{0xFF, 0x09},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x00},
+	{0x44, 0x00},
+	{0x45, 0x00},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x50, 0x30},
+	{0x51, 0x6f},
+	{0x52, 0x67},
+	{0x53, 0x48},
+	{0x54, 0x30},
+	{0x55, 0x6f},
+	{0x56, 0x67},
+	{0x57, 0x48},
+	{0x58, 0x30},
+	{0x59, 0x6f},
+	{0x5a, 0x67},
+	{0x5b, 0x48},
+	{0x5c, 0x30},
+	{0x5d, 0x6f},
+	{0x5e, 0x67},
+	{0x5f, 0x48},
+	{0x96, 0x00},
+	{0x97, 0x00},
+	{0x98, 0x00},
+	{0x99, 0x00},
+	{0x9a, 0x00},
+	{0x9b, 0x00},
+	{0x9c, 0x00},
+	{0x9d, 0x00},
+	{0x9e, 0x00},
+	{0xb6, 0x00},
+	{0xb7, 0x00},
+	{0xb8, 0x00},
+	{0xb9, 0x00},
+	{0xba, 0x00},
+	{0xbb, 0x00},
+	{0xbc, 0x00},
+	{0xbd, 0x00},
+	{0xbe, 0x00},
+	{0xd6, 0x00},
+	{0xd7, 0x00},
+	{0xd8, 0x00},
+	{0xd9, 0x00},
+	{0xda, 0x00},
+	{0xdb, 0x00},
+	{0xdc, 0x00},
+	{0xdd, 0x00},
+	{0xde, 0x00},
+	{0xf6, 0x00},
+	{0xf7, 0x00},
+	{0xf8, 0x00},
+	{0xf9, 0x00},
+	{0xfa, 0x00},
+	{0xfb, 0x00},
+	{0xfc, 0x00},
+	{0xfd, 0x00},
+	{0xfe, 0x00},
+	{0xff, 0x0a},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xff, 0x0b},
+	{0x3d, 0x00},
+	{0x3c, 0x00},
+	{0x30, 0xac},
+	{0x31, 0x78},
+	{0x32, 0x17},
+	{0x33, 0xc1},
+	{0x34, 0x40},
+	{0x35, 0x00},
+	{0x36, 0xc3},
+	{0x37, 0x0a},
+	{0x38, 0x00},
+	{0x39, 0x02},
+	{0x3a, 0x00},
+	{0x3b, 0xb2},
+	{0x25, 0x10},
+	{0x27, 0x1e},
+	{0xbd, 0x00},
+	{0xbc, 0x00},
+	{0xb0, 0xac},
+	{0xb1, 0x78},
+	{0xb2, 0x17},
+	{0xb3, 0xc1},
+	{0xb4, 0x40},
+	{0xb5, 0x00},
+	{0xb6, 0xc3},
+	{0xb7, 0x0a},
+	{0xb8, 0x00},
+	{0xb9, 0x02},
+	{0xba, 0x00},
+	{0xbb, 0xb2},
+	{0xa5, 0x10},
+	{0xa7, 0x1e},
+	{0xFF, 0x21},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0xFF, 0x20},
+	{0x01, 0x00},
+	{0x00, 0x00},
+	{0x40, 0x01},
+	{0x0F, 0x00},
+	{0x0D, 0x01}, //0x01:4lane;0x00:2lane
+	{0x40, 0x00},
+	{0x00, 0xFF}, //ch1/2/3/4 enabled
+	//{0x00, 0x33}, //ch1/2 enabled
+	//{0x00, 0x11}, //ch1 enabled
+	{0xFF, 0x01},
+	{0xC8, 0x00},
+	{0xC9, 0x00},
+	{0xCA, 0x00},
+	{0xCB, 0x00},
+	//pattern enabled
+	{0xFF, 0x00},
+	{0x1C, 0x1A},
+	{0x1D, 0x1A},
+	{0x1E, 0x1A},
+	{0x1F, 0x1A},
+	{0xFF, 0x05},
+	{0x6A, 0x80},
+	{0xFF, 0x06},
+	{0x6A, 0x80},
+	{0xFF, 0x07},
+	{0x6A, 0x80},
+	{0xFF, 0x08},
+	{0x6A, 0x80},
+	SensorEnd
+};
+
+/* format detect open*/
+static struct rk_sensor_reg sensor_open_format_detect[] = {
+	{0xff, 0x13},
+	{0x1f, 0x23},
+	{0x30, 0xff},
+	{0x31, 0xff},
+	{0x32, 0xff},
+	SensorEnd
+};
+
+static void nvp6324_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0;
+
+	switch (cvstd) {
+	case CVSTD_PAL:
+		ad->cfg.width = FORCE_PAL_WIDTH;
+		ad->cfg.height = FORCE_PAL_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_PAL;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 1;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;//25	 30
+		ad->cfg.mipi_freq = JAGUAR1_LINK_FREQ_320M;
+		break;
+	case CVSTD_NTSC:
+		ad->cfg.width = FORCE_NTSC_WIDTH;
+		ad->cfg.height = FORCE_NTSC_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_NTSC;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 1;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;//25	 30
+		ad->cfg.mipi_freq = JAGUAR1_LINK_FREQ_320M;
+		break;
+	case CVSTD_720P25:
+		ad->cfg.width = 1280;
+		ad->cfg.height = 720;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = JAGUAR1_LINK_FREQ_320M;
+		break;
+
+	case CVSTD_1080P25:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = JAGUAR1_LINK_FREQ_640M;
+		break;
+
+	default:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = JAGUAR1_LINK_FREQ_640M;
+		break;
+	}
+	ad->cfg.type = V4L2_MBUS_CSI2_DPHY;
+	ad->cfg.mbus_flags = V4L2_MBUS_CSI2_4_LANE |
+			 V4L2_MBUS_CSI2_CHANNELS;
+	ad->cfg.mbus_code = MEDIA_BUS_FMT_UYVY8_2X8;
+
+	switch (ad->cfg.mbus_flags & V4L2_MBUS_CSI2_LANES) {
+	case V4L2_MBUS_CSI2_1_LANE:
+		ad->cfg.lanes = 1;
+		break;
+	case V4L2_MBUS_CSI2_2_LANE:
+		ad->cfg.lanes = 2;
+		break;
+	case V4L2_MBUS_CSI2_3_LANE:
+		ad->cfg.lanes = 3;
+		break;
+	case V4L2_MBUS_CSI2_4_LANE:
+		ad->cfg.lanes = 4;
+		break;
+	default:
+		ad->cfg.lanes = 1;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+}
+
+static void nvp6324_reg_init(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	struct rk_sensor_reg *sensor;
+	int i;
+
+	switch (cvstd) {
+	case CVSTD_NTSC:
+		VEHICLE_DG("%s, init CVSTD_NTSC mode", __func__);
+		sensor = sensor_preview_data_ntsc_30hz;
+		break;
+	case CVSTD_PAL:
+		VEHICLE_DG("%s, init CVSTD_PAL mode", __func__);
+		sensor = sensor_preview_data_pal_25hz;
+		break;
+	case CVSTD_720P25:
+		VEHICLE_DG("%s, init CVSTD_720P25 mode)", __func__);
+		sensor = sensor_preview_data_720p_25hz;
+		break;
+	case CVSTD_1080P25:
+		VEHICLE_DG("%s, init CVSTD_1080P25 mode", __func__);
+		sensor = sensor_preview_data_1080p_25hz;
+		break;
+	default:
+		VEHICLE_DG("%s, init CVSTD_1080P25 mode", __func__);
+		sensor = sensor_preview_data_1080p_25hz;
+		break;
+	}
+	i = 0;
+	while ((sensor[i].reg != SEQCMD_END) && (sensor[i].reg != 0xFC000000)) {
+		vehicle_sensor_write(ad, sensor[i].reg, sensor[i].val);
+		i++;
+	}
+	/* open format detect*/
+	sensor = sensor_open_format_detect;
+	i = 0;
+	while ((sensor[i].reg != SEQCMD_END) && (sensor[i].reg != 0xFC000000)) {
+		vehicle_sensor_write(ad, sensor[i].reg, sensor[i].val);
+		i++;
+	}
+
+	vehicle_sensor_write(ad, 0xff, 0x05 + ad->ad_chl);
+	vehicle_sensor_write(ad, 0x82, 0xff);
+	vehicle_sensor_write(ad, 0xb8, 0xb9);
+}
+
+void nvp6324_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	unsigned int reg;
+	unsigned char val = 0x00;
+
+	//detect interesting channel
+	reg = channel;
+	ad->ad_chl = channel;
+	VEHICLE_DG("%s, channel set(%d)", __func__, ad->ad_chl);
+	vehicle_sensor_write(ad, 0xff, 0x00);
+	vehicle_sensor_write(ad, reg, val);
+}
+
+int nvp6324_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+
+	if (!nvp6324_g_addev)
+		return -ENODEV;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		nvp6324_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		nvp6324_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		nvp6324_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	nvp6324_g_addev->cfg.ad_ready = true;
+	nvp6324_g_addev->cfg.drop_frames = nvp6324_g_addev->drop_frames;
+
+	*cfg = &nvp6324_g_addev->cfg;
+
+	return 0;
+}
+
+void nvp6324_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	VEHICLE_DG("%s, last_line %d\n", __func__, last_line);
+
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_1080P25) {
+		if (last_line == FORCE_1080P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_720P25) {
+		if (last_line == FORCE_720P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int nvp6324_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	u8 pid;
+
+	ret = vehicle_sensor_write(ad, 0xFF, 0x00);
+	ret |= vehicle_sensor_read(ad, 0xf4, &pid);
+	if (ret)
+		return ret;
+	if (pid != JAGUAR1_CHIP_ID) {
+		VEHICLE_DGERR("%s: expected 0xB0, detected: 0x%02x !",
+		    ad->ad_name, pid);
+		ret = -EINVAL;
+	} else {
+		VEHICLE_INFO("%s Found NVP6324 sensor: id(0x%2x) !\n", __func__, pid);
+	}
+
+	return ret;
+}
+
+static int nvp6324_check_cvstd(struct vehicle_ad_dev *ad, bool activate_check)
+{
+
+	u8 videoloss = 0;
+	int ret = 0;
+	unsigned char cvstd = 0;
+
+	ret = vehicle_sensor_write(ad, 0xFF, 0x00);
+	ret |= vehicle_sensor_read(ad, 0xa4 + ad->ad_chl, &videoloss);
+
+	video_mode = videoloss;
+
+	ret |= vehicle_sensor_write(ad, 0xFF, 0x01);
+	ret |= vehicle_sensor_read(ad, 0x10 + (0x20 * (ad->ad_chl%4)), &cvstd);
+
+	if (ret)
+		return ret;
+
+	if (cvstd == 0x21) {
+		cvstd_mode = CVSTD_720P25;
+		VEHICLE_DG("%s(%d): 720P25\n", __func__, __LINE__);
+	} else if (cvstd == 0x31) {
+		cvstd_mode = CVSTD_1080P25;
+		VEHICLE_DG("%s(%d): 1080P25", __func__, __LINE__);
+	} else if (cvstd == 0x00) {
+		cvstd_mode = CVSTD_NTSC;
+		VEHICLE_DG("%s(%d): 960H NTSC\n", __func__, __LINE__);
+	} else if (cvstd == 0x10) {
+		cvstd_mode = CVSTD_PAL;
+		VEHICLE_DG("%s(%d): 960H PAL\n", __func__, __LINE__);
+	} else if (cvstd == 0xff) {
+		cvstd_mode = cvstd_old;
+		VEHICLE_DG("%s(%d): no ahd plugin!\n", __func__, __LINE__);
+	} else {
+		cvstd_mode = cvstd_old;
+		VEHICLE_DG("%s(%d): not support ahd mode!\n", __func__, __LINE__);
+	}
+
+	return 0;
+}
+
+int nvp6324_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	VEHICLE_DG("%s on(%d)\n", __func__, enable);
+
+	g_nvp6324_streaming = (enable != 0);
+	if (g_nvp6324_streaming) {
+		vehicle_sensor_write(ad, 0xff, 0x21);
+		vehicle_sensor_write(ad, 0x07, 0x0f);
+		if (ad->state_check_work.state_check_wq)
+			queue_delayed_work(ad->state_check_work.state_check_wq,
+				&ad->state_check_work.work, msecs_to_jiffies(200));
+	} else {
+		vehicle_sensor_write(ad, 0xff, 0x21);
+		vehicle_sensor_write(ad, 0x07, 0x8f);
+		if (ad->state_check_work.state_check_wq)
+			cancel_delayed_work_sync(&ad->state_check_work.work);
+	}
+
+	return 0;
+}
+
+static void nvp6324_power_on(struct vehicle_ad_dev *ad)
+{
+	/* gpio_direction_output(ad->power, ad->pwr_active); */
+	VEHICLE_DG("gpio: power(%d), powerdown(%d), reset(%d)",
+		ad->power, ad->powerdown, ad->reset);
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "nvp6324_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "nvp6324_pwd");
+		gpio_direction_output(ad->powerdown, 1);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->reset)) {
+		gpio_request(ad->reset, "nvp6324_rst");
+		gpio_direction_output(ad->reset, 0);
+		usleep_range(1500, 2000);
+		gpio_direction_output(ad->reset, 1);
+	}
+}
+
+static void nvp6324_power_deinit(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->reset))
+		gpio_free(ad->reset);
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static void nvp6324_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = nvp6324_g_addev;
+
+	if (ad->cif_error_last_line > 0) {
+		nvp6324_check_cvstd(ad, true);
+		ad->cif_error_last_line = 0;
+	} else {
+		nvp6324_check_cvstd(ad, false);
+	}
+
+	if (cvstd_old != cvstd_mode ||
+		cvstd_old_state != cvstd_state || (video_old != video_mode)) {
+		VEHICLE_INFO("%s:ad sensor std mode change, cvstd_old(%d), cvstd_mode(%d)\n",
+				 __func__, cvstd_old, cvstd_mode);
+		cvstd_old = cvstd_mode;
+		cvstd_old_state = cvstd_state;
+		video_old = video_mode;
+
+		nvp6324_reinit_parameter(ad, cvstd_mode);
+		nvp6324_reg_init(ad, cvstd_mode);
+		vehicle_ad_stat_change_notify();
+	}
+	if (g_nvp6324_streaming) {
+		queue_delayed_work(ad->state_check_work.state_check_wq,
+				&ad->state_check_work.work, msecs_to_jiffies(100));
+	}
+}
+
+int nvp6324_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = nvp6324_g_addev;
+
+	if (!ad)
+		return -ENODEV;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+	if (ad->irq)
+		free_irq(ad->irq, ad);
+	nvp6324_power_deinit(ad);
+
+	return 0;
+}
+
+static __maybe_unused int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+	case AD_FIX_FORMAT_NTSC:
+	case AD_FIX_FORMAT_720P_50FPS:
+	case AD_FIX_FORMAT_720P_30FPS:
+	case AD_FIX_FORMAT_720P_25FPS:
+		mode = CVSTD_720P25;
+		break;
+	case AD_FIX_FORMAT_1080P_30FPS:
+	case AD_FIX_FORMAT_1080P_25FPS:
+
+	default:
+		mode = CVSTD_720P25;
+		break;
+	}
+
+	return mode;
+}
+
+int nvp6324_ad_init(struct vehicle_ad_dev *ad)
+{
+	int val;
+	int i = 0;
+
+	nvp6324_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	nvp6324_power_on(ad);
+
+	while (++i < 5) {
+		usleep_range(10000, 12000);
+		val = vehicle_generic_sensor_read(ad, 0xf0);
+		if (val != 0xff)
+			break;
+		VEHICLE_DGERR("nvp6324_init i2c_reg_read fail\n");
+	}
+
+	nvp6324_reg_init(ad, cvstd_mode);
+
+	nvp6324_reinit_parameter(ad, cvstd_mode);
+
+	INIT_DELAYED_WORK(&ad->state_check_work.work, nvp6324_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-nvp6324");
+
+	/* nvp6324_check_cvstd(ad, true); */
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.h b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.h
new file mode 100644
index 0000000000000..b287807379692
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_nvp6324.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_NVP6324_H__
+#define __VEHICLE_AD_NVP6324_H__
+
+int nvp6324_ad_init(struct vehicle_ad_dev *ad);
+int nvp6324_ad_deinit(void);
+int nvp6324_ad_get_cfg(struct vehicle_cfg **cfg);
+void nvp6324_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int nvp6324_check_id(struct vehicle_ad_dev *ad);
+int nvp6324_stream(struct vehicle_ad_dev *ad, int enable);
+void nvp6324_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.c b/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.c
new file mode 100644
index 0000000000000..0c10f4ecaccb4
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.c
@@ -0,0 +1,1039 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor tp2825
+ *
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      Zhiqin Wei <wzq@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_tp2825.h"
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+#define FLAG_LOSS				(0x1 << 7)
+#define FLAG_V_LOCKED			(0x1 << 6)
+#define FLAG_H_LOCKED			(0x1 << 5)
+#define FLAG_CARRIER_PLL_LOCKED	(0x1 << 4)
+#define FLAG_VIDEO_DETECTED		(0x1 << 3)
+#define FLAG_EQ_SD_DETECTED		(0x1 << 2)
+#define FLAG_PROGRESSIVE		(0x1 << 1)
+#define FLAG_NO_CARRIER			(0x1 << 0)
+#define FLAG_LOCKED		(FLAG_V_LOCKED | FLAG_H_LOCKED)
+
+static struct vehicle_ad_dev *tp2825_g_addev;
+static int cvstd_mode = CVSTD_720P50;
+static int cvstd_old = CVSTD_720P50;
+static int cvstd_sd = CVSTD_NTSC;
+static int cvstd_state = VIDEO_UNPLUG;
+static int cvstd_old_state = VIDEO_UNPLUG;
+
+#define SENSOR_REGISTER_LEN	1	/* sensor register address bytes*/
+#define SENSOR_VALUE_LEN	1	/* sensor register value bytes*/
+
+struct rk_sensor_reg {
+	unsigned int reg;
+	unsigned int val;
+};
+
+#define SENSOR_CHANNEL_REG		0x41
+
+#define SEQCMD_END  0xFF000000
+#define SensorEnd   {SEQCMD_END, 0x00}
+
+#define SENSOR_DG VEHICLE_DG
+
+/* Preview resolution setting*/
+static struct rk_sensor_reg sensor_preview_data_ntsc[] = {
+	{0x02, 0xCF},
+	{0x06, 0x32},
+	{0x07, 0xC0},
+	{0x08, 0x00},
+	{0x09, 0x24},
+	{0x0A, 0x48},
+	{0x0B, 0xC0},
+	{0x0C, 0x53},
+	{0x0D, 0x10},
+	{0x0E, 0x00},
+	{0x0F, 0x00},
+	{0x10, 0x5e},
+	{0x11, 0x40},
+	{0x12, 0x44},
+	{0x13, 0x00},
+	{0x14, 0x00},
+	{0x15, 0x13},
+	{0x16, 0x4E},
+	{0x17, 0xBC},
+	{0x18, 0x15},
+	{0x19, 0xF0},
+	{0x1A, 0x07},
+	{0x1B, 0x00},
+	{0x1C, 0x09},
+	{0x1D, 0x38},
+	{0x1E, 0x80},
+	{0x1F, 0x80},
+	{0x20, 0xA0},
+	{0x21, 0x86},
+	{0x22, 0x38},
+	{0x23, 0x3C},
+	{0x24, 0x56},
+	{0x25, 0xFF},
+	{0x26, 0x12},
+	{0x27, 0x2D},
+	{0x28, 0x00},
+	{0x29, 0x48},
+	{0x2A, 0x30},
+	{0x2B, 0x70},
+	{0x2C, 0x1A},
+	{0x2D, 0x68},
+	{0x2E, 0x5E},
+	{0x2F, 0x00},
+	{0x30, 0x62},
+	{0x31, 0xBB},
+	{0x32, 0x96},
+	{0x33, 0xC0},
+	{0x34, 0x00},
+	{0x35, 0x65},
+	{0x36, 0xDC},
+	{0x37, 0x00},
+	{0x38, 0x40},
+	{0x39, 0x84},
+	{0x3A, 0x00},
+	{0x3B, 0x03},
+	{0x3C, 0x00},
+	{0x3D, 0x60},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x12},
+	{0x44, 0x07},
+	{0x45, 0x49},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4A, 0x00},
+	{0x4B, 0x00},
+	{0x4C, 0x03},
+	{0x4D, 0x00},
+	{0x4E, 0x37},
+	{0x4F, 0x01},
+	{0xB5, 0x01},
+	{0xB8, 0x00},
+	{0xBA, 0x00},
+	{0xF3, 0x00},
+	{0xF4, 0x00},
+	{0xF5, 0x00},
+	{0xF6, 0x00},
+	{0xF7, 0x00},
+	{0xF8, 0x00},
+	{0xF9, 0x00},
+	{0xFA, 0x00},
+	{0xFB, 0x00},
+	{0xFC, 0xC0},
+	{0xFD, 0x00},
+	SensorEnd
+};
+
+static struct rk_sensor_reg sensor_preview_data_pal[] = {
+	{0x02, 0xCE},
+	{0x06, 0x32},
+	{0x07, 0xC0},
+	{0x08, 0x00},
+	{0x09, 0x24},
+	{0x0A, 0x48},
+	{0x0B, 0xC0},
+	{0x0C, 0x53},
+	{0x0D, 0x11},
+	{0x0E, 0x00},
+	{0x0F, 0x00},
+	{0x10, 0x70},
+	{0x11, 0x4D},
+	{0x12, 0x40},
+	{0x13, 0x00},
+	{0x14, 0x00},
+	{0x15, 0x13},
+	{0x16, 0x67},
+	{0x17, 0xBC},
+	{0x18, 0x16},
+	{0x19, 0x20},
+	{0x1A, 0x17},
+	{0x1B, 0x00},
+	{0x1C, 0x09},
+	{0x1D, 0x48},
+	{0x1E, 0x80},
+	{0x1F, 0x80},
+	{0x20, 0xB0},
+	{0x21, 0x86},
+	{0x22, 0x38},
+	{0x23, 0x3C},
+	{0x24, 0x56},
+	{0x25, 0xFF},
+	{0x26, 0x02},
+	{0x27, 0x2D},
+	{0x28, 0x00},
+	{0x29, 0x48},
+	{0x2A, 0x30},
+	{0x2B, 0x70},
+	{0x2C, 0x1A},
+	{0x2D, 0x60},
+	{0x2E, 0x5E},
+	{0x2F, 0x00},
+	{0x30, 0x7A},
+	{0x31, 0x4A},
+	{0x32, 0x4D},
+	{0x33, 0xF0},
+	{0x34, 0x00},
+	{0x35, 0x65},
+	{0x36, 0xDC},
+	{0x37, 0x00},
+	{0x38, 0x40},
+	{0x39, 0x84},
+	{0x3A, 0x00},
+	{0x3B, 0x03},
+	{0x3C, 0x00},
+	{0x3D, 0x60},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x12},
+	{0x44, 0x07},
+	{0x45, 0x49},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4A, 0x00},
+	{0x4B, 0x00},
+	{0x4C, 0x03},
+	{0x4D, 0x00},
+	{0x4E, 0x37},
+	{0x4F, 0x00},
+	{0xB5, 0x01},
+	{0xB8, 0x00},
+	{0xBA, 0x00},
+	{0xF3, 0x00},
+	{0xF4, 0x00},
+	{0xF5, 0x00},
+	{0xF6, 0x00},
+	{0xF7, 0x00},
+	{0xF8, 0x00},
+	{0xF9, 0x00},
+	{0xFA, 0x00},
+	{0xFB, 0x00},
+	{0xFC, 0xC0},
+	{0xFD, 0x00},
+	SensorEnd
+};
+
+static struct rk_sensor_reg sensor_preview_data_720p_50hz[] = {
+	{0x02, 0xCA},
+	{0x06, 0x32},
+	{0x07, 0xC0},
+	{0x08, 0x00},
+	{0x09, 0x24},
+	{0x0A, 0x48},
+	{0x0B, 0xC0},
+	{0x0C, 0x43},
+	{0x0D, 0x10},
+	{0x0E, 0x00},
+	{0x0F, 0x00},
+	{0x10, 0xf0},
+	{0x11, 0x50},
+	{0x12, 0x60},
+	{0x13, 0x00},
+	{0x14, 0x08},
+	{0x15, 0x13},
+	{0x16, 0x16},
+	{0x17, 0x00},
+	{0x18, 0x18},
+	{0x19, 0xD0},
+	{0x1A, 0x25},
+	{0x1B, 0x00},
+	{0x1C, 0x07},
+	{0x1D, 0xBC},
+	{0x1E, 0x80},
+	{0x1F, 0x80},
+	{0x20, 0x60},
+	{0x21, 0x86},
+	{0x22, 0x38},
+	{0x23, 0x3C},
+	{0x24, 0x56},
+	{0x25, 0xFF},
+	{0x26, 0x02},
+	{0x27, 0x2D},
+	{0x28, 0x00},
+	{0x29, 0x48},
+	{0x2A, 0x30},
+	{0x2B, 0x4A},
+	{0x2C, 0x1A},
+	{0x2D, 0x30},
+	{0x2E, 0x70},
+	{0x2F, 0x00},
+	{0x30, 0x48},
+	{0x31, 0xBB},
+	{0x32, 0x2E},
+	{0x33, 0x90},
+	{0x34, 0x00},
+	{0x35, 0x05},
+	{0x36, 0xDC},
+	{0x37, 0x00},
+	{0x38, 0x40},
+	{0x39, 0x8C},
+	{0x3A, 0x00},
+	{0x3B, 0x03},
+	{0x3C, 0x00},
+	{0x3D, 0x60},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x12},
+	{0x44, 0x07},
+	{0x45, 0x49},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4A, 0x00},
+	{0x4B, 0x00},
+	{0x4C, 0x03},
+	{0x4D, 0x00},
+	{0x4E, 0x03},
+	{0x4F, 0x01},
+	{0xB5, 0x01},
+	{0xB8, 0x00},
+	{0xBA, 0x00},
+	{0xF3, 0x00},
+	{0xF4, 0x00},
+	{0xF5, 0x00},
+	{0xF6, 0x00},
+	{0xF7, 0x00},
+	{0xF8, 0x00},
+	{0xF9, 0x00},
+	{0xFA, 0x00},
+	{0xFB, 0x00},
+	{0xFC, 0xC0},
+	{0xFD, 0x00},
+	SensorEnd
+};
+
+static struct rk_sensor_reg sensor_preview_data_720p_30hz[] = {
+	{0x02, 0xDA},
+	{0x06, 0x32},
+	{0x07, 0xC0},
+	{0x08, 0x00},
+	{0x09, 0x24},
+	{0x0A, 0x48},
+	{0x0B, 0xC0},
+	{0x0C, 0x53},
+	{0x0D, 0x10},
+	{0x0E, 0x00},
+	{0x0F, 0x00},
+	{0x10, 0xf0},
+	{0x11, 0x50},
+	{0x12, 0x60},
+	{0x13, 0x00},
+	{0x14, 0x08},
+	{0x15, 0x13},
+	{0x16, 0x16},
+	{0x17, 0x00},
+	{0x18, 0x19},
+	{0x19, 0xD0},
+	{0x1A, 0x25},
+	{0x1B, 0x00},
+	{0x1C, 0x06},
+	{0x1D, 0x72},
+	{0x1E, 0x80},
+	{0x1F, 0x80},
+	{0x20, 0x60},
+	{0x21, 0x86},
+	{0x22, 0x38},
+	{0x23, 0x3C},
+	{0x24, 0x56},
+	{0x25, 0xFF},
+	{0x26, 0x02},
+	{0x27, 0x2D},
+	{0x28, 0x00},
+	{0x29, 0x48},
+	{0x2A, 0x30},
+	{0x2B, 0x4A},
+	{0x2C, 0x1A},
+	{0x2D, 0x30},
+	{0x2E, 0x70},
+	{0x2F, 0x00},
+	{0x30, 0x48},
+	{0x31, 0xBB},
+	{0x32, 0x2E},
+	{0x33, 0x90},
+	{0x34, 0x00},
+	{0x35, 0x25},
+	{0x36, 0xDC},
+	{0x37, 0x00},
+	{0x38, 0x40},
+	{0x39, 0x88},
+	{0x3A, 0x00},
+	{0x3B, 0x03},
+	{0x3C, 0x00},
+	{0x3D, 0x60},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0x40, 0x03},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x12},
+	{0x44, 0x07},
+	{0x45, 0x49},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4A, 0x00},
+	{0x4B, 0x00},
+	{0x4C, 0x03},
+	{0x4D, 0x00},
+	{0x4E, 0x17},
+	{0x4F, 0x01},
+	{0x85, 0x00},
+	{0x88, 0x00},
+	{0x8A, 0x00},
+	{0xF3, 0x00},
+	{0xF4, 0x00},
+	{0xF5, 0x00},
+	{0xF6, 0x00},
+	{0xF7, 0x00},
+	{0xF8, 0x00},
+	{0xF9, 0x00},
+	{0xFA, 0x00},
+	{0xFB, 0x00},
+	{0xFC, 0xC0},
+	{0xFD, 0x00},
+	SensorEnd
+};
+
+static struct rk_sensor_reg sensor_preview_data_720p_25hz[] = {
+	{0x02, 0xCA},
+	{0x06, 0x32},
+	{0x07, 0xC0},
+	{0x08, 0x00},
+	{0x09, 0x24},
+	{0x0A, 0x48},
+	{0x0B, 0xC0},
+	{0x0C, 0x53},
+	{0x0D, 0x10},
+	{0x0E, 0x00},
+	{0x0F, 0x00},
+	{0x10, 0xf0},
+	{0x11, 0x50},
+	{0x12, 0x60},
+	{0x13, 0x00},
+	{0x14, 0x08},
+	{0x15, 0x13},
+	{0x16, 0x16},
+	{0x17, 0x00},
+	{0x18, 0x19},
+	{0x19, 0xD0},
+	{0x1A, 0x25},
+	{0x1B, 0x00},
+	{0x1C, 0x07},
+	{0x1D, 0xBC},
+	{0x1E, 0x80},
+	{0x1F, 0x80},
+	{0x20, 0x60},
+	{0x21, 0x86},
+	{0x22, 0x38},
+	{0x23, 0x3C},
+	{0x24, 0x56},
+	{0x25, 0xFF},
+	{0x26, 0x02},
+	{0x27, 0x2D},
+	{0x28, 0x00},
+	{0x29, 0x48},
+	{0x2A, 0x30},
+	{0x2B, 0x70},
+	{0x2C, 0x1A},
+	{0x2D, 0x30},
+	{0x2E, 0x70},
+	{0x2F, 0x00},
+	{0x30, 0x48},
+	{0x31, 0xBB},
+	{0x32, 0x2E},
+	{0x33, 0x90},
+	{0x34, 0x00},
+	{0x35, 0x25},
+	{0x36, 0xDC},
+	{0x37, 0x00},
+	{0x38, 0x40},
+	{0x39, 0x88},
+	{0x3A, 0x00},
+	{0x3B, 0x03},
+	{0x3C, 0x00},
+	{0x3D, 0x60},
+	{0x3E, 0x00},
+	{0x3F, 0x00},
+	{0x40, 0x00},
+	{0x41, 0x00},
+	{0x42, 0x00},
+	{0x43, 0x12},
+	{0x44, 0x07},
+	{0x45, 0x49},
+	{0x46, 0x00},
+	{0x47, 0x00},
+	{0x48, 0x00},
+	{0x49, 0x00},
+	{0x4A, 0x00},
+	{0x4B, 0x00},
+	{0x4C, 0x03},
+	{0x4D, 0x00},
+	{0x4E, 0x17},
+	{0x4F, 0x01},
+	{0xB5, 0x01},
+	{0xB8, 0x00},
+	{0xBA, 0x00},
+	{0xF3, 0x00},
+	{0xF4, 0x00},
+	{0xF5, 0x00},
+	{0xF6, 0x00},
+	{0xF7, 0x00},
+	{0xF8, 0x00},
+	{0xF9, 0x00},
+	{0xFA, 0x00},
+	{0xFB, 0x00},
+	{0xFC, 0xC0},
+	{0xFD, 0x00},
+	SensorEnd
+};
+
+static void tp2825_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0, defrect_index = 0;
+
+	switch (cvstd) {
+	case CVSTD_PAL:
+		ad->cfg.width = FORCE_PAL_WIDTH;
+		ad->cfg.height = FORCE_PAL_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_PAL;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+		break;
+	case CVSTD_NTSC:
+		ad->cfg.width = FORCE_NTSC_WIDTH;
+		ad->cfg.height = FORCE_NTSC_HEIGHT;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_NTSC;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 30;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_LOW |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+		break;
+	default:
+		ad->cfg.width = 1280;
+		ad->cfg.height = 720;
+		ad->cfg.start_x = 8;
+		ad->cfg.start_y = 20;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 1;
+		ad->cfg.frame_rate = 50;
+		ad->cfg.type = V4L2_MBUS_PARALLEL;
+		ad->cfg.mbus_flags = V4L2_MBUS_HSYNC_ACTIVE_LOW |
+					V4L2_MBUS_VSYNC_ACTIVE_HIGH |
+					V4L2_MBUS_PCLK_SAMPLE_RISING;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+			defrect_index = i;
+		}
+	}
+
+#ifdef CVBS_DOUBLE_FPS_MODE
+	switch (cvstd) {
+	case CVSTD_PAL:
+		if (!strstr(ad->defrects[defrect_index].interface, "pal")) {
+			ad->cfg.height /= 2;
+			ad->cfg.input_format =
+				CIF_INPUT_FORMAT_PAL_SW_COMPOSITE;
+			ad->cfg.href = 0;
+			ad->cfg.vsync = 1;
+			ad->cfg.frame_rate = 50;
+		}
+	break;
+	case CVSTD_NTSC:
+		if (!strstr(ad->defrects[defrect_index].interface, "ntsc")) {
+			ad->cfg.height /= 2;
+			ad->cfg.input_format =
+				CIF_INPUT_FORMAT_NTSC_SW_COMPOSITE;
+			ad->cfg.href = 0;
+			ad->cfg.vsync = 1;
+			ad->cfg.frame_rate = 60;
+		}
+	break;
+	}
+#endif
+	SENSOR_DG("%s,crop(%d,%d)", __func__, ad->cfg.start_x, ad->cfg.start_y);
+}
+
+static void tp2825_reg_init(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	struct rk_sensor_reg *sensor;
+	int i;
+	unsigned char val[2];
+
+	switch (cvstd) {
+	case CVSTD_720P50:
+		sensor = sensor_preview_data_720p_50hz;
+		break;
+	case CVSTD_720P30:
+		sensor = sensor_preview_data_720p_30hz;
+		break;
+	case CVSTD_720P25:
+		sensor = sensor_preview_data_720p_25hz;
+		break;
+	case CVSTD_PAL:
+		sensor = sensor_preview_data_pal;
+		break;
+	case CVSTD_NTSC:
+		sensor = sensor_preview_data_ntsc;
+		break;
+	default:
+		sensor = sensor_preview_data_720p_50hz;
+		break;
+	}
+	i = 0;
+	while ((sensor[i].reg != SEQCMD_END) && (sensor[i].reg != 0xFC000000)) {
+		if (sensor[i].reg == SENSOR_CHANNEL_REG)
+			sensor[i].val = ad->ad_chl;
+
+		val[0] = sensor[i].val;
+		vehicle_generic_sensor_write(ad, sensor[i].reg, val);
+		i++;
+	}
+}
+
+void tp2825_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	unsigned int reg = 0x41;
+	unsigned char val[0];
+
+	val[0] = channel;
+	ad->ad_chl = channel;
+
+	vehicle_generic_sensor_write(ad, reg, val);
+}
+
+int tp2825_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	if (!tp2825_g_addev)
+		return -1;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		tp2825_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		tp2825_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		tp2825_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	*cfg = &tp2825_g_addev->cfg;
+
+	return 0;
+}
+
+void tp2825_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	SENSOR_DG("%s, last_line %d\n", __func__, last_line);
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+int tp2825_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	int pidh, pidl;
+
+	pidh = vehicle_generic_sensor_read(ad, 0xfe);
+	pidl = vehicle_generic_sensor_read(ad, 0xff);
+	if (pidh != 0x28 || pidl != 0x25) {
+		SENSOR_DG("%s: expected 0x2825, detected 0x%02x 0x%02x\n",
+		    ad->ad_name, pidh, pidl);
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int tp2825_check_cvstd(struct vehicle_ad_dev *ad, bool activate_check)
+{
+	unsigned char cvstd = 0;
+	unsigned char status = 0;
+	static bool is_first = true;
+	static int state = VIDEO_UNPLUG;
+	int check_count = 20;
+	unsigned char v[2];
+
+check_continue:
+	status = vehicle_generic_sensor_read(ad, 0x01);
+
+	if (status & FLAG_LOSS) {
+		state = VIDEO_UNPLUG;
+		v[0] = 0x01;
+		vehicle_generic_sensor_write(ad, 0x26, v);
+	} else if (FLAG_LOCKED == (status & FLAG_LOCKED)) {
+		/* video locked */
+		state = VIDEO_LOCKED;
+		v[0] = 0x02;
+		vehicle_generic_sensor_write(ad, 0x26, v);
+	} else {
+		/* video in but unlocked */
+		state = VIDEO_IN;
+		v[0] = 0x02;
+		vehicle_generic_sensor_write(ad, 0x26, v);
+	}
+
+	if (state == VIDEO_IN) {
+		cvstd = vehicle_generic_sensor_read(ad, 0x03);
+		SENSOR_DG("%s(%d): cvstd_old %d, read 0x03 return 0x%x",
+			  __func__, __LINE__, cvstd_old, cvstd);
+
+		cvstd &= 0x07;
+		if (cvstd == cvstd_old)
+			goto check_end;
+
+		if (cvstd == CVSTD_720P30) {
+			cvstd_mode = CVSTD_720P30;
+			SENSOR_DG("%s(%d): 720P30\n", __func__, __LINE__);
+		} else if (cvstd == CVSTD_720P25) {
+			cvstd_mode = CVSTD_720P25;
+			SENSOR_DG("%s(%d): 720P25\n", __func__, __LINE__);
+		} else if (cvstd == CVSTD_720P60) {
+			SENSOR_DG("%s(%d): 720P60", __func__, __LINE__);
+		} else if (cvstd == CVSTD_720P50) {
+			cvstd_mode = CVSTD_720P50;
+			SENSOR_DG("%s(%d): 720P50\n", __func__, __LINE__);
+		} else if (cvstd == CVSTD_1080P30) {
+			SENSOR_DG("%s(%d): 1080P30", __func__, __LINE__);
+		} else if (cvstd == CVSTD_1080P25) {
+			SENSOR_DG("%s(%d): 1080P25", __func__, __LINE__);
+		} else if (cvstd == CVSTD_SD) {
+			msleep(80);
+			status = vehicle_generic_sensor_read(ad, 0x01);
+			SENSOR_DG("%s(%d): read 0x01 return 0x%x\n",
+				  __func__, __LINE__, status);
+
+			/*
+			 * 1: pal  0: ntsc
+			 */
+			if ((status >> 2) & 0x01)
+				cvstd_sd = CVSTD_PAL;
+			else
+				cvstd_sd = CVSTD_NTSC;
+
+			SENSOR_DG("%s(%d): cvstd_sd is %s\n",
+				  __func__, __LINE__,
+				  (cvstd_sd == CVSTD_PAL) ? "PAL" : "NTSC");
+			cvstd_mode = cvstd_sd;
+		}
+		tp2825_reinit_parameter(ad, cvstd_mode);
+	} else if (state == VIDEO_LOCKED) {
+		goto check_end;
+	} else {
+		SENSOR_DG("%s: check sensor statue failed!\n", __func__);
+		goto check_end;
+	}
+
+	tp2825_reg_init(ad, cvstd_mode);
+check_end:
+	if (check_count && is_first && (state != VIDEO_LOCKED)) {
+		check_count--;
+		if (cvstd == CVSTD_SD)
+			mdelay(100);
+		else
+			mdelay(100);
+		goto check_continue;
+	}
+	is_first = false;
+	cvstd_state = state;
+
+	return 0;
+}
+int tp2825_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	char val;
+
+	if (enable)
+		val = 0x03; //stream on
+	else
+		val = 0x00; //stream off
+	SENSOR_DG("stream write 0x%x to reg 0x4D\n", val);
+	vehicle_generic_sensor_write(ad, 0x4D, &val);
+
+	return 0;
+}
+static void power_on(struct vehicle_ad_dev *ad)
+{
+	/* gpio_direction_output(ad->power, ad->pwr_active); */
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "ad_powerdown");
+		gpio_direction_output(ad->powerdown, !ad->pwdn_active);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "ad_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+}
+
+static void power_off(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static void tp2825_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = tp2825_g_addev;
+
+	if (ad->cif_error_last_line > 0) {
+		tp2825_check_cvstd(ad, true);
+		ad->cif_error_last_line = 0;
+	} else {
+		tp2825_check_cvstd(ad, false);
+	}
+
+	if (cvstd_old != cvstd_mode || cvstd_old_state != cvstd_state) {
+		cvstd_old = cvstd_mode;
+		cvstd_old_state = cvstd_state;
+		SENSOR_DG("ad signal change notify\n");
+		vehicle_ad_stat_change_notify();
+	}
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+}
+
+int tp2825_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = tp2825_g_addev;
+
+	if (!ad)
+		return -1;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+	if (ad->irq)
+		free_irq(ad->irq, ad);
+	power_off(ad);
+
+	return 0;
+}
+
+static int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+		mode = CVSTD_PAL;
+		break;
+	case AD_FIX_FORMAT_NTSC:
+		mode = CVSTD_NTSC;
+		break;
+	case AD_FIX_FORMAT_720P_50FPS:
+		mode = CVSTD_720P50;
+		break;
+	case AD_FIX_FORMAT_720P_30FPS:
+		mode = CVSTD_720P30;
+		break;
+	case AD_FIX_FORMAT_720P_25FPS:
+		mode = CVSTD_720P25;
+		break;
+	default:
+		mode = -1;
+		break;
+	}
+
+	return mode;
+}
+
+int tp2825_ad_init(struct vehicle_ad_dev *ad)
+{
+	int val = 0;
+	int i = 0;
+	int mode;
+
+	tp2825_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	/*  2. ad power on sequence */
+	power_on(ad);
+
+	while (++i < 5) {
+		usleep_range(1000, 1200);
+		val = vehicle_generic_sensor_read(ad, 0x12);
+		if (val != 0xff)
+			break;
+		SENSOR_DG("tp2825_init i2c_reg_read fail\n");
+	}
+
+	/* fix mode */
+	mode = get_ad_mode_from_fix_format(ad->fix_format);
+	if (mode > 0) {
+		SENSOR_DG("fix format %d, fix cvxtd mode %d\n", ad->fix_format, mode);
+		tp2825_reg_init(ad, mode);
+		tp2825_reinit_parameter(ad, mode);
+		SENSOR_DG("%s after init\n", __func__);
+		/* wait for signal locked; */
+		i = 0;
+		while (++i < 10) {
+			msleep(100);
+			val = vehicle_generic_sensor_read(ad, 0x01);
+			if ((FLAG_LOCKED == (val & FLAG_LOCKED)))
+				break;
+		}
+		cvstd_state = VIDEO_LOCKED;
+		return 0;
+	}
+
+	/*  3 .init default format params */
+	tp2825_reg_init(ad, cvstd_mode);
+	tp2825_reinit_parameter(ad, cvstd_mode);
+	SENSOR_DG("%s after reinit init\n", __func__);
+
+	/*  5. create workqueue to detect signal change */
+	INIT_DELAYED_WORK(&ad->state_check_work.work, tp2825_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-tp2825");
+
+	/* tp2825_check_cvstd(ad, true); */
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
+
+
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.h b/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.h
new file mode 100644
index 0000000000000..451912580de80
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_tp2825.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_TP2825_H__
+#define __VEHICLE_AD_TP2825_H__
+
+int tp2825_ad_init(struct vehicle_ad_dev *ad);
+int tp2825_ad_deinit(void);
+int tp2825_ad_get_cfg(struct vehicle_cfg **cfg);
+void tp2825_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int tp2825_check_id(struct vehicle_ad_dev *ad);
+int tp2825_stream(struct vehicle_ad_dev *ad, int enable);
+void tp2825_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.c b/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.c
new file mode 100644
index 0000000000000..8348a3c4772fa
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.c
@@ -0,0 +1,843 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * vehicle sensor tp2855
+ *
+ * Copyright (C) 2024 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *
+ *      Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/sysctl.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/proc_fs.h>
+#include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/uaccess.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include "vehicle_cfg.h"
+#include "vehicle_main.h"
+#include "vehicle_ad.h"
+#include "vehicle_ad_tp2855.h"
+
+enum {
+	CVSTD_720P60 = 0,
+	CVSTD_720P50,
+	CVSTD_1080P30,
+	CVSTD_1080P25,
+	CVSTD_720P30,
+	CVSTD_720P25,
+	CVSTD_SVGAP30,
+	CVSTD_SD,
+	CVSTD_NTSC,
+	CVSTD_PAL
+};
+
+enum {
+	FORCE_PAL_WIDTH = 960,
+	FORCE_PAL_HEIGHT = 576,
+	FORCE_NTSC_WIDTH = 960,
+	FORCE_NTSC_HEIGHT = 480,
+	FORCE_SVGA_WIDTH = 800,
+	FORCE_SVGA_HEIGHT = 600,
+	FORCE_720P_WIDTH = 1280,
+	FORCE_720P_HEIGHT = 720,
+	FORCE_1080P_WIDTH = 1920,
+	FORCE_1080P_HEIGHT = 1080,
+	FORCE_CIF_OUTPUT_FORMAT = CIF_OUTPUT_FORMAT_420,
+};
+
+enum {
+	VIDEO_UNPLUG,
+	VIDEO_IN,
+	VIDEO_LOCKED,
+	VIDEO_UNLOCK
+};
+
+#define TP2855_LINK_FREQ_297M			(297000000UL >> 1)
+#define TP2855_LINK_FREQ_594M			(594000000UL >> 1)
+#define TP2855_XVCLK_FREQ			27000000
+#define TP2855_TEST_PATTERN			0
+
+static struct vehicle_ad_dev *tp2855_g_addev;
+static int cvstd_mode = CVSTD_1080P25;
+static int cvstd_old = CVSTD_720P25;
+static int cvstd_state = VIDEO_UNPLUG;
+// static int cvstd_old_state = VIDEO_UNLOCK;
+
+static bool g_tp2855_streaming;
+
+enum tp2855_support_reso {
+	TP2855_CVSTD_720P_60 = 0,
+	TP2855_CVSTD_720P_50,
+	TP2855_CVSTD_1080P_30,
+	TP2855_CVSTD_1080P_25,
+	TP2855_CVSTD_720P_30,
+	TP2855_CVSTD_720P_25,
+	TP2855_CVSTD_SD,
+	TP2855_CVSTD_OTHER,
+};
+
+struct regval {
+	u8 addr;
+	u8 val;
+};
+
+static __maybe_unused const struct regval common_setting_297M_720p_25fps_regs[] = {
+	{0x02, 0x42},
+	{0x07, 0xc0},
+	{0x0b, 0xc0},
+	{0x0c, 0x13},
+	{0x0d, 0x50},
+	{0x15, 0x13},
+	{0x16, 0x15},
+	{0x17, 0x00},
+	{0x18, 0x19},
+	{0x19, 0xd0},
+	{0x1a, 0x25},
+	{0x1c, 0x07},
+	{0x1d, 0xbc},
+	{0x20, 0x30},
+	{0x21, 0x84},
+	{0x22, 0x36},
+	{0x23, 0x3c},
+	{0x2b, 0x60},
+	{0x2c, 0x0a},
+	{0x2d, 0x30},
+	{0x2e, 0x70},
+	{0x30, 0x48},
+	{0x31, 0xbb},
+	{0x32, 0x2e},
+	{0x33, 0x90},
+	{0x35, 0x25},
+	{0x38, 0x00},
+	{0x39, 0x18},
+
+	{0x02, 0x46},
+	{0x0d, 0x71},
+	{0x18, 0x1b},
+	{0x20, 0x40},
+	{0x21, 0x46},
+	{0x25, 0xfe},
+	{0x26, 0x01},
+	{0x2c, 0x3a},
+	{0x2d, 0x5a},
+	{0x2e, 0x40},
+	{0x30, 0x9e},
+	{0x31, 0x20},
+	{0x32, 0x10},
+	{0x33, 0x90},
+	// {0x23, 0x02}, //vi test ok
+	// {0x23, 0x00},
+};
+
+static __maybe_unused const struct regval mipi_setting_297M_4ch_4lane_regs[] = {
+	{0x40, 0x08},
+	{0x01, 0xf0},
+	{0x02, 0x01},
+	{0x08, 0x0f},
+	{0x20, 0x44},
+	{0x34, 0xe4},
+	{0x14, 0x44},
+	{0x15, 0x0d},
+	{0x25, 0x04},
+	{0x26, 0x03},
+	{0x27, 0x09},
+	{0x29, 0x02},
+	{0x33, 0x0f},
+	{0x33, 0x00},
+	{0x14, 0xc4},
+	{0x14, 0x44},
+};
+
+static __maybe_unused const struct regval mipi_setting_594M_4ch_4lane_regs[] = {
+	{0x40, 0x08},
+	{0x01, 0xf0},
+	{0x02, 0x01},
+	{0x08, 0x0f},
+	{0x20, 0x44},
+	{0x34, 0xe4},
+	{0x15, 0x0C},
+	{0x25, 0x08},
+	{0x26, 0x06},
+	{0x27, 0x11},
+	{0x29, 0x0a},
+	{0x33, 0x0f},
+	{0x33, 0x00},
+	{0x14, 0x33},
+	{0x14, 0xb3},
+	{0x14, 0x33},
+	// {0x23, 0x02}, //vi test ok
+	// {0x23, 0x00},
+};
+
+static __maybe_unused const struct regval common_setting_594M_1080p_25fps_regs[] = {
+	{0x02, 0x40},
+	{0x07, 0xc0},
+	{0x0b, 0xc0},
+	{0x0c, 0x03},
+	{0x0d, 0x50},
+	{0x15, 0x03},
+	{0x16, 0xd2},
+	{0x17, 0x80},
+	{0x18, 0x29},
+	{0x19, 0x38},
+	{0x1a, 0x47},
+	{0x1c, 0x0a},
+	{0x1d, 0x50},
+	{0x20, 0x30},
+	{0x21, 0x84},
+	{0x22, 0x36},
+	{0x23, 0x3c},
+	{0x2b, 0x60},
+	{0x2c, 0x0a},
+	{0x2d, 0x30},
+	{0x2e, 0x70},
+	{0x30, 0x48},
+	{0x31, 0xbb},
+	{0x32, 0x2e},
+	{0x33, 0x90},
+	{0x35, 0x05},
+	{0x38, 0x00},
+	{0x39, 0x1C},
+
+	{0x02, 0x44},
+	{0x0d, 0x73},
+	{0x15, 0x01},
+	{0x16, 0xf0},
+	{0x20, 0x3c},
+	{0x21, 0x46},
+	{0x25, 0xfe},
+	{0x26, 0x0d},
+	{0x2c, 0x3a},
+	{0x2d, 0x54},
+	{0x2e, 0x40},
+	{0x30, 0xa5},
+	{0x31, 0x86},
+	{0x32, 0xfb},
+	{0x33, 0x60},
+};
+
+static void tp2855_reinit_parameter(struct vehicle_ad_dev *ad, unsigned char cvstd)
+{
+	int i = 0;
+
+	switch (cvstd) {
+	case CVSTD_1080P25:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = TP2855_LINK_FREQ_594M;
+		break;
+	case CVSTD_720P25:
+		ad->cfg.width = 1280;
+		ad->cfg.height = 720;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = TP2855_LINK_FREQ_297M;
+		break;
+	default:
+		ad->cfg.width = 1920;
+		ad->cfg.height = 1080;
+		ad->cfg.start_x = 0;
+		ad->cfg.start_y = 0;
+		ad->cfg.input_format = CIF_INPUT_FORMAT_YUV;
+		ad->cfg.output_format = FORCE_CIF_OUTPUT_FORMAT;
+		ad->cfg.field_order = 0;
+		ad->cfg.yuv_order = 0;/*00 - UYVY*/
+		ad->cfg.href = 0;
+		ad->cfg.vsync = 0;
+		ad->cfg.frame_rate = 25;
+		ad->cfg.mipi_freq = TP2855_LINK_FREQ_594M;
+		break;
+	}
+	ad->cfg.type = V4L2_MBUS_CSI2_DPHY;
+	ad->cfg.mbus_flags = V4L2_MBUS_CSI2_4_LANE | V4L2_MBUS_CSI2_CONTINUOUS_CLOCK |
+			 V4L2_MBUS_CSI2_CHANNELS;
+	ad->cfg.mbus_code = MEDIA_BUS_FMT_UYVY8_2X8;
+
+	switch (ad->cfg.mbus_flags & V4L2_MBUS_CSI2_LANES) {
+	case V4L2_MBUS_CSI2_1_LANE:
+		ad->cfg.lanes = 1;
+		break;
+	case V4L2_MBUS_CSI2_2_LANE:
+		ad->cfg.lanes = 2;
+		break;
+	case V4L2_MBUS_CSI2_3_LANE:
+		ad->cfg.lanes = 3;
+		break;
+	case V4L2_MBUS_CSI2_4_LANE:
+		ad->cfg.lanes = 4;
+		break;
+	default:
+		ad->cfg.lanes = 1;
+		break;
+	}
+
+	/* fix crop info from dts config */
+	for (i = 0; i < 4; i++) {
+		if ((ad->defrects[i].width == ad->cfg.width) &&
+		    (ad->defrects[i].height == ad->cfg.height)) {
+			ad->cfg.start_x = ad->defrects[i].crop_x;
+			ad->cfg.start_y = ad->defrects[i].crop_y;
+			ad->cfg.width = ad->defrects[i].crop_width;
+			ad->cfg.height = ad->defrects[i].crop_height;
+		}
+	}
+}
+
+/* sensor register write */
+static int tp2855_write_reg(struct vehicle_ad_dev *ad, u8 reg, u8 val)
+{
+	struct i2c_msg msg;
+	u8 buf[2];
+	int ret;
+
+	buf[0] = reg & 0xFF;
+	buf[1] = val;
+
+	msg.addr = ad->i2c_add;
+	msg.flags = 0;
+	msg.buf = buf;
+	msg.len = sizeof(buf);
+
+	ret = i2c_transfer(ad->adapter, &msg, 1);
+	if (ret >= 0) {
+		usleep_range(300, 400);
+		return 0;
+	}
+
+	VEHICLE_DGERR("tp2855 write reg(0x%x val:0x%x) failed !\n", reg, val);
+
+	return ret;
+}
+
+static int tp2855_write_array(struct vehicle_ad_dev *ad,
+			       const struct regval *regs, int size)
+{
+	int i, ret = 0;
+
+	i = 0;
+	while (i < size) {
+		ret = tp2855_write_reg(ad, regs[i].addr, regs[i].val);
+		if (ret) {
+			VEHICLE_DGERR("%s failed !\n", __func__);
+			break;
+		}
+		i++;
+	}
+
+	return ret;
+}
+
+/* sensor register read */
+static int tp2855_read_reg(struct vehicle_ad_dev *ad, u8 reg, u8 *val)
+{
+	struct i2c_msg msg[2];
+	u8 buf[1];
+	int ret;
+
+	buf[0] = reg & 0xFF;
+
+	msg[0].addr = ad->i2c_add;
+	msg[0].flags = 0;
+	msg[0].buf = buf;
+	msg[0].len = sizeof(buf);
+
+	msg[1].addr = ad->i2c_add;
+	msg[1].flags = 0 | I2C_M_RD;
+	msg[1].buf = buf;
+	msg[1].len = 1;
+
+	ret = i2c_transfer(ad->adapter, msg, 2);
+	if (ret >= 0) {
+		*val = buf[0];
+		return 0;
+	}
+
+	VEHICLE_DGERR("tp2855 read reg(0x%x) failed !\n", reg);
+
+	return ret;
+}
+
+void tp2855_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	ad->ad_chl = channel;
+	VEHICLE_DG("%s, channel set(%d)", __func__, ad->ad_chl);
+}
+
+int tp2855_ad_get_cfg(struct vehicle_cfg **cfg)
+{
+	if (!tp2855_g_addev)
+		return -1;
+
+	switch (cvstd_state) {
+	case VIDEO_UNPLUG:
+		tp2855_g_addev->cfg.ad_ready = false;
+		break;
+	case VIDEO_LOCKED:
+		tp2855_g_addev->cfg.ad_ready = true;
+		break;
+	case VIDEO_IN:
+		tp2855_g_addev->cfg.ad_ready = false;
+		break;
+	}
+
+	tp2855_g_addev->cfg.ad_ready = true;
+
+	*cfg = &tp2855_g_addev->cfg;
+
+	return 0;
+}
+
+void tp2855_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	VEHICLE_INFO("%s, last_line %d\n", __func__, last_line);
+
+	if (last_line < 1)
+		return;
+
+	ad->cif_error_last_line = last_line;
+	if (cvstd_mode == CVSTD_PAL) {
+		if (last_line == FORCE_NTSC_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_NTSC) {
+		if (last_line == FORCE_PAL_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_1080P25) {
+		if (last_line == FORCE_1080P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	} else if (cvstd_mode == CVSTD_720P25) {
+		if (last_line == FORCE_720P_HEIGHT) {
+			if (ad->state_check_work.state_check_wq)
+				queue_delayed_work(
+					ad->state_check_work.state_check_wq,
+					&ad->state_check_work.work,
+					msecs_to_jiffies(0));
+		}
+	}
+}
+
+static int tp2855_set_channel_reso(struct vehicle_ad_dev *ad, int ch,
+						enum tp2855_support_reso reso)
+{
+	int val = reso;
+	u8 tmp = 0;
+	const unsigned char SYS_MODE[5] = { 0x01, 0x02, 0x04, 0x08, 0x0f };
+
+	tp2855_write_reg(ad, 0x40, ch);
+
+	switch (val) {
+	case TP2855_CVSTD_1080P_25:
+		VEHICLE_INFO("set channel %d 1080P_25\n", ch);
+		tp2855_read_reg(ad, 0xf5, &tmp);
+		tmp &= ~SYS_MODE[ch];
+		tp2855_write_reg(ad, 0xf5, tmp);
+		tp2855_write_array(ad, common_setting_594M_1080p_25fps_regs,
+				ARRAY_SIZE(common_setting_594M_1080p_25fps_regs));
+		break;
+	case TP2855_CVSTD_720P_25:
+		VEHICLE_INFO("set channel %d 720P_25\n", ch);
+		tp2855_read_reg(ad, 0xf5, &tmp);
+		tmp |= SYS_MODE[ch];
+		tp2855_write_reg(ad, 0xf5, tmp);
+		tp2855_write_array(ad, common_setting_297M_720p_25fps_regs,
+				ARRAY_SIZE(common_setting_297M_720p_25fps_regs));
+		break;
+	default:
+		VEHICLE_INFO(
+			"set channel %d is not supported, default 1080P_25\n", ch);
+		tp2855_read_reg(ad, 0xf5, &tmp);
+		tmp &= ~SYS_MODE[ch];
+		tp2855_write_reg(ad, 0xf5, tmp);
+		tp2855_write_array(ad, common_setting_594M_1080p_25fps_regs,
+				ARRAY_SIZE(common_setting_594M_1080p_25fps_regs));
+		break;
+	}
+
+	//set color bar
+	if (!ad->last_detect_status)
+		tp2855_write_reg(ad, 0x2a, 0x3c);
+	else
+		tp2855_write_reg(ad, 0x2a, 0x30);
+
+	return 0;
+}
+
+static int tp2855_get_channel_reso(struct vehicle_ad_dev *ad, int ch)
+{
+	u8 detect_fmt = 0xff;
+	u8 reso = 0xff;
+
+	tp2855_write_reg(ad, 0x40, ch);
+	tp2855_read_reg(ad, 0x03, &detect_fmt);
+	reso = detect_fmt & 0x7;
+
+	switch (reso) {
+	case TP2855_CVSTD_1080P_30:
+		VEHICLE_DG("detect channel %d 1080P_30\n", ch);
+		cvstd_mode = CVSTD_1080P30;
+		break;
+	case TP2855_CVSTD_1080P_25:
+		VEHICLE_DG("detect channel %d 1080P_25\n", ch);
+		cvstd_mode = CVSTD_1080P25;
+		break;
+	case TP2855_CVSTD_720P_30:
+		VEHICLE_DG("detect channel %d 720P_30\n", ch);
+		cvstd_mode = CVSTD_720P30;
+		break;
+	case TP2855_CVSTD_720P_25:
+		VEHICLE_DG("detect channel %d 720P_25\n", ch);
+		cvstd_mode = CVSTD_720P25;
+		break;
+	case TP2855_CVSTD_SD:
+		VEHICLE_DG("detect channel %d SD\n", ch);
+		cvstd_mode = CVSTD_SD;
+		break;
+	default:
+		VEHICLE_DG(
+			"detect channel %d is not supported, default 1080P_25\n", ch);
+		reso = TP2855_CVSTD_1080P_25;
+		cvstd_mode = CVSTD_1080P25;
+	}
+
+	return reso;
+}
+
+static __maybe_unused int auto_detect_channel_fmt(struct vehicle_ad_dev *ad)
+{
+	int ch = 0;
+	enum tp2855_support_reso reso = 0xff;
+
+	for (ch = 0; ch < 4; ch++) {
+		reso = tp2855_get_channel_reso(ad, ch);
+		tp2855_set_channel_reso(ad, ch, reso);
+	}
+
+	return 0;
+}
+
+int tp2855_check_id(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	u8 chip_id_h = 0, chip_id_l = 0;
+
+	tp2855_write_reg(ad, 0x40, 0x0);
+	ret = tp2855_read_reg(ad, 0xFE, &chip_id_h);
+	ret |= tp2855_read_reg(ad, 0xFF, &chip_id_l);
+	if (ret)
+		return ret;
+
+	if (chip_id_h != 0x28 || chip_id_l != 0x55) {
+		VEHICLE_DGERR("%s: expected 0x2855, detected: 0x%0x%0x !",
+			ad->ad_name, chip_id_h, chip_id_l);
+		ret = -EINVAL;
+	} else {
+		VEHICLE_INFO("%s Found TP2855 sensor: id(0x%x%x) !\n",
+						__func__, chip_id_h, chip_id_l);
+	}
+
+	return ret;
+}
+
+static int tp2855_get_channel_input_status(struct vehicle_ad_dev *ad, u8 ch)
+{
+	u8 val = 0;
+
+	tp2855_write_reg(ad, 0x40, ch);
+	tp2855_read_reg(ad, 0x01, &val);
+	VEHICLE_DG("input_status ch %d : %x\n", ch, val);
+
+	return (val & 0x80) ? 0 : 1;
+}
+
+static int tp2855_get_all_input_status(struct vehicle_ad_dev *ad, u8 *detect_status)
+{
+	u8 val = 0, i;
+
+	for (i = 0; i < 4; i++) {
+		tp2855_write_reg(ad, 0x40, i);
+		tp2855_read_reg(ad, 0x01, &val);
+		detect_status[i] = tp2855_get_channel_input_status(ad, i);
+	}
+
+	return 0;
+}
+
+static int tp2855_set_decoder_mode(struct vehicle_ad_dev *ad, int ch, int status)
+{
+	u8 val = 0;
+
+	tp2855_write_reg(ad, 0x40, ch);
+	tp2855_read_reg(ad, 0x26, &val);
+	if (status)
+		val |= 0x1;
+	else
+		val &= ~0x1;
+	tp2855_write_reg(ad, 0x26, val);
+
+	return 0;
+}
+
+static int tp2855_detect(struct vehicle_ad_dev *ad)
+{
+	int i;
+	u8 detect_status[4];
+
+	tp2855_get_all_input_status(ad, detect_status);
+	for (i = 0; i < 4; i++)
+		tp2855_set_decoder_mode(ad, i, detect_status[i]);
+
+	return 0;
+}
+
+static int __tp2855_start_stream(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	int array_size = 0;
+
+	auto_detect_channel_fmt(ad);
+	switch (cvstd_mode) {
+	case CVSTD_1080P25:
+		array_size = ARRAY_SIZE(mipi_setting_594M_4ch_4lane_regs);
+		ret = tp2855_write_array(ad, mipi_setting_594M_4ch_4lane_regs, array_size);
+		break;
+	case CVSTD_720P25:
+		array_size = ARRAY_SIZE(mipi_setting_297M_4ch_4lane_regs);
+		ret = tp2855_write_array(ad, mipi_setting_297M_4ch_4lane_regs, array_size);
+		break;
+	}
+	if (ret) {
+		VEHICLE_INFO(" tp2855 start stream: wrote mipi reg failed");
+		return ret;
+	}
+	tp2855_detect(ad);
+
+	msleep(50);
+
+	return 0;
+}
+
+static int __tp2855_stop_stream(struct vehicle_ad_dev *ad)
+{
+	return 0;
+}
+
+static int tp2855_check_cvstd(struct vehicle_ad_dev *ad, bool activate_check)
+{
+	tp2855_get_channel_reso(ad, ad->ad_chl);
+
+	return 0;
+}
+
+int tp2855_stream(struct vehicle_ad_dev *ad, int enable)
+{
+	VEHICLE_INFO("%s on(%d)\n", __func__, enable);
+
+	g_tp2855_streaming = (enable != 0);
+	if (g_tp2855_streaming) {
+		__tp2855_start_stream(ad);
+		if (ad->state_check_work.state_check_wq)
+			queue_delayed_work(ad->state_check_work.state_check_wq,
+				&ad->state_check_work.work, msecs_to_jiffies(200));
+	} else {
+		__tp2855_stop_stream(ad);
+		if (ad->state_check_work.state_check_wq)
+			cancel_delayed_work_sync(&ad->state_check_work.work);
+		VEHICLE_DG("%s(%d): cancel_queue_delayed_work!\n", __func__, __LINE__);
+	}
+
+	return 0;
+}
+
+static void tp2855_power_on(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->power)) {
+		gpio_request(ad->power, "tp2855_power");
+		gpio_direction_output(ad->power, ad->pwr_active);
+		/* gpio_set_value(ad->power, ad->pwr_active); */
+	}
+
+	if (gpio_is_valid(ad->powerdown)) {
+		gpio_request(ad->powerdown, "tp2855_pwd");
+		gpio_direction_output(ad->powerdown, 1);
+		/* gpio_set_value(ad->powerdown, !ad->pwdn_active); */
+	}
+
+	if (gpio_is_valid(ad->reset)) {
+		gpio_request(ad->reset, "tp2855_rst");
+		gpio_direction_output(ad->reset, 0);
+		usleep_range(1500, 2000);
+		gpio_direction_output(ad->reset, 1);
+	}
+	mdelay(100);
+}
+
+static void tp2855_power_off(struct vehicle_ad_dev *ad)
+{
+	if (gpio_is_valid(ad->reset))
+		gpio_free(ad->reset);
+	if (gpio_is_valid(ad->power))
+		gpio_free(ad->power);
+	if (gpio_is_valid(ad->powerdown))
+		gpio_free(ad->powerdown);
+}
+
+static __maybe_unused int tp2855_auto_detect_hotplug(struct vehicle_ad_dev *ad)
+{
+	u8 detect_status = 0;
+
+	tp2855_write_reg(ad, 0x40, 0x04);
+	tp2855_read_reg(ad, 0x01, &detect_status);
+
+	ad->detect_status = (detect_status & 0x80) ? 0 : 1;
+	VEHICLE_DG("input_status %x\n", ad->detect_status);
+
+	return 0;
+}
+
+static void tp2855_check_state_work(struct work_struct *work)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = tp2855_g_addev;
+	tp2855_auto_detect_hotplug(ad);
+	tp2855_check_cvstd(ad, true);
+	if (ad->detect_status != ad->last_detect_status || cvstd_old != cvstd_mode) {
+		ad->last_detect_status = ad->detect_status;
+		cvstd_old = cvstd_mode;
+		vehicle_ad_stat_change_notify();
+	}
+
+	if (g_tp2855_streaming)
+		queue_delayed_work(ad->state_check_work.state_check_wq,
+				   &ad->state_check_work.work, msecs_to_jiffies(100));
+}
+
+int tp2855_ad_deinit(void)
+{
+	struct vehicle_ad_dev *ad;
+
+	ad = tp2855_g_addev;
+
+	if (!ad)
+		return -1;
+
+	if (ad->state_check_work.state_check_wq) {
+		cancel_delayed_work_sync(&ad->state_check_work.work);
+		flush_delayed_work(&ad->state_check_work.work);
+		flush_workqueue(ad->state_check_work.state_check_wq);
+		destroy_workqueue(ad->state_check_work.state_check_wq);
+	}
+
+	tp2855_power_off(ad);
+
+	return 0;
+}
+
+static __maybe_unused int get_ad_mode_from_fix_format(int fix_format)
+{
+	int mode = -1;
+
+	switch (fix_format) {
+	case AD_FIX_FORMAT_PAL:
+	case AD_FIX_FORMAT_NTSC:
+	case AD_FIX_FORMAT_720P_50FPS:
+	case AD_FIX_FORMAT_720P_30FPS:
+	case AD_FIX_FORMAT_720P_25FPS:
+		mode = CVSTD_720P25;
+		break;
+	case AD_FIX_FORMAT_1080P_30FPS:
+	case AD_FIX_FORMAT_1080P_25FPS:
+
+	default:
+		mode = CVSTD_720P25;
+		break;
+	}
+
+	return mode;
+}
+
+int tp2855_ad_init(struct vehicle_ad_dev *ad)
+{
+	int val;
+	int i = 0;
+
+	tp2855_g_addev = ad;
+
+	/*  1. i2c init */
+	while (ad->adapter == NULL) {
+		ad->adapter = i2c_get_adapter(ad->i2c_chl);
+		usleep_range(10000, 12000);
+	}
+	if (ad->adapter == NULL)
+		return -ENODEV;
+
+	if (!i2c_check_functionality(ad->adapter, I2C_FUNC_I2C))
+		return -EIO;
+
+	/*  2. ad power on sequence */
+	tp2855_power_on(ad);
+
+	while (++i < 5) {
+		usleep_range(1000, 1200);
+		val = vehicle_generic_sensor_read(ad, 0xf0);
+		if (val != 0xff)
+			break;
+		VEHICLE_INFO("tp2855_init i2c_reg_read fail\n");
+	}
+
+	tp2855_reinit_parameter(ad, cvstd_mode);
+	ad->last_detect_status = false;
+
+	/*  create workqueue to detect signal change */
+	INIT_DELAYED_WORK(&ad->state_check_work.work, tp2855_check_state_work);
+	ad->state_check_work.state_check_wq =
+		create_singlethread_workqueue("vehicle-ad-tp2855");
+
+	queue_delayed_work(ad->state_check_work.state_check_wq,
+			   &ad->state_check_work.work, msecs_to_jiffies(100));
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.h b/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.h
new file mode 100644
index 0000000000000..52332263d1cab
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_ad_tp2855.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2024 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_AD_TP2855_H__
+#define __VEHICLE_AD_TP2855_H__
+
+int tp2855_ad_init(struct vehicle_ad_dev *ad);
+int tp2855_ad_deinit(void);
+int tp2855_ad_get_cfg(struct vehicle_cfg **cfg);
+void tp2855_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line);
+int tp2855_check_id(struct vehicle_ad_dev *ad);
+int tp2855_stream(struct vehicle_ad_dev *ad, int enable);
+void tp2855_channel_set(struct vehicle_ad_dev *ad, int channel);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_cfg.h b/drivers/video/rockchip/vehicle/vehicle_cfg.h
new file mode 100644
index 0000000000000..7702edd187ec9
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_cfg.h
@@ -0,0 +1,168 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_CFG
+#define __VEHICLE_CFG
+#include <media/v4l2-mediabus.h>
+#include <linux/rk-camera-module.h>
+
+/* Driver information */
+#define VEHICLE_DRIVER_NAME		"Vehicle"
+
+static int vehicle_debug;
+#define VEHICLE_DG(format, ...) do {	\
+	if (vehicle_debug)	\
+		pr_info("%s(%d): " format, __func__, __LINE__, ## __VA_ARGS__);	\
+	} while (0)
+
+#define VEHICLE_DGERR(format, ...)  \
+	pr_info("%s %s(%d):" format, VEHICLE_DRIVER_NAME, __func__, __LINE__, ## __VA_ARGS__)
+#define VEHICLE_INFO(format, ...)  \
+	pr_info("%s %s(%d):" format, VEHICLE_DRIVER_NAME, __func__, __LINE__, ## __VA_ARGS__)
+
+#define MAX_BUF_NUM (6)
+
+#define CVBS_DOUBLE_FPS_MODE	/*PAL 50fps; NTSC 60fps*/
+
+enum {
+	CIF_INPUT_FORMAT_YUV = 0,
+	CIF_INPUT_FORMAT_PAL = 2,
+	CIF_INPUT_FORMAT_NTSC = 3,
+	CIF_INPUT_FORMAT_RAW = 4,
+	CIF_INPUT_FORMAT_JPEG = 5,
+	CIF_INPUT_FORMAT_MIPI = 6,
+	CIF_INPUT_FORMAT_PAL_SW_COMPOSITE = 0xff000000,
+	CIF_INPUT_FORMAT_NTSC_SW_COMPOSITE = 0xfe000000,
+};
+
+enum {
+	CIF_OUTPUT_FORMAT_422 = 0,
+	CIF_OUTPUT_FORMAT_420 = 1,
+};
+
+/* Serial flags */
+/* CSI-2 D-PHY number of data lanes. */
+#define V4L2_MBUS_CSI2_1_LANE			BIT(0)
+#define V4L2_MBUS_CSI2_2_LANE			BIT(1)
+#define V4L2_MBUS_CSI2_3_LANE			BIT(2)
+#define V4L2_MBUS_CSI2_4_LANE			BIT(3)
+/* CSI-2 Virtual Channel identifiers. */
+#define V4L2_MBUS_CSI2_CHANNEL_0		BIT(4)
+#define V4L2_MBUS_CSI2_CHANNEL_1		BIT(5)
+#define V4L2_MBUS_CSI2_CHANNEL_2		BIT(6)
+#define V4L2_MBUS_CSI2_CHANNEL_3		BIT(7)
+/* Clock non-continuous mode support. */
+#define V4L2_MBUS_CSI2_CONTINUOUS_CLOCK		BIT(8)
+
+#define V4L2_MBUS_CSI2_LANES		(V4L2_MBUS_CSI2_1_LANE | \
+					 V4L2_MBUS_CSI2_2_LANE | \
+					 V4L2_MBUS_CSI2_3_LANE | \
+					 V4L2_MBUS_CSI2_4_LANE)
+#define V4L2_MBUS_CSI2_CHANNELS		(V4L2_MBUS_CSI2_CHANNEL_0 | \
+					 V4L2_MBUS_CSI2_CHANNEL_1 | \
+					 V4L2_MBUS_CSI2_CHANNEL_2 | \
+					 V4L2_MBUS_CSI2_CHANNEL_3)
+
+struct vehicle_cfg {
+	/* output */
+	int width;
+	int height;
+	/* sensor output */
+	int src_width;
+	int src_height;
+	/*
+	 * action:	source video data input format.
+	 * 000 - YUV
+	 * 010 - PAL
+	 * 011 - NTSC
+	 * 100 - RAW
+	 * 101 - JPEG
+	 * 110 - MIPI
+	 */
+	int input_format;
+	/*
+	 * 0 - output is 422
+	 * 1 - output is 420
+	 */
+	int output_format;
+	/*
+	 * YUV input order
+	 * 00 - UYVY
+	 * 01 - YVYU
+	 * 10 - VYUY
+	 * 11 - YUYV
+	 */
+	int yuv_order;
+	/*
+	 * ccir input order
+	 * 0 : odd field first
+	 * 1 : even field first
+	 */
+	int field_order;
+
+	/*
+	 * BT.656 not use
+	 * BT.601 hsync polarity
+	 * val:
+	 * 0-low active
+	 * 1-high active
+	 */
+	int href;
+	/*
+	 * BT.656 not use
+	 * BT.601 hsync polarity
+	 * val :
+	 * 0-low active
+	 * 1-high active
+	 */
+	int vsync;
+
+	/*
+	 * enum v4l2_mbus_type - media bus type
+	 * @V4L2_MBUS_PARALLEL: parallel interface with hsync and vsync
+	 * @V4L2_MBUS_BT656:	parallel interface with embedded synchronisation, can
+	 *			also be used for BT.1120
+	 * @V4L2_MBUS_CSI1: MIPI CSI-1 serial interface
+	 * @V4L2_MBUS_CCP2: CCP2 (Compact Camera Port 2)
+	 * @V4L2_MBUS_CSI2: MIPI CSI-2 serial interface
+	 */
+	enum v4l2_mbus_type type;
+
+	/*
+	 * Signal polarity flags
+	 * Note: in BT.656 mode HSYNC, FIELD, and VSYNC are unused
+	 * V4L2_MBUS_[HV]SYNC* flags should be also used for specifying
+	 * configuration of hardware that uses [HV]REF signals
+	 */
+	unsigned int mbus_flags;
+
+	/*
+	 * Note: in BT.656/601 mode mipi_freq are unused
+	 * only used when v4l2_mbus_type is V4L2_MBUS_CSI2
+	 */
+	s64 mipi_freq;
+	/*
+	 * Note: in BT.656/601 mode mipi_freq are unused
+	 * only used when v4l2_mbus_type is V4L2_MBUS_CSI2
+	 */
+	int lanes;
+
+	u32 mbus_code;
+
+	int start_x;
+	int start_y;
+	int frame_rate;
+
+	unsigned int buf_phy_addr[MAX_BUF_NUM];
+	unsigned int buf_num;
+	int ad_ready;
+	/*0:no, 1:90; 2:180; 4:270; 0x10:mirror-y; 0x20:mirror-x*/
+	int rotate_mirror;
+	struct rkmodule_csi_dphy_param *dphy_param;
+	int drop_frames;
+};
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_cif.c b/drivers/video/rockchip/vehicle/vehicle_cif.c
new file mode 100644
index 0000000000000..85a441b1a6ba5
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_cif.c
@@ -0,0 +1,5665 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_cif.c
+ *
+ * mipi_dphy/csi_host/vicap driver for vehicle
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *	Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/platform_device.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/fb.h>
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/reset.h>
+
+#include "vehicle-csi2-dphy-common.h"
+#include "vehicle_cif.h"
+#include "vehicle_flinger.h"
+#include "vehicle_main.h"
+
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <media/v4l2-mediabus.h>
+#include <linux/delay.h>
+#include <linux/pm_runtime.h>
+#include <soc/rockchip/rockchip-system-status.h>
+#include <linux/phy/phy.h>
+#include <linux/uaccess.h>
+#include <linux/bits.h>
+#include "vehicle_samsung_dcphy_common.h"
+
+#define CIF_DG VEHICLE_DG
+#define CIF_ERR VEHICLE_DGERR
+
+static struct vehicle_cif *g_cif;
+
+#define write_reg(base, addr, val) \
+	writel(val, (addr) + (base))
+#define read_reg(base, addr) \
+	readl((addr) + (base))
+
+#define vehicle_write_csihost_reg(base, addr, val)  write_reg(base, addr, val)
+#define vehicle_read_csihost_reg(base, addr) read_reg(base, addr)
+
+//define cif clk and rst
+static const char * const rk3568_cif_clks[] = {
+	"aclk_cif",
+	"hclk_cif",
+	"dclk_cif",
+	"iclk_cif_g",
+};
+
+static const char * const rk3568_cif_rsts[] = {
+	"rst_cif_a",
+	"rst_cif_h",
+	"rst_cif_d",
+	"rst_cif_p",
+	"rst_cif_i",
+};
+
+static const char * const rk3588_cif_clks[] = {
+	"aclk_cif",
+	"hclk_cif",
+	"dclk_cif",
+};
+
+static const char * const rk3588_cif_rsts[] = {
+	"rst_cif_a",
+	"rst_cif_h",
+	"rst_cif_d",
+};
+
+static const char * const rk3562_cif_clks[] = {
+	"aclk_cif",
+	"hclk_cif",
+	"dclk_cif",
+	"csirx0_data",
+	"csirx1_data",
+	"csirx2_data",
+	"csirx3_data",
+};
+
+static const char * const rk3562_cif_rsts[] = {
+	"rst_cif_a",
+	"rst_cif_h",
+	"rst_cif_d",
+	"rst_cif_i0",
+	"rst_cif_i1",
+	"rst_cif_i2",
+	"rst_cif_i3",
+};
+
+static const char * const rk3576_cif_clks[] = {
+	"aclk_cif",
+	"hclk_cif",
+	"dclk_cif",
+	"i0clk_cif",
+	"i1clk_cif",
+	"i2clk_cif",
+	"i3clk_cif",
+	"i4clk_cif",
+};
+
+static const char * const rk3576_cif_rsts[] = {
+	"rst_cif_a",
+	"rst_cif_h",
+	"rst_cif_d",
+	"rst_cif_iclk0",
+	"rst_cif_iclk1",
+	"rst_cif_iclk2",
+	"rst_cif_iclk3",
+	"rst_cif_iclk4",
+};
+
+//define dphy and csi clks/rst
+static struct clk_bulk_data rk3568_csi2_dphy_hw_clks[] = {
+	{ .id = "pclk" },
+};
+
+static struct clk_bulk_data rk3568_csi2_clks[] = {
+	{ .id = "pclk_csi2host" },
+};
+
+static const char * const rk3568_csi2_rsts[] = {
+	"srst_csihost_p",
+};
+
+static struct clk_bulk_data rk3588_csi2_dphy_hw_clks[] = {
+	{ .id = "pclk" },
+};
+
+static const char * const rk3588_csi2_dphy_hw_rsts[] = {
+	"srst_csiphy",
+	"srst_p_csiphy",
+};
+
+static struct clk_bulk_data rk3588_csi2_clks[] = {
+	{ .id = "pclk_csi2host" },
+};
+
+static struct clk_bulk_data rk3588_csi2_dcphy_clks[] = {
+	{ .id = "pclk_csi2host" },
+	{ .id = "iclk_csi2host" },
+};
+
+static const char * const rk3588_csi2_rsts[] = {
+	"srst_csihost_p",
+	"srst_csihost_vicap",
+};
+
+static struct clk_bulk_data rk3562_csi2_dphy_hw_clks[] = {
+	{ .id = "pclk" },
+};
+
+static const char * const rk3562_csi2_dphy_hw_rsts[] = {
+	"srst_p_csiphy",
+};
+
+static struct clk_bulk_data rk3562_csi2_clks[] = {
+	{ .id = "pclk_csi2host" },
+};
+
+static const char * const rk3562_csi2_rsts[] = {
+	"srst_csihost_p",
+};
+
+static struct clk_bulk_data rk3576_csi2_dphy_hw_clks[] = {
+	{ .id = "pclk" },
+};
+
+static const char * const rk3576_csi2_dphy_hw_rsts[] = {
+	"srst_p_csiphy",
+};
+
+static struct clk_bulk_data rk3576_csi2_clks[] = {
+	{ .id = "pclk_csi2host" },
+};
+
+static struct clk_bulk_data rk3576_csi2_dcphy_clks[] = {
+	{ .id = "pclk_csi2host" },
+	{ .id = "iclk_csi2host" },
+};
+
+static const char * const rk3576_csi2_rsts[] = {
+	"srst_csihost_p",
+};
+
+//define cif regs
+static const struct vehicle_cif_reg rk3568_cif_regs[] = {
+	[CIF_REG_DVP_CTRL] = CIF_REG_NAME(CIF_CTRL, "CIF_REG_DVP_CTRL"),
+	[CIF_REG_DVP_INTEN] = CIF_REG_NAME(CIF_INTEN, "CIF_REG_DVP_INTEN"),
+	[CIF_REG_DVP_INTSTAT] = CIF_REG_NAME(CIF_INTSTAT, "CIF_REG_DVP_INTSTAT"),
+	[CIF_REG_DVP_FOR] = CIF_REG_NAME(CIF_FOR, "CIF_REG_DVP_FOR"),
+	[CIF_REG_DVP_MULTI_ID] = CIF_REG_NAME(CIF_MULTI_ID, "CIF_REG_DVP_MULTI_ID"),
+	[CIF_REG_DVP_FRM0_ADDR_Y] = CIF_REG_NAME(CIF_FRM0_ADDR_Y, "CIF_REG_DVP_FRM0_ADDR_Y"),
+	[CIF_REG_DVP_FRM0_ADDR_UV] = CIF_REG_NAME(CIF_FRM0_ADDR_UV, "CIF_REG_DVP_FRM0_ADDR_UV"),
+	[CIF_REG_DVP_FRM1_ADDR_Y] = CIF_REG_NAME(CIF_FRM1_ADDR_Y, "CIF_REG_DVP_FRM1_ADDR_Y"),
+	[CIF_REG_DVP_FRM1_ADDR_UV] = CIF_REG_NAME(CIF_FRM1_ADDR_UV, "CIF_REG_DVP_FRM1_ADDR_UV"),
+	[CIF_REG_DVP_VIR_LINE_WIDTH] = CIF_REG_NAME(CIF_VIR_LINE_WIDTH,
+						"CIF_REG_DVP_VIR_LINE_WIDTH"),
+	[CIF_REG_DVP_SET_SIZE] = CIF_REG_NAME(CIF_SET_SIZE, "CIF_REG_DVP_SET_SIZE"),
+	[CIF_REG_DVP_LINE_INT_NUM] = CIF_REG_NAME(CIF_LINE_INT_NUM, "CIF_REG_DVP_LINE_INT_NUM"),
+	[CIF_REG_DVP_LINE_CNT] = CIF_REG_NAME(CIF_LINE_CNT, "CIF_REG_DVP_LINE_CNT"),
+	[CIF_REG_DVP_CROP] = CIF_REG_NAME(RV1126_CIF_CROP, "CIF_REG_DVP_CROP"),
+	[CIF_REG_DVP_FIFO_ENTRY] = CIF_REG_NAME(RK3568_CIF_FIFO_ENTRY, "CIF_REG_DVP_FIFO_ENTRY"),
+	[CIF_REG_DVP_FRAME_STATUS] = CIF_REG_NAME(RV1126_CIF_FRAME_STATUS,
+						"CIF_REG_DVP_FRAME_STATUS"),
+	[CIF_REG_DVP_CUR_DST] = CIF_REG_NAME(RV1126_CIF_CUR_DST, "CIF_REG_DVP_CUR_DST"),
+	[CIF_REG_DVP_LAST_LINE] = CIF_REG_NAME(RV1126_CIF_LAST_LINE, "CIF_REG_DVP_LAST_LINE"),
+	[CIF_REG_DVP_LAST_PIX] = CIF_REG_NAME(RV1126_CIF_LAST_PIX, "CIF_REG_DVP_LAST_PIX"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID1] = CIF_REG_NAME(CIF_FRM0_ADDR_Y_ID1,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID1"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID1] = CIF_REG_NAME(CIF_FRM0_ADDR_UV_ID1,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID1"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID1] = CIF_REG_NAME(CIF_FRM1_ADDR_Y_ID1,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID1"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID1] = CIF_REG_NAME(CIF_FRM1_ADDR_UV_ID1,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID1"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID2] = CIF_REG_NAME(CIF_FRM0_ADDR_Y_ID2,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID2"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID2] = CIF_REG_NAME(CIF_FRM0_ADDR_UV_ID2,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID2"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID2] = CIF_REG_NAME(CIF_FRM1_ADDR_Y_ID2,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID2"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID2] = CIF_REG_NAME(CIF_FRM1_ADDR_UV_ID2,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID2"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID3] = CIF_REG_NAME(CIF_FRM0_ADDR_Y_ID3,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID3"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID3] = CIF_REG_NAME(CIF_FRM0_ADDR_UV_ID3,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID3"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID3] = CIF_REG_NAME(CIF_FRM1_ADDR_Y_ID3,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID3"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID3] = CIF_REG_NAME(CIF_FRM1_ADDR_UV_ID3,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL0] = CIF_REG_NAME(CIF_CSI_ID0_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL1] = CIF_REG_NAME(CIF_CSI_ID0_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL0] = CIF_REG_NAME(CIF_CSI_ID1_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL1] = CIF_REG_NAME(CIF_CSI_ID1_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL0] = CIF_REG_NAME(CIF_CSI_ID2_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL1] = CIF_REG_NAME(CIF_CSI_ID2_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL0] = CIF_REG_NAME(CIF_CSI_ID3_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL1] = CIF_REG_NAME(CIF_CSI_ID3_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL1"),
+	[CIF_REG_MIPI_LVDS_CTRL] = CIF_REG_NAME(CIF_CSI_MIPI_LVDS_CTRL,
+						"CIF_REG_MIPI_LVDS_CTRL"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID1] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID1] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID1] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID2] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID2] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID2] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3] = CIF_REG_NAME(CIF_CSI_FRM0_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3] = CIF_REG_NAME(CIF_CSI_FRM1_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID3] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID3] = CIF_REG_NAME(CIF_CSI_FRM0_VLW_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID3] = CIF_REG_NAME(CIF_CSI_FRM1_VLW_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_INTEN] = CIF_REG_NAME(CIF_CSI_INTEN, "CIF_REG_MIPI_LVDS_INTEN"),
+	[CIF_REG_MIPI_LVDS_INTSTAT] = CIF_REG_NAME(CIF_CSI_INTSTAT, "CIF_REG_MIPI_LVDS_INTSTAT"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1] = CIF_REG_NAME(CIF_CSI_LINE_INT_NUM_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3] = CIF_REG_NAME(CIF_CSI_LINE_INT_NUM_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1] = CIF_REG_NAME(CIF_CSI_LINE_CNT_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3] = CIF_REG_NAME(CIF_CSI_LINE_CNT_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3"),
+	[CIF_REG_MIPI_LVDS_ID0_CROP_START] = CIF_REG_NAME(CIF_CSI_ID0_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID0_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID1_CROP_START] = CIF_REG_NAME(CIF_CSI_ID1_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID1_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID2_CROP_START] = CIF_REG_NAME(CIF_CSI_ID2_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID2_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID3_CROP_START] = CIF_REG_NAME(CIF_CSI_ID3_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID3_CROP_START"),
+	[CIF_REG_MIPI_FRAME_NUM_VC0] = CIF_REG_NAME(CIF_CSI_FRAME_NUM_VC0,
+						"CIF_REG_MIPI_FRAME_NUM_VC0"),
+	[CIF_REG_MIPI_FRAME_NUM_VC1] = CIF_REG_NAME(CIF_CSI_FRAME_NUM_VC1,
+						"CIF_REG_MIPI_FRAME_NUM_VC1"),
+	[CIF_REG_MIPI_FRAME_NUM_VC2] = CIF_REG_NAME(CIF_CSI_FRAME_NUM_VC2,
+						"CIF_REG_MIPI_FRAME_NUM_VC2"),
+	[CIF_REG_MIPI_FRAME_NUM_VC3] = CIF_REG_NAME(CIF_CSI_FRAME_NUM_VC3,
+						"CIF_REG_MIPI_FRAME_NUM_VC3"),
+	[CIF_REG_Y_STAT_CONTROL] = CIF_REG_NAME(CIF_Y_STAT_CONTROL,
+						"CIF_REG_Y_STAT_CONTROL"),
+	[CIF_REG_Y_STAT_VALUE] = CIF_REG_NAME(CIF_Y_STAT_VALUE, "CIF_REG_Y_STAT_VALUE"),
+	[CIF_REG_MMU_DTE_ADDR] = CIF_REG_NAME(CIF_MMU_DTE_ADDR, "CIF_REG_MMU_DTE_ADDR"),
+	[CIF_REG_MMU_STATUS] = CIF_REG_NAME(CIF_MMU_STATUS, "CIF_REG_MMU_STATUS"),
+	[CIF_REG_MMU_COMMAND] = CIF_REG_NAME(CIF_MMU_COMMAND, "CIF_REG_MMU_COMMAND"),
+	[CIF_REG_MMU_PAGE_FAULT_ADDR] = CIF_REG_NAME(CIF_MMU_PAGE_FAULT_ADDR,
+						"CIF_REG_MMU_PAGE_FAULT_ADDR"),
+	[CIF_REG_MMU_ZAP_ONE_LINE] = CIF_REG_NAME(CIF_MMU_ZAP_ONE_LINE, "CIF_REG_MMU_ZAP_ONE_LINE"),
+	[CIF_REG_MMU_INT_RAWSTAT] = CIF_REG_NAME(CIF_MMU_INT_RAWSTAT, "CIF_REG_MMU_INT_RAWSTAT"),
+	[CIF_REG_MMU_INT_CLEAR] = CIF_REG_NAME(CIF_MMU_INT_CLEAR, "CIF_REG_MMU_INT_CLEAR"),
+	[CIF_REG_MMU_INT_MASK] = CIF_REG_NAME(CIF_MMU_INT_MASK, "CIF_REG_MMU_INT_MASK"),
+	[CIF_REG_MMU_INT_STATUS] = CIF_REG_NAME(CIF_MMU_INT_STATUS, "CIF_REG_MMU_INT_STATUS"),
+	[CIF_REG_MMU_AUTO_GATING] = CIF_REG_NAME(CIF_MMU_AUTO_GATING, "CIF_REG_MMU_AUTO_GATING"),
+	[CIF_REG_GRF_CIFIO_CON] = CIF_REG_NAME(CIF_GRF_VI_CON0, "CIF_REG_GRF_CIFIO_CON"),
+	[CIF_REG_GRF_CIFIO_CON1] = CIF_REG_NAME(CIF_GRF_VI_CON1, "CIF_REG_GRF_CIFIO_CON1"),
+};
+
+static const struct vehicle_cif_reg rk3588_cif_regs[] = {
+	[CIF_REG_DVP_CTRL] = CIF_REG_NAME(DVP_CTRL, "CIF_REG_DVP_CTRL"),
+	[CIF_REG_DVP_INTEN] = CIF_REG_NAME(DVP_INTEN, "CIF_REG_DVP_INTEN"),
+	[CIF_REG_DVP_INTSTAT] = CIF_REG_NAME(DVP_INTSTAT, "CIF_REG_DVP_INTSTAT"),
+	[CIF_REG_DVP_FOR] = CIF_REG_NAME(DVP_FOR, "CIF_REG_DVP_FOR"),
+	[CIF_REG_DVP_MULTI_ID] = CIF_REG_NAME(DVP_MULTI_ID, "CIF_REG_DVP_MULTI_ID"),
+	[CIF_REG_DVP_SAV_EAV] = CIF_REG_NAME(DVP_SAV_EAV, "CIF_REG_DVP_SAV_EAV"),
+	[CIF_REG_DVP_FRM0_ADDR_Y] = CIF_REG_NAME(DVP_FRM0_ADDR_Y_ID0, "CIF_REG_DVP_FRM0_ADDR_Y"),
+	[CIF_REG_DVP_FRM0_ADDR_UV] = CIF_REG_NAME(DVP_FRM0_ADDR_UV_ID0, "CIF_REG_DVP_FRM0_ADDR_UV"),
+	[CIF_REG_DVP_FRM1_ADDR_Y] = CIF_REG_NAME(DVP_FRM1_ADDR_Y_ID0, "CIF_REG_DVP_FRM1_ADDR_Y"),
+	[CIF_REG_DVP_FRM1_ADDR_UV] = CIF_REG_NAME(DVP_FRM1_ADDR_UV_ID0, "CIF_REG_DVP_FRM1_ADDR_UV"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID1] = CIF_REG_NAME(DVP_FRM0_ADDR_Y_ID1,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID1"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID1] = CIF_REG_NAME(DVP_FRM0_ADDR_UV_ID1,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID1"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID1] = CIF_REG_NAME(DVP_FRM1_ADDR_Y_ID1,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID1"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID1] = CIF_REG_NAME(DVP_FRM1_ADDR_UV_ID1,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID1"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID2] = CIF_REG_NAME(DVP_FRM0_ADDR_Y_ID2,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID2"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID2] = CIF_REG_NAME(DVP_FRM0_ADDR_UV_ID2,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID2"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID2] = CIF_REG_NAME(DVP_FRM1_ADDR_Y_ID2,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID2"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID2] = CIF_REG_NAME(DVP_FRM1_ADDR_UV_ID2,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID2"),
+	[CIF_REG_DVP_FRM0_ADDR_Y_ID3] = CIF_REG_NAME(DVP_FRM0_ADDR_Y_ID3,
+						"CIF_REG_DVP_FRM0_ADDR_Y_ID3"),
+	[CIF_REG_DVP_FRM0_ADDR_UV_ID3] = CIF_REG_NAME(DVP_FRM0_ADDR_UV_ID3,
+						"CIF_REG_DVP_FRM0_ADDR_UV_ID3"),
+	[CIF_REG_DVP_FRM1_ADDR_Y_ID3] = CIF_REG_NAME(DVP_FRM1_ADDR_Y_ID3,
+						"CIF_REG_DVP_FRM1_ADDR_Y_ID3"),
+	[CIF_REG_DVP_FRM1_ADDR_UV_ID3] = CIF_REG_NAME(DVP_FRM1_ADDR_UV_ID3,
+						"CIF_REG_DVP_FRM1_ADDR_UV_ID3"),
+	[CIF_REG_DVP_VIR_LINE_WIDTH] = CIF_REG_NAME(DVP_VIR_LINE_WIDTH,
+						"CIF_REG_DVP_VIR_LINE_WIDTH"),
+	[CIF_REG_DVP_SET_SIZE] = CIF_REG_NAME(DVP_CROP_SIZE, "CIF_REG_DVP_SET_SIZE"),
+	[CIF_REG_DVP_CROP] = CIF_REG_NAME(DVP_CROP, "CIF_REG_DVP_CROP"),
+	[CIF_REG_DVP_LINE_INT_NUM] = CIF_REG_NAME(DVP_LINE_INT_NUM_01, "CIF_REG_DVP_LINE_INT_NUM"),
+	[CIF_REG_DVP_LINE_INT_NUM1] = CIF_REG_NAME(DVP_LINE_INT_NUM_23,
+						"CIF_REG_DVP_LINE_INT_NUM1"),
+	[CIF_REG_DVP_LINE_CNT] = CIF_REG_NAME(DVP_LINE_INT_NUM_01, "CIF_REG_DVP_LINE_CNT"),
+	[CIF_REG_DVP_LINE_CNT1] = CIF_REG_NAME(DVP_LINE_INT_NUM_23, "CIF_REG_DVP_LINE_CNT1"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL1"),
+	[CIF_REG_MIPI_LVDS_CTRL] = CIF_REG_NAME(CSI_MIPI0_CTRL, "CIF_REG_MIPI_LVDS_CTRL"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_VLW_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_VLW_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_VLW_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_VLW_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_INTEN] = CIF_REG_NAME(CSI_MIPI0_INTEN, "CIF_REG_MIPI_LVDS_INTEN"),
+	[CIF_REG_MIPI_LVDS_INTSTAT] = CIF_REG_NAME(CSI_MIPI0_INTSTAT, "CIF_REG_MIPI_LVDS_INTSTAT"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3"),
+	[CIF_REG_MIPI_LVDS_ID0_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID0_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID0_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID1_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID1_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID1_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID2_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID2_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID2_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID3_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID3_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID3_CROP_START"),
+	[CIF_REG_MIPI_FRAME_NUM_VC0] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC0,
+						"CIF_REG_MIPI_FRAME_NUM_VC0"),
+	[CIF_REG_MIPI_FRAME_NUM_VC1] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC1,
+						"CIF_REG_MIPI_FRAME_NUM_VC1"),
+	[CIF_REG_MIPI_FRAME_NUM_VC2] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC2,
+						"CIF_REG_MIPI_FRAME_NUM_VC2"),
+	[CIF_REG_MIPI_FRAME_NUM_VC3] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC3,
+						"CIF_REG_MIPI_FRAME_NUM_VC3"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID0] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID0,
+						"CIF_REG_MIPI_EFFECT_CODE_ID0"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID1] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID1,
+						"CIF_REG_MIPI_EFFECT_CODE_ID1"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID2] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID2,
+						"CIF_REG_MIPI_EFFECT_CODE_ID2"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID3] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID3,
+						"CIF_REG_MIPI_EFFECT_CODE_ID3"),
+	[CIF_REG_MIPI_ON_PAD] = CIF_REG_NAME(CSI_MIPI0_ON_PAD, "CIF_REG_MIPI_ON_PAD"),
+	[CIF_REG_GLB_CTRL] = CIF_REG_NAME(GLB_CTRL, "CIF_REG_GLB_CTRL"),
+	[CIF_REG_GLB_INTEN] = CIF_REG_NAME(GLB_INTEN, "CIF_REG_GLB_INTEN"),
+	[CIF_REG_GLB_INTST] = CIF_REG_NAME(GLB_INTST, "CIF_REG_GLB_INTST"),
+	[CIF_REG_SCL_CH_CTRL] = CIF_REG_NAME(SCL_CH_CTRL, "CIF_REG_SCL_CH_CTRL"),
+	[CIF_REG_SCL_CTRL] = CIF_REG_NAME(SCL_CTRL, "CIF_REG_SCL_CTRL"),
+	[CIF_REG_SCL_FRM0_ADDR_CH0] = CIF_REG_NAME(SCL_FRM0_ADDR_CH0,
+						"CIF_REG_SCL_FRM0_ADDR_CH0"),
+	[CIF_REG_SCL_FRM1_ADDR_CH0] = CIF_REG_NAME(SCL_FRM1_ADDR_CH0,
+						"CIF_REG_SCL_FRM1_ADDR_CH0"),
+	[CIF_REG_SCL_VLW_CH0] = CIF_REG_NAME(SCL_VLW_CH0, "CIF_REG_SCL_VLW_CH0"),
+	[CIF_REG_SCL_FRM0_ADDR_CH1] = CIF_REG_NAME(SCL_FRM0_ADDR_CH1,
+						"CIF_REG_SCL_FRM0_ADDR_CH1"),
+	[CIF_REG_SCL_FRM1_ADDR_CH1] = CIF_REG_NAME(SCL_FRM1_ADDR_CH1,
+						"CIF_REG_SCL_FRM1_ADDR_CH1"),
+	[CIF_REG_SCL_VLW_CH1] = CIF_REG_NAME(SCL_VLW_CH1, "CIF_REG_SCL_VLW_CH1"),
+	[CIF_REG_SCL_FRM0_ADDR_CH2] = CIF_REG_NAME(SCL_FRM0_ADDR_CH2,
+						"CIF_REG_SCL_FRM0_ADDR_CH2"),
+	[CIF_REG_SCL_FRM1_ADDR_CH2] = CIF_REG_NAME(SCL_FRM1_ADDR_CH2,
+						"CIF_REG_SCL_FRM1_ADDR_CH2"),
+	[CIF_REG_SCL_VLW_CH2] = CIF_REG_NAME(SCL_VLW_CH2, "CIF_REG_SCL_VLW_CH2"),
+	[CIF_REG_SCL_FRM0_ADDR_CH3] = CIF_REG_NAME(SCL_FRM0_ADDR_CH3, "CIF_REG_SCL_FRM0_ADDR_CH3"),
+	[CIF_REG_SCL_FRM1_ADDR_CH3] = CIF_REG_NAME(SCL_FRM1_ADDR_CH3, "CIF_REG_SCL_FRM1_ADDR_CH3"),
+	[CIF_REG_SCL_VLW_CH3] = CIF_REG_NAME(SCL_VLW_CH3, "CIF_REG_SCL_VLW_CH3"),
+	[CIF_REG_SCL_BLC_CH0] = CIF_REG_NAME(SCL_BLC_CH0, "CIF_REG_SCL_BLC_CH0"),
+	[CIF_REG_SCL_BLC_CH1] = CIF_REG_NAME(SCL_BLC_CH1, "CIF_REG_SCL_BLC_CH1"),
+	[CIF_REG_SCL_BLC_CH2] = CIF_REG_NAME(SCL_BLC_CH2, "CIF_REG_SCL_BLC_CH2"),
+	[CIF_REG_SCL_BLC_CH3] = CIF_REG_NAME(SCL_BLC_CH3, "CIF_REG_SCL_BLC_CH3"),
+	[CIF_REG_TOISP0_CTRL] = CIF_REG_NAME(TOISP0_CH_CTRL, "CIF_REG_TOISP0_CTRL"),
+	[CIF_REG_TOISP0_SIZE] = CIF_REG_NAME(TOISP0_CROP_SIZE, "CIF_REG_TOISP0_SIZE"),
+	[CIF_REG_TOISP0_CROP] = CIF_REG_NAME(TOISP0_CROP, "CIF_REG_TOISP0_CROP"),
+	[CIF_REG_TOISP1_CTRL] = CIF_REG_NAME(TOISP1_CH_CTRL, "CIF_REG_TOISP1_CTRL"),
+	[CIF_REG_TOISP1_SIZE] = CIF_REG_NAME(TOISP1_CROP_SIZE, "CIF_REG_TOISP1_SIZE"),
+	[CIF_REG_TOISP1_CROP] = CIF_REG_NAME(TOISP1_CROP, "CIF_REG_TOISP1_CROP"),
+	[CIF_REG_GRF_CIFIO_CON] = CIF_REG_NAME(CIF_GRF_SOC_CON2, "CIF_REG_GRF_CIFIO_CON"),
+};
+
+static const struct vehicle_cif_reg rk3562_cif_regs[] = {
+	[CIF_REG_MIPI_LVDS_ID0_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL1"),
+	[CIF_REG_MIPI_LVDS_CTRL] = CIF_REG_NAME(CSI_MIPI0_CTRL, "CIF_REG_MIPI_LVDS_CTRL"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_VLW_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_VLW_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_VLW_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_VLW_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_INTEN] = CIF_REG_NAME(CSI_MIPI0_INTEN, "CIF_REG_MIPI_LVDS_INTEN"),
+	[CIF_REG_MIPI_LVDS_INTSTAT] = CIF_REG_NAME(CSI_MIPI0_INTSTAT, "CIF_REG_MIPI_LVDS_INTSTAT"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID0_1,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID2_3,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3"),
+	[CIF_REG_MIPI_LVDS_ID0_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID0_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID0_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID1_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID1_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID1_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID2_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID2_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID2_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID3_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID3_CROP_START,
+						"CIF_REG_MIPI_LVDS_ID3_CROP_START"),
+	[CIF_REG_MIPI_FRAME_NUM_VC0] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC0,
+						"CIF_REG_MIPI_FRAME_NUM_VC0"),
+	[CIF_REG_MIPI_FRAME_NUM_VC1] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC1,
+						"CIF_REG_MIPI_FRAME_NUM_VC1"),
+	[CIF_REG_MIPI_FRAME_NUM_VC2] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC2,
+						"CIF_REG_MIPI_FRAME_NUM_VC2"),
+	[CIF_REG_MIPI_FRAME_NUM_VC3] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC3,
+						"CIF_REG_MIPI_FRAME_NUM_VC3"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID0] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID0,
+						"CIF_REG_MIPI_EFFECT_CODE_ID0"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID1] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID1,
+						"CIF_REG_MIPI_EFFECT_CODE_ID1"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID2] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID2,
+						"CIF_REG_MIPI_EFFECT_CODE_ID2"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID3] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID3,
+						"CIF_REG_MIPI_EFFECT_CODE_ID3"),
+	[CIF_REG_MIPI_ON_PAD] = CIF_REG_NAME(CSI_MIPI0_ON_PAD, "CIF_REG_MIPI_ON_PAD"),
+	[CIF_REG_GLB_CTRL] = CIF_REG_NAME(GLB_CTRL, "CIF_REG_GLB_CTRL"),
+	[CIF_REG_GLB_INTEN] = CIF_REG_NAME(GLB_INTEN, "CIF_REG_GLB_INTEN"),
+	[CIF_REG_GLB_INTST] = CIF_REG_NAME(GLB_INTST, "CIF_REG_GLB_INTST"),
+	[CIF_REG_SCL_CH_CTRL] = CIF_REG_NAME(SCL_CH_CTRL, "CIF_REG_SCL_CH_CTRL"),
+	[CIF_REG_SCL_CTRL] = CIF_REG_NAME(SCL_CTRL, "CIF_REG_SCL_CTRL"),
+	[CIF_REG_SCL_FRM0_ADDR_CH0] = CIF_REG_NAME(SCL_FRM0_ADDR_CH0,
+						"CIF_REG_SCL_FRM0_ADDR_CH0"),
+	[CIF_REG_SCL_FRM1_ADDR_CH0] = CIF_REG_NAME(SCL_FRM1_ADDR_CH0,
+						"CIF_REG_SCL_FRM1_ADDR_CH0"),
+	[CIF_REG_SCL_VLW_CH0] = CIF_REG_NAME(SCL_VLW_CH0, "CIF_REG_SCL_VLW_CH0"),
+	[CIF_REG_SCL_BLC_CH0] = CIF_REG_NAME(SCL_BLC_CH0, "CIF_REG_SCL_BLC_CH0"),
+	[CIF_REG_TOISP0_CTRL] = CIF_REG_NAME(TOISP0_CH_CTRL, "CIF_REG_TOISP0_CTRL"),
+	[CIF_REG_TOISP0_SIZE] = CIF_REG_NAME(TOISP0_CROP_SIZE, "CIF_REG_TOISP0_SIZE"),
+	[CIF_REG_TOISP0_CROP] = CIF_REG_NAME(TOISP0_CROP, "CIF_REG_TOISP0_CROP"),
+};
+
+static const struct vehicle_cif_reg rk3576_cif_regs[] = {
+	[CIF_REG_DVP_CTRL] = CIF_REG_NAME(DVP_CTRL, "CIF_REG_DVP_CTRL"),
+	[CIF_REG_DVP_INTEN] = CIF_REG_NAME(DVP_INTEN, "CIF_REG_DVP_INTEN"),
+	[CIF_REG_DVP_INTSTAT] = CIF_REG_NAME(DVP_INTSTAT, "CIF_REG_DVP_INTSTAT"),
+	[CIF_REG_DVP_FOR] = CIF_REG_NAME(DVP_FOR, "CIF_REG_DVP_FOR"),
+	[CIF_REG_DVP_SAV_EAV] = CIF_REG_NAME(DVP_SAV_EAV, "CIF_REG_DVP_SAV_EAV"),
+	[CIF_REG_DVP_FRM0_ADDR_Y] = CIF_REG_NAME(DVP_FRM0_ADDR_Y_ID0, "CIF_REG_DVP_FRM0_ADDR_Y"),
+	[CIF_REG_DVP_FRM0_ADDR_UV] = CIF_REG_NAME(DVP_FRM0_ADDR_UV_ID0, "CIF_REG_DVP_FRM0_ADDR_UV"),
+	[CIF_REG_DVP_FRM1_ADDR_Y] = CIF_REG_NAME(DVP_FRM1_ADDR_Y_ID0, "CIF_REG_DVP_FRM1_ADDR_Y"),
+	[CIF_REG_DVP_FRM1_ADDR_UV] = CIF_REG_NAME(DVP_FRM1_ADDR_UV_ID0, "CIF_REG_DVP_FRM1_ADDR_UV"),
+	[CIF_REG_DVP_VIR_LINE_WIDTH] = CIF_REG_NAME(DVP_VIR_LINE_WIDTH,
+						"CIF_REG_DVP_VIR_LINE_WIDTH"),
+	[CIF_REG_DVP_SET_SIZE] = CIF_REG_NAME(DVP_CROP_SIZE, "CIF_REG_DVP_SET_SIZE"),
+	[CIF_REG_DVP_CROP] = CIF_REG_NAME(DVP_CROP, "CIF_REG_DVP_CROP"),
+	[CIF_REG_DVP_LINE_INT_NUM] = CIF_REG_NAME(DVP_LINE_INT_NUM_01, "CIF_REG_DVP_LINE_INT_NUM"),
+	[CIF_REG_DVP_LINE_CNT] = CIF_REG_NAME(DVP_LINE_CNT_01, "CIF_REG_DVP_LINE_CNT"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID0_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID0_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID0_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID1_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID1_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID1_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID2_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID2_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID2_CTRL1"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL0] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL0,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL0"),
+	[CIF_REG_MIPI_LVDS_ID3_CTRL1] = CIF_REG_NAME(CSI_MIPI0_ID3_CTRL1,
+						"CIF_REG_MIPI_LVDS_ID3_CTRL1"),
+	[CIF_REG_MIPI_LVDS_CTRL] = CIF_REG_NAME(CSI_MIPI0_CTRL, "CIF_REG_MIPI_LVDS_CTRL"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0] = CIF_REG_NAME(CSI_MIPI0_VLW_ID0,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1] = CIF_REG_NAME(CSI_MIPI0_VLW_ID1,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2] = CIF_REG_NAME(CSI_MIPI0_VLW_ID2,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_Y_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM0_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3] = CIF_REG_NAME(CSI_MIPI0_FRM1_ADDR_UV_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3"),
+	[CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3] = CIF_REG_NAME(CSI_MIPI0_VLW_ID3,
+						"CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3"),
+	[CIF_REG_MIPI_LVDS_INTEN] = CIF_REG_NAME(CSI_MIPI0_INTEN, "CIF_REG_MIPI_LVDS_INTEN"),
+	[CIF_REG_MIPI_LVDS_INTSTAT] = CIF_REG_NAME(CSI_MIPI0_INTSTAT, "CIF_REG_MIPI_LVDS_INTSTAT"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID0_1_RK3576,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_INT_NUM_ID2_3_RK3576,
+						"CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID0_1_RK3576,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1"),
+	[CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3] = CIF_REG_NAME(CSI_MIPI0_LINE_CNT_ID2_3_RK3576,
+						"CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID2_3"),
+	[CIF_REG_MIPI_LVDS_ID0_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID0_CROP_START_RK3576,
+						"CIF_REG_MIPI_LVDS_ID0_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID1_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID1_CROP_START_RK3576,
+						"CIF_REG_MIPI_LVDS_ID1_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID2_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID2_CROP_START_RK3576,
+						"CIF_REG_MIPI_LVDS_ID2_CROP_START"),
+	[CIF_REG_MIPI_LVDS_ID3_CROP_START] = CIF_REG_NAME(CSI_MIPI0_ID3_CROP_START_RK3576,
+						"CIF_REG_MIPI_LVDS_ID3_CROP_START"),
+	[CIF_REG_MIPI_FRAME_NUM_VC0] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC0_RK3576,
+						"CIF_REG_MIPI_FRAME_NUM_VC0"),
+	[CIF_REG_MIPI_FRAME_NUM_VC1] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC1_RK3576,
+						"CIF_REG_MIPI_FRAME_NUM_VC1"),
+	[CIF_REG_MIPI_FRAME_NUM_VC2] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC2_RK3576,
+						"CIF_REG_MIPI_FRAME_NUM_VC2"),
+	[CIF_REG_MIPI_FRAME_NUM_VC3] = CIF_REG_NAME(CSI_MIPI0_FRAME_NUM_VC3_RK3576,
+						"CIF_REG_MIPI_FRAME_NUM_VC3"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID0] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID0_RK3576,
+						"CIF_REG_MIPI_EFFECT_CODE_ID0"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID1] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID1_RK3576,
+						"CIF_REG_MIPI_EFFECT_CODE_ID1"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID2] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID2_RK3576,
+						"CIF_REG_MIPI_EFFECT_CODE_ID2"),
+	[CIF_REG_MIPI_EFFECT_CODE_ID3] = CIF_REG_NAME(CSI_MIPI0_EFFECT_CODE_ID3_RK3576,
+						"CIF_REG_MIPI_EFFECT_CODE_ID3"),
+	[CIF_REG_MIPI_ON_PAD] = CIF_REG_NAME(CSI_MIPI0_ON_PAD_RK3576, "CIF_REG_MIPI_ON_PAD"),
+	[CIF_REG_MIPI_SET_SIZE_ID0] = CIF_REG_NAME(CSI_MIPI0_SET_FRAME_SIZE_ID0_RK3576,
+						"CIF_REG_MIPI_SET_SIZE_ID0"),
+	[CIF_REG_MIPI_SET_SIZE_ID1] = CIF_REG_NAME(CSI_MIPI0_SET_FRAME_SIZE_ID1_RK3576,
+						"CIF_REG_MIPI_SET_SIZE_ID1"),
+	[CIF_REG_MIPI_SET_SIZE_ID2] = CIF_REG_NAME(CSI_MIPI0_SET_FRAME_SIZE_ID2_RK3576,
+						"CIF_REG_MIPI_SET_SIZE_ID2"),
+	[CIF_REG_MIPI_SET_SIZE_ID3] = CIF_REG_NAME(CSI_MIPI0_SET_FRAME_SIZE_ID3_RK3576,
+						"CIF_REG_MIPI_SET_SIZE_ID3"),
+	[CIF_REG_GLB_CTRL] = CIF_REG_NAME(GLB_CTRL, "CIF_REG_GLB_CTRL"),
+	[CIF_REG_GLB_INTEN] = CIF_REG_NAME(GLB_INTEN, "CIF_REG_GLB_INTEN"),
+	[CIF_REG_GLB_INTST] = CIF_REG_NAME(GLB_INTST, "CIF_REG_GLB_INTST"),
+	[CIF_REG_SCL_CH_CTRL] = CIF_REG_NAME(SCL_CH_CTRL, "CIF_REG_SCL_CH_CTRL"),
+	[CIF_REG_SCL_CTRL] = CIF_REG_NAME(SCL_CTRL, "CIF_REG_SCL_CTRL"),
+	[CIF_REG_SCL_FRM0_ADDR_CH0] = CIF_REG_NAME(SCL_FRM0_ADDR_CH0, "CIF_REG_SCL_FRM0_ADDR_CH0"),
+	[CIF_REG_SCL_FRM1_ADDR_CH0] = CIF_REG_NAME(SCL_FRM1_ADDR_CH0, "CIF_REG_SCL_FRM1_ADDR_CH0"),
+	[CIF_REG_SCL_VLW_CH0] = CIF_REG_NAME(SCL_VLW_CH0, "CIF_REG_SCL_VLW_CH0"),
+	[CIF_REG_SCL_BLC_CH0] = CIF_REG_NAME(SCL_BLC_CH0, "CIF_REG_SCL_BLC_CH0"),
+	[CIF_REG_TOISP0_CTRL] = CIF_REG_NAME(TOISP0_CH_CTRL, "CIF_REG_TOISP0_CTRL"),
+	[CIF_REG_TOISP0_SIZE] = CIF_REG_NAME(TOISP0_CROP_SIZE, "CIF_REG_TOISP0_SIZE"),
+	[CIF_REG_TOISP0_CROP] = CIF_REG_NAME(TOISP0_CROP, "CIF_REG_TOISP0_CROP"),
+	[CIF_REG_GRF_CIFIO_CON] = CIF_REG_NAME(CIF_GRF_IOC_MISC_CON1_RK3576,
+							"CIF_REG_GRF_CIFIO_CON"),
+};
+
+//define dphy and csi regs
+static const struct grf_reg rk3568_grf_dphy_regs[] = {
+	[GRF_DPHY_CSI2PHY_FORCERXMODE] = GRF_REG(GRF_VI_CON0, 4, 0),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN] = GRF_REG(GRF_VI_CON0, 4, 4),
+	[GRF_DPHY_CSI2PHY_CLKLANE_EN] = GRF_REG(GRF_VI_CON0, 1, 8),
+	[GRF_DPHY_CLK_INV_SEL] = GRF_REG(GRF_VI_CON0, 1, 9),
+	[GRF_DPHY_CSI2PHY_CLKLANE1_EN] = GRF_REG(GRF_VI_CON0, 1, 10),
+	[GRF_DPHY_CLK1_INV_SEL] = GRF_REG(GRF_VI_CON0, 1, 11),
+	[GRF_DPHY_ISP_CSI2PHY_SEL] = GRF_REG(GRF_VI_CON1, 1, 12),
+	[GRF_DPHY_CIF_CSI2PHY_SEL] = GRF_REG(GRF_VI_CON1, 1, 11),
+	[GRF_DPHY_CSI2PHY_LANE_SEL] = GRF_REG(GRF_VI_CON1, 1, 7),
+};
+
+static const struct csi2dphy_reg rk3568_csi2dphy_regs[] = {
+	[CSI2PHY_REG_CTRL_LANE_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CTRL_LANE_ENABLE),
+	[CSI2PHY_DUAL_CLK_EN] = CSI2PHY_REG(CSI2_DPHY_DUAL_CAL_EN),
+	[CSI2PHY_CLK_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK_WR_THS_SETTLE),
+	[CSI2PHY_CLK_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK_CALIB_EN),
+	[CSI2PHY_LANE0_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_WR_THS_SETTLE),
+	[CSI2PHY_LANE0_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_CALIB_EN),
+	[CSI2PHY_LANE1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_WR_THS_SETTLE),
+	[CSI2PHY_LANE1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_CALIB_EN),
+	[CSI2PHY_LANE2_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_WR_THS_SETTLE),
+	[CSI2PHY_LANE2_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_CALIB_EN),
+	[CSI2PHY_LANE3_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_WR_THS_SETTLE),
+	[CSI2PHY_LANE3_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_CALIB_EN),
+	[CSI2PHY_CLK1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_WR_THS_SETTLE),
+	[CSI2PHY_CLK1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_CALIB_EN),
+};
+
+static const struct grf_reg rk3588_grf_dphy_regs[] = {
+	[GRF_DPHY_CSI2PHY_FORCERXMODE] = GRF_REG(GRF_DPHY_CON0, 4, 0),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN] = GRF_REG(GRF_DPHY_CON0, 4, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN0] = GRF_REG(GRF_DPHY_CON0, 2, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN1] = GRF_REG(GRF_DPHY_CON0, 2, 6),
+	[GRF_DPHY_CSI2PHY_CLKLANE_EN] = GRF_REG(GRF_DPHY_CON0, 1, 8),
+	[GRF_DPHY_CLK_INV_SEL] = GRF_REG(GRF_DPHY_CON0, 1, 9),
+	[GRF_DPHY_CSI2PHY_CLKLANE1_EN] = GRF_REG(GRF_DPHY_CON0, 1, 10),
+	[GRF_DPHY_CLK1_INV_SEL] = GRF_REG(GRF_DPHY_CON0, 1, 11),
+	[GRF_DPHY_CSI2PHY_LANE_SEL] = GRF_REG(GRF_SOC_CON2, 1, 6),
+	[GRF_DPHY_CSI2PHY1_LANE_SEL] = GRF_REG(GRF_SOC_CON2, 1, 7),
+	[GRF_DPHY_CSIHOST2_SEL] = GRF_REG(GRF_SOC_CON2, 1, 8),
+	[GRF_DPHY_CSIHOST3_SEL] = GRF_REG(GRF_SOC_CON2, 1, 9),
+	[GRF_DPHY_CSIHOST4_SEL] = GRF_REG(GRF_SOC_CON2, 1, 10),
+	[GRF_DPHY_CSIHOST5_SEL] = GRF_REG(GRF_SOC_CON2, 1, 11),
+};
+
+static const struct csi2dphy_reg rk3588_csi2dphy_regs[] = {
+	[CSI2PHY_REG_CTRL_LANE_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CTRL_LANE_ENABLE),
+	[CSI2PHY_DUAL_CLK_EN] = CSI2PHY_REG(CSI2_DPHY_DUAL_CAL_EN),
+	[CSI2PHY_CLK_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK_WR_THS_SETTLE),
+	[CSI2PHY_CLK_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK_CALIB_EN),
+	[CSI2PHY_LANE0_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_WR_THS_SETTLE),
+	[CSI2PHY_LANE0_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_CALIB_EN),
+	[CSI2PHY_LANE1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_WR_THS_SETTLE),
+	[CSI2PHY_LANE1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_CALIB_EN),
+	[CSI2PHY_LANE2_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_WR_THS_SETTLE),
+	[CSI2PHY_LANE2_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_CALIB_EN),
+	[CSI2PHY_LANE3_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_WR_THS_SETTLE),
+	[CSI2PHY_LANE3_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_CALIB_EN),
+	[CSI2PHY_CLK1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_WR_THS_SETTLE),
+	[CSI2PHY_CLK1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_CALIB_EN),
+	[CSI2PHY_CLK1_LANE_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_LANE_EN),
+};
+
+static const struct grf_reg rk3588_grf_dcphy_regs[] = {
+	[GRF_CPHY_MODE] = GRF_REG(GRF_DCPHY_CON0, 9, 0),
+};
+
+static const struct csi2dphy_reg rk3588_csi2dcphy_regs[] = {
+	[CSI2PHY_CLK_THS_SETTLE] = CSI2PHY_REG(CSI2_DCPHY_CLK_WR_THS_SETTLE),
+	[CSI2PHY_LANE0_THS_SETTLE] = CSI2PHY_REG(CSI2_DCPHY_LANE0_WR_THS_SETTLE),
+	[CSI2PHY_LANE0_ERR_SOT_SYNC] = CSI2PHY_REG(CSI2_DCPHY_LANE0_WR_ERR_SOT_SYNC),
+	[CSI2PHY_LANE1_THS_SETTLE] = CSI2PHY_REG(CSI2_DCPHY_LANE1_WR_THS_SETTLE),
+	[CSI2PHY_LANE1_ERR_SOT_SYNC] = CSI2PHY_REG(CSI2_DCPHY_LANE1_WR_ERR_SOT_SYNC),
+	[CSI2PHY_LANE2_THS_SETTLE] = CSI2PHY_REG(CSI2_DCPHY_LANE2_WR_THS_SETTLE),
+	[CSI2PHY_LANE2_ERR_SOT_SYNC] = CSI2PHY_REG(CSI2_DCPHY_LANE2_WR_ERR_SOT_SYNC),
+	[CSI2PHY_LANE3_THS_SETTLE] = CSI2PHY_REG(CSI2_DCPHY_LANE3_WR_THS_SETTLE),
+	[CSI2PHY_LANE3_ERR_SOT_SYNC] = CSI2PHY_REG(CSI2_DCPHY_LANE3_WR_ERR_SOT_SYNC),
+	[CSI2PHY_CLK_LANE_ENABLE] = CSI2PHY_REG(CSI2_DCPHY_CLK_LANE_ENABLE),
+	[CSI2PHY_DATA_LANE0_ENABLE] = CSI2PHY_REG(CSI2_DCPHY_DATA_LANE0_ENABLE),
+	[CSI2PHY_DATA_LANE1_ENABLE] = CSI2PHY_REG(CSI2_DCPHY_DATA_LANE1_ENABLE),
+	[CSI2PHY_DATA_LANE2_ENABLE] = CSI2PHY_REG(CSI2_DCPHY_DATA_LANE2_ENABLE),
+	[CSI2PHY_DATA_LANE3_ENABLE] = CSI2PHY_REG(CSI2_DCPHY_DATA_LANE3_ENABLE),
+	[CSI2PHY_S0C_GNR_CON1] = CSI2PHY_REG(CSI2_DCPHY_S0C_GNR_CON1),
+	[CSI2PHY_S0C_ANA_CON1] = CSI2PHY_REG(CSI2_DCPHY_S0C_ANA_CON1),
+	[CSI2PHY_S0C_ANA_CON2] = CSI2PHY_REG(CSI2_DCPHY_S0C_ANA_CON2),
+	[CSI2PHY_S0C_ANA_CON3] = CSI2PHY_REG(CSI2_DCPHY_S0C_ANA_CON3),
+	[CSI2PHY_COMBO_S0D0_GNR_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_GNR_CON1),
+	[CSI2PHY_COMBO_S0D0_ANA_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_ANA_CON1),
+	[CSI2PHY_COMBO_S0D0_ANA_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_ANA_CON2),
+	[CSI2PHY_COMBO_S0D0_ANA_CON3] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_ANA_CON3),
+	[CSI2PHY_COMBO_S0D0_ANA_CON6] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_ANA_CON6),
+	[CSI2PHY_COMBO_S0D0_ANA_CON7] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_ANA_CON7),
+	[CSI2PHY_COMBO_S0D0_DESKEW_CON0] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_DESKEW_CON0),
+	[CSI2PHY_COMBO_S0D0_DESKEW_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_DESKEW_CON2),
+	[CSI2PHY_COMBO_S0D0_DESKEW_CON4] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_DESKEW_CON4),
+	[CSI2PHY_COMBO_S0D0_CRC_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_CRC_CON1),
+	[CSI2PHY_COMBO_S0D0_CRC_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D0_CRC_CON2),
+	[CSI2PHY_COMBO_S0D1_GNR_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_GNR_CON1),
+	[CSI2PHY_COMBO_S0D1_ANA_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_ANA_CON1),
+	[CSI2PHY_COMBO_S0D1_ANA_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_ANA_CON2),
+	[CSI2PHY_COMBO_S0D1_ANA_CON3] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_ANA_CON3),
+	[CSI2PHY_COMBO_S0D1_ANA_CON6] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_ANA_CON6),
+	[CSI2PHY_COMBO_S0D1_ANA_CON7] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_ANA_CON7),
+	[CSI2PHY_COMBO_S0D1_DESKEW_CON0] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_DESKEW_CON0),
+	[CSI2PHY_COMBO_S0D1_DESKEW_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_DESKEW_CON2),
+	[CSI2PHY_COMBO_S0D1_DESKEW_CON4] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_DESKEW_CON4),
+	[CSI2PHY_COMBO_S0D1_CRC_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_CRC_CON1),
+	[CSI2PHY_COMBO_S0D1_CRC_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D1_CRC_CON2),
+	[CSI2PHY_COMBO_S0D2_GNR_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_GNR_CON1),
+	[CSI2PHY_COMBO_S0D2_ANA_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_ANA_CON1),
+	[CSI2PHY_COMBO_S0D2_ANA_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_ANA_CON2),
+	[CSI2PHY_COMBO_S0D2_ANA_CON3] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_ANA_CON3),
+	[CSI2PHY_COMBO_S0D2_ANA_CON6] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_ANA_CON6),
+	[CSI2PHY_COMBO_S0D2_ANA_CON7] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_ANA_CON7),
+	[CSI2PHY_COMBO_S0D2_DESKEW_CON0] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_DESKEW_CON0),
+	[CSI2PHY_COMBO_S0D2_DESKEW_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_DESKEW_CON2),
+	[CSI2PHY_COMBO_S0D2_DESKEW_CON4] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_DESKEW_CON4),
+	[CSI2PHY_COMBO_S0D2_CRC_CON1] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_CRC_CON1),
+	[CSI2PHY_COMBO_S0D2_CRC_CON2] = CSI2PHY_REG(CSI2_DCPHY_COMBO_S0D2_CRC_CON2),
+	[CSI2PHY_S0D3_GNR_CON1] = CSI2PHY_REG(CSI2_DCPHY_S0D3_GNR_CON1),
+	[CSI2PHY_S0D3_ANA_CON1] = CSI2PHY_REG(CSI2_DCPHY_S0D3_ANA_CON1),
+	[CSI2PHY_S0D3_ANA_CON2] = CSI2PHY_REG(CSI2_DCPHY_S0D3_ANA_CON2),
+	[CSI2PHY_S0D3_ANA_CON3] = CSI2PHY_REG(CSI2_DCPHY_S0D3_ANA_CON3),
+	[CSI2PHY_S0D3_DESKEW_CON0] = CSI2PHY_REG(CSI2_DCPHY_S0D3_DESKEW_CON0),
+	[CSI2PHY_S0D3_DESKEW_CON2] = CSI2PHY_REG(CSI2_DCPHY_S0D3_DESKEW_CON2),
+	[CSI2PHY_S0D3_DESKEW_CON4] = CSI2PHY_REG(CSI2_DCPHY_S0D3_DESKEW_CON4),
+};
+
+static const struct grf_reg rk3562_grf_dphy_regs[] = {
+	[GRF_DPHY_CSI2PHY_FORCERXMODE] = GRF_REG(RK3562_GRF_VI_CON0, 4, 0),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN] = GRF_REG(RK3562_GRF_VI_CON0, 4, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN0] = GRF_REG(RK3562_GRF_VI_CON0, 2, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN1] = GRF_REG(RK3562_GRF_VI_CON0, 2, 6),
+	[GRF_DPHY_CSI2PHY_CLKLANE_EN] = GRF_REG(RK3562_GRF_VI_CON0, 1, 8),
+	[GRF_DPHY_CLK_INV_SEL] = GRF_REG(RK3562_GRF_VI_CON0, 1, 9),
+	[GRF_DPHY_CSI2PHY_CLKLANE1_EN] = GRF_REG(RK3562_GRF_VI_CON0, 1, 10),
+	[GRF_DPHY_CLK1_INV_SEL] = GRF_REG(RK3562_GRF_VI_CON0, 1, 11),
+	[GRF_DPHY_CSI2PHY_LANE_SEL] = GRF_REG(RK3562_GRF_VI_CON0, 1, 12),
+	[GRF_DPHY_CSI2PHY1_LANE_SEL] = GRF_REG(RK3562_GRF_VI_CON0, 1, 13),
+	[GRF_DPHY1_CSI2PHY_FORCERXMODE] = GRF_REG(RK3562_GRF_VI_CON1, 4, 0),
+	[GRF_DPHY1_CSI2PHY_DATALANE_EN] = GRF_REG(RK3562_GRF_VI_CON1, 4, 4),
+	[GRF_DPHY1_CSI2PHY_DATALANE_EN0] = GRF_REG(RK3562_GRF_VI_CON1, 2, 4),
+	[GRF_DPHY1_CSI2PHY_DATALANE_EN1] = GRF_REG(RK3562_GRF_VI_CON1, 2, 6),
+	[GRF_DPHY1_CSI2PHY_CLKLANE_EN] = GRF_REG(RK3562_GRF_VI_CON1, 1, 8),
+	[GRF_DPHY1_CLK_INV_SEL] = GRF_REG(RK3562_GRF_VI_CON1, 1, 9),
+	[GRF_DPHY1_CSI2PHY_CLKLANE1_EN] = GRF_REG(RK3562_GRF_VI_CON1, 1, 10),
+	[GRF_DPHY1_CLK1_INV_SEL] = GRF_REG(RK3562_GRF_VI_CON1, 1, 11),
+};
+
+static const struct csi2dphy_reg rk3562_csi2dphy_regs[] = {
+	[CSI2PHY_REG_CTRL_LANE_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CTRL_LANE_ENABLE),
+	[CSI2PHY_DUAL_CLK_EN] = CSI2PHY_REG(CSI2_DPHY_DUAL_CAL_EN),
+	[CSI2PHY_CLK_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK_WR_THS_SETTLE),
+	[CSI2PHY_CLK_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK_CALIB_EN),
+	[CSI2PHY_LANE0_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_WR_THS_SETTLE),
+	[CSI2PHY_LANE0_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE0_CALIB_EN),
+	[CSI2PHY_LANE1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_WR_THS_SETTLE),
+	[CSI2PHY_LANE1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE1_CALIB_EN),
+	[CSI2PHY_LANE2_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_WR_THS_SETTLE),
+	[CSI2PHY_LANE2_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE2_CALIB_EN),
+	[CSI2PHY_LANE3_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_WR_THS_SETTLE),
+	[CSI2PHY_LANE3_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_LANE3_CALIB_EN),
+	[CSI2PHY_CLK1_THS_SETTLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_WR_THS_SETTLE),
+	[CSI2PHY_CLK1_CALIB_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_CALIB_EN),
+	[CSI2PHY_CLK1_LANE_ENABLE] = CSI2PHY_REG(CSI2_DPHY_CLK1_LANE_EN),
+};
+
+static const struct grf_reg rk3576_grf_dphy_regs[] = {
+	[GRF_DPHY_CSI2PHY_FORCERXMODE] = GRF_REG(GRF_DPHY_CON0, 4, 0),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN] = GRF_REG(GRF_DPHY_CON0, 4, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN0] = GRF_REG(GRF_DPHY_CON0, 2, 4),
+	[GRF_DPHY_CSI2PHY_DATALANE_EN1] = GRF_REG(GRF_DPHY_CON0, 2, 6),
+	[GRF_DPHY_CSI2PHY_CLKLANE_EN] = GRF_REG(GRF_DPHY_CON0, 1, 8),
+	[GRF_DPHY_CLK_INV_SEL] = GRF_REG(GRF_DPHY_CON0, 1, 9),
+	[GRF_DPHY_CSI2PHY_CLKLANE1_EN] = GRF_REG(GRF_DPHY_CON0, 1, 10),
+	[GRF_DPHY_CLK1_INV_SEL] = GRF_REG(GRF_DPHY_CON0, 1, 11),
+	[GRF_DPHY_CSI2PHY_LANE_SEL] = GRF_REG(GRF_SOC_CON5_RK3576, 1, 1),
+	[GRF_DPHY_CSI2PHY1_LANE_SEL] = GRF_REG(GRF_SOC_CON5_RK3576, 1, 2),
+};
+
+//define dcphy params
+static struct rkmodule_csi_dphy_param rk3588_dcphy_param = {
+	.vendor = PHY_VENDOR_SAMSUNG,
+	.lp_vol_ref = 3,
+	.lp_hys_sw = {3, 0, 0, 0},
+	.lp_escclk_pol_sel = {1, 0, 0, 0},
+	.skew_data_cal_clk = {0, 3, 3, 3},
+	.clk_hs_term_sel = 2,
+	.data_hs_term_sel = {2, 2, 2, 2},
+	.reserved = {0},
+};
+
+/* These tables must be sorted by .range_h ascending. */
+static const struct hsfreq_range rk3568_csi2_dphy_hw_hsfreq_ranges[] = {
+	{ 109, 0x02}, { 149, 0x03}, { 199, 0x06}, { 249, 0x06},
+	{ 299, 0x06}, { 399, 0x08}, { 499, 0x0b}, { 599, 0x0e},
+	{ 699, 0x10}, { 799, 0x12}, { 999, 0x16}, {1199, 0x1e},
+	{1399, 0x23}, {1599, 0x2d}, {1799, 0x32}, {1999, 0x37},
+	{2199, 0x3c}, {2399, 0x41}, {2499, 0x46}
+};
+
+/* These tables must be sorted by .range_h ascending. */
+static const struct hsfreq_range rk3588_csi2_dcphy_d_hw_hsfreq_ranges[] = {
+	{ 80,  0x105}, { 100, 0x106}, { 120, 0x107}, { 140, 0x108},
+	{ 160, 0x109}, { 180, 0x10a}, { 200, 0x10b}, { 220, 0x10c},
+	{ 240, 0x10d}, { 270, 0x10e}, { 290, 0x10f}, { 310, 0x110},
+	{ 330, 0x111}, { 350, 0x112}, { 370, 0x113}, { 390, 0x114},
+	{ 410, 0x115}, { 430, 0x116}, { 450, 0x117}, { 470, 0x118},
+	{ 490, 0x119}, { 510, 0x11a}, { 540, 0x11b}, { 560, 0x11c},
+	{ 580, 0x11d}, { 600, 0x11e}, { 620, 0x11f}, { 640, 0x120},
+	{ 660, 0x121}, { 680, 0x122}, { 700, 0x123}, { 720, 0x124},
+	{ 740, 0x125}, { 760, 0x126}, { 790, 0x127}, { 810, 0x128},
+	{ 830, 0x129}, { 850, 0x12a}, { 870, 0x12b}, { 890, 0x12c},
+	{ 910, 0x12d}, { 930, 0x12e}, { 950, 0x12f}, { 970, 0x130},
+	{ 990, 0x131}, {1010, 0x132}, {1030, 0x133}, {1060, 0x134},
+	{1080, 0x135}, {1100, 0x136}, {1120, 0x137}, {1140, 0x138},
+	{1160, 0x139}, {1180, 0x13a}, {1200, 0x13b}, {1220, 0x13c},
+	{1240, 0x13d}, {1260, 0x13e}, {1280, 0x13f}, {1310, 0x140},
+	{1330, 0x141}, {1350, 0x142}, {1370, 0x143}, {1390, 0x144},
+	{1410, 0x145}, {1430, 0x146}, {1450, 0x147}, {1470, 0x148},
+	{1490, 0x149}, {1580, 0x007}, {1740, 0x008}, {1910, 0x009},
+	{2070, 0x00a}, {2240, 0x00b}, {2410, 0x00c}, {2570, 0x00d},
+	{2740, 0x00e}, {2910, 0x00f}, {3070, 0x010}, {3240, 0x011},
+	{3410, 0x012}, {3570, 0x013}, {3740, 0x014}, {3890, 0x015},
+	{4070, 0x016}, {4240, 0x017}, {4400, 0x018}, {4500, 0x019},
+};
+
+static struct csi2_dphy_hw rk3568_csi2_dphy_hw = {
+	.dphy_clks = rk3568_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3568_csi2_dphy_hw_clks),
+	.csi2_clks = rk3568_csi2_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3568_csi2_clks),
+	.csi2_rsts = rk3568_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3568_csi2_rsts),
+	.hsfreq_ranges = rk3568_csi2_dphy_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3568_csi2_dphy_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3568_csi2dphy_regs,
+	.grf_regs = rk3568_grf_dphy_regs,
+	.chip_id = CHIP_ID_RK3568,
+};
+
+static struct csi2_dphy_hw rk3588_csi2_dphy_hw = {
+	.dphy_clks = rk3588_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3588_csi2_dphy_hw_clks),
+	.dphy_rsts = rk3588_csi2_dphy_hw_rsts,
+	.num_dphy_rsts = ARRAY_SIZE(rk3588_csi2_dphy_hw_rsts),
+	.csi2_clks = rk3588_csi2_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3588_csi2_clks),
+	.csi2_rsts = rk3588_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3588_csi2_rsts),
+	.hsfreq_ranges = rk3568_csi2_dphy_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3568_csi2_dphy_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3588_csi2dphy_regs,
+	.grf_regs = rk3588_grf_dphy_regs,
+	.chip_id = CHIP_ID_RK3588,
+};
+
+static struct csi2_dphy_hw rk3588_csi2_dcphy_hw = {
+	.dphy_clks = rk3588_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3588_csi2_dphy_hw_clks),
+	.csi2_clks = rk3588_csi2_dcphy_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3588_csi2_dcphy_clks),
+	.csi2_rsts = rk3588_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3588_csi2_rsts),
+	.hsfreq_ranges = rk3588_csi2_dcphy_d_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3588_csi2_dcphy_d_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3588_csi2dcphy_regs,
+	.grf_regs = rk3588_grf_dcphy_regs,
+	.chip_id = CHIP_ID_RK3588_DCPHY,
+};
+
+static struct csi2_dphy_hw rk3562_csi2_dphy_hw = {
+	.dphy_clks = rk3562_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3562_csi2_dphy_hw_clks),
+	.dphy_rsts = rk3562_csi2_dphy_hw_rsts,
+	.num_dphy_rsts = ARRAY_SIZE(rk3562_csi2_dphy_hw_rsts),
+	.csi2_clks = rk3562_csi2_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3562_csi2_clks),
+	.csi2_rsts = rk3562_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3562_csi2_rsts),
+	.hsfreq_ranges = rk3568_csi2_dphy_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3568_csi2_dphy_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3562_csi2dphy_regs,
+	.grf_regs = rk3562_grf_dphy_regs,
+	.chip_id = CHIP_ID_RK3562,
+};
+
+static struct csi2_dphy_hw rk3576_csi2_dphy_hw = {
+	.dphy_clks = rk3576_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3576_csi2_dphy_hw_clks),
+	.dphy_rsts = rk3576_csi2_dphy_hw_rsts,
+	.num_dphy_rsts = ARRAY_SIZE(rk3576_csi2_dphy_hw_rsts),
+	.csi2_clks = rk3576_csi2_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3576_csi2_clks),
+	.csi2_rsts = rk3576_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3576_csi2_rsts),
+	.hsfreq_ranges = rk3568_csi2_dphy_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3568_csi2_dphy_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3588_csi2dphy_regs,
+	.grf_regs = rk3576_grf_dphy_regs,
+	.chip_id = CHIP_ID_RK3576,
+};
+
+static struct csi2_dphy_hw rk3576_csi2_dcphy_hw = {
+	.dphy_clks = rk3576_csi2_dphy_hw_clks,
+	.num_dphy_clks = ARRAY_SIZE(rk3576_csi2_dphy_hw_clks),
+	.csi2_clks = rk3576_csi2_dcphy_clks,
+	.num_csi2_clks = ARRAY_SIZE(rk3576_csi2_dcphy_clks),
+	.csi2_rsts = rk3576_csi2_rsts,
+	.num_csi2_rsts = ARRAY_SIZE(rk3576_csi2_rsts),
+	.hsfreq_ranges = rk3588_csi2_dcphy_d_hw_hsfreq_ranges,
+	.num_hsfreq_ranges = ARRAY_SIZE(rk3588_csi2_dcphy_d_hw_hsfreq_ranges),
+	.csi2dphy_regs = rk3588_csi2dcphy_regs,
+	.grf_regs = rk3588_grf_dcphy_regs,
+	.chip_id = CHIP_ID_RK3588_DCPHY,
+};
+
+static const struct cif_input_fmt in_fmts[] = {
+	{
+		.mbus_code	= MEDIA_BUS_FMT_YUYV8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_YUYV,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_YUYV,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_YUYV8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_YUYV,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_YUYV,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_INTERLACED,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_YVYU8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_YVYU,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_YVYU,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_YVYU8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_YVYU,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_YVYU,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_INTERLACED,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_UYVY8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_UYVY,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_UYVY,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_UYVY8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_UYVY,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_UYVY,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_INTERLACED,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_VYUY8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_VYUY,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_VYUY,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_VYUY8_2X8,
+		.dvp_fmt_val	= YUV_INPUT_422 | YUV_INPUT_ORDER_VYUY,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_YUV422,
+		.csi_yuv_order	= CSI_YUV_INPUT_ORDER_VYUY,
+		.fmt_type	= CIF_FMT_TYPE_YUV,
+		.field		= V4L2_FIELD_INTERLACED,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SBGGR8_1X8,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_8,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW8,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGBRG8_1X8,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_8,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW8,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGRBG8_1X8,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_8,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW8,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SRGGB8_1X8,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_8,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW8,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SBGGR10_1X10,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_10,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW10,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGBRG10_1X10,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_10,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW10,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGRBG10_1X10,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_10,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW10,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SRGGB10_1X10,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_10,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW10,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SBGGR12_1X12,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_12,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW12,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGBRG12_1X12,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_12,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW12,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SGRBG12_1X12,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_12,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW12,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_SRGGB12_1X12,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_12,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW12,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_RGB888_1X24,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RGB888,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_Y8_1X8,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_8,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW8,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_Y10_1X10,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_10,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW10,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}, {
+		.mbus_code	= MEDIA_BUS_FMT_Y12_1X12,
+		.dvp_fmt_val	= INPUT_MODE_RAW | RAW_DATA_WIDTH_12,
+		.csi_fmt_val	= CSI_WRDDR_TYPE_RAW12,
+		.fmt_type	= CIF_FMT_TYPE_RAW,
+		.field		= V4L2_FIELD_NONE,
+	}
+};
+
+static const struct cif_output_fmt out_fmts[] = {
+	{
+		.fourcc = V4L2_PIX_FMT_NV16,
+		.cplanes = 2,
+		.mplanes = 1,
+		.fmt_val = YUV_OUTPUT_422 | UV_STORAGE_ORDER_UVUV,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_YUV422,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_NV61,
+		.fmt_val = YUV_OUTPUT_422 | UV_STORAGE_ORDER_VUVU,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_YUV422,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_NV12,
+		.fmt_val = YUV_OUTPUT_420 | UV_STORAGE_ORDER_UVUV,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_YUV420SP,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_NV21,
+		.fmt_val = YUV_OUTPUT_420 | UV_STORAGE_ORDER_VUVU,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_YUV420SP,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_YUYV,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_YVYU,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_UYVY,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_VYUY,
+		.cplanes = 2,
+		.mplanes = 1,
+		.bpp = { 8, 16 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_YUV,
+	}, {
+		.fourcc = V4L2_PIX_FMT_RGB24,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 24 },
+		.csi_fmt_val = CSI_WRDDR_TYPE_RGB888,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_RGB565,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_BGR666,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 18 },
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SRGGB8,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 8 },
+		.raw_bpp = 8,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGRBG8,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 8 },
+		.raw_bpp = 8,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGBRG8,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 8 },
+		.raw_bpp = 8,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SBGGR8,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 8 },
+		.raw_bpp = 8,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW8,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SRGGB10,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 10,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW10,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGRBG10,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 10,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW10,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGBRG10,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 10,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW10,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SBGGR10,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 10,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW10,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SRGGB12,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 12,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW12,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGRBG12,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 12,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW12,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SGBRG12,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 12,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW12,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SBGGR12,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 12,
+		.csi_fmt_val = CSI_WRDDR_TYPE_RAW12,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_SBGGR16,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.raw_bpp = 16,
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}, {
+		.fourcc = V4L2_PIX_FMT_Y16,
+		.cplanes = 1,
+		.mplanes = 1,
+		.bpp = { 16 },
+		.fmt_type = CIF_FMT_TYPE_RAW,
+	}
+
+	/* TODO: We can support NV12M/NV21M/NV16M/NV61M too */
+};
+
+static void rkcif_write_reg(struct vehicle_cif *cif,
+			  enum cif_reg_index index, u32 val)
+{
+	void __iomem *base = cif->base;
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+	int csi_offset = 0;
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS &&
+	    index >= CIF_REG_MIPI_LVDS_ID0_CTRL0 &&
+	    index <= CIF_REG_MIPI_ON_PAD) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			csi_offset = cif->csi_host_idx * 0x100;
+		} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 3)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x500;
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 2)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x100 + cif->csi_host_idx * 0x100;
+		}
+	}
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index == CIF_REG_GLB_CTRL || index == CIF_REG_DVP_CTRL || reg->offset != 0x0) {
+			write_reg(base, reg->offset + csi_offset, val);
+		} else {
+			VEHICLE_INFO("write index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				 index, reg->name, val);
+		}
+	}
+	VEHICLE_DG("@%s register[%s] offset(0x%x) csi_offset(0x%x) value:0x%x !\n",
+				__func__, reg->name, reg->offset, csi_offset, val);
+}
+
+static void rkcif_write_reg_or(struct vehicle_cif *cif,
+			  enum cif_reg_index index, u32 val)
+{
+	void __iomem *base = cif->base;
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+	unsigned int reg_val = 0x0;
+	int csi_offset = 0;
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS &&
+	    index >= CIF_REG_MIPI_LVDS_ID0_CTRL0 &&
+	    index <= CIF_REG_MIPI_ON_PAD) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			csi_offset = cif->csi_host_idx * 0x100;
+		} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 3)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x500;
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 2)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x100 + cif->csi_host_idx * 0x100;
+		}
+	}
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index == CIF_REG_GLB_CTRL || index == CIF_REG_DVP_CTRL || reg->offset != 0x0) {
+			reg_val = read_reg(base, reg->offset + csi_offset);
+			reg_val |= val;
+			write_reg(base, reg->offset + csi_offset, reg_val);
+		} else {
+			VEHICLE_INFO("write index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				 index, reg->name, val);
+		}
+	}
+	VEHICLE_DG("@%s register[%s] offset(0x%x)  csi_offset(0x%x) value:0x%x !\n",
+				__func__, reg->name, reg->offset, csi_offset, reg_val);
+}
+
+static void rkcif_write_reg_and(struct vehicle_cif *cif,
+			  enum cif_reg_index index, u32 val)
+{
+	void __iomem *base = cif->base;
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+	unsigned int reg_val = 0x0;
+	int csi_offset = 0;
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS &&
+	    index >= CIF_REG_MIPI_LVDS_ID0_CTRL0 &&
+	    index <= CIF_REG_MIPI_ON_PAD) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			csi_offset = cif->csi_host_idx * 0x100;
+		} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 3)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x500;
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 2)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x100 + cif->csi_host_idx * 0x100;
+		}
+	}
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index == CIF_REG_GLB_CTRL || index == CIF_REG_DVP_CTRL || reg->offset != 0x0) {
+			reg_val = read_reg(base, reg->offset + csi_offset);
+			reg_val &= val;
+			write_reg(base, reg->offset + csi_offset, reg_val);
+		} else {
+			VEHICLE_INFO("write index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				 index, reg->name, val);
+		}
+	}
+	VEHICLE_DG("@%s register[%s] offset(0x%x) csi_offset(0x%x) value:0x%x !\n",
+				__func__, reg->name, reg->offset, csi_offset, reg_val);
+}
+
+static unsigned int rkcif_read_reg(struct vehicle_cif *cif,
+				 enum cif_reg_index index)
+{
+	unsigned int val = 0x0;
+	void __iomem *base = cif->base;
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+	int csi_offset = 0;
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS &&
+	    index >= CIF_REG_MIPI_LVDS_ID0_CTRL0 &&
+	    index <= CIF_REG_MIPI_ON_PAD) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			csi_offset = cif->csi_host_idx * 0x100;
+		} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 3)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x500;
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->csi_host_idx < 2)
+				csi_offset = cif->csi_host_idx * 0x200;
+			else
+				csi_offset = 0x100 + cif->csi_host_idx * 0x100;
+		}
+	}
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index == CIF_REG_GLB_CTRL || index == CIF_REG_DVP_CTRL || reg->offset != 0x0)
+			val = read_reg(base, reg->offset + csi_offset);
+		else
+			VEHICLE_INFO("read index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				 index, reg->name, val);
+	}
+	VEHICLE_DG("@%s register[%s] offset(0x%x) csi_offset(0x%x) value:0x%x !\n",
+				__func__, reg->name, reg->offset, csi_offset, val);
+	return val;
+}
+
+static void rkvehicle_cif_write_grf_reg(struct vehicle_cif *cif,
+			 enum cif_reg_index index, u32 val)
+{
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index > CIF_REG_DVP_CTRL) {
+			if (!IS_ERR(cif->regmap_grf))
+				regmap_write(cif->regmap_grf, reg->offset, val);
+		} else {
+			VEHICLE_INFO("write index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				 index, reg->name, val);
+		}
+		VEHICLE_DG("@%s reg[%s] offset(0x%x): 0x%x !\n",
+				__func__, reg->name, reg->offset, val);
+	}
+}
+
+static u32 rkvehicle_cif_read_grf_reg(struct vehicle_cif *cif,
+			   enum cif_reg_index index)
+{
+	const struct vehicle_cif_reg *reg = &cif->cif_regs[index];
+	u32 val = 0xffff;
+
+	if (index < CIF_REG_INDEX_MAX) {
+		if (index > CIF_REG_DVP_CTRL) {
+			if (!IS_ERR(cif->regmap_grf))
+				regmap_read(cif->regmap_grf, reg->offset, &val);
+		} else {
+			VEHICLE_INFO("read index(%d) reg[%s]: 0x%x failed, maybe useless!!!\n",
+				index, reg->name, val);
+		}
+		VEHICLE_DG("@%s reg[%s] offset(0x%x): 0x%x !\n",
+				__func__, reg->name, reg->offset, val);
+	}
+
+	return val;
+}
+
+static inline void write_csi2_dphy_reg(struct csi2_dphy_hw *hw,
+					    int index, u32 value)
+{
+	const struct csi2dphy_reg *reg = &hw->csi2dphy_regs[index];
+
+	if ((index == CSI2PHY_REG_CTRL_LANE_ENABLE) ||
+	    (index == CSI2PHY_CLK_LANE_ENABLE) ||
+	    ((index != CSI2PHY_REG_CTRL_LANE_ENABLE) &&
+	     (reg->offset != 0x0)))
+		writel(value, hw->csi2_dphy_base + reg->offset);
+
+	VEHICLE_DG("@%s offset(0x%x) reg val: 0x%x !\n",
+				__func__, reg->offset, value);
+}
+
+static inline void write_csi2_dphy_reg_mask(struct csi2_dphy_hw *hw,
+					    int index, u32 value, u32 mask)
+{
+	const struct csi2dphy_reg *reg = &hw->csi2dphy_regs[index];
+	u32 read_val = 0;
+
+	read_val = readl(hw->csi2_dphy_base + reg->offset);
+	read_val &= ~mask;
+	read_val |= value;
+	writel(read_val, hw->csi2_dphy_base + reg->offset);
+}
+
+static inline void read_csi2_dphy_reg(struct csi2_dphy_hw *hw,
+					   int index, u32 *value)
+{
+	const struct csi2dphy_reg *reg = &hw->csi2dphy_regs[index];
+
+	if ((index == CSI2PHY_REG_CTRL_LANE_ENABLE) ||
+	    (index == CSI2PHY_CLK_LANE_ENABLE) ||
+	    ((index != CSI2PHY_REG_CTRL_LANE_ENABLE) &&
+	     (reg->offset != 0x0)))
+		*value = readl(hw->csi2_dphy_base + reg->offset);
+
+	VEHICLE_DG("@%s offset(0x%x) reg val: 0x%x !\n",
+				__func__, reg->offset, *value);
+}
+
+static void csi_mipidphy_wr_ths_settle(struct csi2_dphy_hw *hw,
+					      int hsfreq,
+					      enum csi2_dphy_lane lane)
+{
+	unsigned int val = 0;
+	unsigned int offset;
+
+	switch (lane) {
+	case CSI2_DPHY_LANE_CLOCK:
+		offset = CSI2PHY_CLK_THS_SETTLE;
+		break;
+	case CSI2_DPHY_LANE_CLOCK1:
+		offset = CSI2PHY_CLK1_THS_SETTLE;
+		break;
+	case CSI2_DPHY_LANE_DATA0:
+		offset = CSI2PHY_LANE0_THS_SETTLE;
+		break;
+	case CSI2_DPHY_LANE_DATA1:
+		offset = CSI2PHY_LANE1_THS_SETTLE;
+		break;
+	case CSI2_DPHY_LANE_DATA2:
+		offset = CSI2PHY_LANE2_THS_SETTLE;
+		break;
+	case CSI2_DPHY_LANE_DATA3:
+		offset = CSI2PHY_LANE3_THS_SETTLE;
+		break;
+	default:
+		return;
+	}
+
+	read_csi2_dphy_reg(hw, offset, &val);
+	val = (val & ~0x7f) | hsfreq;
+	write_csi2_dphy_reg(hw, offset, val);
+}
+
+static void rkvehicle_cif_cfg_dvp_clk_sampling_edge(struct vehicle_cif *cif,
+					enum rkcif_clk_edge edge)
+{
+	u32 val = 0x0;
+
+	if (!IS_ERR(cif->regmap_grf)) {
+		if (cif->chip_id == CHIP_RK3568_VEHICLE_CIF) {
+			if (edge == RKCIF_CLK_RISING)
+				val = RK3568_CIF_PCLK_SAMPLING_EDGE_RISING;
+			else
+				val = RK3568_CIF_PCLK_SAMPLING_EDGE_FALLING;
+		}
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			if (edge == RKCIF_CLK_RISING)
+				val = RK3588_CIF_PCLK_SAMPLING_EDGE_RISING;
+			else
+				val = RK3588_CIF_PCLK_SAMPLING_EDGE_FALLING;
+		}
+		rkvehicle_cif_write_grf_reg(cif, CIF_REG_GRF_CIFIO_CON, val);
+	}
+}
+
+static int rkcif_dvp_get_input_yuv_order(struct vehicle_cfg *cfg)
+{
+	unsigned int mask;
+
+	switch (cfg->mbus_code) {
+	case MEDIA_BUS_FMT_UYVY8_2X8:
+		mask = CSI_YUV_INPUT_ORDER_UYVY >> 11;
+		break;
+	case MEDIA_BUS_FMT_VYUY8_2X8:
+		mask = CSI_YUV_INPUT_ORDER_VYUY >> 11;
+		break;
+	case MEDIA_BUS_FMT_YUYV8_2X8:
+		mask = CSI_YUV_INPUT_ORDER_YUYV >> 11;
+		break;
+	case MEDIA_BUS_FMT_YVYU8_2X8:
+		mask = CSI_YUV_INPUT_ORDER_YVYU >> 11;
+		break;
+	default:
+		mask = CSI_YUV_INPUT_ORDER_UYVY >> 11;
+		break;
+	}
+	return mask;
+}
+
+static int cif_stream_setup(struct vehicle_cif *cif)
+{
+	struct vehicle_cfg *cfg = &cif->cif_cfg;
+	u32 val, mbus_flags,
+	    xfer_mode = 0, yc_swap = 0,
+	    inputmode = 0, mipimode = 0,
+	    input_format = 0, output_format = 0, crop = 0,
+	    out_fmt_mask = 0,
+	    multi_id_en = BT656_1120_MULTI_ID_DISABLE,
+	    multi_id_mode = BT656_1120_MULTI_ID_MODE_1,
+	    multi_id_sel = BT656_1120_MULTI_ID_SEL_LSB,
+	    bt1120_edge_mode = BT1120_CLOCK_SINGLE_EDGES;
+	u32 sav_detect = BT656_DETECT_SAV;
+	u32 in_fmt_yuv_order = 0;
+
+	mbus_flags = cfg->mbus_flags;
+	/* set dvp clk sample edge */
+	if (mbus_flags & V4L2_MBUS_PCLK_SAMPLE_RISING)
+		rkvehicle_cif_cfg_dvp_clk_sampling_edge(cif, RKCIF_CLK_RISING);
+	else
+		rkvehicle_cif_cfg_dvp_clk_sampling_edge(cif, RKCIF_CLK_FALLING);
+
+	inputmode = cfg->input_format<<2; //INPUT_MODE_YUV or INPUT_MODE_BT656_YUV422
+	//YUV_INPUT_ORDER_UYVY, MEDIA_BUS_FMT_UYVY8_2X8, CCIR_INPUT_ORDER_ODD
+	input_format = (cfg->yuv_order<<5) | YUV_INPUT_422 | (cfg->field_order<<9);
+	if (cfg->output_format == CIF_OUTPUT_FORMAT_420)
+		output_format = YUV_OUTPUT_420 | UV_STORAGE_ORDER_UVUV;
+	else
+		output_format = YUV_OUTPUT_422 | UV_STORAGE_ORDER_UVUV;
+
+	if (cif->chip_id == CHIP_RK3568_VEHICLE_CIF) {
+		val = cfg->vsync | (cfg->href<<1) | inputmode | mipimode
+		   | input_format | output_format
+		   | xfer_mode | yc_swap | multi_id_en
+		   | multi_id_sel | multi_id_mode | bt1120_edge_mode;
+	} else {
+		out_fmt_mask = (CSI_WRDDR_TYPE_YUV420SP_RK3588 << 11) |
+				(CSI_YUV_OUTPUT_ORDER_UYVY << 1);
+		in_fmt_yuv_order = rkcif_dvp_get_input_yuv_order(cfg);
+		val = cfg->vsync | (cfg->href<<1) | inputmode
+		   | in_fmt_yuv_order | out_fmt_mask
+		   | yc_swap | multi_id_en | multi_id_sel
+		   | sav_detect | multi_id_mode | bt1120_edge_mode;
+	}
+
+	if (cif->chip_id >= CHIP_RK3576_VEHICLE_CIF)
+		val |= DVP_UVDS_EN;
+
+	rkcif_write_reg(cif, CIF_REG_DVP_FOR, val);
+
+	rkcif_write_reg(cif, CIF_REG_DVP_VIR_LINE_WIDTH, cfg->width);
+	rkcif_write_reg(cif, CIF_REG_DVP_SET_SIZE,
+		      cfg->width | (cfg->height << 16));
+
+	crop = (cfg->start_x | (cfg->start_y<<16));
+	rkcif_write_reg(cif, CIF_REG_DVP_CROP, crop);
+
+	rkcif_write_reg(cif, CIF_REG_DVP_FRAME_STATUS, FRAME_STAT_CLS);
+
+	if (cif->chip_id < CHIP_RK3588_VEHICLE_CIF) {
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, INTSTAT_CLS);
+		rkcif_write_reg(cif, CIF_REG_DVP_SCL_CTRL, ENABLE_YUV_16BIT_BYPASS);
+		rkcif_write_reg(cif, CIF_REG_DVP_INTEN,
+			FRAME_END_EN | INTSTAT_ERR |
+			PST_INF_FRAME_END);
+		/* enable line int for sof */
+		rkcif_write_reg(cif, CIF_REG_DVP_LINE_INT_NUM, 0x1);
+		rkcif_write_reg(cif, CIF_REG_DVP_INTEN, LINE_INT_EN);
+	} else if (cif->chip_id < CHIP_RK3576_VEHICLE_CIF) {
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, 0x3c3ffff);
+		rkcif_write_reg_or(cif, CIF_REG_DVP_INTEN, 0x033ffff);//0x3c3ffff
+	} else {
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, 0x3c3ffff);
+		rkcif_write_reg_or(cif, CIF_REG_DVP_INTEN, 0x3c3ff0f);
+	}
+
+	cif->interlaced_enable = false;
+
+	return 0;
+}
+
+static inline void csi2_dphy_write_sys_grf_reg(struct csi2_dphy_hw *hw,
+				     int index, u8 value)
+{
+	const struct grf_reg *reg = &hw->grf_regs[index];
+	unsigned int val = HIWORD_UPDATE(value, reg->mask, reg->shift);
+
+	if (reg->shift)
+		regmap_write(hw->regmap_sys_grf, reg->offset, val);
+}
+
+static inline void csi2_dphy_write_grf_reg(struct csi2_dphy_hw *hw,
+				     int index, u8 value)
+{
+	const struct grf_reg *reg = &hw->grf_regs[index];
+	unsigned int val = HIWORD_UPDATE(value, reg->mask, reg->shift);
+
+	if (reg->shift)
+		regmap_write(hw->regmap_grf, reg->offset, val);
+}
+
+static inline u32 csi2_dphy_read_grf_reg(struct csi2_dphy_hw *hw, int index)
+{
+	const struct grf_reg *reg = &hw->grf_regs[index];
+	unsigned int val = 0;
+
+	if (reg->shift) {
+		regmap_read(hw->regmap_grf, reg->offset, &val);
+		val = (val >> reg->shift) & reg->mask;
+	}
+
+	return val;
+}
+
+static void csi2_dphy_config_dual_mode(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	u32 val;
+
+	val = ~GRF_CSI2PHY_LANE_SEL_SPLIT;
+	if (cif->dphy_hw->phy_index < 3) {
+		csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_DATALANE_EN,
+				GENMASK(cif->cif_cfg.lanes - 1, 0));
+		csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_CLKLANE_EN, 0x1);
+		if (cif->chip_id != CHIP_RK3588_VEHICLE_CIF &&
+				cif->chip_id != CHIP_RK3576_VEHICLE_CIF)
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_LANE_SEL, val);
+		else
+			csi2_dphy_write_sys_grf_reg(hw, GRF_DPHY_CSI2PHY_LANE_SEL, val);
+	} else {
+		if (cif->chip_id <= CHIP_RK3588_VEHICLE_CIF ||
+				cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_DATALANE_EN,
+					GENMASK(cif->cif_cfg.lanes - 1, 0));
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_CLKLANE_EN, 0x1);
+		} else {
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY1_CSI2PHY_DATALANE_EN,
+					GENMASK(cif->cif_cfg.lanes - 1, 0));
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY1_CSI2PHY_CLKLANE_EN, 0x1);
+		}
+		if (cif->chip_id != CHIP_RK3588_VEHICLE_CIF &&
+				cif->chip_id != CHIP_RK3576_VEHICLE_CIF)
+			csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY1_LANE_SEL, val);
+		else
+			csi2_dphy_write_sys_grf_reg(hw, GRF_DPHY_CSI2PHY1_LANE_SEL, val);
+	}
+}
+
+static int vehicle_csi2_dphy_stream_start(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	const struct hsfreq_range *hsfreq_ranges = hw->hsfreq_ranges;
+	int num_hsfreq_ranges = hw->num_hsfreq_ranges;
+	int i, hsfreq = 0;
+	u32 val = 0, pre_val;
+
+
+	mutex_lock(&hw->mutex);
+
+	/* set data lane num and enable clock lane */
+	/*
+	 * for rk356x: dphy0 is used just for full mode,
+	 *             dphy1 is used just for split mode,uses lane0_1,
+	 *             dphy2 is used just for split mode,uses lane2_3
+	 */
+	read_csi2_dphy_reg(hw, CSI2PHY_REG_CTRL_LANE_ENABLE, &pre_val);
+	val |= (GENMASK(cif->cif_cfg.lanes - 1, 0) <<
+		CSI2_DPHY_CTRL_DATALANE_ENABLE_OFFSET_BIT) |
+		(0x1 << CSI2_DPHY_CTRL_CLKLANE_ENABLE_OFFSET_BIT);
+
+	val |= pre_val;
+	write_csi2_dphy_reg(hw, CSI2PHY_REG_CTRL_LANE_ENABLE, val);
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		write_csi2_dphy_reg(hw, CSI2PHY_DUAL_CLK_EN, 0x1e);
+		write_csi2_dphy_reg(hw, CSI2PHY_DUAL_CLK_EN, 0x1f);
+		csi2_dphy_config_dual_mode(cif);
+	}
+
+	/* not into receive mode/wait stopstate */
+	csi2_dphy_write_grf_reg(hw, GRF_DPHY_CSI2PHY_FORCERXMODE, 0x0);
+
+	/* enable calibration */
+	if (hw->data_rate_mbps > 1500) {
+		write_csi2_dphy_reg(hw, CSI2PHY_CLK_CALIB_ENABLE, 0x80);
+		if (cif->cif_cfg.lanes > 0x00)
+			write_csi2_dphy_reg(hw, CSI2PHY_LANE0_CALIB_ENABLE, 0x80);
+		if (cif->cif_cfg.lanes > 0x01)
+			write_csi2_dphy_reg(hw, CSI2PHY_LANE1_CALIB_ENABLE, 0x80);
+		if (cif->cif_cfg.lanes > 0x02)
+			write_csi2_dphy_reg(hw, CSI2PHY_LANE2_CALIB_ENABLE, 0x80);
+		if (cif->cif_cfg.lanes > 0x03)
+			write_csi2_dphy_reg(hw, CSI2PHY_LANE3_CALIB_ENABLE, 0x80);
+	}
+
+	/* set clock lane and data lane */
+	for (i = 0; i < num_hsfreq_ranges; i++) {
+		if (hsfreq_ranges[i].range_h >= hw->data_rate_mbps) {
+			hsfreq = hsfreq_ranges[i].cfg_bit;
+			break;
+		}
+	}
+
+	if (i == num_hsfreq_ranges) {
+		i = num_hsfreq_ranges - 1;
+		dev_warn(hw->dev, "data rate: %lld mbps, max support %d mbps",
+			 hw->data_rate_mbps, hsfreq_ranges[i].range_h + 1);
+		hsfreq = hsfreq_ranges[i].cfg_bit;
+	}
+
+	VEHICLE_DG("mipi data_rate_mbps %lld, matched bit(0x%0x), lanes(%d)\n",
+			hw->data_rate_mbps, hsfreq, cif->cif_cfg.lanes);
+
+	csi_mipidphy_wr_ths_settle(hw, hsfreq, CSI2_DPHY_LANE_CLOCK);
+	if (cif->cif_cfg.lanes > 0x00)
+		csi_mipidphy_wr_ths_settle(hw, hsfreq, CSI2_DPHY_LANE_DATA0);
+	if (cif->cif_cfg.lanes > 0x01)
+		csi_mipidphy_wr_ths_settle(hw, hsfreq, CSI2_DPHY_LANE_DATA1);
+	if (cif->cif_cfg.lanes > 0x02)
+		csi_mipidphy_wr_ths_settle(hw, hsfreq, CSI2_DPHY_LANE_DATA2);
+	if (cif->cif_cfg.lanes > 0x03)
+		csi_mipidphy_wr_ths_settle(hw, hsfreq, CSI2_DPHY_LANE_DATA3);
+
+	atomic_inc(&hw->stream_cnt);
+
+	mutex_unlock(&hw->mutex);
+
+	return 0;
+}
+
+static void vehicle_samsung_dcphy_rx_config_settle(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+	const struct hsfreq_range *hsfreq_ranges = NULL;
+	int num_hsfreq_ranges = 0;
+	int i, hsfreq = 0;
+	u32 sot_sync = 0;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		hsfreq_ranges = hw->hsfreq_ranges;
+		num_hsfreq_ranges = hw->num_hsfreq_ranges;
+		sot_sync = 0x03;
+	}
+
+	/* set data lane */
+	for (i = 0; i < num_hsfreq_ranges; i++) {
+		if (hsfreq_ranges[i].range_h >= hw->data_rate_mbps) {
+			hsfreq = hsfreq_ranges[i].cfg_bit;
+			break;
+		}
+	}
+
+	/*clk settle fix to 0x301*/
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY)
+		regmap_write(samsung->regmap, RX_CLK_THS_SETTLE, 0x301);
+
+	if (cif->cif_cfg.lanes > 0x00) {
+		regmap_update_bits(samsung->regmap, RX_LANE0_THS_SETTLE, 0x1ff, hsfreq);
+		regmap_update_bits(samsung->regmap, RX_LANE0_ERR_SOT_SYNC, 0xff, sot_sync);
+	}
+	if (cif->cif_cfg.lanes > 0x01) {
+		regmap_update_bits(samsung->regmap, RX_LANE1_THS_SETTLE, 0x1ff, hsfreq);
+		regmap_update_bits(samsung->regmap, RX_LANE1_ERR_SOT_SYNC, 0xff, sot_sync);
+	}
+	if (cif->cif_cfg.lanes > 0x02) {
+		regmap_update_bits(samsung->regmap, RX_LANE2_THS_SETTLE, 0x1ff, hsfreq);
+		regmap_update_bits(samsung->regmap, RX_LANE2_ERR_SOT_SYNC, 0xff, sot_sync);
+	}
+	if (cif->cif_cfg.lanes > 0x03) {
+		regmap_update_bits(samsung->regmap, RX_LANE3_THS_SETTLE, 0x1ff, hsfreq);
+		regmap_update_bits(samsung->regmap, RX_LANE3_ERR_SOT_SYNC, 0xff, sot_sync);
+	}
+}
+
+static int vehicle_samsung_dcphy_rx_config_common(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+	u32 dlysel = 0;
+	int i = 0;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		if (hw->data_rate_mbps < 1500)
+			dlysel = 0;
+		else if (hw->data_rate_mbps < 2000)
+			dlysel = 3 << 8;
+		else if (hw->data_rate_mbps < 3000)
+			dlysel = 2 << 8;
+		else if (hw->data_rate_mbps < 4000)
+			dlysel = 1 << 8;
+		else if (hw->data_rate_mbps < 6500)
+			dlysel = 0;
+		if (hw->dphy_param->clk_hs_term_sel > 0x7) {
+			dev_err(hw->dev, "clk_hs_term_sel error param %d\n",
+				hw->dphy_param->clk_hs_term_sel);
+			return -EINVAL;
+		}
+		for (i = 0; i < cif->cif_cfg.lanes; i++) {
+			if (hw->dphy_param->data_hs_term_sel[i] > 0x7) {
+				dev_err(hw->dev, "data_hs_term_sel[%d] error param %d\n",
+					i,
+					hw->dphy_param->data_hs_term_sel[i]);
+				return -EINVAL;
+			}
+			if (hw->dphy_param->lp_hys_sw[i] > 0x3) {
+				dev_err(hw->dev, "lp_hys_sw[%d] error param %d\n",
+					i,
+					hw->dphy_param->lp_hys_sw[i]);
+				return -EINVAL;
+			}
+			if (hw->dphy_param->lp_escclk_pol_sel[i] > 0x1) {
+				dev_err(hw->dev, "lp_escclk_pol_sel[%d] error param %d\n",
+					i,
+					hw->dphy_param->lp_escclk_pol_sel[i]);
+				return -EINVAL;
+			}
+			if (hw->dphy_param->skew_data_cal_clk[i] > 0x1f) {
+				dev_err(hw->dev, "skew_data_cal_clk[%d] error param %d\n",
+					i,
+					hw->dphy_param->skew_data_cal_clk[i]);
+				return -EINVAL;
+			}
+		}
+		regmap_write(samsung->regmap, RX_S0C_GNR_CON1, 0x1450);
+		regmap_write(samsung->regmap, RX_S0C_ANA_CON1, 0x8000);
+		regmap_write(samsung->regmap, RX_S0C_ANA_CON2, hw->dphy_param->clk_hs_term_sel);
+		regmap_write(samsung->regmap, RX_S0C_ANA_CON3, 0x0600);
+		if (cif->cif_cfg.lanes > 0x00) {
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_GNR_CON1, 0x1450);
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_ANA_CON1, 0x8000);
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_ANA_CON2, dlysel |
+				     hw->dphy_param->data_hs_term_sel[0]);
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_ANA_CON3, 0x0600 |
+				     (hw->dphy_param->lp_hys_sw[0] << 4) |
+				     (hw->dphy_param->lp_escclk_pol_sel[0] << 11));
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_ANA_CON7, 0x40);
+			regmap_write(samsung->regmap, RX_COMBO_S0D0_DESKEW_CON2,
+				     hw->dphy_param->skew_data_cal_clk[0]);
+			if (hw->data_rate_mbps >= 1500 &&
+				cif->chip_id >= CHIP_RK3576_VEHICLE_CIF) {
+				regmap_write(samsung->regmap, RX_COMBO_S0D0_DESKEW_CON0, BIT(0));
+				regmap_write(samsung->regmap, RX_COMBO_S0D0_DESKEW_CON4, 0x81A);
+			}
+		}
+		if (cif->cif_cfg.lanes > 0x01) {
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_GNR_CON1, 0x1450);
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_ANA_CON1, 0x8000);
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_ANA_CON2, dlysel |
+				     hw->dphy_param->data_hs_term_sel[1]);
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_ANA_CON3, 0x0600 |
+				     (hw->dphy_param->lp_hys_sw[1] << 4) |
+				     (hw->dphy_param->lp_escclk_pol_sel[1] << 11));
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_ANA_CON7, 0x40);
+			regmap_write(samsung->regmap, RX_COMBO_S0D1_DESKEW_CON2,
+				     hw->dphy_param->skew_data_cal_clk[1]);
+			if (hw->data_rate_mbps >= 1500 &&
+				cif->chip_id >= CHIP_RK3576_VEHICLE_CIF) {
+				regmap_write(samsung->regmap, RX_COMBO_S0D1_DESKEW_CON0, BIT(0));
+				regmap_write(samsung->regmap, RX_COMBO_S0D1_DESKEW_CON4, 0x81A);
+			}
+		}
+		if (cif->cif_cfg.lanes > 0x02) {
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_GNR_CON1, 0x1450);
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_ANA_CON1, 0x8000);
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_ANA_CON2, dlysel |
+				     hw->dphy_param->data_hs_term_sel[2]);
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_ANA_CON3, 0x0600 |
+				     (hw->dphy_param->lp_hys_sw[2] << 4) |
+				     (hw->dphy_param->lp_escclk_pol_sel[2] << 11));
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_ANA_CON7, 0x40);
+			regmap_write(samsung->regmap, RX_COMBO_S0D2_DESKEW_CON2,
+				     hw->dphy_param->skew_data_cal_clk[2]);
+			if (hw->data_rate_mbps >= 1500 &&
+				cif->chip_id >= CHIP_RK3576_VEHICLE_CIF) {
+				regmap_write(samsung->regmap, RX_COMBO_S0D2_DESKEW_CON0, BIT(0));
+				regmap_write(samsung->regmap, RX_COMBO_S0D2_DESKEW_CON4, 0x81A);
+			}
+		}
+		if (cif->cif_cfg.lanes > 0x03) {
+			regmap_write(samsung->regmap, RX_S0D3_GNR_CON1, 0x1450);
+			regmap_write(samsung->regmap, RX_S0D3_ANA_CON1, 0x8000);
+			regmap_write(samsung->regmap, RX_S0D3_ANA_CON2, dlysel |
+				     hw->dphy_param->data_hs_term_sel[3]);
+			regmap_write(samsung->regmap, RX_S0D3_ANA_CON3, 0x0600 |
+				     (hw->dphy_param->lp_hys_sw[3] << 4) |
+				     (hw->dphy_param->lp_escclk_pol_sel[3] << 11));
+			regmap_write(samsung->regmap, RX_S0D3_DESKEW_CON2,
+				     hw->dphy_param->skew_data_cal_clk[3]);
+			if (hw->data_rate_mbps >= 1500 &&
+				cif->chip_id >= CHIP_RK3576_VEHICLE_CIF) {
+				regmap_write(samsung->regmap, RX_S0D3_DESKEW_CON0, BIT(0));
+				regmap_write(samsung->regmap, RX_S0D3_DESKEW_CON4, 0x81A);
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int vehicle_samsung_dcphy_rx_lane_enable(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+	u32 sts;
+	int ret = 0;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY)
+		regmap_update_bits(samsung->regmap, RX_CLK_LANE_ENABLE, PHY_ENABLE, PHY_ENABLE);
+
+	if (cif->cif_cfg.lanes > 0x00)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE0_ENABLE, PHY_ENABLE, PHY_ENABLE);
+	if (cif->cif_cfg.lanes > 0x01)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE1_ENABLE, PHY_ENABLE, PHY_ENABLE);
+	if (cif->cif_cfg.lanes > 0x02)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE2_ENABLE, PHY_ENABLE, PHY_ENABLE);
+	if (cif->cif_cfg.lanes > 0x03)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE3_ENABLE, PHY_ENABLE, PHY_ENABLE);
+
+	/*wait for clk lane ready*/
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		ret = regmap_read_poll_timeout(samsung->regmap, RX_CLK_LANE_ENABLE,
+				       sts, (sts & PHY_READY), 200, 4000);
+		if (ret < 0) {
+			dev_err(samsung->dev, "phy rx clk lane is not locked\n");
+			return -EINVAL;
+		}
+	}
+
+	/*wait for data lane ready*/
+	if (cif->cif_cfg.lanes > 0x00) {
+		ret = regmap_read_poll_timeout(samsung->regmap, RX_DATA_LANE0_ENABLE,
+				       sts, (sts & PHY_READY), 200, 2000);
+		if (ret < 0) {
+			dev_err(samsung->dev, "phy rx data lane 0 is not locked\n");
+			return -EINVAL;
+		}
+	}
+	if (cif->cif_cfg.lanes > 0x01) {
+		ret = regmap_read_poll_timeout(samsung->regmap, RX_DATA_LANE1_ENABLE,
+				       sts, (sts & PHY_READY), 200, 2000);
+		if (ret < 0) {
+			dev_err(samsung->dev, "phy rx data lane 1 is not locked\n");
+			return -EINVAL;
+		}
+	}
+	if (cif->cif_cfg.lanes > 0x02) {
+		ret = regmap_read_poll_timeout(samsung->regmap, RX_DATA_LANE2_ENABLE,
+				       sts, (sts & PHY_READY), 200, 2000);
+		if (ret < 0) {
+			dev_err(samsung->dev, "phy rx data lane 2 is not locked\n");
+			return -EINVAL;
+		}
+	}
+
+	if (cif->cif_cfg.lanes > 0x03) {
+		ret = regmap_read_poll_timeout(samsung->regmap, RX_DATA_LANE3_ENABLE,
+				       sts, (sts & PHY_READY), 200, 2000);
+		if (ret < 0) {
+			dev_err(samsung->dev, "phy rx data lane 3 is not locked\n");
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+static void vehicle_samsung_mipi_dcphy_bias_block_enable(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+	struct csi2_dphy_hw *csi_dphy = samsung->dphy_vehicle[0];
+	u32 bias_con2 = 0x3223;
+
+	if (csi_dphy &&
+	    csi_dphy->dphy_param->lp_vol_ref != 3 &&
+	    csi_dphy->dphy_param->lp_vol_ref < 0x7) {
+		bias_con2 &= 0xfffffff8;
+		bias_con2 |= csi_dphy->dphy_param->lp_vol_ref;
+		dev_info(samsung->dev,
+			 "rx change lp_vol_ref to %d, it may cause tx exception\n",
+			 csi_dphy->dphy_param->lp_vol_ref);
+	}
+	regmap_write(samsung->regmap, BIAS_CON0, 0x0010);
+	regmap_write(samsung->regmap, BIAS_CON1, 0x0110);
+	regmap_write(samsung->regmap, BIAS_CON2, bias_con2);
+
+	/* default output voltage select:
+	 * dphy: 400mv
+	 * cphy: 530mv
+	 */
+	if (samsung->c_option)
+		regmap_update_bits(samsung->regmap, BIAS_CON4,
+				   I_MUX_SEL_MASK, I_MUX_SEL(2));
+}
+
+static int vehicle_csi2_dcphy_stream_start(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+	int ret = 0;
+
+	dev_info(hw->dev, "mipi dcphy stream on\n");
+	mutex_lock(&hw->mutex);
+
+	if (samsung->s_phy_rst)
+		reset_control_assert(samsung->s_phy_rst);
+
+	vehicle_samsung_mipi_dcphy_bias_block_enable(cif);
+	ret = vehicle_samsung_dcphy_rx_config_common(cif);
+	if (ret)
+		goto out_streamon;
+
+	vehicle_samsung_dcphy_rx_config_settle(cif);
+
+	ret = vehicle_samsung_dcphy_rx_lane_enable(cif);
+	if (ret)
+		goto out_streamon;
+
+	if (samsung->s_phy_rst)
+		reset_control_deassert(samsung->s_phy_rst);
+	atomic_inc(&hw->stream_cnt);
+	mutex_unlock(&hw->mutex);
+
+	return 0;
+
+out_streamon:
+	if (samsung->s_phy_rst)
+		reset_control_deassert(samsung->s_phy_rst);
+	mutex_unlock(&hw->mutex);
+	dev_err(hw->dev, "stream on error\n");
+	return -EINVAL;
+}
+
+static int vehicle_csi2_dcphy_stream_stop(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct samsung_mipi_dcphy *samsung = hw->samsung_phy;
+
+	dev_info(hw->dev, "mipi dcphy stream off\n");
+	if (atomic_dec_return(&hw->stream_cnt))
+		return 0;
+
+	mutex_lock(&hw->mutex);
+
+	if (samsung->s_phy_rst)
+		reset_control_assert(samsung->s_phy_rst);
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY)
+		regmap_update_bits(samsung->regmap, RX_CLK_LANE_ENABLE, PHY_ENABLE, 0);
+
+	if (cif->cif_cfg.lanes > 0x00)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE0_ENABLE, PHY_ENABLE, 0);
+	if (cif->cif_cfg.lanes > 0x01)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE1_ENABLE, PHY_ENABLE, 0);
+	if (cif->cif_cfg.lanes > 0x02)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE2_ENABLE, PHY_ENABLE, 0);
+	if (cif->cif_cfg.lanes > 0x03)
+		regmap_update_bits(samsung->regmap, RX_DATA_LANE3_ENABLE, PHY_ENABLE, 0);
+
+	if (samsung->s_phy_rst)
+		reset_control_deassert(samsung->s_phy_rst);
+	usleep_range(500, 1000);
+
+	mutex_unlock(&hw->mutex);
+
+	return 0;
+}
+
+static void vehicle_csi2_disable(struct vehicle_cif *cif)
+{
+	void __iomem *base = cif->csi2_base;
+
+	vehicle_write_csihost_reg(base, CSIHOST_RESETN, 0);
+	vehicle_write_csihost_reg(base, CSIHOST_MSK1, 0xffffffff);
+	vehicle_write_csihost_reg(base, CSIHOST_MSK2, 0xffffffff);
+}
+
+static void vehicle_csi2_enable(struct vehicle_cif *cif,
+				enum host_type_t host_type)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	void __iomem *base = hw->csi2_base;
+	int lanes = cif->cif_cfg.lanes;
+
+	vehicle_write_csihost_reg(base, CSIHOST_N_LANES, lanes - 1);
+
+	if (host_type == RK_DSI_RXHOST) {
+		vehicle_write_csihost_reg(base, CSIHOST_CONTROL,
+				  SW_CPHY_EN(0) | SW_DSI_EN(1) |
+				  SW_DATATYPE_FS(0x01) | SW_DATATYPE_FE(0x11) |
+				  SW_DATATYPE_LS(0x21) | SW_DATATYPE_LE(0x31));
+		/* Disable some error interrupt when HOST work on DSI RX mode */
+		vehicle_write_csihost_reg(base, CSIHOST_MSK1, 0xe00000f0);
+		vehicle_write_csihost_reg(base, CSIHOST_MSK2, 0xff00);
+	} else {
+		vehicle_write_csihost_reg(base, CSIHOST_CONTROL,
+				  SW_CPHY_EN(0) | SW_DSI_EN(0) |
+				  SW_DATATYPE_FS(0x0) | SW_DATATYPE_FE(0x01) |
+				  SW_DATATYPE_LS(0x02) | SW_DATATYPE_LE(0x03));
+		vehicle_write_csihost_reg(base, CSIHOST_MSK1, 0);
+		vehicle_write_csihost_reg(base, CSIHOST_MSK2, 0xf000);
+	}
+
+	vehicle_write_csihost_reg(base, CSIHOST_RESETN, 1);
+}
+
+static int vehicle_csi2_stream_start(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	enum host_type_t host_type;
+	int i;
+
+	host_type = RK_CSI_RXHOST;
+	vehicle_csi2_enable(cif, host_type);
+	for (i = 0; i < RK_CSI2_ERR_MAX; i++)
+		hw->err_list[i].cnt = 0;
+
+	return 0;
+}
+
+static void vehicle_cif_csi_get_vc_num(struct vehicle_cif *cif)
+{
+	int vc_num = 0;
+	unsigned int mbus_flags = cif->cif_cfg.mbus_flags;
+
+	for (vc_num = 0; vc_num < RKCIF_MAX_CSI_CHANNEL; vc_num++) {
+		if (mbus_flags & V4L2_MBUS_CSI2_CHANNEL_0) {
+			cif->channels[vc_num].vc = vc_num;
+			mbus_flags ^= V4L2_MBUS_CSI2_CHANNEL_0;
+			continue;
+		}
+		if (mbus_flags & V4L2_MBUS_CSI2_CHANNEL_1) {
+			cif->channels[vc_num].vc = vc_num;
+			mbus_flags ^= V4L2_MBUS_CSI2_CHANNEL_1;
+			continue;
+		}
+
+		if (mbus_flags & V4L2_MBUS_CSI2_CHANNEL_2) {
+			cif->channels[vc_num].vc = vc_num;
+			mbus_flags ^= V4L2_MBUS_CSI2_CHANNEL_2;
+			continue;
+		}
+
+		if (mbus_flags & V4L2_MBUS_CSI2_CHANNEL_3) {
+			cif->channels[vc_num].vc = vc_num;
+			mbus_flags ^= V4L2_MBUS_CSI2_CHANNEL_3;
+			continue;
+		}
+	}
+
+	cif->num_channels = vc_num ? (vc_num - 1) : 1;
+	if (cif->num_channels == 1)
+		cif->channels[0].vc = 0;
+}
+
+static const struct
+cif_input_fmt *find_input_fmt(u32 mbus_code)
+{
+	const struct cif_input_fmt *fmt;
+	u32 i;
+
+	for (i = 0; i < ARRAY_SIZE(in_fmts); i++) {
+		fmt = &in_fmts[i];
+		if (mbus_code == fmt->mbus_code)
+			return fmt;
+	}
+
+	return NULL;
+}
+
+static const struct
+cif_output_fmt *find_output_fmt(u32 pixelfmt)
+{
+	const struct cif_output_fmt *fmt;
+	u32 i;
+
+	for (i = 0; i < ARRAY_SIZE(out_fmts); i++) {
+		fmt = &out_fmts[i];
+		if (fmt->fourcc == pixelfmt)
+			return fmt;
+	}
+
+	return NULL;
+}
+
+static enum cif_reg_index get_reg_index_of_id_ctrl0(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_ID0_CTRL0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_ID1_CTRL0;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_ID2_CTRL0;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_ID3_CTRL0;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_ID0_CTRL0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_id_ctrl1(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_ID0_CTRL1;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_ID1_CTRL1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_ID2_CTRL1;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_ID3_CTRL1;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_ID0_CTRL1;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm0_y_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm_num(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_FRAME_NUM_VC0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_FRAME_NUM_VC1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_FRAME_NUM_VC2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_FRAME_NUM_VC3;
+		break;
+	default:
+		index = CIF_REG_MIPI_FRAME_NUM_VC0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm1_y_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm0_uv_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm1_uv_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm0_y_vlw(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm1_y_vlw(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm0_uv_vlw(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_frm1_uv_vlw(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_reg_index_of_id_crop_start(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_MIPI_LVDS_ID0_CROP_START;
+		break;
+	case 1:
+		index = CIF_REG_MIPI_LVDS_ID1_CROP_START;
+		break;
+	case 2:
+		index = CIF_REG_MIPI_LVDS_ID2_CROP_START;
+		break;
+	case 3:
+		index = CIF_REG_MIPI_LVDS_ID3_CROP_START;
+		break;
+	default:
+		index = CIF_REG_MIPI_LVDS_ID0_CROP_START;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_dvp_reg_index_of_frm0_y_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_DVP_FRM0_ADDR_Y;
+		break;
+	case 1:
+		index = CIF_REG_DVP_FRM0_ADDR_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_DVP_FRM0_ADDR_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_DVP_FRM0_ADDR_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_DVP_FRM0_ADDR_Y;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_dvp_reg_index_of_frm1_y_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_DVP_FRM1_ADDR_Y;
+		break;
+	case 1:
+		index = CIF_REG_DVP_FRM1_ADDR_Y_ID1;
+		break;
+	case 2:
+		index = CIF_REG_DVP_FRM1_ADDR_Y_ID2;
+		break;
+	case 3:
+		index = CIF_REG_DVP_FRM1_ADDR_Y_ID3;
+		break;
+	default:
+		index = CIF_REG_DVP_FRM0_ADDR_Y;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_dvp_reg_index_of_frm0_uv_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_DVP_FRM0_ADDR_UV;
+		break;
+	case 1:
+		index = CIF_REG_DVP_FRM0_ADDR_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_DVP_FRM0_ADDR_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_DVP_FRM0_ADDR_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_DVP_FRM0_ADDR_UV;
+		break;
+	}
+
+	return index;
+}
+
+static enum cif_reg_index get_dvp_reg_index_of_frm1_uv_addr(int channel_id)
+{
+	enum cif_reg_index index;
+
+	switch (channel_id) {
+	case 0:
+		index = CIF_REG_DVP_FRM1_ADDR_UV;
+		break;
+	case 1:
+		index = CIF_REG_DVP_FRM1_ADDR_UV_ID1;
+		break;
+	case 2:
+		index = CIF_REG_DVP_FRM1_ADDR_UV_ID2;
+		break;
+	case 3:
+		index = CIF_REG_DVP_FRM1_ADDR_UV_ID3;
+		break;
+	default:
+		index = CIF_REG_DVP_FRM1_ADDR_UV;
+		break;
+	}
+
+	return index;
+}
+
+static unsigned char get_data_type(u32 pixelformat, u8 cmd_mode_en)
+{
+	switch (pixelformat) {
+	/* csi raw8 */
+	case MEDIA_BUS_FMT_SBGGR8_1X8:
+	case MEDIA_BUS_FMT_SGBRG8_1X8:
+	case MEDIA_BUS_FMT_SGRBG8_1X8:
+	case MEDIA_BUS_FMT_SRGGB8_1X8:
+		return 0x2a;
+	/* csi raw10 */
+	case MEDIA_BUS_FMT_SBGGR10_1X10:
+	case MEDIA_BUS_FMT_SGBRG10_1X10:
+	case MEDIA_BUS_FMT_SGRBG10_1X10:
+	case MEDIA_BUS_FMT_SRGGB10_1X10:
+		return 0x2b;
+	/* csi raw12 */
+	case MEDIA_BUS_FMT_SBGGR12_1X12:
+	case MEDIA_BUS_FMT_SGBRG12_1X12:
+	case MEDIA_BUS_FMT_SGRBG12_1X12:
+	case MEDIA_BUS_FMT_SRGGB12_1X12:
+		return 0x2c;
+	/* csi uyvy 422 */
+	case MEDIA_BUS_FMT_UYVY8_2X8:
+	case MEDIA_BUS_FMT_VYUY8_2X8:
+	case MEDIA_BUS_FMT_YUYV8_2X8:
+	case MEDIA_BUS_FMT_YVYU8_2X8:
+		return 0x1e;
+	case MEDIA_BUS_FMT_RGB888_1X24: {
+		if (cmd_mode_en) /* dsi command mode*/
+			return 0x39;
+		else /* dsi video mode */
+			return 0x3e;
+	}
+
+	default:
+		return 0x2b;
+	}
+}
+
+#define UV_OFFSET (cif->cif_cfg.width * cif->cif_cfg.height)
+
+static int vehicle_cif_init_buffer(struct vehicle_cif *cif,
+					     int init, int csi_ch)
+{
+	struct vehicle_rkcif_dummy_buffer *dummy_buf = &cif->dummy_buf;
+	u32 frm0_addr_y, frm0_addr_uv;
+	u32 frm1_addr_y, frm1_addr_uv;
+	unsigned long y_addr, uv_addr;
+	int i;
+
+	if (cif->cif_cfg.buf_num < 2)
+		return -EINVAL;
+
+	if (cif->cif_cfg.buf_num > MAX_BUF_NUM)
+		cif->cif_cfg.buf_num = MAX_BUF_NUM;
+
+	for (i = 0 ; i < cif->cif_cfg.buf_num; i++) {
+		cif->frame_buf[i] = cif->cif_cfg.buf_phy_addr[i];
+		if (cif->frame_buf[i] == 0)
+			return -EINVAL;
+	}
+
+	cif->last_buf_index = 0;
+	cif->current_buf_index = 1;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		frm0_addr_y = get_reg_index_of_frm0_y_addr(csi_ch);
+		frm0_addr_uv = get_reg_index_of_frm0_uv_addr(csi_ch);
+		frm1_addr_y = get_reg_index_of_frm1_y_addr(csi_ch);
+		frm1_addr_uv = get_reg_index_of_frm1_uv_addr(csi_ch);
+	} else {
+		frm0_addr_y = get_dvp_reg_index_of_frm0_y_addr(csi_ch);
+		frm0_addr_uv = get_dvp_reg_index_of_frm0_uv_addr(csi_ch);
+		frm1_addr_y = get_dvp_reg_index_of_frm1_y_addr(csi_ch);
+		frm1_addr_uv = get_dvp_reg_index_of_frm1_uv_addr(csi_ch);
+	}
+
+	spin_lock(&cif->vbq_lock);
+
+	y_addr = vehicle_flinger_request_cif_buffer();
+	if (y_addr) {
+		uv_addr = y_addr + UV_OFFSET;
+		rkcif_write_reg(cif, frm0_addr_y, y_addr);
+		rkcif_write_reg(cif, frm0_addr_uv, uv_addr);
+		cif->active[0] = y_addr;
+	} else {
+		rkcif_write_reg(cif, frm0_addr_y, dummy_buf->dma_addr);
+		rkcif_write_reg(cif, frm0_addr_uv, dummy_buf->dma_addr);
+		cif->active[0] = y_addr;
+	}
+
+	y_addr = vehicle_flinger_request_cif_buffer();
+	if (y_addr) {
+		uv_addr = y_addr + UV_OFFSET;
+		rkcif_write_reg(cif, frm1_addr_y, y_addr);
+		rkcif_write_reg(cif, frm1_addr_uv, uv_addr);
+		cif->active[1] = y_addr;
+	} else {
+		rkcif_write_reg(cif, frm1_addr_y, dummy_buf->dma_addr);
+		rkcif_write_reg(cif, frm1_addr_uv, dummy_buf->dma_addr);
+		cif->active[1] = y_addr;
+	}
+
+	if (cif->cif_cfg.type != V4L2_MBUS_CSI2_DPHY) {
+		int ch_id;
+
+		for (ch_id = 0; ch_id < 4; ch_id++) {
+			if (ch_id == csi_ch)
+				continue;
+
+			rkcif_write_reg(cif, get_dvp_reg_index_of_frm0_y_addr(ch_id),
+					dummy_buf->dma_addr);
+			rkcif_write_reg(cif, get_dvp_reg_index_of_frm1_y_addr(ch_id),
+					dummy_buf->dma_addr);
+			rkcif_write_reg(cif, get_dvp_reg_index_of_frm0_uv_addr(ch_id),
+					dummy_buf->dma_addr);
+			rkcif_write_reg(cif, get_dvp_reg_index_of_frm1_uv_addr(ch_id),
+					dummy_buf->dma_addr);
+		}
+	}
+
+	spin_unlock(&cif->vbq_lock);
+
+	return 0;
+}
+
+static int vehicle_cif_csi_channel_init(struct vehicle_cif *cif,
+		   struct vehicle_csi_channel_info *channel)
+{
+	struct vehicle_cfg *cfg = &cif->cif_cfg;
+	const struct cif_output_fmt *fmt;
+	u32 fourcc;
+
+	channel->enable = 1;
+	channel->width = cfg->width;
+	channel->height = cfg->height;
+	cif->interlaced_enable = false;
+	channel->cmd_mode_en = 0; /* default use DSI Video Mode */
+
+	channel->crop_en = 1;
+	channel->crop_st_x = cfg->start_x;
+	channel->crop_st_y = cfg->start_y;
+	channel->width = cfg->width;
+	channel->height = cfg->height;
+	if (cfg->output_format == CIF_OUTPUT_FORMAT_420) {
+		fmt = find_output_fmt(V4L2_PIX_FMT_NV12);
+		if (!fmt) {
+			VEHICLE_DGERR("can not find output format: 0x%x", V4L2_PIX_FMT_NV12);
+			return -EINVAL;
+		}
+	} else {
+		fmt = find_output_fmt(V4L2_PIX_FMT_NV16);
+		if (!fmt) {
+			VEHICLE_DGERR("can not find output format: 0x%x", V4L2_PIX_FMT_NV16);
+			return -EINVAL;
+		}
+	}
+
+	channel->fmt_val = fmt->csi_fmt_val;
+	VEHICLE_INFO("%s, LINE=%d, channel->fmt_val = 0x%x, fmt->csi_fmt_val= 0x%x",
+				__func__, __LINE__, channel->fmt_val, fmt->csi_fmt_val);
+	/*
+	 * for mipi or lvds, when enable compact, the virtual width of raw10/raw12
+	 * needs aligned with :ALIGN(bits_per_pixel * width / 8, 8), if enable 16bit mode
+	 * needs aligned with :ALIGN(bits_per_pixel * width * 2, 8), to optimize reading and
+	 * writing of ddr, aligned with 256
+	 */
+
+	if (fmt->fmt_type == CIF_FMT_TYPE_RAW && channel->fmt_val != CSI_WRDDR_TYPE_RAW8)
+		channel->virtual_width = ALIGN(channel->width * 2, 8);
+	else
+		channel->virtual_width = ALIGN(channel->width * fmt->bpp[0] / 8, 8);
+
+	if (channel->fmt_val == CSI_WRDDR_TYPE_RGB888)
+		channel->width = channel->width * fmt->bpp[0] / 8;
+	/*
+	 * rk cif don't support output yuyv fmt data
+	 * if user request yuyv fmt, the input mode must be RAW8
+	 * and the width is double Because the real input fmt is
+	 * yuyv
+	 */
+	fourcc = fmt->fourcc;
+	if (fourcc == V4L2_PIX_FMT_YUYV || fourcc == V4L2_PIX_FMT_YVYU ||
+	    fourcc == V4L2_PIX_FMT_UYVY || fourcc == V4L2_PIX_FMT_VYUY) {
+		channel->fmt_val = CSI_WRDDR_TYPE_RAW8;
+		channel->width *= 2;
+		channel->virtual_width *= 2;
+	}
+	VEHICLE_DG("%s, LINE=%d, channel->fmt_val = 0x%x", __func__, __LINE__, channel->fmt_val);
+	if (cfg->input_format == CIF_INPUT_FORMAT_PAL ||
+		cfg->input_format == CIF_INPUT_FORMAT_NTSC) {
+		VEHICLE_INFO("CVBS IN PAL or NTSC config.");
+		cif->interlaced_enable = true;
+		if (!cif->use_hw_interlace) {
+			channel->virtual_width *= 2;
+			cif->interlaced_offset = channel->width;
+			cif->interlaced_counts = 0;
+			cif->interlaced_buffer = 0;
+			channel->height /= 2;
+		}
+
+		VEHICLE_INFO("do denterlaced.\n");
+	}
+
+	channel->data_type = get_data_type(cfg->mbus_code,
+					   channel->cmd_mode_en);
+
+	return 0;
+}
+
+static int vehicle_cif_csi_channel_set(struct vehicle_cif *cif,
+				       struct vehicle_csi_channel_info *channel,
+				       enum v4l2_mbus_type mbus_type)
+
+{
+	unsigned int val = 0x0;
+
+	if (channel->id >= 4)
+		return -EINVAL;
+
+	if (!channel->enable) {
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(channel->id),
+				CSI_DISABLE_CAPTURE);
+		return 0;
+	}
+
+	rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTSTAT,
+				~(CSI_START_INTSTAT(channel->id) |
+				CSI_DMA_END_INTSTAT(channel->id) |
+				CSI_LINE_INTSTAT(channel->id)));
+
+	/* 0. need set CIF_CSI_INTEN to 0x0 first */
+	rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTEN, 0x0);
+
+	/* enable id0 frame start int for sof(long frame, for hdr)
+	 * vehicle don't need this
+	 */
+	if (channel->id == RKCIF_STREAM_MIPI_ID0)
+		rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+					CSI_START_INTEN(channel->id));
+
+	rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1,
+			     0x3fff << 16 | 0x3fff);
+	rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3,
+			     0x3fff << 16 | 0x3fff);
+
+	rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+				CSI_DMA_END_INTEN(channel->id));
+
+	rkcif_write_reg(cif, CIF_REG_MIPI_WATER_LINE,
+			     CIF_MIPI_LVDS_SW_WATER_LINE_25_RK1808 |
+			     CIF_MIPI_LVDS_SW_WATER_LINE_ENABLE_RK1808 |
+			     CIF_MIPI_LVDS_SW_HURRY_VALUE_RK1808(0x3) |
+			     CIF_MIPI_LVDS_SW_HURRY_ENABLE_RK1808);
+
+	val = CIF_MIPI_LVDS_SW_PRESS_VALUE(0x3) |
+		CIF_MIPI_LVDS_SW_PRESS_ENABLE |
+		CIF_MIPI_LVDS_SW_HURRY_VALUE(0x3) |
+		CIF_MIPI_LVDS_SW_HURRY_ENABLE |
+		CIF_MIPI_LVDS_SW_WATER_LINE_25 |
+		CIF_MIPI_LVDS_SW_WATER_LINE_ENABLE;
+
+	val &= ~CIF_MIPI_LVDS_SW_SEL_LVDS;
+
+	rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_CTRL, val);
+
+	rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+				CSI_ALL_ERROR_INTEN);
+
+	rkcif_write_reg(cif, get_reg_index_of_id_ctrl1(channel->id),
+			     channel->width | (channel->height << 16));
+
+	rkcif_write_reg(cif, get_reg_index_of_frm0_y_vlw(channel->id),
+			     channel->virtual_width);
+	rkcif_write_reg(cif, get_reg_index_of_frm1_y_vlw(channel->id),
+			     channel->virtual_width);
+	rkcif_write_reg(cif, get_reg_index_of_frm0_uv_vlw(channel->id),
+			     channel->virtual_width);
+	rkcif_write_reg(cif, get_reg_index_of_frm1_uv_vlw(channel->id),
+			     channel->virtual_width);
+
+	if (channel->crop_en)
+		rkcif_write_reg(cif, get_reg_index_of_id_crop_start(channel->id),
+				     channel->crop_st_y << 16 | channel->crop_st_x);
+
+	return 0;
+}
+
+/*config reg for rk3588*/
+static int vehicle_cif_csi_channel_set_v1(struct vehicle_cif *cif,
+				       struct vehicle_csi_channel_info *channel,
+				       enum v4l2_mbus_type mbus_type)
+{
+	unsigned int val = 0x0;
+
+	if (channel->id >= 4)
+		return -EINVAL;
+
+	if (!channel->enable) {
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(channel->id),
+				CSI_DISABLE_CAPTURE);
+		return 0;
+	}
+
+	rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTSTAT,
+				 ~(CSI_START_INTSTAT(channel->id) |
+				 CSI_DMA_END_INTSTAT(channel->id) |
+				 CSI_LINE_INTSTAT_V1(channel->id)));
+
+	/* enable id0 frame start int for sof(long frame, for hdr)
+	 * vehicle don't need this
+	 */
+	if (channel->id == RKCIF_STREAM_MIPI_ID0)
+		rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+					CSI_START_INTEN(channel->id));
+
+	rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1,
+			     0x3fff << 16 | 0x3fff);
+	rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3,
+			     0x3fff << 16 | 0x3fff);
+
+	rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+				CSI_DMA_END_INTEN(channel->id));
+
+	if (cif->chip_id > CHIP_RK3562_VEHICLE_CIF) {
+		val = CIF_MIPI_LVDS_SW_WATER_LINE_ENABLE |
+			(CIF_MIPI_LVDS_SW_WATER_LINE_25 << 19);
+
+		rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_CTRL, val);
+	} else {
+		val = CIF_MIPI_LVDS_SW_PRESS_VALUE_RK3588(0x3) |
+			CIF_MIPI_LVDS_SW_PRESS_ENABLE |
+			CIF_MIPI_LVDS_SW_HURRY_VALUE_RK3588(0x3) |
+			CIF_MIPI_LVDS_SW_HURRY_ENABLE |
+			CIF_MIPI_LVDS_SW_WATER_LINE_25 |
+			CIF_MIPI_LVDS_SW_WATER_LINE_ENABLE;
+
+		rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_CTRL, val);
+	}
+
+	rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTEN,
+				CSI_ALL_ERROR_INTEN_V1);
+
+	if (cif->chip_id < CHIP_RK3576_VEHICLE_CIF)
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl1(channel->id),
+				     (channel->width) | (channel->height << 16));
+	else
+		rkcif_write_reg(cif, CIF_REG_MIPI_SET_SIZE_ID0 + channel->id,
+				     (channel->width) | (channel->height << 16));
+
+	rkcif_write_reg(cif, get_reg_index_of_frm0_y_vlw(channel->id),
+			     channel->virtual_width);
+
+	if (channel->crop_en)
+		rkcif_write_reg(cif, get_reg_index_of_id_crop_start(channel->id),
+				     channel->crop_st_y << 16 | channel->crop_st_x);
+
+	return 0;
+}
+
+static int vehicle_cif_stream_start(struct vehicle_cif *cif)
+{
+	struct vehicle_csi_channel_info *channel;
+
+	vehicle_cif_csi_get_vc_num(cif);
+
+	/* just need init virtual channel 0 */
+	channel = &cif->channels[0];
+	channel->id = 0;
+	vehicle_cif_csi_channel_init(cif, channel);
+	if (cif->chip_id < CHIP_RK3588_VEHICLE_CIF)
+		vehicle_cif_csi_channel_set(cif, channel, V4L2_MBUS_CSI2_DPHY);
+	else
+		vehicle_cif_csi_channel_set_v1(cif, channel, V4L2_MBUS_CSI2_DPHY);
+
+	return 0;
+}
+
+static int cif_csi_stream_setup(struct vehicle_cif *cif)
+{
+	vehicle_csi2_stream_start(cif);
+	if (cif->dphy_hw->chip_id == CHIP_ID_RK3588_DCPHY)
+		vehicle_csi2_dcphy_stream_start(cif);
+	else
+		vehicle_csi2_dphy_stream_start(cif);
+	vehicle_cif_stream_start(cif);
+
+	return 0;
+}
+
+static void vehicle_csi2_dphy_hw_do_reset(struct vehicle_cif  *cif)
+{
+	unsigned int i;
+	struct csi2_dphy_hw *dphy_hw = cif->dphy_hw;
+
+	for (i = 0; i < dphy_hw->num_dphy_rsts; i++)
+		if (dphy_hw->dphy_rst[i])
+			reset_control_assert(dphy_hw->dphy_rst[i]);
+	udelay(5);
+	for (i = 0; i < dphy_hw->num_dphy_rsts; i++)
+		if (dphy_hw->dphy_rst[i])
+			reset_control_deassert(dphy_hw->dphy_rst[i]);
+}
+
+static void vehicle_csi2_hw_soft_reset(struct vehicle_cif  *cif)
+{
+	unsigned int i;
+	struct csi2_dphy_hw *dphy_hw = cif->dphy_hw;
+
+	for (i = 0; i < dphy_hw->num_csi2_rsts; i++)
+		if (dphy_hw->csi2_rst[i])
+			reset_control_assert(dphy_hw->csi2_rst[i]);
+	udelay(5);
+	for (i = 0; i < dphy_hw->num_csi2_rsts; i++)
+		if (dphy_hw->csi2_rst[i])
+			reset_control_deassert(dphy_hw->csi2_rst[i]);
+}
+
+static int vehicle_csi2_dphy_stream_stop(struct vehicle_cif *cif)
+{
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+
+	mutex_lock(&hw->mutex);
+
+	write_csi2_dphy_reg(hw, CSI2PHY_REG_CTRL_LANE_ENABLE, 0x01);
+	if (cif->dphy_hw->chip_id >= CHIP_ID_RK3588)
+		vehicle_csi2_dphy_hw_do_reset(cif);
+	usleep_range(500, 1000);
+
+	mutex_unlock(&hw->mutex);
+
+	return 0;
+}
+
+static void vehicle_rkcif_disable_sys_clk(struct rk_cif_clk *clk)
+{
+	int i;
+
+	for (i = clk->clks_num - 1; i >= 0; i--)
+		clk_disable_unprepare(clk->clks[i]);
+}
+
+static int vehicle_rkcif_enable_sys_clk(struct rk_cif_clk *clk)
+{
+	int i, ret = -EINVAL;
+
+	for (i = 0; i < clk->clks_num; i++) {
+		ret = clk_prepare_enable(clk->clks[i]);
+		if (ret < 0)
+			goto err;
+	}
+
+	return 0;
+err:
+	for (--i; i >= 0; --i)
+		clk_disable_unprepare(clk->clks[i]);
+
+	return ret;
+}
+
+static int rk_cif_mclk_ctrl(struct vehicle_cif *cif, int on)
+{
+	int err = 0;
+
+	struct rk_cif_clk *clk = &cif->clk;
+
+	if (on && !clk->on) {
+		vehicle_rkcif_enable_sys_clk(clk);
+		clk->on = true;
+	} else if (!on && clk->on) {
+		vehicle_rkcif_disable_sys_clk(clk);
+		clk->on = false;
+	}
+
+	return err;
+}
+
+static void csi2_disable_dphy_clk(struct csi2_dphy_hw *hw)
+{
+	int i;
+
+	for (i = hw->num_dphy_clks - 1; i >= 0; i--) {
+		clk_disable_unprepare(hw->dphy_clks[i].clk);
+		VEHICLE_INFO("%s(%d) disable dphy clk: %s\n",
+			__func__, __LINE__, hw->dphy_clks[i].id);
+	}
+}
+
+static int csi2_enable_dphy_clk(struct csi2_dphy_hw *hw)
+{
+	int i, ret = -EINVAL;
+
+	for (i = 0; i < hw->num_dphy_clks; i++) {
+		ret = clk_prepare_enable(hw->dphy_clks[i].clk);
+		if (ret < 0)
+			goto err;
+		VEHICLE_INFO("%s(%d) enable dphy clk: %s\n",
+			__func__, __LINE__, hw->dphy_clks[i].id);
+	}
+
+	return 0;
+err:
+	VEHICLE_DGERR("%s(%d) enable dphy clk: %s err\n",
+			__func__, __LINE__, hw->dphy_clks[i].id);
+	for (--i; i >= 0; --i)
+		clk_disable_unprepare(hw->dphy_clks[i].clk);
+
+	return ret;
+}
+
+static void csi2_disable_clk(struct csi2_dphy_hw *hw)
+{
+	int i;
+
+	for (i = hw->num_csi2_clks - 1; i >= 0; i--) {
+		clk_disable_unprepare(hw->csi2_clks[i].clk);
+		VEHICLE_INFO("%s(%d) disable csi2 clk: %s\n",
+				__func__, __LINE__, hw->csi2_clks[i].id);
+	}
+}
+
+static int csi2_enable_clk(struct csi2_dphy_hw *hw)
+{
+	int i, ret = -EINVAL;
+
+	for (i = 0; i < hw->num_csi2_clks; i++) {
+		ret = clk_prepare_enable(hw->csi2_clks[i].clk);
+		if (ret < 0)
+			goto err;
+		VEHICLE_INFO("%s(%d) enable csi2 clk: %s\n",
+			__func__, __LINE__, hw->csi2_clks[i].id);
+	}
+
+	return 0;
+err:
+	VEHICLE_DGERR("%s(%d) enable csi2 clk: %s err\n",
+			__func__, __LINE__, hw->csi2_clks[i].id);
+	for (--i; i >= 0; --i)
+		clk_disable_unprepare(hw->csi2_clks[i].clk);
+
+	return ret;
+}
+
+static int vehicle_csi2_clk_ctrl(struct vehicle_cif *cif, int on)
+{
+	int ret = 0;
+	struct csi2_dphy_hw *dphy_hw = cif->dphy_hw;
+
+	on = !!on;
+	if (on) {
+		ret = csi2_enable_dphy_clk(dphy_hw);
+		if (ret < 0) {
+			VEHICLE_DGERR("enable csi dphy clk failed!");
+			goto err;
+		}
+		ret = csi2_enable_clk(dphy_hw);
+		if (ret < 0) {
+			VEHICLE_DGERR("enable csi dphy clk failed!");
+			goto err;
+		}
+		dphy_hw->on = true;
+	} else {
+		csi2_disable_dphy_clk(dphy_hw);
+		csi2_disable_clk(dphy_hw);
+		dphy_hw->on = false;
+	}
+
+	return 0;
+err:
+	return ret;
+}
+
+static int vehicle_csi2_stream_stop(struct vehicle_cif *cif)
+{
+	vehicle_csi2_disable(cif);
+
+	return 0;
+}
+
+static int vehicle_cif_stream_stop(struct vehicle_cif *cif)
+{
+	return 0;
+}
+
+static int vehicle_cif_csi_stream_stop(struct vehicle_cif *cif)
+{
+	vehicle_cif_stream_stop(cif);
+	vehicle_csi2_stream_stop(cif);
+	if (cif->dphy_hw->chip_id == CHIP_ID_RK3588_DCPHY)
+		vehicle_csi2_dcphy_stream_stop(cif);
+	else
+		vehicle_csi2_dphy_stream_stop(cif);
+
+	return 0;
+}
+
+static int vehicle_cif_csi2_s_stream(struct vehicle_cif *cif,
+				int enable,
+				enum v4l2_mbus_type mbus_type)
+
+{
+	unsigned int val = 0x0;
+	const struct cif_input_fmt *infmt;
+	struct vehicle_csi_channel_info *channel;
+	int id;
+
+	channel = &cif->channels[0];
+
+	if (enable) {
+		val = CSI_ENABLE_CAPTURE | channel->fmt_val |
+		      channel->cmd_mode_en << 4 | channel->crop_en << 5 |
+		      channel->id << 8 | channel->data_type << 10;
+
+		val &= ~CSI_ENABLE_MIPI_COMPACT;
+
+		infmt = find_input_fmt(cif->cif_cfg.mbus_code);
+		if (!infmt) {
+			VEHICLE_INFO("Input fmt is invalid, use default!\n");
+			val |= CSI_YUV_INPUT_ORDER_UYVY;
+		} else {
+			val |= infmt->csi_yuv_order;
+		}
+
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(channel->id), val);
+		cif->state = RKCIF_STATE_STREAMING;
+	} else {
+		id = channel->id;
+		val = rkcif_read_reg(cif, get_reg_index_of_id_ctrl0(id));
+		val &= ~CSI_ENABLE_CAPTURE;
+
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(id), val);
+
+		rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTSTAT,
+					CSI_START_INTSTAT(id) |
+					CSI_DMA_END_INTSTAT(id) |
+					CSI_LINE_INTSTAT(id));
+
+		rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTEN,
+					 ~(CSI_START_INTEN(id) |
+					   CSI_DMA_END_INTEN(id) |
+					   CSI_LINE_INTEN(id)));
+
+		rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTEN,
+					~CSI_ALL_ERROR_INTEN);
+		cif->state = RKCIF_STATE_READY;
+	}
+
+	return 0;
+}
+
+static int vehicle_cif_csi2_s_stream_v1(struct vehicle_cif *cif,
+				int enable,
+				enum v4l2_mbus_type mbus_type)
+
+{
+	unsigned int val = 0x0;
+	const struct cif_input_fmt *infmt;
+	struct vehicle_csi_channel_info *channel;
+	struct vehicle_cfg *cfg = &cif->cif_cfg;
+	int id;
+
+	channel = &cif->channels[0];
+
+	if (enable) {
+		if (cif->chip_id <= CHIP_RK3562_VEHICLE_CIF) {
+			val = CSI_ENABLE_CAPTURE | CSI_DMA_ENABLE |
+			      channel->cmd_mode_en << 26 | CSI_ENABLE_CROP_V1 |
+			      channel->id << 8 | channel->data_type << 10;
+
+			infmt = find_input_fmt(cif->cif_cfg.mbus_code);
+			if (!infmt) {
+				VEHICLE_INFO("Input fmt is invalid, use default!\n");
+				val |= CSI_YUV_INPUT_ORDER_UYVY;
+			} else {
+				val |= infmt->csi_yuv_order | infmt->csi_fmt_val;
+			}
+
+			if (cfg->output_format == CIF_OUTPUT_FORMAT_420) {
+				if (find_output_fmt(V4L2_PIX_FMT_NV12))
+					val |= CSI_WRDDR_TYPE_YUV420SP_RK3588 |
+						CSI_YUV_OUTPUT_ORDER_UYVY;
+			} else {
+				if (find_output_fmt(V4L2_PIX_FMT_NV16))
+					val |= CSI_WRDDR_TYPE_YUV422SP_RK3588 |
+						CSI_YUV_OUTPUT_ORDER_UYVY;
+			}
+
+			rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(channel->id), val);
+		} else {
+			val = CSI_ENABLE_CAPTURE | CSI_DMA_ENABLE_RK3576 |
+			      CSI_ENABLE_CROP_RK3576;
+
+			infmt = find_input_fmt(cif->cif_cfg.mbus_code);
+			if (!infmt) {
+				VEHICLE_INFO("Input fmt is invalid, use default!\n");
+				val |= CSI_YUV_INPUT_ORDER_UYVY;
+			} else if (cif->interlaced_enable) {
+				val |= (infmt->csi_yuv_order >> 4) |
+						((infmt->csi_fmt_val + 1) << 4);
+			} else {
+				val |= (infmt->csi_yuv_order >> 4) | (infmt->csi_fmt_val << 4);
+			}
+
+			if (cfg->output_format == CIF_OUTPUT_FORMAT_420) {
+				if (find_output_fmt(V4L2_PIX_FMT_NV12))
+					val |= CSI_WRDDR_TYPE_YUV420SP_RK3588 << 3 |
+						CSI_YUV_OUTPUT_ORDER_UYVY >> 4;
+			} else {
+				if (find_output_fmt(V4L2_PIX_FMT_NV16))
+					val |= CSI_WRDDR_TYPE_YUV422SP_RK3588 << 3 |
+						CSI_YUV_OUTPUT_ORDER_UYVY >> 4;
+			}
+
+			if (cfg->output_format == CIF_OUTPUT_FORMAT_420)
+				val |= CSI_UVDS_EN;
+			rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(channel->id), val);
+
+			val = channel->data_type << 2;
+			rkcif_write_reg(cif, get_reg_index_of_id_ctrl1(channel->id), val);
+
+		}
+
+		rkcif_write_reg(cif, CIF_REG_MIPI_EFFECT_CODE_ID0, 0x02410251);
+		rkcif_write_reg(cif, CIF_REG_MIPI_EFFECT_CODE_ID1, 0x02420252);
+		cif->state = RKCIF_STATE_STREAMING;
+	} else {
+		id = channel->id;
+		val = rkcif_read_reg(cif, get_reg_index_of_id_ctrl0(id));
+		if (cif->chip_id >= CHIP_RK3576_VEHICLE_CIF)
+			val &= ~(CSI_ENABLE_CAPTURE | CSI_DMA_ENABLE_RK3576);
+		else
+			val &= ~(CSI_ENABLE_CAPTURE | CSI_DMA_ENABLE);
+
+		rkcif_write_reg(cif, get_reg_index_of_id_ctrl0(id), val);
+
+		rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_INTSTAT,
+					CSI_START_INTSTAT(id) |
+					CSI_DMA_END_INTSTAT(id) |
+					CSI_LINE_INTSTAT(id));
+
+		rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTEN,
+					 ~(CSI_START_INTEN(id) |
+					   CSI_DMA_END_INTEN(id) |
+					   CSI_LINE_INTEN(id)));
+
+		rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_INTEN,
+					~CSI_ALL_ERROR_INTEN_V1);
+		rkcif_write_reg_and(cif, CIF_REG_MIPI_LVDS_CTRL,
+						~CSI_ENABLE_CAPTURE);
+		cif->state = RKCIF_STATE_READY;
+	}
+
+	return 0;
+}
+
+static int cif_interrupt_setup(struct vehicle_cif *cif)
+{
+	rkcif_write_reg(cif, CIF_REG_DVP_INTEN,
+			     FRAME_END_EN | INTSTAT_ERR |
+			     PST_INF_FRAME_END);
+
+	/* enable line int for sof */
+	rkcif_write_reg(cif, CIF_REG_DVP_LINE_INT_NUM, 0x1);
+	rkcif_write_reg(cif, CIF_REG_DVP_INTEN, LINE_INT_EN);
+
+	return 0;
+}
+
+static void vehicle_cif_dvp_dump_regs(struct vehicle_cif *cif)
+{
+	int val;
+
+	if (!vehicle_debug)
+		return;
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_CTRL);
+	VEHICLE_DG("CIF_REG_DVP_CTRL = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+	VEHICLE_DG("CIF_REG_DVP_INTEN = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_INTSTAT);
+	VEHICLE_DG("CIF_REG_DVP_INTSTAT = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FOR);
+	VEHICLE_DG("CIF_REG_DVP_FOR = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_MULTI_ID);
+	VEHICLE_DG("CIF_REG_DVP_MULTI_ID = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LINE_NUM_ADDR);
+	VEHICLE_DG("CIF_REG_DVP_LINE_NUM_ADDR = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FRM0_ADDR_Y);
+	VEHICLE_DG("CIF_REG_DVP_FRM0_ADDR_Y = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FRM0_ADDR_UV);
+	VEHICLE_DG("CIF_REG_DVP_FRM0_ADDR_UV = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FRM1_ADDR_Y);
+	VEHICLE_DG("CIF_REG_DVP_FRM1_ADDR_Y = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FRM1_ADDR_UV);
+	VEHICLE_DG("CIF_REG_DVP_FRM1_ADDR_UV = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_VIR_LINE_WIDTH);
+	VEHICLE_DG("CIF_REG_DVP_VIR_LINE_WIDTH = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_SET_SIZE);
+	VEHICLE_DG("CIF_REG_DVP_SET_SIZE = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LINE_INT_NUM);
+	VEHICLE_DG("CIF_REG_DVP_LINE_INT_NUM = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LINE_CNT);
+	VEHICLE_DG("CIF_REG_DVP_LINE_CNT = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_CROP);
+	VEHICLE_DG("CIF_REG_DVP_CROP = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_SCL_CTRL);
+	VEHICLE_DG("CIF_REG_DVP_SCL_CTRL = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_FRAME_STATUS);
+	VEHICLE_DG("CIF_REG_DVP_FRAME_STATUS = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_CUR_DST);
+	VEHICLE_DG("CIF_REG_DVP_CUR_DST = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LAST_LINE);
+	VEHICLE_DG("CIF_REG_DVP_LAST_LINE = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LAST_PIX);
+	VEHICLE_DG("CIF_REG_DVP_LAST_PIX = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_SCL_VALID_NUM);
+	VEHICLE_DG("CIF_REG_DVP_SCL_VALID_NUM = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_DVP_LINE_NUM_ADDR);
+	VEHICLE_DG("CIF_REG_DVP_LINE_NUM_ADDR = 0X%x\r\n", val);
+
+	/* read dvp clk sample edge */
+	val = rkvehicle_cif_read_grf_reg(cif, CIF_REG_GRF_CIFIO_CON);
+	VEHICLE_DG("CIF_REG_GRF_CIFIO_CON = 0X%x\r\n", val);
+}
+
+static void vehicle_cif_csi2_dump_regs(struct vehicle_cif *cif)
+{
+	int val = 0;
+	void __iomem *csi2_base = cif->csi2_base;
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+
+	if (!vehicle_debug)
+		return;
+
+	/* 1. dump csi2-dphy regs */
+	if (cif->dphy_hw->chip_id == CHIP_ID_RK3588) {
+		VEHICLE_DG("\n\n DUMP CSI-DPHY REGS: \r\n");
+		read_csi2_dphy_reg(hw, CSI2PHY_REG_CTRL_LANE_ENABLE, &val);
+		VEHICLE_DG("CSI2PHY_REG_CTRL_LANE_ENABLE = 0x%x\r\n", val);
+
+		read_csi2_dphy_reg(hw, CSI2PHY_DUAL_CLK_EN, &val);
+		VEHICLE_DG("CSI2PHY_DUAL_CLK_EN = 0x%x\r\n", val);
+
+		val = csi2_dphy_read_grf_reg(hw, GRF_DPHY_CSI2PHY_FORCERXMODE);
+		VEHICLE_DG("GRF_DPHY_CSI2PHY_FORCERXMODE = 0x%x\r\n", val);
+
+		val = csi2_dphy_read_grf_reg(hw, GRF_DPHY_CSI2PHY_LANE_SEL);
+		VEHICLE_DG("GRF_DPHY_CSI2PHY_LANE_SEL = 0x%x\r\n", val);
+
+		val = csi2_dphy_read_grf_reg(hw, GRF_DPHY_CSI2PHY_DATALANE_EN);
+		VEHICLE_DG("GRF_DPHY_CSI2PHY_DATALANE_EN = 0x%x\r\n", val);
+
+		val = csi2_dphy_read_grf_reg(hw, GRF_DPHY_CSI2PHY_CLKLANE_EN);
+		VEHICLE_DG("GRF_DPHY_CSI2PHY_CLKLANE_EN = 0x%x\r\n", val);
+	}
+
+	/* 2. dump csi2 regs */
+	VEHICLE_DG("\n\n DUMP CSI2 REGS: \r\n");
+	val = vehicle_read_csihost_reg(csi2_base, CSIHOST_N_LANES);
+	VEHICLE_DG("CSIHOST_N_LANES = 0x%x\r\n", val);
+
+	val = vehicle_read_csihost_reg(csi2_base, CSIHOST_CONTROL);
+	VEHICLE_DG("CSIHOST_CONTROL = 0x%x\r\n", val);
+
+	val = vehicle_read_csihost_reg(csi2_base, CSIHOST_MSK1);
+	VEHICLE_DG("CSIHOST_MSK1 = 0x%x\r\n", val);
+
+	val = vehicle_read_csihost_reg(csi2_base, CSIHOST_MSK2);
+	VEHICLE_DG("CSIHOST_MSK2 = 0x%x\r\n", val);
+
+	val = vehicle_read_csihost_reg(csi2_base, CSIHOST_RESETN);
+	VEHICLE_DG("CSIHOST_RESETN = 0x%x\r\n", val);
+
+	/* 3. dump cif regs */
+	VEHICLE_DG("\n\n DUMP MIPI CIF REGS: \r\n");
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_CTRL);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_CTRL = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_INTEN);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_INTEN = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_INTSTAT);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_INTSTAT = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_ID0_CTRL0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_ID0_CTRL0 = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_ID0_CTRL1);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_ID0_CTRL1 = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID0_1 = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_LINE_INT_NUM_ID2_3 = 0x%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME0_VLW_Y_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME0_VLW_UV_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME1_VLW_Y_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME1_VLW_UV_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME0_ADDR_Y_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME0_ADDR_UV_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME1_ADDR_Y_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_FRAME1_ADDR_UV_ID0 = 0X%x\r\n", val);
+
+	val = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_ID0_CROP_START);
+	VEHICLE_DG("CIF_REG_MIPI_LVDS_ID0_CROP_START = 0X%x\r\n", val);
+
+	/* read dvp clk sample edge */
+	val = rkvehicle_cif_read_grf_reg(cif, CIF_REG_GRF_CIFIO_CON);
+	VEHICLE_DG("CIF_REG_GRF_CIFIO_CON = 0X%x\r\n", val);
+}
+
+static int vehicle_cif_s_stream(struct vehicle_cif *cif, int enable)
+{
+	int cif_ctrl_val;
+	unsigned int dma_en = 0;
+
+	cif->is_enabled = enable;
+
+	VEHICLE_INFO("%s enable=%d\n", __func__, enable);
+
+	if (enable) {
+		cif->irqinfo.cifirq_idx = 0;
+		cif->irqinfo.cifirq_normal_idx = 0;
+		cif->irqinfo.cifirq_abnormal_idx = 0;
+		cif->irqinfo.dmairq_idx = 0;
+		cif->irqinfo.all_err_cnt = 0;
+		cif->irqinfo.dvp_bus_err_cnt = 0;
+		cif->irqinfo.dvp_overflow_cnt = 0;
+		cif->irqinfo.dvp_pix_err_cnt = 0;
+		cif->irqinfo.dvp_line_err_cnt = 0;
+		cif->irqinfo.dvp_size_err_cnt = 0;
+		cif->irqinfo.dvp_bwidth_lack_cnt = 0;
+		cif->irqinfo.csi_size_err_cnt = 0;
+
+		rkcif_write_reg(cif, CIF_REG_DVP_INTEN,
+				FRAME_END_EN | INTSTAT_ERR |
+				PST_INF_FRAME_END);
+
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			rkcif_write_reg(cif, CIF_REG_DVP_LINE_INT_NUM, 0x1);
+			rkcif_write_reg_or(cif, CIF_REG_DVP_INTEN, 0x033ffff);
+		}
+
+		dma_en = DVP_DMA_EN;
+		if (cif->chip_id < CHIP_RK3588_VEHICLE_CIF) {
+			rkcif_write_reg(cif, CIF_REG_DVP_CTRL,
+				AXI_BURST_16 | MODE_PINGPONG | ENABLE_CAPTURE);
+		} else if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			rkcif_write_reg(cif, CIF_REG_DVP_CTRL,
+			     DVP_SW_WATER_LINE_25
+			     | dma_en
+			     | DVP_PRESS_EN
+			     | DVP_HURRY_EN
+			     | DVP_SW_WATER_LINE_25
+			     | DVP_SW_PRESS_VALUE(3)
+			     | DVP_SW_HURRY_VALUE(3)
+			     | ENABLE_CAPTURE);
+		} else {
+			dma_en = DVP_SW_DMA_EN_RK3676(0);
+			rkcif_write_reg(cif, CIF_REG_DVP_CTRL,
+			     DVP_SW_WATER_LINE_25_RK3576
+			     | DVP_SW_CAP_EN_RK3576(0)
+			     | dma_en
+			     | ENABLE_CAPTURE);
+		}
+
+		cif->frame_idx = 0;
+		cif->state = RKCIF_STATE_STREAMING;
+	} else {
+		cif_ctrl_val = rkcif_read_reg(cif, CIF_REG_DVP_CTRL);
+		cif_ctrl_val &= ~ENABLE_CAPTURE;
+		rkcif_write_reg(cif, CIF_REG_DVP_CTRL, cif_ctrl_val);
+		rkcif_write_reg(cif, CIF_REG_DVP_INTEN, 0);
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, 0x3ff);
+		rkcif_write_reg(cif, CIF_REG_DVP_FRAME_STATUS, 0x0);
+		cif->state = RKCIF_STATE_READY;
+	}
+
+	return 0;
+}
+
+static int vehicle_cif_create_dummy_buf(struct vehicle_cif *cif)
+{
+	struct vehicle_rkcif_dummy_buffer *dummy_buf = &cif->dummy_buf;
+	struct vehicle_cfg *cfg = &cif->cif_cfg;
+
+	/* get a maximum plane size */
+	dummy_buf->size = cfg->width * cfg->height * 2;
+
+	dummy_buf->vaddr = dma_alloc_coherent(cif->dev, dummy_buf->size,
+					      &dummy_buf->dma_addr,
+					      GFP_KERNEL);
+	if (!dummy_buf->vaddr) {
+		VEHICLE_DGERR("Failed to allocate the memory for dummy buffer\n");
+		return -ENOMEM;
+	}
+
+	VEHICLE_INFO("Allocate dummy buffer, size: 0x%08x\n", dummy_buf->size);
+
+	return 0;
+}
+
+static void vehicle_cif_destroy_dummy_buf(struct vehicle_cif *cif)
+{
+	struct vehicle_rkcif_dummy_buffer *dummy_buf = &cif->dummy_buf;
+
+	VEHICLE_INFO("Destroy dummy buffer, size: 0x%08x\n", dummy_buf->size);
+
+	if (dummy_buf->vaddr)
+		dma_free_coherent(cif->dev, dummy_buf->size,
+				  dummy_buf->vaddr, dummy_buf->dma_addr);
+	dummy_buf->dma_addr = 0;
+	dummy_buf->vaddr = NULL;
+}
+
+static void vehicle_cif_hw_soft_reset(struct vehicle_cif  *cif)
+{
+	unsigned int i;
+	struct rk_cif_clk *clk = &cif->clk;
+
+	for (i = 0; i < clk->rsts_num; i++)
+		if (clk->cif_rst[i])
+			reset_control_assert(clk->cif_rst[i]);
+	udelay(10);
+	for (i = 0; i < clk->rsts_num; i++)
+		if (clk->cif_rst[i])
+			reset_control_deassert(clk->cif_rst[i]);
+}
+
+static void vehicle_rkcif_do_soft_reset(struct vehicle_cif  *cif)
+{
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY)
+		rkcif_write_reg_or(cif, CIF_REG_MIPI_LVDS_CTRL, 0x000A0000);
+	else
+		rkcif_write_reg_or(cif, CIF_REG_DVP_CTRL, 0x000A0000);
+	usleep_range(10, 20);
+	VEHICLE_INFO("vicap do soft reset 0x%x\n", 0x000A0000);
+}
+
+static int vehicle_cif_do_stop_stream(struct vehicle_cif  *cif)
+{
+	if (!cif)
+		return -1;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF) {
+			vehicle_cif_csi2_s_stream_v1(cif, 0, V4L2_MBUS_CSI2_DPHY);
+			vehicle_cif_csi_stream_stop(cif);
+		} else {
+			vehicle_cif_csi2_s_stream(cif, 0, V4L2_MBUS_CSI2_DPHY);
+			vehicle_cif_csi_stream_stop(cif);
+		}
+	} else {
+		vehicle_cif_s_stream(cif, 0);
+	}
+	if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF)
+		vehicle_rkcif_do_soft_reset(cif);
+	vehicle_cif_destroy_dummy_buf(cif);
+
+	return 0;
+}
+
+static int vehicle_cif_do_start_stream(struct vehicle_cif  *cif)
+{
+	int ret = 0;
+
+	if (!cif)
+		return -ENODEV;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+
+		/*  1. stream setup */
+		cif_csi_stream_setup(cif);
+
+		/*  2. create dummy buf */
+		ret = vehicle_cif_create_dummy_buf(cif);
+		if (ret < 0)
+			VEHICLE_DGERR("Failed to create dummy_buf, %d\n", ret);
+
+		/*  3. cif init buffer */
+		if (vehicle_cif_init_buffer(cif, 1, cif->channels[0].id) < 0)
+			return -EINVAL;
+
+		/*  4. dump cif regs */
+		vehicle_cif_csi2_dump_regs(cif);
+
+		/*  5. start stream */
+		if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF)
+			vehicle_cif_csi2_s_stream_v1(cif, 1, V4L2_MBUS_CSI2_DPHY);
+		else
+			vehicle_cif_csi2_s_stream(cif, 1, V4L2_MBUS_CSI2_DPHY);
+
+	} else {
+		/*  1. stream setup */
+		cif_stream_setup(cif);
+
+		/*  2. create dummy buf */
+		ret = vehicle_cif_create_dummy_buf(cif);
+		if (ret < 0)
+			VEHICLE_DGERR("Failed to create dummy_buf, %d\n", ret);
+
+		/*  3. cif init buffer */
+		if (vehicle_cif_init_buffer(cif, 1, 0) < 0)
+			return -EINVAL;
+
+		/*  4. enable interrupts */
+		if (cif->chip_id < CHIP_RK3588_VEHICLE_CIF)
+			cif_interrupt_setup(cif);
+
+		/*  5. dump cif regs */
+		vehicle_cif_dvp_dump_regs(cif);
+
+		/*  6. start stream */
+		vehicle_cif_s_stream(cif, 1);
+	}
+
+	return 0;
+}
+
+static void vehicle_rkcif_disable_sys_clk(struct rk_cif_clk *clk);
+static int vehicle_rkcif_enable_sys_clk(struct rk_cif_clk *clk);
+
+static void vehicle_cif_reset(struct vehicle_cif  *cif, int only_rst)
+{
+	int ret = 0;
+
+	mutex_lock(&cif->stream_lock);
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		VEHICLE_DG("%s enter, V4L2_MBUS_CSI2 reset need to do!\n", __func__);
+
+		// goto unlock_reset;
+		if (only_rst == 1) {
+			vehicle_cif_hw_soft_reset(cif);
+		} else {
+			vehicle_cif_do_stop_stream(cif);
+			vehicle_cif_hw_soft_reset(cif);
+			vehicle_cif_do_start_stream(cif);
+		}
+	} else {
+		int ctrl_reg, inten_reg, crop_reg, set_size_reg, for_reg;
+		int vir_line_width_reg, scl_reg;
+		int y0_reg, uv0_reg, y1_reg, uv1_reg;
+
+		VEHICLE_DG("%s enter, do reset!\n", __func__);
+		if (only_rst == 1) {
+			vehicle_cif_hw_soft_reset(cif);
+		} else {
+			ctrl_reg = rkcif_read_reg(cif, CIF_REG_DVP_CTRL);
+			if (ctrl_reg & ENABLE_CAPTURE)
+				rkcif_write_reg(cif, CIF_REG_DVP_CTRL,
+					      ctrl_reg & ~ENABLE_CAPTURE);
+
+			crop_reg = rkcif_read_reg(cif, CIF_REG_DVP_CROP);
+			set_size_reg = rkcif_read_reg(cif, CIF_REG_DVP_SET_SIZE);
+			inten_reg = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+			for_reg = rkcif_read_reg(cif, CIF_REG_DVP_FOR);
+			vir_line_width_reg = rkcif_read_reg(cif,
+							  CIF_REG_DVP_VIR_LINE_WIDTH);
+			scl_reg = rkcif_read_reg(cif, CIF_REG_DVP_SCL_CTRL);
+			y0_reg = rkcif_read_reg(cif, CIF_REG_DVP_FRM0_ADDR_Y);
+			uv0_reg = rkcif_read_reg(cif, CIF_REG_DVP_FRM0_ADDR_UV);
+			y1_reg = rkcif_read_reg(cif, CIF_REG_DVP_FRM1_ADDR_Y);
+			uv1_reg = rkcif_read_reg(cif, CIF_REG_DVP_FRM1_ADDR_UV);
+
+			udelay(20);
+			vehicle_cif_hw_soft_reset(cif);
+			vehicle_rkcif_disable_sys_clk(&cif->clk);
+			udelay(5);
+			ret = vehicle_rkcif_enable_sys_clk(&cif->clk);
+			if (ret < 0) {
+				VEHICLE_DGERR("@%s, resume cif clk failed!\n", __func__);
+				goto unlock_reset;
+			}
+
+			rkcif_write_reg(cif, CIF_REG_DVP_CTRL,
+				      ctrl_reg & ~ENABLE_CAPTURE);
+			rkcif_write_reg(cif, CIF_REG_DVP_INTEN, inten_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_CROP, crop_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_SET_SIZE, set_size_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_FOR, for_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_VIR_LINE_WIDTH,
+				      vir_line_width_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_SCL_CTRL, scl_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_FRM0_ADDR_Y, y0_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_FRM0_ADDR_UV, uv0_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_FRM1_ADDR_Y, y1_reg);
+			rkcif_write_reg(cif, CIF_REG_DVP_FRM1_ADDR_UV, uv1_reg);
+		}
+	}
+unlock_reset:
+	mutex_unlock(&cif->stream_lock);
+}
+
+static void vehicle_cif_reset_delay(struct vehicle_cif *cif)
+{
+	mdelay(10);
+	vehicle_cif_reset(cif, 0);
+	mdelay(10);
+	vehicle_cif_s_stream(cif, 1);
+}
+
+static void cif_capture_en(char *reg, int enable)
+{
+	int val = 0;
+
+	val = read_reg(reg, CIF_REG_DVP_CTRL);
+	if (enable == 1)
+		write_reg(reg, CIF_REG_DVP_CTRL, val | ENABLE_CAPTURE);
+	else
+		write_reg(reg, CIF_REG_DVP_CTRL, val & (~ENABLE_CAPTURE));
+}
+
+static void vehicle_cif_reset_work_func(struct work_struct *work)
+{
+	struct vehicle_cif *cif = container_of(work, struct vehicle_cif,
+			work.work);
+
+	if (cif->stopping)
+		return;
+
+	atomic_set(&cif->reset_status, 1);
+	vehicle_cif_reset_delay(cif);
+	atomic_set(&cif->reset_status, 0);
+	wake_up(&cif->wq_stopped);
+}
+
+int vehicle_wait_cif_reset_done(void)
+{
+	struct vehicle_cif *cif = g_cif;
+	int ret = 0, retry = 2;
+
+	for (retry = 2; retry >= 0; retry--) {
+		ret = wait_event_timeout(cif->wq_stopped,
+				   !atomic_read(&cif->reset_status),
+				   msecs_to_jiffies(200));
+		if (!ret) {
+			VEHICLE_DG("%s wait cif reset timeout, left try times(%d)!\n",
+				__func__, retry);
+		} else {
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static int cif_irq_error_process(struct vehicle_cif *cif, unsigned int reg_intstat)
+{
+	VEHICLE_DG("%s cif->irqinfo.all_err_cnt(%lld)\n", __func__,
+					cif->irqinfo.all_err_cnt);
+	if (reg_intstat & INTSTAT_ERR) {
+		cif->irqinfo.all_err_cnt++;
+
+		if (reg_intstat & BUS_ERR) {
+			cif->irqinfo.dvp_bus_err_cnt++;
+			VEHICLE_DGERR("dvp bus err\n");
+		}
+
+		if (reg_intstat & DVP_ALL_OVERFLOW) {
+			cif->irqinfo.dvp_overflow_cnt++;
+			VEHICLE_DGERR("dvp overflow err\n");
+		}
+
+		if (reg_intstat & LINE_ERR) {
+			cif->irqinfo.dvp_line_err_cnt++;
+			VEHICLE_DGERR("dvp line err\n");
+		}
+
+		if (reg_intstat & PIX_ERR) {
+			cif->irqinfo.dvp_pix_err_cnt++;
+			VEHICLE_DGERR("dvp pix err\n");
+		}
+
+		if (cif->irqinfo.all_err_cnt < 10) {
+			u32 mask;
+
+			VEHICLE_DGERR("ERROR: DVP_ALL_ERROR:0x%x!!\n", reg_intstat);
+			mask = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+			mask &= ~INTSTAT_ERR;
+			rkcif_write_reg(cif, CIF_REG_DVP_INTEN, mask);
+			return -2;
+		} else if (cif->irqinfo.all_err_cnt >= 10) {
+			u32 mask;
+
+			mask = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+			mask &= ~INTSTAT_ERR;
+			rkcif_write_reg(cif, CIF_REG_DVP_INTEN, mask);
+			VEHICLE_DGERR("ERROR: DVP_ALL_ERROR:0x%x!!\n", reg_intstat);
+			return -2;
+		}
+	}
+
+	return 0;
+}
+
+static int vehicle_cif_csi2_g_mipi_id(unsigned int intstat)
+{
+	if (intstat & CSI_FRAME_END_ID0) {
+		if ((intstat & CSI_FRAME_END_ID0) == CSI_FRAME_END_ID0)
+			VEHICLE_DG("frame0/1 trigger simultaneously in ID0\n");
+		return RKCIF_STREAM_MIPI_ID0;
+	}
+
+	if (intstat & CSI_FRAME_END_ID1) {
+		if ((intstat & CSI_FRAME_END_ID1) == CSI_FRAME_END_ID1)
+			VEHICLE_DG("frame0/1 trigger simultaneously in ID1\n");
+		return RKCIF_STREAM_MIPI_ID1;
+	}
+
+	if (intstat & CSI_FRAME_END_ID2) {
+		if ((intstat & CSI_FRAME_END_ID2) == CSI_FRAME_END_ID2)
+			VEHICLE_DG("frame0/1 trigger simultaneously in ID2\n");
+		return RKCIF_STREAM_MIPI_ID2;
+	}
+
+	if (intstat & CSI_FRAME_END_ID3) {
+		if ((intstat & CSI_FRAME_END_ID3) == CSI_FRAME_END_ID3)
+			VEHICLE_DG("frame0/1 trigger simultaneously in ID3\n");
+		return RKCIF_STREAM_MIPI_ID3;
+	}
+
+	return -EINVAL;
+}
+
+static __maybe_unused int rkcif_dvp_g_ch_id_by_fe(unsigned int intstat)
+{
+	if (intstat & DVP_ALL_END_ID0) {
+		if ((intstat & DVP_ALL_END_ID0) ==
+		    DVP_ALL_END_ID0)
+			VEHICLE_DG("frame0/1 trigger simultaneously in DVP ID0\n");
+		return RKCIF_STREAM_MIPI_ID0;
+	}
+
+	if (intstat & DVP_ALL_END_ID1) {
+		if ((intstat & DVP_ALL_END_ID1) ==
+		    DVP_ALL_END_ID1)
+			VEHICLE_DG("frame0/1 trigger simultaneously in DVP ID1\n");
+		return RKCIF_STREAM_MIPI_ID1;
+	}
+
+	if (intstat & DVP_ALL_END_ID2) {
+		if ((intstat & DVP_ALL_END_ID2) ==
+		    DVP_ALL_END_ID2)
+			VEHICLE_DG("frame0/1 trigger simultaneously in DVP ID2\n");
+		return RKCIF_STREAM_MIPI_ID2;
+	}
+
+	if (intstat & DVP_ALL_END_ID3) {
+		if ((intstat & DVP_ALL_END_ID3) ==
+		    DVP_ALL_END_ID3)
+			VEHICLE_DG("frame0/1 trigger simultaneously in DVP ID3\n");
+		return RKCIF_STREAM_MIPI_ID3;
+	}
+
+	return -EINVAL;
+}
+
+static int vehicle_cif_next_buffer(struct vehicle_cif *cif, u32 frame_ready, int mipi_id)
+{
+	u32 frm0_addr_y, frm0_addr_uv;
+	u32 frm1_addr_y, frm1_addr_uv;
+	unsigned long y_addr = 0, uv_addr = 0;
+	static unsigned long temp_y_addr, temp_uv_addr;
+	int commit_buf = 0;
+	struct vehicle_rkcif_dummy_buffer *dummy_buf = &cif->dummy_buf;
+	u32 frm_num_reg, frame_id = 0;
+	VEHICLE_DG("@%s, enter, mipi_id(%d)\n", __func__, mipi_id);
+
+	if ((frame_ready > 1) || (cif->cif_cfg.buf_num < 2) ||
+		(cif->cif_cfg.buf_num > MAX_BUF_NUM))
+		return 0;
+
+	cif->last_buf_index = cif->current_buf_index;
+	cif->current_buf_index = (cif->current_buf_index + 1) %
+				 cif->cif_cfg.buf_num;
+
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		frm0_addr_y = get_reg_index_of_frm0_y_addr(mipi_id);
+		frm0_addr_uv = get_reg_index_of_frm0_uv_addr(mipi_id);
+		frm1_addr_y = get_reg_index_of_frm1_y_addr(mipi_id);
+		frm1_addr_uv = get_reg_index_of_frm1_uv_addr(mipi_id);
+		frm_num_reg = get_reg_index_of_frm_num(mipi_id);
+		frame_id = rkcif_read_reg(cif, frm_num_reg);
+		VEHICLE_DG("@%s, frm_num_reg(0x%x), frame_id:0x%x\n", __func__,
+			   frm_num_reg, frame_id);
+	} else {
+		frm0_addr_y = get_dvp_reg_index_of_frm0_y_addr(mipi_id);
+		frm0_addr_uv = get_dvp_reg_index_of_frm0_uv_addr(mipi_id);
+		frm1_addr_y = get_dvp_reg_index_of_frm1_y_addr(mipi_id);
+		frm1_addr_uv = get_dvp_reg_index_of_frm1_uv_addr(mipi_id);
+	}
+
+	spin_lock(&cif->vbq_lock);
+
+	if (!cif->interlaced_enable || cif->use_hw_interlace) {
+		temp_y_addr = vehicle_flinger_request_cif_buffer();
+		if (temp_y_addr == 0) {
+			VEHICLE_INFO("%s,warnning request buffer failed\n", __func__);
+			spin_unlock(&cif->vbq_lock);
+			if (dummy_buf->vaddr) {
+				if (frame_ready == 0) {
+					rkcif_write_reg(cif, frm0_addr_y, dummy_buf->dma_addr);
+					rkcif_write_reg(cif, frm0_addr_uv, dummy_buf->dma_addr);
+				} else {
+					rkcif_write_reg(cif, frm1_addr_y, dummy_buf->dma_addr);
+					rkcif_write_reg(cif, frm1_addr_uv, dummy_buf->dma_addr);
+				}
+				VEHICLE_INFO("frame Drop to dummy buf\n");
+			} else {
+				VEHICLE_INFO("dummy buf is null!\n");
+			}
+			return -1;
+		}
+		temp_uv_addr = temp_y_addr + UV_OFFSET;
+		y_addr = temp_y_addr;
+		uv_addr = temp_uv_addr;
+		commit_buf = 0;
+	} else if (cif->interlaced_enable && !cif->use_hw_interlace) {
+		if ((frame_id != 0 && (frame_id & 0xffff) % 2 == 0) ||
+		    (frame_id == 0 && (cif->interlaced_counts % 2 == 0))) {
+			temp_y_addr = vehicle_flinger_request_cif_buffer();
+			if (temp_y_addr == 0) {
+				VEHICLE_DGERR("%s,warnning request buffer failed\n", __func__);
+				spin_unlock(&cif->vbq_lock);
+				return -1;
+			}
+			temp_uv_addr = temp_y_addr + UV_OFFSET;
+			y_addr = temp_y_addr;
+			uv_addr = temp_uv_addr;
+			commit_buf = -1; //not ok yet
+		} else {
+			y_addr = temp_y_addr + cif->interlaced_offset;
+			//uv_addr = temp_uv_addr;
+			uv_addr = temp_uv_addr + cif->interlaced_offset;
+			commit_buf = 0; //even & odd field add
+			if (temp_y_addr == 0) {
+				VEHICLE_DGERR("%s,warnning temp_y_addr is NULL!\n", __func__);
+				spin_unlock(&cif->vbq_lock);
+				return -1;
+			}
+		}
+		WARN_ON(y_addr == cif->interlaced_offset);
+		WARN_ON(uv_addr == cif->interlaced_offset);
+	}
+
+	if (frame_ready == 0) {
+		rkcif_write_reg(cif, frm0_addr_y, y_addr);
+		rkcif_write_reg(cif, frm0_addr_uv, uv_addr);
+		cif->active[0] = temp_y_addr;
+	} else {
+		rkcif_write_reg(cif, frm1_addr_y, y_addr);
+		rkcif_write_reg(cif, frm1_addr_uv, uv_addr);
+		cif->active[1] = temp_y_addr;
+	}
+	cif->interlaced_counts++;
+	spin_unlock(&cif->vbq_lock);
+
+	return commit_buf;
+}
+
+/***************************** irq operation ******************************/
+//discard the first few frames to solve display abnormality after different model camera switch
+static int drop_frames_number;
+static irqreturn_t rk_camera_irq(int irq, void *data)
+{
+	struct vehicle_cif *cif = (struct vehicle_cif *)data;
+	u32 lastline, lastpix, ctl;
+	u32 cif_frmst, frmid, int_en;
+	unsigned int intstat, i = 0xff;
+	int frame_ready = 0;
+	int frame_phase = 0;
+	unsigned long addr;
+	int mipi_id = 0;
+
+	if (drop_frames_number > 0) {
+		VEHICLE_INFO("%s discard the first few frames!\n", __func__);
+		drop_frames_number--;
+		goto IRQ_EXIT;
+	}
+
+	VEHICLE_DG("%s enter, cifirq_normal_idx(%ld) cif->frame_idx(%d)!\n", __func__,
+					cif->irqinfo.cifirq_normal_idx, cif->frame_idx);
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		if (!cif->stopping) {
+			if (cif->irqinfo.cifirq_normal_idx == cif->frame_idx) {
+				cif->irqinfo.cifirq_abnormal_idx++;
+			} else {
+				cif->irqinfo.cifirq_normal_idx = cif->frame_idx;
+				cif->irqinfo.cifirq_abnormal_idx = 0;
+			}
+		}
+
+		intstat = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_INTSTAT);
+		lastline = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1);
+
+		/* clear all interrupts that has been triggered */
+		rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_INTSTAT, intstat);
+
+		/* when not detect new FRAME_END continue over 5 irq, reset, it's abnormal */
+		if (cif->irqinfo.cifirq_abnormal_idx >= 5) {
+			VEHICLE_DGERR(
+			 "ERROR: cifirq_abnormal_idx reach(%ld) consecutive, do reset work!!\n",
+			  cif->irqinfo.cifirq_abnormal_idx);
+//			mod_delayed_work(system_wq, &cif->work,
+//					 msecs_to_jiffies(1));
+			cif->irqinfo.cifirq_abnormal_idx = 0;
+			vehicle_cif_stat_change_notify();
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_FIFO_OVERFLOW) {
+			cif->irqinfo.csi_overflow_cnt++;
+			VEHICLE_DGERR(
+				 "ERROR: csi fifo overflow, intstat:0x%x, lastline:%d!!\n",
+				  intstat, lastline);
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_BANDWIDTH_LACK) {
+			cif->irqinfo.csi_bwidth_lack_cnt++;
+			VEHICLE_DGERR(
+				 "ERROR: csi bandwidth lack, intstat:0x%x!!\n",
+				 intstat);
+			if (cif->irqinfo.csi_bwidth_lack_cnt >= 5) {
+				//do reset work
+//				mod_delayed_work(system_wq, &cif->work,
+//						 msecs_to_jiffies(1));
+			}
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_ALL_ERROR_INTEN) {
+			cif->irqinfo.all_err_cnt++;
+			VEHICLE_DGERR(
+				 "ERROR: CSI_ALL_ERROR_INTEN:0x%x!!\n", intstat);
+			goto IRQ_EXIT;
+		}
+
+		/* if do not reach frame dma end, return irq */
+		mipi_id = vehicle_cif_csi2_g_mipi_id(intstat);
+		if (mipi_id < 0)
+			goto IRQ_EXIT;
+
+		for (i = 0; i < RKCIF_MAX_STREAM_MIPI; i++) {
+			mipi_id = vehicle_cif_csi2_g_mipi_id(intstat);
+
+			VEHICLE_DG(" i(%d)  mipi_id(%d)\n", i, mipi_id);
+			if (mipi_id < 0)
+				continue;
+
+			if (cif->stopping) {
+				vehicle_cif_csi2_s_stream(cif, 0, V4L2_MBUS_CSI2_DPHY);
+				cif->stopping = false;
+				wake_up(&cif->wq_stopped);
+				continue;
+			}
+
+			if (cif->state != RKCIF_STATE_STREAMING)
+				continue;
+
+			switch (mipi_id) {
+			case RKCIF_STREAM_MIPI_ID0:
+				frame_phase = SW_FRM_END_ID0(intstat);
+				intstat &= ~CSI_FRAME_END_ID0;
+				break;
+			case RKCIF_STREAM_MIPI_ID1:
+				frame_phase = SW_FRM_END_ID1(intstat);
+				intstat &= ~CSI_FRAME_END_ID1;
+				break;
+			case RKCIF_STREAM_MIPI_ID2:
+				frame_phase = SW_FRM_END_ID2(intstat);
+				intstat &= ~CSI_FRAME_END_ID2;
+				break;
+			case RKCIF_STREAM_MIPI_ID3:
+				frame_phase = SW_FRM_END_ID3(intstat);
+				intstat &= ~CSI_FRAME_END_ID3;
+				break;
+			}
+
+			if (frame_phase & CIF_CSI_FRAME1_READY)
+				frame_ready = 1;
+			else if (frame_phase & CIF_CSI_FRAME0_READY)
+				frame_ready = 0;
+
+			addr = cif->active[frame_ready];
+			if (vehicle_cif_next_buffer(cif, frame_ready, mipi_id) < 0)
+				VEHICLE_DG("cif_nex_buffer error, do not commit %lx\n", addr);
+			else
+				vehicle_flinger_commit_cif_buffer(addr);
+		}
+		cif->frame_idx++;
+	} else {
+		intstat = rkcif_read_reg(cif, CIF_REG_DVP_INTSTAT);
+		cif_frmst = rkcif_read_reg(cif, CIF_REG_DVP_FRAME_STATUS);
+		lastline = rkcif_read_reg(cif, CIF_REG_DVP_LAST_LINE);
+		lastline = CIF_FETCH_Y_LAST_LINE(lastline);
+		lastpix = rkcif_read_reg(cif, CIF_REG_DVP_LAST_PIX);
+		lastpix =  CIF_FETCH_Y_LAST_LINE(lastpix);
+		ctl = rkcif_read_reg(cif, CIF_REG_DVP_CTRL);
+		VEHICLE_DG("lastline:%d, lastpix:%d, ctl:%d\n",
+					  lastline, lastpix, ctl);
+
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, intstat);
+
+		if ((intstat & LINE_INT_END) && !(intstat & (FRAME_END))) {
+			if ((intstat & (PRE_INF_FRAME_END | PST_INF_FRAME_END)) == 0x0) {
+				if ((intstat & INTSTAT_ERR) == 0x0) {
+					int_en = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+					int_en &= ~LINE_INT_EN;
+					rkcif_write_reg(cif, CIF_REG_DVP_INTEN, int_en);
+				}
+			}
+		}
+
+		/* 0. error process */
+		if (cif_irq_error_process(cif, intstat) < 0) {
+			VEHICLE_DGERR("irq error, to do... reset, intstat=%x\n", intstat);
+//			mod_delayed_work(system_wq, &cif->work,
+//					 msecs_to_jiffies(1));
+			vehicle_cif_stat_change_notify();
+			goto IRQ_EXIT;
+		}
+
+		/* There are two irqs enabled:
+		 *	- PST_INF_FRAME_END: cif FIFO is ready,
+		 *	  this is prior to FRAME_END
+		 *	- FRAME_END: cif has saved frame to memory,
+		 *	  a frame ready
+		 */
+		if ((intstat & PST_INF_FRAME_END)) {
+			cif->irqinfo.cifirq_idx++;
+			if (cif->stopping) {
+			/* To stop CIF ASAP, before FRAME_END irq */
+				vehicle_cif_s_stream(cif, 0);
+				cif->stopping = false;
+				wake_up(&cif->wq_stopped);
+				goto IRQ_EXIT;
+			}
+		}
+
+		if ((intstat & FRAME_END)) {
+			int_en = rkcif_read_reg(cif, CIF_REG_DVP_INTEN);
+			int_en |= LINE_INT_EN;
+			rkcif_write_reg(cif, CIF_REG_DVP_INTEN, int_en);
+
+			if (cif->stopping) {
+				vehicle_cif_s_stream(cif, 0);
+				cif->stopping = false;
+				wake_up(&cif->wq_stopped);
+				goto IRQ_EXIT;
+			}
+
+			frmid = CIF_GET_FRAME_ID(cif_frmst);
+			if ((cif_frmst == 0xfffd0002) || (cif_frmst == 0xfffe0002)) {
+				VEHICLE_DG("frmid:%d, frmstat:0x%x\n",
+					  frmid, cif_frmst);
+				rkcif_write_reg(cif, CIF_REG_DVP_FRAME_STATUS,
+							 FRAME_STAT_CLS);
+			}
+
+			if ((!(cif_frmst & CIF_F0_READY) && !(cif_frmst & CIF_F1_READY))) {
+				VEHICLE_DG("err f0 && f1 not ready\n");
+				cif_capture_en(cif->base, 0);
+				rkcif_write_reg(cif, CIF_REG_DVP_INTEN, 0);
+				mod_delayed_work(system_wq, &cif->work,
+						 msecs_to_jiffies(1));
+				goto IRQ_EXIT;
+			}
+
+			if (cif_frmst & CIF_F0_READY)
+				frame_ready = 0;
+			else
+				frame_ready = 1;
+			addr = cif->active[frame_ready];
+			if (vehicle_cif_next_buffer(cif, frame_ready, mipi_id) < 0)
+				CIF_DG("cif_nex_buffer error, do not commit %lx\n", addr);
+			else
+				vehicle_flinger_commit_cif_buffer(addr);
+			cif->frame_idx++;
+		}
+	}
+	cif->irqinfo.all_frm_end_cnt++;
+
+IRQ_EXIT:
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t rk_camera_irq_v1(int irq, void *data)
+{
+	struct vehicle_cif *cif = (struct vehicle_cif *)data;
+	u32 lastline;
+	unsigned int intstat, i = 0xff, bak_intstat = 0;
+	int frame_ready = 0;
+	int frame_phase = 0;
+	unsigned long addr;
+	int mipi_id = 0;
+
+	if (drop_frames_number > 0) {
+		VEHICLE_INFO("%s discard the first few frames!\n", __func__);
+		drop_frames_number--;
+		goto IRQ_EXIT;
+	}
+
+	VEHICLE_DG("%s enter, cifirq_normal_idx(%ld) cif->frame_idx(%d)!\n", __func__,
+					cif->irqinfo.cifirq_normal_idx, cif->frame_idx);
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		if (!cif->stopping) {
+			if (cif->irqinfo.cifirq_normal_idx == cif->frame_idx) {
+				cif->irqinfo.cifirq_abnormal_idx++;
+			} else {
+				cif->irqinfo.cifirq_normal_idx = cif->frame_idx;
+				cif->irqinfo.cifirq_abnormal_idx = 0;
+			}
+		}
+
+		intstat = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_INTSTAT);
+		lastline = rkcif_read_reg(cif, CIF_REG_MIPI_LVDS_LINE_LINE_CNT_ID0_1);
+
+		/* clear all interrupts that has been triggered */
+		if (intstat) {
+			bak_intstat = intstat;
+			VEHICLE_DG("%s bak_intstat = %d!\n", __func__, bak_intstat);
+			rkcif_write_reg(cif, CIF_REG_MIPI_LVDS_INTSTAT, intstat);
+		} else {
+			goto IRQ_EXIT;
+		}
+
+		/* when not detect new FRAME_END continue over 5 irq, reset, it's abnormal */
+		if (cif->irqinfo.cifirq_abnormal_idx >= 5) {
+			VEHICLE_DGERR(
+				"ERROR: cifirq_abnormal_idx reach(%ld) consecutive, do reset work!!\n",
+				cif->irqinfo.cifirq_abnormal_idx);
+			cif->irqinfo.cifirq_abnormal_idx = 0;
+			vehicle_cif_stat_change_notify();
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_SIZE_ERR) {
+			cif->irqinfo.csi_size_err_cnt++;
+			VEHICLE_DGERR("ERROR: csi size error, intstat:0x%x, lastline:%d!!\n",
+				intstat, lastline);
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_FIFO_OVERFLOW_V1) {
+			cif->irqinfo.csi_overflow_cnt++;
+			VEHICLE_DGERR("ERROR: csi fifo overflow, intstat:0x%x, lastline:%d!!\n",
+				intstat, lastline);
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_BANDWIDTH_LACK_V1) {
+			cif->irqinfo.csi_bwidth_lack_cnt++;
+			VEHICLE_DGERR("ERROR: csi bandwidth lack, intstat:0x%x!!\n",
+				intstat);
+			goto IRQ_EXIT;
+		}
+
+		if (intstat & CSI_ALL_ERROR_INTEN_V1) {
+			cif->irqinfo.all_err_cnt++;
+			VEHICLE_DGERR("ERROR: CSI_ALL_ERROR_INTEN:0x%x!!\n", intstat);
+			goto IRQ_EXIT;
+		}
+
+		/* if do not reach frame dma end, return irq */
+		mipi_id = vehicle_cif_csi2_g_mipi_id(intstat);
+		if (mipi_id < 0)
+			goto IRQ_EXIT;
+
+		for (i = 0; i < RKCIF_MAX_STREAM_MIPI; i++) {
+			mipi_id = vehicle_cif_csi2_g_mipi_id(intstat);
+
+			VEHICLE_DG(" i(%d)  mipi_id(%d)\n", i, mipi_id);
+			if (mipi_id < 0)
+				continue;
+
+			if (cif->stopping) {
+				vehicle_cif_csi2_s_stream_v1(cif, 0, V4L2_MBUS_CSI2_DPHY);
+				cif->stopping = false;
+				wake_up(&cif->wq_stopped);
+				continue;
+			}
+
+			if (cif->state != RKCIF_STATE_STREAMING)
+				continue;
+
+			switch (mipi_id) {
+			case RKCIF_STREAM_MIPI_ID0:
+				frame_phase = SW_FRM_END_ID0(intstat);
+				intstat &= ~CSI_FRAME_END_ID0;
+				break;
+			case RKCIF_STREAM_MIPI_ID1:
+				frame_phase = SW_FRM_END_ID1(intstat);
+				intstat &= ~CSI_FRAME_END_ID1;
+				break;
+			case RKCIF_STREAM_MIPI_ID2:
+				frame_phase = SW_FRM_END_ID2(intstat);
+				intstat &= ~CSI_FRAME_END_ID2;
+				break;
+			case RKCIF_STREAM_MIPI_ID3:
+				frame_phase = SW_FRM_END_ID3(intstat);
+				intstat &= ~CSI_FRAME_END_ID3;
+				break;
+			}
+
+			if (frame_phase & CIF_CSI_FRAME1_READY)
+				frame_ready = 1;
+			else if (frame_phase & CIF_CSI_FRAME0_READY)
+				frame_ready = 0;
+
+			addr = cif->active[frame_ready];
+			if (vehicle_cif_next_buffer(cif, frame_ready, mipi_id) < 0)
+				VEHICLE_DGERR("cif_nex_buffer error, do not commit %lx\n", addr);
+			else
+				vehicle_flinger_commit_cif_buffer(addr);
+		}
+		cif->frame_idx++;
+	} else {
+		int ch_id;
+
+		intstat = rkcif_read_reg(cif, CIF_REG_DVP_INTSTAT);
+
+		rkcif_write_reg(cif, CIF_REG_DVP_INTSTAT, intstat);
+
+		if (intstat & DVP_SIZE_ERR) {
+			cif->irqinfo.dvp_size_err_cnt++;
+			VEHICLE_DGERR("dvp size err intstat 0x%x\n", intstat);
+		}
+
+		if (intstat & DVP_FIFO_OVERFLOW) {
+			cif->irqinfo.dvp_overflow_cnt++;
+			VEHICLE_DGERR("dvp fifo overflow err intstat 0x%x\n", intstat);
+		}
+
+		if (intstat & DVP_BANDWIDTH_LACK) {
+			cif->irqinfo.dvp_bwidth_lack_cnt++;
+			VEHICLE_DGERR("dvp bandwidth lack err intstat 0x%x\n", intstat);
+		}
+
+		if (intstat & INTSTAT_ERR_RK3588) {
+			cif->irqinfo.all_err_cnt++;
+			VEHICLE_DGERR("ERROR: DVP_ALL_ERROR_INTEN:0x%x!!\n", intstat);
+		}
+		for (i = 0; i < RKCIF_MAX_STREAM_DVP; i++) {
+			ch_id = rkcif_dvp_g_ch_id_by_fe(intstat);
+
+			if (ch_id < 0)
+				continue;
+
+			if (cif->stopping) {
+				vehicle_cif_s_stream(cif, 0);
+				cif->stopping = false;
+				wake_up(&cif->wq_stopped);
+				continue;
+			}
+
+			if (cif->state != RKCIF_STATE_STREAMING)
+				continue;
+
+			switch (ch_id) {
+			case RKCIF_STREAM_MIPI_ID0:
+				frame_phase = SW_FRM_END_ID0(intstat);
+				intstat &= ~DVP_ALL_END_ID0;
+				break;
+			case RKCIF_STREAM_MIPI_ID1:
+				frame_phase = SW_FRM_END_ID1(intstat);
+				intstat &= ~DVP_ALL_END_ID1;
+				break;
+			case RKCIF_STREAM_MIPI_ID2:
+				frame_phase = SW_FRM_END_ID2(intstat);
+				intstat &= ~DVP_ALL_END_ID2;
+				break;
+			case RKCIF_STREAM_MIPI_ID3:
+				frame_phase = SW_FRM_END_ID3(intstat);
+				intstat &= ~DVP_ALL_END_ID3;
+				break;
+			}
+
+			if (frame_phase & CIF_F0_READY)
+				frame_ready = 0;
+			else
+				frame_ready = 1;
+
+			addr = cif->active[frame_ready];
+			if (vehicle_cif_next_buffer(cif, frame_ready, ch_id) < 0)
+				VEHICLE_DGERR("cif_nex_buffer error, do not commit %lx\n", addr);
+			else
+				vehicle_flinger_commit_cif_buffer(addr);
+
+			cif->frame_idx++;
+		}
+	}
+	cif->irqinfo.all_frm_end_cnt++;
+
+IRQ_EXIT:
+	return IRQ_HANDLED;
+}
+
+#define vehicle_csi2_err_strncat(dst_str, src_str) {\
+	if (strlen(dst_str) + strlen(src_str) < CSI_ERRSTR_LEN)\
+		strncat(dst_str, src_str, strlen(src_str)); }
+
+static void vehicle_csi2_find_err_vc(int val, char *vc_info)
+{
+	int i;
+	char cur_str[CSI_VCINFO_LEN] = {0};
+
+	memset(vc_info, 0, sizeof(*vc_info));
+	for (i = 0; i < 4; i++) {
+		if ((val >> i) & 0x1) {
+			snprintf(cur_str, CSI_VCINFO_LEN, " %d", i);
+			if (strlen(vc_info) + strlen(cur_str) < CSI_VCINFO_LEN)
+				strncat(vc_info, cur_str, strlen(cur_str));
+		}
+	}
+}
+
+static void vehicle_csi2_err_print_work(struct work_struct *work)
+{
+	struct vehicle_csi2_err_state_work *err_state = container_of(work,
+							struct vehicle_csi2_err_state_work,
+							work);
+
+	pr_err("mipi_csi2: ERR%d:0x%x %s\n", err_state->err_num,
+		err_state->err_val, err_state->err_str);
+	if (err_state->err_num == 1)
+		pr_info("mipi_csi2: err_stat:0x%lx\n", err_state->err_stat);
+}
+
+static irqreturn_t vehicle_csirx_irq1(int irq, void *data)
+{
+	struct vehicle_cif *cif = (struct vehicle_cif *)data;
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	struct csi2_err_stats *err_list = NULL;
+	unsigned long err_stat = 0;
+	u32 val;
+	char err_str[CSI_ERRSTR_LEN] = {0};
+	char cur_str[CSI_ERRSTR_LEN] = {0};
+	char vc_info[CSI_VCINFO_LEN] = {0};
+
+	val = read_reg(hw->csi2_base, CSIHOST_ERR1);
+	if (val) {
+		write_reg(hw->csi2_base,
+				  CSIHOST_ERR1, 0x0);
+
+		if (val & CSIHOST_ERR1_PHYERR_SPTSYNCHS) {
+			err_list = &hw->err_list[RK_CSI2_ERR_SOTSYN];
+			err_list->cnt++;
+
+			vehicle_csi2_find_err_vc(val & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(sot sync,lane:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+
+		if (val & CSIHOST_ERR1_ERR_BNDRY_MATCH) {
+			err_list = &hw->err_list[RK_CSI2_ERR_FS_FE_MIS];
+			err_list->cnt++;
+			vehicle_csi2_find_err_vc((val >> 4) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(fs/fe miss,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+
+		}
+
+		if (val & CSIHOST_ERR1_ERR_SEQ) {
+			err_list = &hw->err_list[RK_CSI2_ERR_FRM_SEQ_ERR];
+			err_list->cnt++;
+			vehicle_csi2_find_err_vc((val >> 8) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(f_seq,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+
+		}
+
+		if (val & CSIHOST_ERR1_ERR_FRM_DATA) {
+			err_list = &hw->err_list[RK_CSI2_ERR_CRC_ONCE];
+			err_list->cnt++;
+			vehicle_csi2_find_err_vc((val >> 12) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(err_data,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+
+		}
+
+		if (val & CSIHOST_ERR1_ERR_CRC) {
+			err_list = &hw->err_list[RK_CSI2_ERR_CRC];
+			err_list->cnt++;
+			vehicle_csi2_find_err_vc((val >> 24) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(crc,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+
+		}
+
+		if (val & CSIHOST_ERR1_ERR_ECC2) {
+			err_list = &hw->err_list[RK_CSI2_ERR_CRC];
+			err_list->cnt++;
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(ecc2) ");
+			vehicle_csi2_err_strncat(err_str, cur_str);
+
+		}
+		if (val & CSIHOST_ERR1_ERR_CTRL) {
+			vehicle_csi2_find_err_vc((val >> 16) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(ctrl,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		hw->err_list[RK_CSI2_ERR_ALL].cnt++;
+		err_stat = ((hw->err_list[RK_CSI2_ERR_FS_FE_MIS].cnt & 0xff) << 8) |
+			    ((hw->err_list[RK_CSI2_ERR_ALL].cnt) & 0xff);
+
+		cif->err_state.err_val = val;
+		cif->err_state.err_num = 1;
+		cif->err_state.err_stat = err_stat;
+		strscpy(cif->err_state.err_str, err_str, CSI_ERRSTR_LEN);
+		queue_work(cif->err_state.err_print_wq, &cif->err_state.work);
+
+	}
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t vehicle_csirx_irq2(int irq, void *data)
+{
+	struct vehicle_cif *cif = (struct vehicle_cif *)data;
+	struct csi2_dphy_hw *hw = cif->dphy_hw;
+	u32 val;
+	char cur_str[CSI_ERRSTR_LEN] = {0};
+	char err_str[CSI_ERRSTR_LEN] = {0};
+	char vc_info[CSI_VCINFO_LEN] = {0};
+
+	val = read_reg(hw->csi2_base, CSIHOST_ERR2);
+	if (val) {
+		if (val & CSIHOST_ERR2_PHYERR_ESC) {
+			vehicle_csi2_find_err_vc(val & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(ULPM,lane:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		if (val & CSIHOST_ERR2_PHYERR_SOTHS) {
+			vehicle_csi2_find_err_vc((val >> 4) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(sot,lane:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		if (val & CSIHOST_ERR2_ECC_CORRECTED) {
+			vehicle_csi2_find_err_vc((val >> 8) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(ecc,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		if (val & CSIHOST_ERR2_ERR_ID) {
+			vehicle_csi2_find_err_vc((val >> 12) & 0xf, vc_info);
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(err id,vc:%s) ", vc_info);
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		if (val & CSIHOST_ERR2_PHYERR_CODEHS) {
+			snprintf(cur_str, CSI_ERRSTR_LEN, "(err code) ");
+			vehicle_csi2_err_strncat(err_str, cur_str);
+		}
+		cif->err_state.err_val = val;
+		cif->err_state.err_num = 2;
+		strscpy(cif->err_state.err_str, err_str, CSI_ERRSTR_LEN);
+		queue_work(cif->err_state.err_print_wq, &cif->err_state.work);
+
+	}
+
+	return IRQ_HANDLED;
+}
+
+int vehicle_cif_reverse_open(struct vehicle_cfg *v_cfg)
+{
+	int ret = 0;
+	struct vehicle_cif *cif = g_cif;
+
+	if (!cif)
+		return -ENODEV;
+
+	mutex_lock(&cif->stream_lock);
+	memcpy(&cif->cif_cfg, v_cfg, sizeof(struct vehicle_cfg));
+	ret = pm_runtime_get_sync(cif->dev);
+	if (ret < 0) {
+		pm_runtime_put_noidle(cif->dev);
+		VEHICLE_DGERR("%s pm_runtime_get_sync failed\n", __func__);
+		goto exit;
+	}
+
+	/*get dcphy param*/
+	if (cif->dphy_hw->chip_id == CHIP_ID_RK3588_DCPHY) {
+		if (cif->cif_cfg.dphy_param) {
+			cif->dphy_hw->dphy_param = cif->cif_cfg.dphy_param;
+			dev_info(cif->dev, "-----get dphy param from sensor----\n");
+		} else {
+			cif->dphy_hw->dphy_param = &rk3588_dcphy_param;
+			dev_info(cif->dev, "fail to get dphy param, used default value\n");
+		}
+	}
+	/* set ddr fix freq */
+	rockchip_set_system_status(SYS_STATUS_CIF0);
+	vehicle_cif_hw_soft_reset(cif);
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		/* 0. set mipi-dphy data rate */
+		cif->dphy_hw->data_rate_mbps = cif->cif_cfg.mipi_freq * 2 / 1000 / 1000;
+
+		/* 0. set csi2 & dphy clk */
+		vehicle_csi2_hw_soft_reset(cif);
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588 ||
+			cif->dphy_hw->chip_id >= CHIP_ID_RK3562)
+			vehicle_csi2_dphy_hw_do_reset(cif);
+
+		if (!cif->dphy_hw->on)
+			vehicle_csi2_clk_ctrl(cif, 1);
+
+		/*  1. stream setup */
+		cif_csi_stream_setup(cif);
+
+		/*  2. create dummy buf */
+		ret = vehicle_cif_create_dummy_buf(cif);
+		if (ret < 0)
+			VEHICLE_DGERR("Failed to create dummy_buf, %d\n", ret);
+
+		/*  3. cif init buffer */
+		if (vehicle_cif_init_buffer(cif, 1, cif->channels[0].id) < 0)
+			goto exit;
+
+		/*  4. dump cif regs */
+		vehicle_cif_csi2_dump_regs(cif);
+
+		/*  5. start stream */
+		if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF)
+			vehicle_cif_csi2_s_stream_v1(cif, 1, V4L2_MBUS_CSI2_DPHY);
+		else
+			vehicle_cif_csi2_s_stream(cif, 1, V4L2_MBUS_CSI2_DPHY);
+
+	} else {
+		/*  1. stream setup */
+		cif_stream_setup(cif);
+
+		/*  2. create dummy buf */
+		ret = vehicle_cif_create_dummy_buf(cif);
+		if (ret < 0)
+			VEHICLE_DGERR("Failed to create dummy_buf, %d\n", ret);
+
+		/*  2. cif init buffer */
+		if (vehicle_cif_init_buffer(cif, 1, 0) < 0)
+			goto exit;
+
+		/*  3. enable interrupts */
+		if (cif->chip_id < CHIP_RK3588_VEHICLE_CIF)
+			cif_interrupt_setup(cif);
+
+		/*  4. dump cif regs */
+		vehicle_cif_dvp_dump_regs(cif);
+
+		/*  5. start stream */
+		vehicle_cif_s_stream(cif, 1);
+	}
+
+	cif->stopping = false;
+	drop_frames_number = cif->drop_frames;
+
+	mutex_unlock(&cif->stream_lock);
+
+	return 0;
+
+exit:
+	mutex_unlock(&cif->stream_lock);
+	return -1;
+}
+
+int vehicle_cif_reverse_close(void)
+{
+	int ret = 0;
+	struct vehicle_cif *cif = g_cif;
+
+	if (!cif)
+		return -ENODEV;
+
+	mutex_lock(&cif->stream_lock);
+
+	VEHICLE_DG("%s cif reverse start closing\n", __func__);
+	cif->stopping = true;
+	cancel_delayed_work_sync(&(cif->work));
+	flush_delayed_work(&(cif->work));
+	cancel_work_sync(&cif->err_state.work);
+
+	ret = wait_event_timeout(cif->wq_stopped,
+				 cif->state != RKCIF_STATE_STREAMING,
+				 msecs_to_jiffies(100));
+	if (!ret) {
+		VEHICLE_DGERR("%s wait stream stop timeout!\n", __func__);
+		if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+			if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF)
+				vehicle_cif_csi2_s_stream_v1(cif, 0, V4L2_MBUS_CSI2_DPHY);
+			else
+				vehicle_cif_csi2_s_stream(cif, 0, V4L2_MBUS_CSI2_DPHY);
+		} else {
+			vehicle_cif_s_stream(cif, 0);
+		}
+		//cif->stopping = false;
+	}
+	if (cif->cif_cfg.type == V4L2_MBUS_CSI2_DPHY) {
+		vehicle_cif_csi_stream_stop(cif);
+		vehicle_csi2_hw_soft_reset(cif);
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588 ||
+			cif->dphy_hw->chip_id >= CHIP_ID_RK3562)
+			vehicle_csi2_dphy_hw_do_reset(cif);
+		if (cif->dphy_hw->on)
+			vehicle_csi2_clk_ctrl(cif, 0);
+	}
+
+	vehicle_cif_destroy_dummy_buf(cif);
+	//vehicle_csi2_hw_soft_reset(cif);
+	//vehicle_cif_hw_soft_reset(cif);
+	rockchip_clear_system_status(SYS_STATUS_CIF0);
+	mutex_unlock(&cif->stream_lock);
+	cif->stopping = false;
+
+	return 0;
+}
+
+static int cif_parse_dt(struct vehicle_cif *cif)
+{
+	struct device *dev = cif->dev;
+	struct device_node *node;
+	struct device_node *phy_node;
+	struct device_node *cif_node;
+	struct device_node *cis2_node;
+	const char *status = NULL;
+
+	if (of_property_read_u32(dev->of_node, "cif,drop-frames",
+				 &cif->drop_frames)) {
+		VEHICLE_INFO("%s:Get cif, drop-frames failed!\n", __func__);
+		cif->drop_frames = 0; //default drop frames;
+	}
+
+	if (of_property_read_u32(dev->of_node, "cif,chip-id",
+				 &cif->chip_id)) {
+		VEHICLE_INFO("%s:Get cif, chip_id failed!\n", __func__);
+		cif->chip_id = CHIP_RK3588_VEHICLE_CIF; //default rk3588;
+	}
+
+	cif_node = of_parse_phandle(dev->of_node, "rockchip,cif", 0);
+	cif->base = (char *)of_iomap(cif_node, 0);
+
+	node = of_parse_phandle(dev->of_node, "rockchip,cru", 0);
+	cif->cru_base = of_iomap(node, 0);
+	of_node_put(node);
+
+	node = of_parse_phandle(dev->of_node, "rockchip,grf", 0);
+	cif->grf_base = of_iomap(node, 0);
+	of_node_put(node);
+
+	cif->regmap_grf = syscon_regmap_lookup_by_phandle(dev->of_node, "rockchip,grf");
+	if (IS_ERR(cif->regmap_grf))
+		VEHICLE_DGERR("unable to get rockchip,grf\n");
+
+	cif->irq = irq_of_parse_and_map(cif_node, 0);
+	of_node_put(cif_node);
+	if (cif->irq < 0) {
+		VEHICLE_DGERR("%s: request cif irq failed\n", __func__);
+		goto unmap_cif;
+	}
+
+	node = of_parse_phandle(dev->of_node, "rockchip,cif-phy", 0);
+	if (!node) {
+		VEHICLE_DGERR("get cif-phy dts failed\n");
+		goto unmap_cif;
+	}
+
+	for_each_child_of_node(node, phy_node) {
+		of_property_read_string(phy_node, "status", &status);
+		if (status && !strcmp(status, "disabled"))
+			continue;
+		else
+			cif->phy_node = phy_node;
+		VEHICLE_INFO("status: %s %s\n", cif->phy_node->name, status);
+	}
+	of_node_put(node);
+	if (of_property_read_u32(cif->phy_node, "csihost-idx", &cif->csi_host_idx)) {
+		VEHICLE_INFO("Get %s csihost-idx failed! sensor link to dvp!!\n",
+				cif->phy_node->name);
+		cif->inf_id = RKCIF_DVP;
+	} else {
+		cif->inf_id = RKCIF_MIPI_LVDS;
+		VEHICLE_INFO("sensor link to %s!!\n", cif->phy_node->name);
+	}
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF &&
+		    !(cif->csi_host_idx == RKCIF_MIPI0_CSI2 ||
+		      cif->csi_host_idx == RKCIF_MIPI1_CSI2)) {
+			node = of_parse_phandle(cif->phy_node, "rockchip,csi2-dphy", 0);
+			cif->csi2_dphy_base = of_iomap(node, 0);
+			of_node_put(node);
+
+			cif->regmap_dphy_grf =
+				syscon_regmap_lookup_by_phandle(cif->phy_node, "rockchip,dphy-grf");
+			if (IS_ERR(cif->regmap_dphy_grf))
+				VEHICLE_INFO("unable to get rockchip,dphy-grf\n");
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF &&
+			cif->csi_host_idx != RKCIF_MIPI0_CSI2) {
+			node = of_parse_phandle(cif->phy_node, "rockchip,csi2-dphy", 0);
+			cif->csi2_dphy_base = of_iomap(node, 0);
+			of_node_put(node);
+
+			cif->regmap_dphy_grf =
+				syscon_regmap_lookup_by_phandle(cif->phy_node, "rockchip,dphy-grf");
+			if (IS_ERR(cif->regmap_dphy_grf))
+				VEHICLE_INFO("unable to get rockchip,dphy-grf\n");
+
+			cif->dphy_sys_grf =
+				syscon_regmap_lookup_by_phandle(cif->phy_node, "rockchip,sys-grf");
+			if (IS_ERR(cif->dphy_sys_grf))
+				VEHICLE_INFO("unable to get rockchip,sys-grf\n");
+		} else if (cif->chip_id != CHIP_RK3588_VEHICLE_CIF &&
+				cif->chip_id != CHIP_RK3576_VEHICLE_CIF) {
+			node = of_parse_phandle(cif->phy_node, "rockchip,csi2-dphy", 0);
+			cif->csi2_dphy_base = of_iomap(node, 0);
+			of_node_put(node);
+		}
+
+		cis2_node = of_parse_phandle(cif->phy_node, "rockchip,csi2", 0);
+		cif->csi2_base = of_iomap(cis2_node, 0);
+
+		cif->csi2_irq1 = irq_of_parse_and_map(cis2_node, 0);
+		if (cif->csi2_irq1 < 0) {
+			VEHICLE_DGERR("%s: request csi-intr1 failed\n", __func__);
+			of_node_put(cis2_node);
+			goto unmap_csi;
+		}
+
+		cif->csi2_irq2 = irq_of_parse_and_map(cis2_node, 1);
+		of_node_put(cis2_node);
+		if (cif->csi2_irq2 < 0) {
+			VEHICLE_DGERR("%s: request csi-intr2 failed\n", __func__);
+			goto unmap_csi;
+		}
+	}
+
+	VEHICLE_DG("%s, drop_frames = %d\n", __func__, cif->drop_frames);
+
+	return 0;
+
+unmap_csi:
+	iounmap(cif->csi2_dphy_base);
+	iounmap(cif->csi2_base);
+unmap_cif:
+	iounmap(cif->base);
+	iounmap(cif->cru_base);
+	iounmap(cif->grf_base);
+
+	return -ENODEV;
+}
+
+int vehicle_cif_init(struct vehicle_cif *cif)
+{
+	int ret;
+	struct device *dev;
+	struct rk_cif_clk *clk;
+	struct csi2_dphy_hw *dphy_hw;
+	struct clk *tmp_cif_clk = NULL;
+	int i;
+	int inf_id;
+
+	if (!cif)
+		return -ENODEV;
+
+	dev = cif->dev;
+	clk = &cif->clk;
+	g_cif = cif;
+
+	/* 0. dts parse */
+	if (cif_parse_dt(cif) < -1) {
+		VEHICLE_DGERR("%s: cif_parse_dt failed\n", __func__);
+		return -ENODEV;
+	}
+
+	inf_id = cif->inf_id;
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			if (cif->csi_host_idx == RKCIF_MIPI0_CSI2 ||
+			    cif->csi_host_idx == RKCIF_MIPI1_CSI2)
+				dphy_hw = &rk3588_csi2_dcphy_hw;
+			else
+				dphy_hw = &rk3588_csi2_dphy_hw;
+		} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+			dphy_hw = &rk3562_csi2_dphy_hw;
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->csi_host_idx == RKCIF_MIPI0_CSI2)
+				dphy_hw = &rk3576_csi2_dcphy_hw;
+			else
+				dphy_hw = &rk3576_csi2_dphy_hw;
+		} else {
+			dphy_hw = &rk3568_csi2_dphy_hw;
+		}
+	}
+
+	/*  1. cif/csi2-dphy/csi2 clk setup */
+	if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+		clk->clks_num = ARRAY_SIZE(rk3588_cif_clks);
+		clk->rsts_num = ARRAY_SIZE(rk3588_cif_rsts);
+	} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+		clk->clks_num = ARRAY_SIZE(rk3562_cif_clks);
+		clk->rsts_num = ARRAY_SIZE(rk3562_cif_rsts);
+	} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+		clk->clks_num = ARRAY_SIZE(rk3576_cif_clks);
+		clk->rsts_num = ARRAY_SIZE(rk3576_cif_rsts);
+	} else {
+		clk->clks_num = ARRAY_SIZE(rk3568_cif_clks);
+		clk->rsts_num = ARRAY_SIZE(rk3568_cif_rsts);
+	}
+
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		cif->dphy_hw = dphy_hw;
+		dphy_hw->dev = cif->dev;
+		/*get phy_index*/
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588) {
+			if (cif->csi_host_idx >= RKCIF_MIPI4_CSI2)
+				cif->dphy_hw->phy_index = 3;
+			else
+				cif->dphy_hw->phy_index = 0;
+		} else if (cif->dphy_hw->chip_id == CHIP_ID_RK3562) {
+			if (cif->csi_host_idx >= RKCIF_MIPI2_CSI2)
+				cif->dphy_hw->phy_index = 3;
+			else
+				cif->dphy_hw->phy_index = 0;
+		} else if (cif->dphy_hw->chip_id == CHIP_ID_RK3576) {
+			if (cif->csi_host_idx >= RKCIF_MIPI3_CSI2)
+				cif->dphy_hw->phy_index = 3;
+			else
+				cif->dphy_hw->phy_index = 0;
+		} else {
+			cif->dphy_hw->phy_index = 0;
+		}
+		/*get mipi dcphy*/
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588_DCPHY) {
+			struct phy *dcphy = NULL;
+			struct samsung_mipi_dcphy *dcphy_hw = NULL;
+
+			dcphy = of_phy_get(cif->phy_node, "dcphy");
+			if (IS_ERR(dcphy)) {
+				ret = PTR_ERR(dcphy);
+				dev_err(dev, "failed to get mipi dcphy: %d\n", ret);
+				return ret;
+			}
+			dcphy_hw = phy_get_drvdata(dcphy);
+			dcphy_hw->dphy_vehicle[dcphy_hw->dphy_vehicle_num] = cif->dphy_hw;
+			dcphy_hw->dphy_vehicle_num++;
+			cif->dphy_hw->samsung_phy = dcphy_hw;
+		}
+		/* csi2 mipidphy rsts */
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3562 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3576) {
+			for (i = 0; i < dphy_hw->num_dphy_rsts; i++) {
+				struct reset_control *rst = NULL;
+
+				rst = of_reset_control_get(cif->phy_node, dphy_hw->dphy_rsts[i]);
+				if (IS_ERR(rst)) {
+					dev_err(dev, "failed to get %s\n", dphy_hw->dphy_rsts[i]);
+					return PTR_ERR(rst);
+				}
+				dphy_hw->dphy_rst[i] = rst;
+			}
+		} else {
+			dev_info(dev, "use mipi dcphy, no need request rst\n");
+		}
+
+		/* csi2 mipidphy clks */
+		for (i = 0; i < dphy_hw->num_dphy_clks; i++) {
+			struct clk *tmp_clk =
+				of_clk_get_by_name(cif->phy_node, dphy_hw->dphy_clks[i].id);
+
+			if (IS_ERR(tmp_clk)) {
+				dev_err(dev, "failed to get %s\n", dphy_hw->dphy_clks[i].id);
+				return PTR_ERR(tmp_clk);
+			}
+			dev_info(dev, "clk get %s\n", dphy_hw->dphy_clks[i].id);
+			dphy_hw->dphy_clks[i].clk = tmp_clk;
+		}
+
+		/* csi2 clks */
+		for (i = 0; i < dphy_hw->num_csi2_clks; i++) {
+			struct clk *tmp_clk =
+				of_clk_get_by_name(cif->phy_node, dphy_hw->csi2_clks[i].id);
+
+			if (IS_ERR(tmp_clk)) {
+				dev_err(dev, "failed to get %s\n", dphy_hw->csi2_clks[i].id);
+				return PTR_ERR(tmp_clk);
+			}
+			dev_info(dev, "clk get %s\n", dphy_hw->csi2_clks[i].id);
+			dphy_hw->csi2_clks[i].clk = tmp_clk;
+		}
+
+		/* csi2 rsts */
+		for (i = 0; i < dphy_hw->num_csi2_rsts; i++) {
+			struct reset_control *rst = NULL;
+
+			rst = of_reset_control_get(cif->phy_node, dphy_hw->csi2_rsts[i]);
+			if (IS_ERR(rst)) {
+				dev_err(dev, "failed to get %s\n", dphy_hw->csi2_rsts[i]);
+				return PTR_ERR(rst);
+			}
+			dphy_hw->csi2_rst[i] = rst;
+		}
+		dphy_hw->on = false;
+	}
+	/* vicap clks */
+	if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+		for (i = 0; i < clk->clks_num; i++) {
+			tmp_cif_clk = devm_clk_get(dev, rk3588_cif_clks[i]);
+
+			if (IS_ERR(tmp_cif_clk)) {
+				dev_err(dev, "failed to get %s\n", rk3588_cif_clks[i]);
+				return PTR_ERR(tmp_cif_clk);
+			}
+			clk->clks[i] = tmp_cif_clk;
+			clk->on = false;
+		}
+	} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+		for (i = 0; i < clk->clks_num; i++) {
+			tmp_cif_clk = devm_clk_get(dev, rk3562_cif_clks[i]);
+
+			if (IS_ERR(tmp_cif_clk)) {
+				dev_err(dev, "failed to get %s\n", rk3562_cif_clks[i]);
+				return PTR_ERR(tmp_cif_clk);
+			}
+			clk->clks[i] = tmp_cif_clk;
+			clk->on = false;
+		}
+	} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+		for (i = 0; i < clk->clks_num; i++) {
+			tmp_cif_clk = devm_clk_get(dev, rk3576_cif_clks[i]);
+
+			if (IS_ERR(tmp_cif_clk)) {
+				dev_err(dev, "failed to get %s\n", rk3576_cif_clks[i]);
+				return PTR_ERR(tmp_cif_clk);
+			}
+			clk->clks[i] = tmp_cif_clk;
+			clk->on = false;
+		}
+	} else {
+		for (i = 0; i < clk->clks_num; i++) {
+			tmp_cif_clk = devm_clk_get(dev, rk3568_cif_clks[i]);
+
+			if (IS_ERR(tmp_cif_clk)) {
+				dev_err(dev, "failed to get %s\n", rk3568_cif_clks[i]);
+				return PTR_ERR(tmp_cif_clk);
+			}
+			clk->clks[i] = tmp_cif_clk;
+			clk->on = false;
+		}
+	}
+
+	/* vicap rsts */
+	if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+		for (i = 0; i < clk->rsts_num; i++) {
+			struct reset_control *rst = NULL;
+
+			if (rk3588_cif_rsts[i])
+				rst = devm_reset_control_get(dev, rk3588_cif_rsts[i]);
+			if (IS_ERR(rst)) {
+				dev_err(dev, "failed to get %s\n", rk3588_cif_rsts[i]);
+				return PTR_ERR(rst);
+			}
+			clk->cif_rst[i] = rst;
+		}
+	} else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF) {
+		for (i = 0; i < clk->rsts_num; i++) {
+			struct reset_control *rst = NULL;
+
+			if (rk3562_cif_rsts[i])
+				rst = devm_reset_control_get(dev, rk3562_cif_rsts[i]);
+			if (IS_ERR(rst)) {
+				dev_err(dev, "failed to get %s\n", rk3562_cif_rsts[i]);
+				return PTR_ERR(rst);
+			}
+			clk->cif_rst[i] = rst;
+		}
+	} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+		for (i = 0; i < clk->rsts_num; i++) {
+			struct reset_control *rst = NULL;
+
+			if (rk3576_cif_rsts[i])
+				rst = devm_reset_control_get(dev, rk3576_cif_rsts[i]);
+			if (IS_ERR(rst)) {
+				dev_err(dev, "failed to get %s\n", rk3576_cif_rsts[i]);
+				return PTR_ERR(rst);
+			}
+			clk->cif_rst[i] = rst;
+		}
+	} else {
+		for (i = 0; i < clk->rsts_num; i++) {
+			struct reset_control *rst = NULL;
+
+			if (rk3568_cif_rsts[i])
+				rst = devm_reset_control_get(dev, rk3568_cif_rsts[i]);
+			if (IS_ERR(rst)) {
+				dev_err(dev, "failed to get %s\n", rk3568_cif_rsts[i]);
+				return PTR_ERR(rst);
+			}
+			clk->cif_rst[i] = rst;
+		}
+	}
+
+	/*  2. set cif clk & sensor mclk */
+	rk_cif_mclk_ctrl(cif, 1);
+	INIT_DELAYED_WORK(&cif->work, vehicle_cif_reset_work_func);
+
+	if (inf_id == RKCIF_MIPI_LVDS)
+		/*  2. set csi2 & dphy clk */
+		if (!cif->dphy_hw->on)
+			vehicle_csi2_clk_ctrl(cif, 1);
+
+	/*  3. request cif irq & mipi csi irq1-2 */
+	if (cif->chip_id >= CHIP_RK3588_VEHICLE_CIF) {
+		ret = devm_request_irq(dev, cif->irq,
+				rk_camera_irq_v1, IRQF_SHARED, "vehicle_cif", cif);
+		if (ret < 0) {
+			VEHICLE_DGERR("request cif irq failed!\n");
+			return -EINVAL;
+		}
+	} else {
+		ret = devm_request_irq(dev, cif->irq,
+				rk_camera_irq, IRQF_SHARED, "vehicle_cif", cif);
+		if (ret < 0) {
+			VEHICLE_DGERR("request cif irq failed!\n");
+			return -EINVAL;
+		}
+	}
+
+	VEHICLE_DG("%s(%d):\n", __func__, __LINE__);
+
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		ret = devm_request_irq(dev, cif->csi2_irq1, vehicle_csirx_irq1,
+				IRQF_SHARED, "vehicle_csi_intr1", cif);
+		if (ret < 0) {
+			VEHICLE_DGERR("request csirx irq1 failed!\n");
+			return -EINVAL;
+		}
+
+		ret = devm_request_irq(dev, cif->csi2_irq2, vehicle_csirx_irq2,
+				IRQF_SHARED, "vehicle_csi_intr2", cif);
+		if (ret < 0) {
+			VEHICLE_DGERR("request csirx irq2 failed!\n");
+			return -EINVAL;
+		}
+	}
+	/*  4. set cif regs */
+	if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF)
+		cif->cif_regs = rk3588_cif_regs;
+	else if (cif->chip_id == CHIP_RK3562_VEHICLE_CIF)
+		cif->cif_regs = rk3562_cif_regs;
+	else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF)
+		cif->cif_regs = rk3576_cif_regs;
+	else
+		cif->cif_regs = rk3568_cif_regs;
+
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		/* 5. set csi2-mipi-dphy reg */
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3568 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3562 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3576)
+			cif->dphy_hw->csi2_dphy_base = cif->csi2_dphy_base;
+
+		/* 7. set mipi-csi2 reg */
+		cif->dphy_hw->csi2_base = cif->csi2_base;
+
+		/* 8. set dphy grf regmap */
+		if (cif->chip_id == CHIP_RK3588_VEHICLE_CIF) {
+			if (cif->dphy_hw->chip_id == CHIP_ID_RK3588) {
+				cif->dphy_hw->regmap_grf = cif->regmap_dphy_grf;
+				cif->dphy_hw->regmap_sys_grf = cif->regmap_grf;
+			}
+		} else if (cif->chip_id == CHIP_RK3576_VEHICLE_CIF) {
+			if (cif->dphy_hw->chip_id == CHIP_ID_RK3576) {
+				cif->dphy_hw->regmap_grf = cif->regmap_dphy_grf;
+				cif->dphy_hw->regmap_sys_grf = cif->dphy_sys_grf;
+			}
+		} else {
+			cif->dphy_hw->regmap_grf = cif->regmap_grf;
+		}
+		mutex_init(&dphy_hw->mutex);
+	}
+
+	if (cif->inf_id == RKCIF_MIPI_LVDS && cif->chip_id <= CHIP_RK3562_VEHICLE_CIF)
+		cif->use_hw_interlace = false;
+	else
+		cif->use_hw_interlace = true;
+
+	/* 9. init waitqueue */
+	atomic_set(&cif->reset_status, 0);
+	init_waitqueue_head(&cif->wq_stopped);
+
+	spin_lock_init(&cif->vbq_lock);
+
+	INIT_WORK(&cif->err_state.work, vehicle_csi2_err_print_work);
+	cif->err_state.err_print_wq = create_workqueue("cis2_err_print_queue");
+	if (cif->err_state.err_print_wq == NULL) {
+		dev_err(dev, "%s: %s create failed.\n", __func__,
+			"csi2_err_print_wq");
+	}
+
+	return 0;
+}
+
+int vehicle_cif_deinit(struct vehicle_cif *cif)
+{
+	struct rk_cif_clk *clk = &cif->clk;
+	struct device *dev = cif->dev;
+	int i;
+	struct csi2_dphy_hw *dphy_hw = cif->dphy_hw;
+	int inf_id = cif->inf_id;
+
+	// vehicle_cif_s_stream(cif, 0);
+	// vehicle_cif_do_stop_stream(cif);
+
+	/* set csi2-dphy csi cif clk */
+	rk_cif_mclk_ctrl(cif, 0);
+	if (inf_id == RKCIF_MIPI_LVDS)
+		if (cif->dphy_hw->on)
+			vehicle_csi2_clk_ctrl(cif, 0);
+
+	/* vicap rsts release */
+	for (i = 0; i < clk->rsts_num; i++)
+		reset_control_put(clk->cif_rst[i]);
+
+	/* vicap clk release */
+	for (i = 0; i < clk->clks_num; i++)
+		devm_clk_put(dev, clk->clks[i]);
+
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		/*dcphy put*/
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588_DCPHY) {
+			struct samsung_mipi_dcphy *dcphy_hw = cif->dphy_hw->samsung_phy;
+			struct csi2_dphy_hw *csi2_dphy = NULL;
+
+			for (i = 0; i < dcphy_hw->dphy_vehicle_num; i++) {
+				csi2_dphy = dcphy_hw->dphy_vehicle[i];
+				if (csi2_dphy) {
+					dcphy_hw->dphy_vehicle[i] = NULL;
+					dcphy_hw->dphy_vehicle_num--;
+					break;
+				}
+			}
+		}
+		/* dphy clks release */
+		for (i = 0; i < dphy_hw->num_dphy_clks; i++)
+			clk_put(dphy_hw->dphy_clks[i].clk);
+		/* dphy rsts release */
+		if (cif->dphy_hw->chip_id == CHIP_ID_RK3588 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3562 ||
+		    cif->dphy_hw->chip_id == CHIP_ID_RK3576) {
+			for (i = 0; i < dphy_hw->num_dphy_rsts; i++)
+				reset_control_put(dphy_hw->dphy_rst[i]);
+		}
+		/* csi2 clks release */
+		for (i = 0; i < dphy_hw->num_csi2_clks; i++)
+			clk_put(dphy_hw->csi2_clks[i].clk);
+		/* csi2 resets release */
+		for (i = 0; i < dphy_hw->num_csi2_rsts; i++)
+			reset_control_put(dphy_hw->csi2_rst[i]);
+
+		mutex_destroy(&dphy_hw->mutex);
+	}
+
+	devm_free_irq(dev, cif->irq, cif);
+	if (inf_id == RKCIF_MIPI_LVDS) {
+		devm_free_irq(dev, cif->csi2_irq1, cif);
+		devm_free_irq(dev, cif->csi2_irq2, cif);
+	}
+	if (cif->err_state.err_print_wq) {
+		flush_workqueue(cif->err_state.err_print_wq);
+		destroy_workqueue(cif->err_state.err_print_wq);
+	}
+
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_cif.h b/drivers/video/rockchip/vehicle/vehicle_cif.h
new file mode 100644
index 0000000000000..372a30da80a25
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_cif.h
@@ -0,0 +1,191 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_CIF_H
+#define __VEHICLE_CIF_H
+
+#include "vehicle_cfg.h"
+#include "vehicle_cif_regs.h"
+#include "../../../media/platform/rockchip/cif/dev.h"
+#include <linux/dma-mapping.h>
+
+enum vehicle_rkcif_chip_id {
+	CHIP_RK3568_VEHICLE_CIF = 0x0,
+	CHIP_RK3588_VEHICLE_CIF,
+	CHIP_RK3562_VEHICLE_CIF,
+	CHIP_RK3576_VEHICLE_CIF,
+};
+
+enum rkcif_csi_host_idx {
+	RKCIF_MIPI0_CSI2 = 0x0,
+	RKCIF_MIPI1_CSI2,
+	RKCIF_MIPI2_CSI2,
+	RKCIF_MIPI3_CSI2,
+	RKCIF_MIPI4_CSI2,
+	RKCIF_MIPI5_CSI2,
+};
+
+struct vehicle_rkcif_dummy_buffer {
+	void *vaddr;
+	dma_addr_t dma_addr;
+	u32 size;
+};
+
+struct rk_cif_clk {
+	/************clk************/
+	struct clk	*clks[RKCIF_MAX_BUS_CLK];
+	int		clks_num;
+	/************reset************/
+	struct reset_control	*cif_rst[RKCIF_MAX_RESET];
+	int		rsts_num;
+	/*  spinlock_t lock; */
+	bool		on;
+};
+
+struct rk_cif_irqinfo {
+	unsigned int irq;
+	unsigned long cifirq_idx;
+	unsigned long cifirq_normal_idx;
+	unsigned long cifirq_abnormal_idx;
+	unsigned long dmairq_idx;
+
+	/* @csi_overflow_cnt: count of csi overflow irq
+	 * @csi_bwidth_lack_cnt: count of csi bandwidth lack irq
+	 * @dvp_bus_err_cnt: count of dvp bus err irq
+	 * @dvp_overflow_cnt: count dvp overflow irq
+	 * @dvp_line_err_cnt: count dvp line err irq
+	 * @dvp_pix_err_cnt: count dvp pix err irq
+	 * @all_frm_end_cnt: raw frame end count
+	 * @all_err_cnt: all err count
+	 * @
+	 */
+
+	u64 csi_overflow_cnt;
+	u64 csi_bwidth_lack_cnt;
+	u64 dvp_bus_err_cnt;
+	u64 dvp_overflow_cnt;
+	u64 dvp_line_err_cnt;
+	u64 dvp_pix_err_cnt;
+	u64 all_frm_end_cnt;
+	u64 all_err_cnt;
+	u64 dvp_size_err_cnt;
+	u64 dvp_bwidth_lack_cnt;
+	u64 csi_size_err_cnt;
+};
+
+#define RKCIF_MAX_CSI_CHANNEL	4
+struct vehicle_csi_channel_info {
+	unsigned char	id;
+	unsigned char	enable;	/* capture enable */
+	unsigned char	vc;
+	unsigned char	data_type;
+	unsigned char	crop_en;
+	unsigned char	cmd_mode_en;
+	unsigned char	fmt_val;
+	unsigned int	width;
+	unsigned int	height;
+	unsigned int	virtual_width;
+	unsigned int	crop_st_x;
+	unsigned int	crop_st_y;
+};
+
+struct vehicle_csi2_err_state_work {
+	struct workqueue_struct *err_print_wq;
+	struct work_struct work;
+	char err_str[CSI_ERRSTR_LEN];
+	u32 err_val;
+	u32 err_num;
+	unsigned long err_stat;
+};
+
+struct vehicle_cif {
+	struct		device *dev;
+	struct		device_node *phy_node;
+	struct		rk_cif_clk clk;
+	struct		vehicle_cfg cif_cfg;
+	char		*base;  /*cif base addr*/
+	//unsigned long cru_base;
+	//unsigned long grf_base;
+	void __iomem	*cru_base; /*cru base addr*/
+	void __iomem	*grf_base; /*grf base addr*/
+	void __iomem	*csi2_dphy_base; /*csi2_dphy base addr*/
+	void __iomem	*csi2_base; /*csi2 base addr*/
+	struct		delayed_work work;
+
+	bool		is_enabled;
+	u32		frame_buf[MAX_BUF_NUM];
+	u32		current_buf_index;
+	u32		last_buf_index;
+	u32		active[2];
+	int		irq;
+	int		csi2_irq1;
+	int		csi2_irq2;
+	int		drop_frames;
+	struct		rk_cif_irqinfo irqinfo;
+	const		struct vehicle_cif_reg *cif_regs;
+	struct		regmap *regmap_grf;
+	struct		regmap *regmap_dphy_grf;
+	struct		regmap *dphy_sys_grf;
+	unsigned int	frame_idx;
+	struct	vehicle_rkcif_dummy_buffer	dummy_buf;
+	struct csi2_dphy_hw	*dphy_hw;
+	int		num_channels;
+	int		chip_id;
+	int		inf_id;
+	unsigned int	csi_host_idx;
+	struct		vehicle_csi_channel_info channels[RKCIF_MAX_CSI_CHANNEL];
+	spinlock_t	vbq_lock; /* vfd lock */
+	bool		interlaced_enable;
+	unsigned int	interlaced_offset;
+	unsigned int	interlaced_counts;
+	unsigned long	*interlaced_buffer;
+	atomic_t	reset_status;
+	wait_queue_head_t	wq_stopped;
+	bool		stopping;
+	struct mutex	stream_lock;
+	enum rkcif_state	state;
+	struct vehicle_csi2_err_state_work err_state;
+	bool		use_hw_interlace;
+};
+
+int vehicle_cif_init_mclk(struct vehicle_cif *cif);
+int vehicle_cif_init(struct vehicle_cif *cif);
+int vehicle_cif_deinit(struct vehicle_cif *cif);
+
+int vehicle_cif_reverse_open(struct vehicle_cfg *v_cfg);
+
+int vehicle_cif_reverse_close(void);
+int vehicle_wait_cif_reset_done(void);
+
+/* CIF IRQ STAT*/
+#define DMA_FRAME_END					(0x01 << 0)
+#define LINE_END					(0x01 << 1)
+#define IFIFO_OF					(0x01 << 4)
+#define DFIFO_OF					(0x01 << 5)
+#define PRE_INF_FRAME_END				(0x01 << 8)
+#define PST_INF_FRAME_END				(0x01 << 9)
+
+enum rk_camera_signal_polarity {
+	RK_CAMERA_DEVICE_SIGNAL_HIGH_LEVEL = 1,
+	RK_CAMERA_DEVICE_SIGNAL_LOW_LEVEL = 0,
+};
+
+enum rk_camera_device_type {
+	RK_CAMERA_DEVICE_BT601_8	= 0x10000011,
+	RK_CAMERA_DEVICE_BT601_10	= 0x10000012,
+	RK_CAMERA_DEVICE_BT601_12	= 0x10000014,
+	RK_CAMERA_DEVICE_BT601_16	= 0x10000018,
+
+	RK_CAMERA_DEVICE_BT656_8	= 0x10000021,
+	RK_CAMERA_DEVICE_BT656_10	= 0x10000022,
+	RK_CAMERA_DEVICE_BT656_12	= 0x10000024,
+	RK_CAMERA_DEVICE_BT656_16	= 0x10000028,
+
+	RK_CAMERA_DEVICE_CVBS_NTSC	= 0x20000001,
+	RK_CAMERA_DEVICE_CVBS_PAL	= 0x20000002
+};
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_cif_regs.h b/drivers/video/rockchip/vehicle/vehicle_cif_regs.h
new file mode 100644
index 0000000000000..08e1b1f892d20
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_cif_regs.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle Driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+
+#ifndef _VEHICLE_RKCIF_REGS_H
+#define _VEHICLE_RKCIF_REGS_H
+#include "../../../media/platform/rockchip/cif/regs.h"
+
+struct vehicle_cif_reg {
+	u32 offset;
+	char *name;
+};
+
+#define CIF_REG_NAME(_offset, _name)	{ .offset = (_offset), .name = (_name), }
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_dev.c b/drivers/video/rockchip/vehicle/vehicle_dev.c
new file mode 100644
index 0000000000000..6e96f1b30f146
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_dev.c
@@ -0,0 +1,116 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_dev.c
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      Zhiqin Wei <wzq@rock-chips.com>
+ *
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/mm.h>
+#include <linux/miscdevice.h>
+#include <linux/proc_fs.h>
+
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/uaccess.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+#include <linux/proc_fs.h>
+#include <linux/poll.h>
+#include <linux/bitops.h>
+#include <linux/moduleparam.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include "vehicle_main.h"
+#include "vehicle_cfg.h"
+
+static int vechile_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int vechile_close(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static ssize_t vechile_write(struct file *file, const char __user *buf,
+			     size_t size, loff_t *ppos)
+{
+	int ret = 0;
+	char data[22] = "";
+
+	ret = copy_from_user(data, buf, 18);
+
+	if (ret)
+		return -1;
+	if (memcmp(data, "88", 2) == 0) {
+		vehicle_android_is_ready_notify();
+		VEHICLE_INFO("android already up, set vehicle in bottom\n");
+	} else {
+		vehicle_apk_state_change(data);
+		VEHICLE_INFO("apk_state_change, open dvr\n");
+	}
+
+	return size;
+}
+
+static ssize_t
+vechile_read(struct file *file, char __user *buf, size_t size, loff_t *ppos)
+{
+	return 1;
+}
+
+static const struct file_operations vechile_fops = {
+	.owner      = THIS_MODULE,
+	/*.compat_ioctl      = vechile_ioctl,*/
+	.open       = vechile_open,
+	.release    = vechile_close,
+	.write  = vechile_write,
+	.read = vechile_read,
+};
+
+static struct miscdevice vechile_dev = {
+	.minor		= MISC_DYNAMIC_MINOR,
+	.name		= "vehicle",
+	.fops		= &vechile_fops,
+};
+
+static int __init vechile_module_init(void)
+{
+	int ret = 0;
+
+	/* register misc device*/
+	ret = misc_register(&vechile_dev);
+	if (ret) {
+		VEHICLE_DGERR("ERROR: could not register vechile dev\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+void vechile_module_exit(void)
+{
+	misc_deregister(&vechile_dev);
+}
+
+module_init(vechile_module_init);
+
+MODULE_LICENSE("GPL");
diff --git a/drivers/video/rockchip/vehicle/vehicle_flinger.c b/drivers/video/rockchip/vehicle/vehicle_flinger.c
new file mode 100644
index 0000000000000..f645bfa9346d6
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_flinger.c
@@ -0,0 +1,1519 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_flinger.c
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *	Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/atomic.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/kthread.h>
+#include <linux/fb.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <asm/div64.h>
+#include <linux/uaccess.h>
+#include <linux/linux_logo.h>
+#include <linux/dma-mapping.h>
+#include <linux/regulator/consumer.h>
+#include <linux/of_address.h>
+#include <linux/memblock.h>
+#include <linux/kthread.h>
+#include <linux/fdtable.h>
+#include <linux/miscdevice.h>
+#ifdef CONFIG_OF
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_gpio.h>
+#include <video/of_display_timing.h>
+#include <video/display_timing.h>
+#endif
+
+#include "vehicle_flinger.h"
+#include "../../../gpu/drm/rockchip/rockchip_drm_direct_show.h"
+#include "../drivers/video/rockchip/rga3/include/rga_drv.h"
+
+static int vehicle_dump_cif;
+static int vehicle_dump_rga;
+static int vehicle_dump_vop;
+
+enum force_value {
+	FORCE_WIDTH = 1920,
+	FORCE_HEIGHT = 1080,
+	FORCE_STRIDE = 1920,
+	FORCE_XOFFSET = 0,
+	FORCE_YOFFSET = 0,
+	FORCE_FORMAT = HAL_PIXEL_FORMAT_YCrCb_NV12,
+	FORCE_ROTATION = RGA_TRANSFORM_ROT_0,
+};
+
+enum {
+	NUM_SOURCE_BUFFERS = 5, /*5 src buffer for cif*/
+	NUM_TARGET_BUFFERS = 3, /*3 dst buffer rga*/
+};
+
+enum buffer_state {
+	UNKNOWN = 0,
+	FREE,
+	DEQUEUE,
+	QUEUE,
+	ACQUIRE,
+	DISPLAY,
+};
+
+struct rect {
+	size_t x;
+	size_t y;
+	size_t w;
+	size_t h;
+	size_t s;
+	size_t f;
+};
+
+struct graphic_buffer {
+	struct list_head list;
+	uint32_t handle;
+	struct rockchip_drm_direct_show_buffer *drm_buffer;
+	int fd;
+	struct sync_fence *rel_fence;
+	struct rect src;
+	struct rect dst;
+	enum buffer_state state;
+	unsigned long phy_addr;
+	void *vir_addr;
+	int rotation;
+	int offset;
+	int len;
+	int width;
+	int height;
+	int stride;
+	int format;
+	struct work_struct render_work;
+	ktime_t timestamp;
+};
+
+struct queue_buffer {
+	struct list_head list;
+	struct graphic_buffer *buffer;
+};
+
+struct flinger {
+	struct device *dev;
+	struct ion_client *ion_client;
+	struct work_struct init_work;
+	struct work_struct render_work;
+	struct workqueue_struct *render_workqueue;
+	struct mutex source_buffer_lock;/*src buffer lock*/
+	struct mutex target_buffer_lock;/*dst buffer lock*/
+	struct graphic_buffer source_buffer[NUM_SOURCE_BUFFERS];
+	struct graphic_buffer target_buffer[NUM_TARGET_BUFFERS];
+	struct mutex queue_buffer_lock;
+	struct list_head queue_buffer_list;
+	wait_queue_head_t worker_wait;
+	atomic_t worker_cond_atomic;
+	atomic_t worker_running_atomic;
+	int source_index;
+	int target_index;
+	struct vehicle_cfg v_cfg;
+	int cvbs_field_count;
+	struct graphic_buffer *last_src_buffer;
+	/*debug*/
+	int debug_cif_count;
+	int debug_vop_count;
+	bool running;
+	struct drm_device *drm_dev;
+	struct drm_crtc *crtc;
+	struct drm_plane *plane;
+	const char *crtc_name;
+	const char *plane_name;
+};
+
+static struct flinger *flinger;
+
+static int rk_flinger_queue_work(struct flinger *flinger,
+				 struct graphic_buffer *src_buffer);
+
+static int rk_flinger_alloc_bpp(int format)
+{
+	int width = 4;
+
+	switch (format) {
+	case HAL_PIXEL_FORMAT_RGB_565:
+		width = 2;
+		break;
+	case HAL_PIXEL_FORMAT_RGB_888:
+		width =  3;
+		break;
+	case HAL_PIXEL_FORMAT_RGBA_8888:
+		width =  4;
+		break;
+	case HAL_PIXEL_FORMAT_RGBX_8888:
+		width =  4;
+		break;
+	case HAL_PIXEL_FORMAT_BGRA_8888:
+		width =  4;
+		break;
+	case HAL_PIXEL_FORMAT_YCrCb_NV12:
+		width =  2;
+		break;
+	default:
+		VEHICLE_INFO("%s: unsupported format: 0x%x\n", __func__, format);
+		break;
+	}
+
+	return width;
+}
+
+static int rk_flinger_HAL_format_to_DRM(int format)
+{
+	int drm_format = 0;
+
+	switch (format) {
+	case HAL_PIXEL_FORMAT_RGBX_8888:
+		drm_format =  DRM_FORMAT_XRGB8888;
+		break;
+	case HAL_PIXEL_FORMAT_YCrCb_NV12:
+		drm_format =  DRM_FORMAT_NV12;
+		break;
+	case HAL_PIXEL_FORMAT_RGB_888:
+		drm_format =  DRM_FORMAT_RGB888;
+		break;
+	case HAL_PIXEL_FORMAT_RGB_565:
+		drm_format =  DRM_FORMAT_RGB565;
+		break;
+	default:
+		VEHICLE_INFO("%s: unsupported format: 0x%x\n", __func__, format);
+		break;
+	}
+
+	return drm_format;
+}
+
+static int rk_flinger_alloc_buffer(struct flinger *flg,
+				   struct graphic_buffer *buffer,
+				   int w, int h,
+				   int s, int f)
+{
+	unsigned long phy_addr;
+	size_t len;
+	int bpp;
+	int ret = 0;
+	struct rockchip_drm_direct_show_buffer *create_buffer;
+
+	VEHICLE_DG("------------alloc buffer start---------\n");
+	if (!flg)
+		return -ENODEV;
+
+	if (!buffer)
+		return -EINVAL;
+
+	bpp = rk_flinger_alloc_bpp(f);
+	len = s * h * bpp;
+
+	create_buffer = kmalloc(sizeof(struct rockchip_drm_direct_show_buffer), GFP_KERNEL);
+	if (!create_buffer)
+		return -ENOMEM;
+	create_buffer->width = w;
+	create_buffer->height = h;
+	create_buffer->pixel_format = rk_flinger_HAL_format_to_DRM(f);
+	create_buffer->flag = ROCKCHIP_BO_CONTIG;
+
+	ret = rockchip_drm_direct_show_alloc_buffer(flg->drm_dev, create_buffer);
+	if (ret)
+		VEHICLE_DGERR("error: failed to alloc drm buffer\n");
+
+	VEHICLE_DG("-----creat buffer over-----\n");
+	buffer->vir_addr = create_buffer->vir_addr[0];
+	buffer->handle = create_buffer->dmabuf_fd;
+	phy_addr = create_buffer->phy_addr[0];
+	buffer->fd = create_buffer->dmabuf_fd;
+	buffer->drm_buffer = create_buffer;
+
+	buffer->rel_fence = NULL;
+	buffer->phy_addr = phy_addr;
+	buffer->rotation = 0;
+	buffer->width = w;
+	buffer->height = h;
+	buffer->stride = s;
+	buffer->format = f;
+	buffer->len = len;
+
+	return ret;
+}
+
+static int rk_flinger_free_buffer(struct flinger *flinger,
+			   struct graphic_buffer *buffer)
+{
+	if (!flinger)
+		return -ENODEV;
+
+	if (!buffer)
+		return -EINVAL;
+
+	if (buffer->drm_buffer)
+		rockchip_drm_direct_show_free_buffer(flinger->drm_dev,
+							buffer->drm_buffer);
+
+	return 0;
+}
+
+static int rk_flinger_create_worker(struct flinger *flinger)
+{
+	struct workqueue_struct *wq = NULL;
+
+	wq = create_singlethread_workqueue("flinger-render");
+	if (!wq) {
+		VEHICLE_DGERR("wzqtest Failed to create flinger workqueue\n");
+		return -ENODEV;
+	}
+	flinger->render_workqueue = wq;
+
+	return 0;
+}
+
+static int rk_flinger_destroy_worker(struct flinger *flinger)
+{
+	if (!flinger)
+		return -ENODEV;
+
+	if (flinger->render_workqueue)
+		destroy_workqueue(flinger->render_workqueue);
+
+	return 0;
+}
+
+static int vehicle_flinger_parse_dt(struct flinger *flinger)
+{
+	struct device *dev = flinger->dev;
+
+	if (of_property_read_string(dev->of_node, "vehicle,crtc_name", &flinger->crtc_name)) {
+		dev_info(dev, "%s: get crtc_name failed, use default!\n", __func__);
+		flinger->crtc_name = "video_port3";
+	} else {
+		dev_info(dev, "%s: get crtc name from dts, crtc-name = %s\n",
+							__func__, flinger->crtc_name);
+	}
+
+	if (of_property_read_string(dev->of_node, "vehicle,plane_name", &flinger->plane_name)) {
+		dev_info(dev, "%s: get crtc_name failed, use default!\n", __func__);
+		flinger->plane_name = "Esmart3-win0";
+	} else {
+		dev_info(dev, "%s: get crtc name from dts, crtc-name = %s\n",
+							__func__, flinger->plane_name);
+	}
+
+	return 0;
+}
+
+int vehicle_flinger_init(struct device *dev, struct vehicle_cfg *v_cfg)
+{
+	struct graphic_buffer *buffer;
+	struct flinger *flg = NULL;
+	int i, ret, w, h, s, f;
+	static bool inited;
+
+	if (inited)
+		return 0;
+
+	VEHICLE_INFO("%s: v_cfg->rotate_mirror(0x%x)\n", __func__, v_cfg->rotate_mirror);
+
+	// if (FORCE_ROTATION == RGA_TRANSFORM_ROT_270 || FORCE_ROTATION == RGA_TRANSFORM_ROT_90) {
+	if ((v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK) == 0x01 ||
+	    (v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK) == 0x04) {
+		w = FORCE_WIDTH;
+		h = ALIGN(FORCE_HEIGHT, 64);
+		s = ALIGN(FORCE_HEIGHT, 64);
+		f = FORCE_FORMAT;
+	} else {
+		w = ALIGN(FORCE_WIDTH, 64);
+		h = FORCE_HEIGHT;
+		s = ALIGN(FORCE_STRIDE, 64);
+		f = FORCE_FORMAT;
+	}
+
+	flg = kzalloc(sizeof(*flg), GFP_KERNEL);
+	if (!flg) {
+		VEHICLE_DGERR("flinger is NULL\n");
+		return -ENOMEM;
+	}
+
+	if (!flg->drm_dev)
+		flg->drm_dev = rockchip_drm_get_dev();
+	if (!flg->drm_dev) {
+		VEHICLE_DGERR("------drm device is not ready!!!-----\n");
+		kfree(flg);
+		return -ENODEV;
+	}
+
+	mutex_init(&flg->queue_buffer_lock);
+	mutex_init(&flg->source_buffer_lock);
+	mutex_init(&flg->target_buffer_lock);
+	INIT_LIST_HEAD(&flg->queue_buffer_list);
+	init_waitqueue_head(&flg->worker_wait);
+	atomic_set(&flg->worker_cond_atomic, 0);
+	atomic_set(&flg->worker_running_atomic, 1);
+
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++) {
+		flg->source_buffer[i].handle = 0;
+		flg->source_buffer[i].phy_addr = 0;
+		flg->source_buffer[i].fd = -1;
+	}
+	for (i = 0; i < NUM_TARGET_BUFFERS; i++) {
+		flg->target_buffer[i].phy_addr = 0;
+		flg->target_buffer[i].handle = 0;
+		flg->target_buffer[i].fd = -1;
+	}
+
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++) {
+		buffer = &(flg->source_buffer[i]);
+		ret = rk_flinger_alloc_buffer(flg, buffer, w, h, s, f);
+		if (ret) {
+			VEHICLE_DGERR("rk_flinger alloc src buffer failed(%d)\n",
+					ret);
+			goto free_dst_alloc;
+		}
+		buffer->state = FREE;
+	}
+	for (i = 0; i < NUM_TARGET_BUFFERS; i++) {
+		buffer = &(flg->target_buffer[i]);
+		// f = HAL_PIXEL_FORMAT_RGBX_8888;
+		// if (FORCE_ROTATION == RGA_TRANSFORM_ROT_270 ||
+		//	FORCE_ROTATION == RGA_TRANSFORM_ROT_90)
+		if ((v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK) == 0x01 ||
+		    (v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK) == 0x04)
+			ret = rk_flinger_alloc_buffer(flg, buffer, h, w, s, f);
+		else
+			ret = rk_flinger_alloc_buffer(flg, buffer, w, h, s, f);
+		// ret = rk_flinger_alloc_buffer(flg, buffer, w, h, s, f);
+		if (ret) {
+			VEHICLE_DGERR("rk_flinger alloc dst buffer failed\n");
+			goto free_src_alloc;
+		}
+		buffer->state = FREE;
+	}
+
+	ret = rk_flinger_create_worker(flg);
+	if (ret) {
+		VEHICLE_DGERR("rk_flinger create worker failed\n");
+		goto free_dst_alloc;
+	}
+	flinger = flg;
+
+	memcpy(&flg->v_cfg, v_cfg, sizeof(struct vehicle_cfg));
+	rk_flinger_queue_work(flg, NULL);
+	flg->dev = dev;
+
+	ret = vehicle_flinger_parse_dt(flg);
+	if (ret) {
+		VEHICLE_DGERR("vehicle flinger parse dts failed\n");
+		goto free_dst_alloc;
+	}
+
+	VEHICLE_INFO("vehicle flinger init ok\n");
+	inited = true;
+
+	return 0;
+free_dst_alloc:
+	for (i = 0; i < NUM_TARGET_BUFFERS; i++)
+		rk_flinger_free_buffer(flg, &(flg->target_buffer[i]));
+
+free_src_alloc:
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++)
+		rk_flinger_free_buffer(flg, &(flg->source_buffer[i]));
+
+	return -EINVAL;
+}
+__maybe_unused int vehicle_flinger_deinit(void)
+{
+	struct flinger *flg = flinger;
+	int i;
+
+	if (!flg)
+		return -ENODEV;
+
+	atomic_set(&flg->worker_running_atomic, 0);
+	atomic_inc(&flg->worker_cond_atomic);
+	wake_up(&flg->worker_wait);
+	flush_work(&flg->render_work);
+	flush_workqueue(flg->render_workqueue);
+	rk_flinger_destroy_worker(flg);
+
+	flinger = NULL;
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++)
+		rk_flinger_free_buffer(flg, &flg->source_buffer[i]);
+
+	for (i = 0; i < NUM_TARGET_BUFFERS; i++)
+		rk_flinger_free_buffer(flg, &flg->target_buffer[i]);
+
+	kfree(flg);
+
+	return 0;
+}
+
+static int rk_flinger_format_hal_to_rga(int format)
+{
+	int rga_format = -1;
+
+	switch (format) {
+	case HAL_PIXEL_FORMAT_RGB_565:
+		rga_format =  RGA_FORMAT_RGB_565;
+		break;
+	case HAL_PIXEL_FORMAT_RGB_888:
+		rga_format =  RGA_FORMAT_RGB_888;
+		break;
+	case HAL_PIXEL_FORMAT_RGBA_8888:
+		rga_format =  RGA_FORMAT_RGBA_8888;
+		break;
+	case HAL_PIXEL_FORMAT_RGBX_8888:
+		rga_format =  RGA_FORMAT_RGBX_8888;
+		break;
+	case HAL_PIXEL_FORMAT_BGRA_8888:
+		rga_format =  RGA_FORMAT_BGRA_8888;
+		break;
+	case HAL_PIXEL_FORMAT_YCrCb_NV12:
+		rga_format =  RGA_FORMAT_YCrCb_420_SP;
+		break;
+	case HAL_PIXEL_FORMAT_YCbCr_422_SP:
+		rga_format =  RGA_FORMAT_YCbCr_422_SP;
+		break;
+	default:
+		break;
+	}
+
+	return rga_format;
+}
+
+static int rk_flinger_set_rect(struct rect *rect, int x, size_t y,
+			       int w, int h, int s, int f)
+{
+	if (!rect)
+		return -EINVAL;
+
+	rect->x = x;
+	rect->y = y;
+	rect->w = w;
+	rect->h = h;
+	rect->s = s;
+	rect->f = f;
+
+	return 0;
+}
+
+static int
+rk_flinger_set_buffer_rotation(struct graphic_buffer *buffer, int r)
+{
+	if (!buffer)
+		return -EINVAL;
+
+	buffer->rotation = r;
+
+	return buffer->rotation;
+}
+
+static int
+rk_flinger_cacultae_dst_rect_by_rotation(struct graphic_buffer *buffer)
+{
+	struct rect *src_rect, *dst_rect;
+
+	if (!buffer)
+		return -EINVAL;
+
+	src_rect = &buffer->src;
+	dst_rect = &buffer->dst;
+
+	switch (buffer->rotation & RGA_TRANSFORM_ROT_MASK) {
+	case RGA_TRANSFORM_ROT_90:
+	case RGA_TRANSFORM_ROT_270:
+		dst_rect->x = src_rect->x;
+		dst_rect->y = src_rect->y;
+		dst_rect->h = src_rect->w;
+		dst_rect->w = src_rect->h;
+		dst_rect->s = src_rect->h;
+		break;
+	case RGA_TRANSFORM_ROT_0:
+	case RGA_TRANSFORM_ROT_180:
+	case RGA_TRANSFORM_FLIP_H:
+	case RGA_TRANSFORM_FLIP_V:
+	default:
+		dst_rect->x = src_rect->x;
+		dst_rect->y = src_rect->y;
+		dst_rect->w = src_rect->w;
+		dst_rect->h = src_rect->h;
+		dst_rect->s = src_rect->s;
+		break;
+	}
+
+	return 0;
+}
+
+static int rk_flinger_fill_buffer_rects(struct graphic_buffer *buffer,
+					struct rect *src_rect,
+					struct rect *dst_rect)
+{
+	if (!buffer)
+		return -EINVAL;
+
+	if (src_rect)
+		memcpy(&buffer->src, src_rect, sizeof(struct rect));
+	if (dst_rect)
+		memcpy(&buffer->dst, dst_rect, sizeof(struct rect));
+
+	return 0;
+}
+
+static int rk_flinger_iep_deinterlace(struct flinger *flinger,
+				      struct graphic_buffer *src_buffer,
+				      struct graphic_buffer *dst_buffer)
+{
+	struct rga_req rga_request;
+	int ret;
+
+	memset(&rga_request, 0, sizeof(rga_request));
+
+	if (!src_buffer || !dst_buffer)
+		return -EINVAL;
+
+	rga_request.rotate_mode = 0;
+	rga_request.sina = 0;
+	rga_request.cosa = 0;
+
+	rga_request.src.act_w = src_buffer->src.w;
+	rga_request.src.act_h = src_buffer->src.h;
+	rga_request.src.x_offset = 0;
+	rga_request.src.y_offset = 0;
+	rga_request.src.vir_w = src_buffer->src.w;
+	rga_request.src.vir_h = src_buffer->src.h;
+	rga_request.src.yrgb_addr = src_buffer->fd;
+	rga_request.src.uv_addr = 0;
+	rga_request.src.v_addr = 0;
+	rga_request.src.format = RGA_FORMAT_YCrCb_420_SP;
+	if (src_buffer->rotation == RGA_TRANSFORM_ROT_0 ||
+		src_buffer->rotation == RGA_TRANSFORM_ROT_180) {
+		rga_request.dst.act_w = src_buffer->src.w;
+		rga_request.dst.act_h = src_buffer->src.h / 2;
+		rga_request.dst.vir_w = src_buffer->src.w;
+		rga_request.dst.vir_h = src_buffer->src.h / 2;
+	} else {
+		rga_request.dst.act_w = src_buffer->src.w / 2;
+		rga_request.dst.act_h = src_buffer->src.h;
+		rga_request.dst.vir_w = src_buffer->src.w / 2;
+		rga_request.dst.vir_h = src_buffer->src.h;
+	}
+	rga_request.dst.x_offset = 0;
+	rga_request.dst.y_offset = 0;
+
+	rga_request.dst.yrgb_addr = dst_buffer->fd;
+	rga_request.dst.uv_addr = 0;
+	rga_request.dst.v_addr = 0;
+	rga_request.dst.format = RGA_FORMAT_YCrCb_420_SP;
+
+	rga_request.scale_mode = 1;
+
+	rga_request.mmu_info.mmu_en = 1;
+	rga_request.mmu_info.mmu_flag = ((2 & 0x3) << 4) |
+			 1 | (1 << 31 | 1 << 8 | 1 << 10);
+
+	rga_request.src.rd_mode = RGA_RASTER_MODE;
+	rga_request.dst.rd_mode = RGA_RASTER_MODE;
+
+	ret = rga_kernel_commit(&rga_request);
+	if (ret)
+		VEHICLE_DGERR("RGA_BLIT_SYNC failed(%d)\n", ret);
+
+	dst_buffer->width = src_buffer->width;
+	dst_buffer->height = src_buffer->height;
+	dst_buffer->src.f = src_buffer->src.f;
+
+	if (src_buffer->rotation == RGA_TRANSFORM_ROT_0 ||
+		src_buffer->rotation == RGA_TRANSFORM_ROT_180) {
+		dst_buffer->src.w = src_buffer->src.w;
+		dst_buffer->src.h = src_buffer->src.h / 2;
+	} else {
+		dst_buffer->src.w = src_buffer->src.w / 2;
+		dst_buffer->src.h = src_buffer->src.h;
+	}
+	dst_buffer->src.x = 0;
+	dst_buffer->src.y = 0;
+
+	src_buffer->state = FREE;
+
+	return 0;
+}
+
+static int rk_flinger_rga_scaler(struct flinger *flinger,
+				 struct graphic_buffer *src_buffer,
+				 struct graphic_buffer *dst_buffer)
+{
+	struct rga_req rga_request;
+	int ret;
+
+	memset(&rga_request, 0, sizeof(rga_request));
+
+	if (!src_buffer || !dst_buffer)
+		return -EINVAL;
+
+	rga_request.rotate_mode = 0;
+	rga_request.sina = 0;
+	rga_request.cosa = 0;
+
+	rga_request.yuv2rgb_mode = 0x0 << 0; // yuvtoyuv config 0
+	/* yuv to rgb color space transform if need  */
+	//rga_request.yuv2rgb_mode = 0x1 << 0; // limit range
+	//rga_request.yuv2rgb_mode = 0x2 << 0; // full range
+
+	rga_request.src.act_w = src_buffer->src.w;
+	rga_request.src.act_h = src_buffer->src.h;
+	rga_request.src.x_offset = 0;
+	rga_request.src.y_offset = 0;
+	rga_request.src.vir_w = src_buffer->src.w;
+	rga_request.src.vir_h = src_buffer->src.h;
+	rga_request.src.yrgb_addr = src_buffer->fd;
+	rga_request.src.uv_addr = 0;
+	rga_request.src.v_addr = 0;
+	rga_request.src.format = RGA_FORMAT_YCrCb_420_SP;
+
+	rga_request.dst.act_w = dst_buffer->width;
+	rga_request.dst.act_h = dst_buffer->height;
+	rga_request.dst.x_offset = 0;
+	rga_request.dst.y_offset = 0;
+	rga_request.dst.vir_w = dst_buffer->width;
+	rga_request.dst.vir_h = dst_buffer->height;
+	rga_request.dst.yrgb_addr = dst_buffer->fd;
+	rga_request.dst.uv_addr = 0;
+	rga_request.dst.v_addr = 0;
+	rga_request.dst.format =  RGA_FORMAT_YCrCb_420_SP;
+
+	rga_request.scale_mode = 1;
+
+	rga_request.mmu_info.mmu_en = 1;
+	rga_request.mmu_info.mmu_flag = ((2 & 0x3) << 4) |
+		   1 | (1 << 31 | 1 << 8 | 1 << 10);
+
+	rga_request.src.rd_mode = RGA_RASTER_MODE;
+	rga_request.dst.rd_mode = RGA_RASTER_MODE;
+
+	ret = rga_kernel_commit(&rga_request);
+	if (ret)
+		VEHICLE_DGERR("RGA_BLIT_SYNC failed(%d)\n", ret);
+
+	dst_buffer->src.f = dst_buffer->format;
+	dst_buffer->src.w = dst_buffer->width;
+	dst_buffer->src.h = dst_buffer->height;
+	dst_buffer->src.x = 0;
+	dst_buffer->src.y = 0;
+	/* save rga in buffer */
+	if (vehicle_dump_rga) {
+		struct file *filep = NULL;
+		loff_t pos = 0;
+		static bool file_ready;
+		static int frame_count;
+
+		VEHICLE_DG("@%s src->vir_addr[0](%d) addr[100](%d)\n",
+				__func__, ((char *)(src_buffer->vir_addr))[0],
+					((char *)(src_buffer->vir_addr))[100]);
+		if (!file_ready) {
+			int frame_len = src_buffer->src.w * src_buffer->src.h * 3 / 2;
+			char path[128] = {0};
+
+			VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+							frame_count++, frame_len);
+			sprintf(path, "/data/rga_scaler_in_%zu_%zu.yuv",
+					src_buffer->src.w, src_buffer->src.h);
+			filep = filp_open(path, O_CREAT | O_RDWR, 0666);
+			if (IS_ERR(filep)) {
+				VEHICLE_DGERR(" %s filp_open failed!\n", path);
+				file_ready = false;
+			} else {
+				kernel_write(filep, src_buffer->vir_addr, frame_len, &pos);
+				filp_close(filep, NULL);
+				VEHICLE_INFO(" %s file saved ok!\n", path);
+				file_ready = true;
+			}
+		}
+	}
+	/* save rga out buffer */
+	if (vehicle_dump_rga) {
+		struct file *filep = NULL;
+		loff_t pos = 0;
+		static bool file_ready;
+		static int frame_count;
+
+		VEHICLE_DG("@%s dst->vir_addr[0](%d) addr[100](%d)\n",
+				__func__, ((char *)(dst_buffer->vir_addr))[0],
+					((char *)(dst_buffer->vir_addr))[100]);
+		if (!file_ready) {
+			/* NV12 */
+			int frame_len = dst_buffer->src.w * dst_buffer->src.h * 3 / 2;
+			char path[128] = {0};
+
+			VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+							frame_count++, frame_len);
+			sprintf(path, "/data/rga_scaler_out_%zu_%zu.yuv",
+					dst_buffer->src.w, dst_buffer->src.h);
+			filep = filp_open(path, O_CREAT | O_RDWR, 0666);
+			if (IS_ERR(filep)) {
+				VEHICLE_DGERR(" %s filp_open failed!\n", path);
+				file_ready = false;
+			} else {
+				kernel_write(filep, dst_buffer->vir_addr, frame_len, &pos);
+				filp_close(filep, NULL);
+				VEHICLE_INFO(" %s file saved ok!\n", path);
+				file_ready = true;
+			}
+		}
+	}
+
+	src_buffer->state = FREE;
+
+	return 0;
+}
+
+static int rk_flinger_rga_blit(struct flinger *flinger,
+			       struct graphic_buffer *src_buffer,
+			       struct graphic_buffer *dst_buffer)
+{
+	struct rga_req rga_request;
+	int sx, sy, sw, sh, ss, sf;
+	int dx, dy, dw, dh, ds, df;
+	int orientation;
+	int ret;
+	int src_fd, dst_fd;
+
+	if (!src_buffer || !dst_buffer)
+		return -EINVAL;
+
+	src_fd = src_buffer->fd;
+	dst_fd = dst_buffer->fd;
+
+	memset(&rga_request, 0, sizeof(rga_request));
+
+	orientation = src_buffer->rotation;
+	dst_buffer->rotation = src_buffer->rotation;
+
+	sx = src_buffer->src.x;
+	sy = src_buffer->src.y;
+	sw = src_buffer->src.w;
+	ss = src_buffer->src.s;
+	sh = src_buffer->src.h;
+	sf = rk_flinger_format_hal_to_rga(src_buffer->src.f);
+	VEHICLE_DG("%s src: sx:%d, sy:%d, sw:%d, ss:%d, sh:%d\n",
+				__func__, sx, sy, sw, ss, sh);
+	dx = src_buffer->dst.x;
+	dy = src_buffer->dst.y;
+	dw = src_buffer->dst.w;
+	ds = src_buffer->dst.s;
+	dh = src_buffer->dst.h;
+	df = rk_flinger_format_hal_to_rga(src_buffer->dst.f);
+	VEHICLE_DG("%s dst: dx:%d, dy:%d, dw:%d, ds:%d, dh:%d\n",
+				__func__, dx, dy, dw, ds, dh);
+	if (src_buffer->offset) {
+		sh += src_buffer->offset / src_buffer->len * sh;
+		sx = src_buffer->offset / src_buffer->len * sh;
+		src_fd = 0;
+	}
+	VEHICLE_DG("%s src: sx:%d, sy:%d, sw:%d, ss:%d, sh:%d\n",
+				__func__, sx, sy, sw, ss, sh);
+	switch (orientation) {
+	case RGA_TRANSFORM_ROT_0:
+		rga_request.rotate_mode = 0;
+		rga_request.sina = 0;
+		rga_request.cosa = 0;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dw;
+		rga_request.dst.act_h = dh;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	case RGA_TRANSFORM_FLIP_H:/*x mirror*/
+		rga_request.rotate_mode = 2;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dw;
+		rga_request.dst.act_h = dh;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	case RGA_TRANSFORM_FLIP_V:/*y mirror*/
+		rga_request.rotate_mode = 3;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dw;
+		rga_request.dst.act_h = dh;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	case RGA_TRANSFORM_ROT_90:
+		rga_request.rotate_mode = 1;
+		rga_request.sina = 65536;
+		rga_request.cosa = 0;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dh;
+		rga_request.dst.act_h = dw;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	case RGA_TRANSFORM_ROT_180:
+		rga_request.rotate_mode = 1;
+		rga_request.sina = 0;
+		rga_request.cosa = -65536;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dw;
+		rga_request.dst.act_h = dh;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	case RGA_TRANSFORM_ROT_270:
+		rga_request.rotate_mode = 1;
+		rga_request.sina = -65536;
+		rga_request.cosa = 0;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dh;
+		rga_request.dst.act_h = dw;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	default:
+		rga_request.rotate_mode = 0;
+		rga_request.sina = 0;
+		rga_request.cosa = 0;
+		rga_request.dst.vir_w = ds;
+		rga_request.dst.vir_h = dh;
+		rga_request.dst.act_w = dw;
+		rga_request.dst.act_h = dh;
+		rga_request.dst.x_offset = 0;
+		rga_request.dst.y_offset = 0;
+		break;
+	}
+
+	rga_request.src.yrgb_addr = src_fd;
+	rga_request.src.uv_addr = 0;
+	rga_request.src.v_addr = 0;
+
+	rga_request.dst.yrgb_addr = dst_fd;
+	rga_request.dst.uv_addr = 0;
+	rga_request.dst.v_addr = 0;
+
+	rga_request.src.vir_w = ss;
+	rga_request.src.vir_h = sh;
+	rga_request.src.format = sf;
+	rga_request.src.act_w = sw;
+	rga_request.src.act_h = sh;
+	rga_request.src.x_offset = 0;
+	rga_request.src.y_offset = 0;
+
+	rga_request.dst.format = df;
+
+	rga_request.clip.xmin = 0;
+	rga_request.clip.xmax = dw - 1;
+	rga_request.clip.ymin = 0;
+	rga_request.clip.ymax = dh - 1;
+	rga_request.scale_mode = 0;
+
+	rga_request.yuv2rgb_mode = 0x0 << 0; // yuvtoyuv config 0
+	/* yuv to rgb color space transform if need  */
+	//rga_request.yuv2rgb_mode = 0x1 << 0; // limit range
+	//rga_request.yuv2rgb_mode = 0x2 << 0; // full range
+
+	rga_request.mmu_info.mmu_en = 1;
+	rga_request.mmu_info.mmu_flag = ((2 & 0x3) << 4) |
+		 1 | (1 << 31 | 1 << 8 | 1 << 10);
+
+	rga_request.src.rd_mode = RGA_RASTER_MODE;
+	rga_request.dst.rd_mode = RGA_RASTER_MODE;
+
+	VEHICLE_DG("%s src_buffer->src.f(%zu) src_buffer->dst.f(%zu)",
+				__func__, src_buffer->src.f, src_buffer->dst.f);
+	ret = rga_kernel_commit(&rga_request);
+	if (ret)
+		VEHICLE_DGERR("RGA_BLIT_SYNC failed(%d)\n", ret);
+
+	return 0;
+}
+
+static int rk_flinger_rga_render(struct flinger *flinger,
+				 struct graphic_buffer *src_buffer,
+				 struct graphic_buffer *dst_buffer,
+				 struct graphic_buffer *tmp_buffer)
+{
+	int rotation;
+
+	if (!flinger || !src_buffer || !dst_buffer)
+		return -EINVAL;
+
+	if (dst_buffer && dst_buffer->rel_fence)
+		dst_buffer->rel_fence = NULL;
+
+	if ((src_buffer->rotation & RGA_TRANSFORM_ROT_MASK) &&
+		(src_buffer->rotation & RGA_TRANSFORM_FLIP_MASK)) {
+
+		rotation = flinger->v_cfg.rotate_mirror;
+		/* 1. rotate */
+		src_buffer->rotation = rotation & RGA_TRANSFORM_ROT_MASK;
+		rk_flinger_rga_blit(flinger, src_buffer, tmp_buffer);
+		rk_flinger_fill_buffer_rects(tmp_buffer, &src_buffer->dst,
+					     &src_buffer->dst);
+		tmp_buffer->src.f = src_buffer->dst.f;
+		tmp_buffer->rotation = rotation & RGA_TRANSFORM_FLIP_MASK;
+		/* 2. mirror */
+		rk_flinger_rga_blit(flinger, tmp_buffer, dst_buffer);
+		rk_flinger_fill_buffer_rects(dst_buffer, &tmp_buffer->dst,
+					     &tmp_buffer->dst);
+		dst_buffer->src.f = src_buffer->dst.f;
+
+		src_buffer->rotation = rotation;
+	} else {
+		rk_flinger_rga_blit(flinger, src_buffer, dst_buffer);
+		rk_flinger_fill_buffer_rects(dst_buffer, &src_buffer->dst,
+					     &src_buffer->dst);
+		dst_buffer->src.f = src_buffer->dst.f;
+	}
+	/* save rga out buffer */
+	if (vehicle_dump_rga) {
+		struct file *filep = NULL;
+		loff_t pos = 0;
+		static bool file_ready;
+		static int frame_count;
+
+		VEHICLE_DG("@%s dst->vir_addr[0](%d) addr[100](%d)\n",
+				__func__, ((char *)(dst_buffer->vir_addr))[0],
+					((char *)(dst_buffer->vir_addr))[100]);
+		if (!file_ready) {
+			int frame_len = dst_buffer->src.w * dst_buffer->src.h * 3 / 2;//NV12
+			char path[128] = {0};
+
+			VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+							frame_count++, frame_len);
+			sprintf(path, "/data/rga_render_%zu_%zu.yuv",
+					dst_buffer->src.w, dst_buffer->src.h);
+			filep = filp_open(path, O_CREAT | O_RDWR, 0666);
+			if (IS_ERR(filep)) {
+				VEHICLE_DGERR(" %s filp_open failed!\n", path);
+				file_ready = false;
+			} else {
+				kernel_write(filep, dst_buffer->vir_addr, frame_len, &pos);
+				filp_close(filep, NULL);
+				VEHICLE_INFO(" %s file saved ok!\n", path);
+				file_ready = true;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static void rk_drm_vehicle_commit(struct flinger *flinger, struct graphic_buffer *buffer)
+{
+	struct rockchip_drm_direct_show_commit_info commit_info;
+	int hdisplay = flinger->crtc->state->adjusted_mode.hdisplay;
+	int vdisplay = flinger->crtc->state->adjusted_mode.vdisplay;
+
+	commit_info.crtc = flinger->crtc;
+	commit_info.plane = flinger->plane;
+
+	commit_info.src_x = 0;
+	commit_info.src_y = 0;
+	commit_info.src_w = buffer->src.w;
+	commit_info.src_h = buffer->src.h;
+	// commit_info.src_w = buffer->drm_buffer->width;
+	// commit_info.src_h = buffer->drm_buffer->height;
+
+	/*center display*/
+	// commit_info.dst_x = (hdisplay - BUFFER_WIDTH) / 2;
+	// commit_info.dst_y = (vdisplay - BUFFER_HEIGHT) / 2;
+	// commit_info.dst_w = commit_info.src_w;
+	// commit_info.dst_h = commit_info.src_h;
+
+	/*full screen display */
+	commit_info.dst_x = 0;
+	commit_info.dst_y = 0;
+	commit_info.dst_w = hdisplay;
+	commit_info.dst_h = vdisplay;
+
+	commit_info.top_zpos  = true;
+
+	commit_info.buffer = buffer->drm_buffer;
+
+	if (vehicle_dump_vop) {
+		struct file *filep = NULL;
+		loff_t pos = 0;
+		static bool file_ready;
+		static int frame_count;
+
+		if (!file_ready) {
+			int frame_len = buffer->drm_buffer->width *
+					buffer->drm_buffer->height * 3 / 2;//NV12
+			char path[128] = {0};
+
+			VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+							frame_count++, frame_len);
+			sprintf(path, "/data/vop_commit_%d_%d.yuv",
+						buffer->drm_buffer->width,
+						buffer->drm_buffer->height);
+			filep = filp_open(path, O_CREAT | O_RDWR, 0666);
+			if (IS_ERR(filep)) {
+				VEHICLE_DGERR(" %s filp_open failed!\n", path);
+				file_ready = false;
+			} else {
+				kernel_write(filep,
+					buffer->drm_buffer->vir_addr[0], frame_len, &pos);
+				filp_close(filep, NULL);
+				VEHICLE_INFO(" %s file saved ok!\n", path);
+				file_ready = true;
+			}
+		}
+	}
+	rockchip_drm_direct_show_commit(flinger->drm_dev, &commit_info);
+}
+
+static int drop_frames_number;
+static int rk_flinger_vop_show(struct flinger *flinger,
+			       struct graphic_buffer *buffer)
+{
+	if (!flinger || !buffer)
+		return -EINVAL;
+
+	VEHICLE_DG("flinger vop show buffer wxh(%zux%zu)\n",
+					buffer->src.w, buffer->src.h);
+	if (drop_frames_number > 0) {
+		VEHICLE_INFO("%s discard the frame num(%d)!\n", __func__, drop_frames_number);
+		drop_frames_number--;
+		return 0;
+	}
+
+	if (!flinger->running)
+		return 0;
+
+	/* get crtc and plane */
+	flinger->crtc = rockchip_drm_direct_show_get_crtc(flinger->drm_dev, flinger->crtc_name);
+	if (flinger->crtc == NULL) {
+		VEHICLE_DGERR("error: failed to get crtc\n");
+		return -EINVAL;
+	}
+
+	flinger->plane = rockchip_drm_direct_show_get_plane(flinger->drm_dev, flinger->plane_name);
+	if (flinger->plane == NULL) {
+		VEHICLE_DGERR("error: failed to get plane\n");
+		return -EINVAL;
+	}
+
+	rk_drm_vehicle_commit(flinger, buffer);
+
+	flinger->debug_vop_count++;
+	/* save vop show buffer */
+	if (vehicle_dump_vop) {
+		struct file *filep = NULL;
+		loff_t pos = 0;
+		static bool file_ready;
+		static int frame_count;
+
+		VEHICLE_DG("@%s buffer->vir_addr[0](%d) addr[100](%d)\n",
+				__func__, ((char *)(buffer->vir_addr))[0],
+					((char *)(buffer->vir_addr))[100]);
+		if (!file_ready) {
+			int frame_len = buffer->src.w * buffer->src.h * 3 / 2;//NV12
+			char path[128] = {0};
+
+			VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+							frame_count++, frame_len);
+			sprintf(path, "/data/vop_show_%zu_%zu.yuv",
+						buffer->src.w, buffer->src.h);
+			filep = filp_open(path, O_CREAT | O_RDWR, 0666);
+			if (IS_ERR(filep)) {
+				VEHICLE_DGERR(" %s filp_open failed!\n", path);
+				file_ready = false;
+			} else {
+				kernel_write(filep, buffer->vir_addr, frame_len, &pos);
+				filp_close(filep, NULL);
+				VEHICLE_INFO(" %s file saved ok!\n", path);
+				file_ready = true;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static void rk_flinger_first_done(struct work_struct *work)
+{
+	struct graphic_buffer *buffer;
+	struct flinger *flg = flinger;
+	int i;
+	struct flinger *flg_test =
+		 container_of(work, struct flinger, init_work);
+	struct vehicle_cfg *v_cfg = &flg_test->v_cfg;
+
+	if (!flg)
+		return;
+
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++) {
+		if (flg->source_buffer[i].state == FREE) {
+			buffer = &(flg->source_buffer[i]);
+			rk_flinger_set_rect(&buffer->src,
+					    FORCE_XOFFSET, FORCE_YOFFSET,
+					    v_cfg->width, v_cfg->height,
+					    v_cfg->width, FORCE_FORMAT);
+			rk_flinger_set_buffer_rotation(buffer, v_cfg->rotate_mirror);
+			rk_flinger_cacultae_dst_rect_by_rotation(buffer);
+			buffer->dst.f = buffer->src.f;
+			VEHICLE_INFO("buffer[%d]->rotation(%d).\n",
+				      i, buffer->rotation);
+		}
+	}
+}
+
+static void rk_flinger_render_show(struct work_struct *work)
+{
+	struct graphic_buffer *src_buffer, *dst_buffer, *iep_buffer, *buffer;
+	/* struct queue_buffer *cur = NULL, *next = NULL; */
+	struct flinger *flg = flinger;
+	int i, found = 0;
+	static int count = -1;
+	static int last_src_index = -1;
+	bool cvbs_flag = true;
+	struct flinger *flg_test =
+			container_of(work, struct flinger, render_work);
+	struct vehicle_cfg *v_cfg = &flg_test->v_cfg;
+
+	src_buffer = NULL;
+	dst_buffer = NULL;
+	flg->source_index = 0;
+
+	do {
+try_again:
+		wait_event_interruptible_timeout(flg->worker_wait,
+						 atomic_read(&flg->worker_cond_atomic),
+						 msecs_to_jiffies(1000000));
+		VEHICLE_DG("wake up enter, v_cfg.w*h(%dx%d)\n",
+				v_cfg->width, v_cfg->height);
+
+		if (atomic_read(&flg->worker_running_atomic) == 0) {
+			VEHICLE_INFO("%s loop exit\n", __func__);
+			break;
+		}
+		if (atomic_read(&flg->worker_cond_atomic) <= 0) {
+			/*printk("waiting 'worker_cond_atomic' timed out.");*/
+			goto try_again;
+		}
+		atomic_dec(&flg->worker_cond_atomic);
+
+		/*  1. find src buffer */
+		src_buffer = NULL;
+		found = last_src_index + 1;
+		for (i = 1; i < NUM_SOURCE_BUFFERS; i++, found++) {
+			found = found % NUM_SOURCE_BUFFERS;
+			if (flg->source_buffer[found].state == QUEUE) {
+				src_buffer = &flg->source_buffer[found];
+				last_src_index = found;
+				break;
+			}
+		}
+
+		if (!src_buffer || !src_buffer->fd) {
+			usleep_range(3000, 3100);
+			VEHICLE_DGERR("[%s:%d] error, no buffer\n", __func__, __LINE__);
+			goto try_again;
+		}
+
+		count++;
+		src_buffer->state = ACQUIRE;
+		/* save rkcif buffer */
+		if (vehicle_dump_cif) {
+			// struct file *filep = NULL;
+			struct file *filep;
+			loff_t pos = 0;
+			static bool file_ready;
+			static int frame_count;
+
+			VEHICLE_DG("src_buffer->vir_addr[0](%d) addr[100](%d)\n",
+						((char *)(src_buffer->vir_addr))[0],
+						((char *)(src_buffer->vir_addr))[100]);
+
+			if (!file_ready) {
+				//nv12 frame_len=w*h*3/2
+				int frame_len = src_buffer->src.w * src_buffer->src.h * 3 / 2;
+				char path[128] = {0};
+
+				VEHICLE_DG("save vop frame(%d) frame_len(%d)\n",
+								frame_count++, frame_len);
+				sprintf(path, "/data/cif_out_%zu_%zu.yuv",
+							src_buffer->src.w, src_buffer->src.h);
+				filep = filp_open(path, O_RDWR | O_CREAT, 0666);
+				if (IS_ERR(filep)) {
+					VEHICLE_DGERR(" %s filp_open failed!\n", path);
+					file_ready = false;
+				} else {
+					kernel_write(filep, src_buffer->vir_addr, frame_len, &pos);
+					filp_close(filep, NULL);
+					VEHICLE_INFO(" %s file saved ok!\n", path);
+					file_ready = true;
+				}
+			}
+		}
+
+		/*  2. find dst buffer */
+		dst_buffer = NULL;
+		iep_buffer = NULL;
+		/*get iep, rga, vop buffer*/
+		if (1) { //rotation by rga
+			if (flg->v_cfg.input_format == CIF_INPUT_FORMAT_PAL ||
+			    flg->v_cfg.input_format == CIF_INPUT_FORMAT_NTSC) {
+				iep_buffer = &(flg->target_buffer
+					       [NUM_TARGET_BUFFERS - 1]);
+				iep_buffer->state = ACQUIRE;
+				cvbs_flag = true;
+			} else {
+				cvbs_flag = false;
+			}
+			dst_buffer = &(flg->target_buffer
+				       [count % (NUM_TARGET_BUFFERS - 1)]);
+			dst_buffer->state = ACQUIRE;
+		} else if (flg->v_cfg.input_format == CIF_INPUT_FORMAT_PAL ||
+			   flg->v_cfg.input_format == CIF_INPUT_FORMAT_NTSC) {
+			iep_buffer = &(flg->target_buffer
+				       [count % NUM_TARGET_BUFFERS]);
+			iep_buffer->state = ACQUIRE;
+		}
+		if (!iep_buffer || !iep_buffer->fd) {
+			if (iep_buffer)
+				iep_buffer->state = FREE;
+		}
+
+		/* 3 do deinterlace & rotation & display*/
+		if (!cvbs_flag) {
+			// YPbPr
+			VEHICLE_DG("it is ypbpr signal\n");
+			iep_buffer = &(flg->target_buffer[NUM_TARGET_BUFFERS - 1]);
+			iep_buffer->state = ACQUIRE;
+			//scaler by rga to force widthxheight display
+			rk_flinger_rga_render(flg, src_buffer, iep_buffer, dst_buffer);
+			src_buffer->state = FREE;
+			rk_flinger_rga_scaler(flg, iep_buffer, dst_buffer);
+			iep_buffer->state = FREE;
+			rk_flinger_vop_show(flg, dst_buffer);
+			for (i = 0; i < NUM_TARGET_BUFFERS; i++) {
+				buffer = &(flinger->target_buffer[i]);
+				if (buffer->state == DISPLAY)
+					buffer->state = FREE;
+			}
+
+			dst_buffer->state = DISPLAY;
+		} else {
+			// cvbs
+			VEHICLE_DG("it is a cvbs signal\n");
+			rk_flinger_rga_render(flg, src_buffer, dst_buffer, iep_buffer);
+			src_buffer->state = FREE;
+			rk_flinger_iep_deinterlace(flg, dst_buffer, iep_buffer);
+			dst_buffer->state = FREE;
+			rk_flinger_rga_scaler(flg, iep_buffer, dst_buffer);
+			rk_flinger_vop_show(flg, dst_buffer);
+			iep_buffer->state = FREE;
+
+			for (i = 0; i < NUM_TARGET_BUFFERS; i++) {
+				buffer = &(flinger->target_buffer[i]);
+				if (buffer->state == DISPLAY)
+					buffer->state = FREE;
+			}
+			dst_buffer->state = DISPLAY;
+		}
+	} while (1);
+}
+
+static int rk_flinger_queue_work(struct flinger *flinger,
+				 struct graphic_buffer *src_buffer)
+{
+	if (!flinger)
+		return -ENODEV;
+
+	if (!src_buffer) {
+		if (flinger->render_workqueue) {
+			INIT_WORK(&flinger->init_work, rk_flinger_first_done);
+			queue_work(flinger->render_workqueue,
+				   &flinger->init_work);
+		}
+	}
+
+	if (flinger->render_workqueue) {
+		INIT_WORK(&flinger->render_work, rk_flinger_render_show);
+		queue_work(flinger->render_workqueue, &flinger->render_work);
+	}
+
+	return 0;
+}
+
+static struct graphic_buffer *
+rk_flinger_lookup_buffer_by_phy_addr(unsigned long phy_addr)
+{
+	struct graphic_buffer *buffer = NULL;
+	struct flinger *flg = flinger;
+	int i;
+
+	VEHICLE_DG("%s:phy_addr=%lx\n", __func__, phy_addr);
+	for (i = 1; i < NUM_SOURCE_BUFFERS; i++) {
+		if (flg->source_buffer[i].state == DEQUEUE) {
+			buffer = &(flg->source_buffer[i]);
+			if (buffer && (buffer->offset +
+			    buffer->phy_addr == phy_addr)) {
+				buffer->state = QUEUE;
+				break;
+			}
+		}
+	}
+	if (i < NUM_SOURCE_BUFFERS)
+		return buffer;
+	else
+		return NULL;
+}
+
+static bool vehicle_rotation_param_check(struct vehicle_cfg *v_cfg)
+{
+	switch (v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK) {
+	case RGA_TRANSFORM_ROT_90:
+	case RGA_TRANSFORM_ROT_270:
+	case RGA_TRANSFORM_ROT_0:
+	case RGA_TRANSFORM_ROT_180:
+		return true;
+	default:
+		VEHICLE_INFO("invalid rotate-mirror param %d\n",
+					v_cfg->rotate_mirror);
+		v_cfg->rotate_mirror = v_cfg->rotate_mirror & RGA_TRANSFORM_FLIP_MASK;
+		return false;
+	}
+
+	switch (v_cfg->rotate_mirror & RGA_TRANSFORM_FLIP_MASK) {
+	case RGA_TRANSFORM_FLIP_H:
+	case RGA_TRANSFORM_FLIP_V:
+		return true;
+	default:
+		VEHICLE_INFO("invalid rotate-mirror param %d\n",
+					v_cfg->rotate_mirror);
+		v_cfg->rotate_mirror = v_cfg->rotate_mirror & RGA_TRANSFORM_ROT_MASK;
+		return false;
+	}
+}
+int vehicle_flinger_reverse_open(struct vehicle_cfg *v_cfg,
+				bool android_is_ready)
+{
+	int i;
+	int width;
+	int height;
+	struct flinger *flg = flinger;
+	struct graphic_buffer *buffer;
+	int hal_format;
+
+	width = v_cfg->width;
+	height = v_cfg->height;
+
+	if (!flinger)
+		return -ENODEV;
+
+	vehicle_rotation_param_check(v_cfg);
+
+	if (v_cfg->output_format == CIF_OUTPUT_FORMAT_422)
+		hal_format = HAL_PIXEL_FORMAT_YCbCr_422_SP;
+	else
+		hal_format = HAL_PIXEL_FORMAT_YCrCb_NV12;
+
+	/*  1. reinit buffer format */
+	for (i = 0; i < NUM_SOURCE_BUFFERS; i++) {
+		buffer = &(flg->source_buffer[i]);
+		rk_flinger_set_rect(&buffer->src,
+				    0, 0, width,
+				    height, width, hal_format);
+		rk_flinger_set_buffer_rotation(buffer, v_cfg->rotate_mirror);
+		rk_flinger_cacultae_dst_rect_by_rotation(buffer);
+		buffer->dst.f = buffer->src.f;
+		buffer->state = FREE;
+	}
+
+	for (i = 0; i < NUM_TARGET_BUFFERS; i++) {
+		buffer = &(flg->target_buffer[i]);
+		buffer->state = FREE;
+	}
+
+	/*2. fill buffer info*/
+	for (i = 0; i < NUM_SOURCE_BUFFERS && i < MAX_BUF_NUM; i++) {
+		v_cfg->buf_phy_addr[i] = flinger->source_buffer[i].phy_addr;
+		VEHICLE_DG("buf_phy_addr=%x, i=%d", v_cfg->buf_phy_addr[i], i);
+	}
+
+	v_cfg->buf_num = NUM_SOURCE_BUFFERS;
+
+	flg->cvbs_field_count = 0;
+	memcpy(&flg->v_cfg, v_cfg, sizeof(struct vehicle_cfg));
+	flg->running = true;
+	drop_frames_number = v_cfg->drop_frames;
+
+	return 0;
+}
+
+int vehicle_flinger_reverse_close(bool android_is_ready)
+{
+	struct flinger *flg = flinger;
+
+	flg->running = false;
+	if (flg->drm_dev && flg->plane)
+		rockchip_drm_direct_show_disable_plane(flg->drm_dev, flg->plane);
+	VEHICLE_DG("%s(%d) done\n", __func__, __LINE__);
+
+	return 0;
+}
+
+unsigned long vehicle_flinger_request_cif_buffer(void)
+{
+	struct graphic_buffer *src_buffer = NULL;
+	struct flinger *flg = flinger;
+	static int last_src_index = -1;
+	int found;
+	int i;
+
+	src_buffer = NULL;
+	for (i = 1; i < NUM_SOURCE_BUFFERS; i++) {
+		found = (last_src_index + i) % NUM_SOURCE_BUFFERS;
+		VEHICLE_DG("%s,flg->source_buffer[%d].state(%d)",
+			__func__, found, flg->source_buffer[found].state);
+		if (flg->source_buffer[found].state == FREE) {
+			src_buffer = &flg->source_buffer[found];
+			last_src_index = found;
+			src_buffer->state = DEQUEUE;
+			break;
+		}
+	}
+
+	if (i < NUM_SOURCE_BUFFERS)
+		return src_buffer->phy_addr;
+	else
+		return 0;
+}
+
+void vehicle_flinger_commit_cif_buffer(u32 buf_phy_addr)
+{
+	struct graphic_buffer *buffer = NULL;
+	struct flinger *flg = flinger;
+
+	if (!flg)
+		return;
+
+	buffer = rk_flinger_lookup_buffer_by_phy_addr(buf_phy_addr);
+	if (buffer) {
+		buffer->timestamp = ktime_get();
+		atomic_inc(&flg->worker_cond_atomic);
+		flg->debug_cif_count++;
+		wake_up(&flg->worker_wait);
+	} else {
+		VEHICLE_DGERR("%x, no free buffer\n", buf_phy_addr);
+	}
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_flinger.h b/drivers/video/rockchip/vehicle/vehicle_flinger.h
new file mode 100644
index 0000000000000..06e7d4ea811d6
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_flinger.h
@@ -0,0 +1,115 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * drivers/video/rockchip/flinger/flinger.c
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ *
+ */
+#ifndef __VEHICLE_FLINGER_H
+#define __VEHICLE_FLINGER_H
+
+#include "vehicle_cfg.h"
+#include "../rga3/include/rga.h"
+#include <linux/types.h>
+#include <linux/dma-mapping.h>
+
+int vehicle_flinger_init(struct device *dev, struct vehicle_cfg *v_cfg);
+int vehicle_flinger_deinit(void);
+int vehicle_flinger_reverse_open(struct vehicle_cfg *cfg,
+				bool android_already);
+int vehicle_flinger_reverse_close(bool android_already);
+unsigned long vehicle_flinger_request_cif_buffer(void);
+void vehicle_flinger_commit_cif_buffer(u32 buf_phy_addr);
+
+enum {
+	RGA_TRANSFORM_ROT_MASK   =   0x0000000F,
+	RGA_TRANSFORM_ROT_0      =   0x00000000,
+	RGA_TRANSFORM_ROT_90     =   0x00000001,
+	RGA_TRANSFORM_ROT_180    =   0x00000002,
+	RGA_TRANSFORM_ROT_270    =   0x00000004,
+
+	RGA_TRANSFORM_FLIP_MASK  =   0x000000F0,
+	RGA_TRANSFORM_FLIP_H     =   0x00000020,
+	RGA_TRANSFORM_FLIP_V     =   0x00000010,
+};
+/*
+ * pixel format definitions,this is copy from android/system/core/include/system/graphics.h
+ */
+enum {
+	HAL_PIXEL_FORMAT_RGBA_8888 = 1,
+	HAL_PIXEL_FORMAT_RGBX_8888 = 2,
+	HAL_PIXEL_FORMAT_RGB_888 = 3,
+	HAL_PIXEL_FORMAT_RGB_565 = 4,
+	HAL_PIXEL_FORMAT_BGRA_8888 = 5,
+	HAL_PIXEL_FORMAT_RGBA_5551 = 6,
+	HAL_PIXEL_FORMAT_RGBA_4444 = 7,
+
+	/* 0x8 - 0xFF range unavailable */
+
+	/*
+	 * 0x100 - 0x1FF
+	 *
+	 * This range is reserved for pixel formats that are specific to the HAL
+	 * implementation.  Implementations can use any value in this range to
+	 * communicate video pixel formats between their HAL modules.  These formats
+	 * must not have an alpha channel.  Additionally, an EGLimage created from a
+	 * gralloc buffer of one of these formats must be supported for use with the
+	 * GL_OES_EGL_image_external OpenGL ES extension.
+	 */
+
+	/*
+	 * Android YUV format:
+	 *
+	 * This format is exposed outside of the HAL to software decoders and
+	 * applications.  EGLImageKHR must support it in conjunction with the
+	 * OES_EGL_image_external extension.
+	 *
+	 * YV12 is a 4:2:0 YCrCb planar format comprised of a WxH Y plane followed
+	 * by (W/2) x (H/2) Cr and Cb planes.
+	 *
+	 * This format assumes
+	 * - an even width
+	 * - an even height
+	 * - a horizontal stride multiple of 16 pixels
+	 * - a vertical stride equal to the height
+	 *
+	 *   y_size = stride * height
+	 *   c_size = ALIGN(stride/2, 16) * height/2
+	 *   size = y_size + c_size * 2
+	 *   cr_offset = y_size
+	 *   cb_offset = y_size + c_size
+	 *
+	 */
+	HAL_PIXEL_FORMAT_YV12 = 0x32315659, // YCrCb 4:2:0 Planar
+
+	/* Legacy formats (deprecated), used by ImageFormat.java */
+
+	/*
+	 *YCbCr format default is BT601.
+	 */
+	HAL_PIXEL_FORMAT_YCbCr_422_SP = 0x10,   // NV16
+	HAL_PIXEL_FORMAT_YCrCb_420_SP = 0x11,   // NV21
+	HAL_PIXEL_FORMAT_YCbCr_422_I = 0x14,    // YUY2
+	HAL_PIXEL_FORMAT_YCrCb_NV12 = 0x20, // YUY2
+	HAL_PIXEL_FORMAT_YCrCb_NV12_VIDEO = 0x21,   // YUY2
+
+	HAL_PIXEL_FORMAT_YCrCb_NV12_10      = 0x22, // YUV420_1obit
+	HAL_PIXEL_FORMAT_YCbCr_422_SP_10    = 0x23, // YUV422_1obit
+	HAL_PIXEL_FORMAT_YCrCb_444_SP_10    = 0x24, //YUV444_1obit
+
+	HAL_PIXEL_FORMAT_YCrCb_444 = 0x25,  //yuv444
+	HAL_PIXEL_FORMAT_FBDC_RGB565    = 0x26,
+	HAL_PIXEL_FORMAT_FBDC_U8U8U8U8  = 0x27, /*ARGB888*/
+	HAL_PIXEL_FORMAT_FBDC_U8U8U8    = 0x28, /*RGBP888*/
+	HAL_PIXEL_FORMAT_FBDC_RGBA888   = 0x29, /*ABGR888*/
+	HAL_PIXEL_FORMAT_BGRX_8888 = 0x30,
+	HAL_PIXEL_FORMAT_BGR_888 = 0x31,
+	HAL_PIXEL_FORMAT_BGR_565 = 0x32,
+
+	HAL_PIXEL_FORMAT_YUYV422 = 0x33,
+	HAL_PIXEL_FORMAT_YUYV420 = 0x34,
+	HAL_PIXEL_FORMAT_UYVY422 = 0x35,
+	HAL_PIXEL_FORMAT_UYVY420 = 0x36,
+};
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_generic_sensor.c b/drivers/video/rockchip/vehicle/vehicle_generic_sensor.c
new file mode 100644
index 0000000000000..e277e61ddc922
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_generic_sensor.c
@@ -0,0 +1,488 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_generic_sensor.c
+ *
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *      Zhiqin Wei <wzq@rock-chips.com>
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/i2c.h>
+#include <linux/of_gpio.h>
+#include <linux/delay.h>
+#include <linux/clk.h>
+#include "vehicle_ad.h"
+#include "vehicle_ad_7181.h"
+#include "vehicle_ad_tp2855.h"
+#include "vehicle_ad_tp2825.h"
+#include "vehicle_ad_gc2145.h"
+#include "vehicle_ad_nvp6324.h"
+#include "vehicle_ad_nvp6188.h"
+#include "vehicle_ad_max96714.h"
+#include <linux/moduleparam.h>
+#include "../../../../drivers/media/i2c/jaguar1_drv/jaguar1_v4l2.h"
+#include "../../../../drivers/media/i2c/nvp6188.h"
+#include "../../../../drivers/media/i2c/max96714.h"
+#include "../../../../drivers/media/i2c/tp2855.h"
+
+struct vehicle_sensor_ops {
+	const char *name;
+	int (*sensor_init)(struct vehicle_ad_dev *ad);
+	int (*sensor_deinit)(void);
+	int (*sensor_stream)(struct vehicle_ad_dev *ad, int value);
+	int (*sensor_get_cfg)(struct vehicle_cfg **cfg);
+	void (*sensor_check_cif_error)(struct vehicle_ad_dev *ad, int last_line);
+	int (*sensor_check_id_cb)(struct vehicle_ad_dev *ad);
+	void (*sensor_set_channel)(struct vehicle_ad_dev *ad, int channel);
+	int (*sensor_mod_init)(void);
+};
+static struct vehicle_sensor_ops *sensor_cb;
+
+static struct vehicle_sensor_ops sensor_cb_series[] = {
+	{
+		.name = "adv7181",
+#ifdef CONFIG_VIDEO_REVERSE_AD7181
+		.sensor_init = adv7181_ad_init,
+		.sensor_deinit = adv7181_ad_deinit,
+		.sensor_stream = adv7181_stream,
+		.sensor_get_cfg = adv7181_ad_get_cfg,
+		.sensor_check_cif_error = adv7181_ad_check_cif_error,
+		.sensor_check_id_cb = adv7181_check_id,
+		.sensor_set_channel = adv7181_channel_set
+#endif
+	},
+	{
+		.name = "tp2825",
+#ifdef CONFIG_VIDEO_REVERSE_TP2825
+		.sensor_init = tp2825_ad_init,
+		.sensor_deinit = tp2825_ad_deinit,
+		.sensor_stream = tp2825_stream,
+		.sensor_get_cfg = tp2825_ad_get_cfg,
+		.sensor_check_cif_error = tp2825_ad_check_cif_error,
+		.sensor_check_id_cb = tp2825_check_id,
+		.sensor_set_channel = tp2825_channel_set
+#endif
+	},
+	{
+		.name = "gc2145",
+#ifdef CONFIG_VIDEO_REVERSE_GC2145
+		.sensor_init = gc2145_ad_init,
+		.sensor_deinit = gc2145_ad_deinit,
+		.sensor_stream = gc2145_stream,
+		.sensor_get_cfg = gc2145_ad_get_cfg,
+		.sensor_check_cif_error = gc2145_ad_check_cif_error,
+		.sensor_check_id_cb = gc2145_check_id,
+		.sensor_set_channel = gc2145_channel_set,
+#endif
+	},
+	{
+		.name = "nvp6324",
+#ifdef CONFIG_VIDEO_REVERSE_NVP6324
+		.sensor_init = nvp6324_ad_init,
+		.sensor_deinit = nvp6324_ad_deinit,
+		.sensor_stream = nvp6324_stream,
+		.sensor_get_cfg = nvp6324_ad_get_cfg,
+		.sensor_check_cif_error = nvp6324_ad_check_cif_error,
+		.sensor_check_id_cb = nvp6324_check_id,
+		.sensor_set_channel = nvp6324_channel_set,
+#ifdef CONFIG_VIDEO_NVP6324
+		.sensor_mod_init = nvp6324_sensor_mod_init
+#endif
+#endif
+	},
+	{
+		.name = "max96714",
+#ifdef CONFIG_VIDEO_REVERSE_MAX96714
+		.sensor_init = max96714_ad_init,
+		.sensor_deinit = max96714_ad_deinit,
+		.sensor_stream = max96714_stream,
+		.sensor_get_cfg = max96714_ad_get_cfg,
+		.sensor_check_cif_error = max96714_ad_check_cif_error,
+		.sensor_check_id_cb = max96714_check_id,
+		.sensor_set_channel = max96714_channel_set,
+#ifdef CONFIG_VIDEO_MAX96714
+		.sensor_mod_init = max96714_sensor_mod_init
+#endif
+#endif
+	},
+	{
+		.name = "nvp6188",
+#ifdef CONFIG_VIDEO_REVERSE_NVP6188
+		.sensor_init = nvp6188_ad_init,
+		.sensor_deinit = nvp6188_ad_deinit,
+		.sensor_stream = nvp6188_stream,
+		.sensor_get_cfg = nvp6188_ad_get_cfg,
+		.sensor_check_cif_error = nvp6188_ad_check_cif_error,
+		.sensor_check_id_cb = nvp6188_check_id,
+		.sensor_set_channel = nvp6188_channel_set,
+#ifdef CONFIG_VIDEO_NVP6188
+		.sensor_mod_init = nvp6188_sensor_mod_init
+#endif
+#endif
+	},
+	{
+		.name = "tp2855",
+#ifdef CONFIG_VIDEO_REVERSE_TP2855
+		.sensor_init = tp2855_ad_init,
+		.sensor_deinit = tp2855_ad_deinit,
+		.sensor_stream = tp2855_stream,
+		.sensor_get_cfg = tp2855_ad_get_cfg,
+		.sensor_check_cif_error = tp2855_ad_check_cif_error,
+		.sensor_check_id_cb = tp2855_check_id,
+		.sensor_set_channel = tp2855_channel_set,
+#ifdef CONFIG_VIDEO_TP2855
+		.sensor_mod_init = tp2855_sensor_mod_init
+#endif
+#endif
+	}
+};
+
+int vehicle_generic_sensor_write(struct vehicle_ad_dev *ad, char reg, char *pval)
+{
+	struct i2c_msg msg;
+	int ret;
+
+	char *tx_buf = kmalloc(2, GFP_KERNEL);
+
+	if (!tx_buf)
+		return -ENOMEM;
+
+	memcpy(tx_buf, &reg, 1);
+	memcpy(tx_buf+1, (char *)pval, 1);
+
+	msg.addr = ad->i2c_add;
+	msg.flags = 0;
+	msg.len = 2;
+	msg.buf = (char *)tx_buf;
+//	msg.scl_rate = ad->i2c_rate;
+
+	ret = i2c_transfer(ad->adapter, &msg, 1);
+	kfree(tx_buf);
+
+	return (ret == 1) ? 4 : ret;
+}
+
+int vehicle_sensor_write(struct vehicle_ad_dev *ad, u8 reg, u8 val)
+{
+	struct i2c_msg msg;
+	u8 buf[2];
+	int ret;
+
+	//SENSOR_DG("write reg(0x%x val:0x%x)!\n", reg, val);
+	buf[0] = reg & 0xFF;
+	buf[1] = val;
+
+	msg.addr = ad->i2c_add;
+	msg.flags = 0;
+	msg.buf = buf;
+	msg.len = sizeof(buf);
+
+	ret = i2c_transfer(ad->adapter, &msg, 1);
+	if (ret >= 0)
+		return 0;
+
+	VEHICLE_DGERR("write reg(0x%x val:0x%x) failed !\n", reg, val);
+	return ret;
+}
+
+int vehicle_generic_sensor_read(struct vehicle_ad_dev *ad, char reg)
+{
+	struct i2c_msg msgs[2];
+	int ret;
+	char reg_buf[2];
+	char pval;
+
+	memcpy(reg_buf, &reg, 1);
+
+	msgs[0].addr =	ad->i2c_add;
+	msgs[0].flags = 0;
+	msgs[0].len = 1;
+	msgs[0].buf = reg_buf;
+//	msgs[0].scl_rate = ad->i2c_rate;
+
+	msgs[1].addr = ad->i2c_add;
+	msgs[1].flags = I2C_M_RD;
+	msgs[1].len = 1;
+	msgs[1].buf = &pval;
+//	msgs[1].scl_rate = ad->i2c_rate;
+
+	ret = i2c_transfer(ad->adapter, msgs, 2);
+	if (ret)
+		return ret;
+
+	return pval;
+}
+
+/* sensor register read */
+int vehicle_sensor_read(struct vehicle_ad_dev *ad, u8 reg, u8 *val)
+{
+	struct i2c_msg msg[2];
+	u8 buf[1];
+	int ret;
+
+	buf[0] = reg & 0xFF;
+
+	msg[0].addr = ad->i2c_add;
+	msg[0].flags = 0;
+	msg[0].buf = buf;
+	msg[0].len = sizeof(buf);
+
+	msg[1].addr = ad->i2c_add;
+	msg[1].flags = I2C_M_RD;
+	msg[1].buf = buf;
+	msg[1].len = 1;
+
+	ret = i2c_transfer(ad->adapter, msg, 2);
+	if (ret >= 0) {
+		*val = buf[0];
+		return 0;
+	}
+
+	dev_err(ad->dev,
+		"read reg:0x%x failed !\n", reg);
+
+	return ret;
+}
+
+int vehicle_ad_stream(struct vehicle_ad_dev *ad, int val)
+{
+	int ret = 0;
+
+	if (sensor_cb && sensor_cb->sensor_stream) {
+		ret = sensor_cb->sensor_stream(ad, val);
+		if (ret < 0)
+			VEHICLE_DGERR("%s sensor_init failed!\n", ad->ad_name);
+	}
+
+	return ret;
+}
+
+int vehicle_parse_sensor(struct vehicle_ad_dev *ad)
+{
+	struct device *dev = ad->dev;
+	struct device_node *node = NULL;
+	struct device_node *cp = NULL;
+	enum of_gpio_flags flags;
+	const char *status = NULL;
+	int i;
+	int ret = 0;
+
+	if (of_property_read_u32(dev->of_node, "ad,fix-format",
+				 &ad->fix_format))
+		VEHICLE_DGERR("get fix-format failed!\n");
+
+	if (of_property_read_u32(dev->of_node, "vehicle,rotate-mirror",
+				 &ad->cfg.rotate_mirror))
+		VEHICLE_DGERR("get rotate-mirror failed!\n");
+
+	node = of_parse_phandle(dev->of_node, "rockchip,cif-sensor", 0);
+	if (!node) {
+		VEHICLE_DGERR("get cif-sensor dts failed\n");
+		return -ENODEV;
+	}
+
+	for_each_child_of_node(node, cp) {
+		of_property_read_string(cp, "status", &status);
+		if (status && !strcmp(status, "disabled"))
+			continue;
+		VEHICLE_DG("status: %s\n", status);
+
+//		if (of_property_read_u32(cp, "i2c_rata", &ad->i2c_rate))
+//			SENSOR_DG("Get %s i2c_rata failed!\n", cp->name);
+		if (of_property_read_u32(cp, "i2c_chl", &ad->i2c_chl))
+			VEHICLE_DGERR("Get %s i2c_chl failed!", cp->name);
+		if (of_property_read_u32(cp, "ad_chl", &ad->ad_chl))
+			VEHICLE_DGERR("Get %s ad_chl failed!", cp->name);
+
+		if (ad->ad_chl > 4 || ad->ad_chl < 0) {
+			VEHICLE_DGERR("error, ad_chl %d !\n", ad->ad_chl);
+			ad->ad_chl = 0;
+		}
+		if (of_property_read_u32(cp, "mclk_rate", &ad->mclk_rate))
+			VEHICLE_DGERR("Get %s mclk_rate failed!\n", cp->name);
+
+		if (of_property_read_u32(cp, "drop_frames",
+					 &ad->drop_frames)) {
+			VEHICLE_DGERR("%s:Get sensor, drop-frames failed!\n", __func__);
+			ad->drop_frames = 0; //default drop frames;
+		}
+
+		if (of_property_read_u32(cp, "rst_active", &ad->rst_active))
+			VEHICLE_DGERR("Get %s rst_active failed!", cp->name);
+
+		ad->reset = of_get_named_gpio_flags(cp, "reset-gpios",
+							0, &flags);
+
+		if (of_property_read_u32(cp, "pwr_active", &ad->pwr_active))
+			VEHICLE_DGERR("Get %s pwr_active failed!\n", cp->name);
+
+		if (of_property_read_u32(cp, "pwdn_active", &ad->pwdn_active))
+			VEHICLE_DGERR("Get %s pwdn_active failed!\n", cp->name);
+
+		ad->power = of_get_named_gpio_flags(cp, "power-gpios",
+						    0, &flags);
+		ad->powerdown = of_get_named_gpio_flags(cp,
+							"powerdown-gpios",
+							0, &flags);
+		ad->reset = of_get_named_gpio_flags(cp, "reset-gpios",
+						0, &flags);
+
+		ad->xvclk = of_clk_get_by_name(cp, "xvclk");
+		if (IS_ERR(ad->xvclk)) {
+			ad->xvclk = NULL;
+			VEHICLE_DGERR("Failed to get sensor xvclk, maybe unuse\n");
+		}
+
+		if (of_property_read_u32(cp, "i2c_add", &ad->i2c_add))
+			VEHICLE_DGERR("Get %s i2c_add failed!\n", cp->name);
+
+		ad->i2c_add = (ad->i2c_add >> 1);
+
+		if (of_property_read_u32(cp, "resolution", &ad->resolution))
+			VEHICLE_DGERR("Get %s resolution failed!\n", cp->name);
+
+		of_property_read_u32_array(cp,
+				"rockchip,camera-module-defrect0",
+				(unsigned int *)&ad->defrects[0], 6);
+		of_property_read_u32_array(cp,
+				"rockchip,camera-module-defrect1",
+				(unsigned int *)&ad->defrects[1], 6);
+		of_property_read_u32_array(cp,
+				"rockchip,camera-module-defrect2",
+				(unsigned int *)&ad->defrects[2], 6);
+		of_property_read_u32_array(cp,
+				"rockchip,camera-module-defrect3",
+				(unsigned int *)&ad->defrects[3], 6);
+
+		of_property_read_string(cp,
+				"rockchip,camera-module-interface0",
+				&ad->defrects[0].interface);
+		of_property_read_string(cp,
+				"rockchip,camera-module-interface1",
+				&ad->defrects[1].interface);
+		of_property_read_string(cp,
+				"rockchip,camera-module-interface2",
+				&ad->defrects[2].interface);
+		of_property_read_string(cp,
+				"rockchip,camera-module-interface3",
+				&ad->defrects[3].interface);
+
+		ad->ad_name = cp->name;
+		for (i = 0; i < ARRAY_SIZE(sensor_cb_series); i++) {
+			if (!strcmp(ad->ad_name, sensor_cb_series[i].name))
+				sensor_cb = sensor_cb_series + i;
+		}
+
+		VEHICLE_DG("%s: ad_chl=%d,,ad_addr=%x,fix_for=%d\n", ad->ad_name,
+		    ad->ad_chl, ad->i2c_add, ad->fix_format);
+		VEHICLE_DG("gpio power:%d, active:%d\n", ad->power, ad->pwr_active);
+		VEHICLE_DG("gpio powerdown:%d, active:%d\n",
+		    ad->powerdown, ad->pwdn_active);
+		break;
+	}
+
+	if (!ad->ad_name)
+		ret = -EINVAL;
+
+	return ret;
+}
+
+static void vehicle_ad_mclk_set(struct vehicle_ad_dev *ad, int on)
+{
+	int err = 0;
+	int clk_rate = ad->mclk_rate * 1000000;
+
+	if (on) {
+		err = clk_set_rate(ad->xvclk, clk_rate);
+		if (err < 0)
+			VEHICLE_DGERR("Failed to set xvclk rate (%dMHz)\n", ad->mclk_rate);
+		clk_prepare_enable(ad->xvclk);
+		if (err < 0)
+			VEHICLE_DGERR("Failed to enable xvclk\n");
+	} else {
+		clk_disable_unprepare(ad->xvclk);
+	}
+	usleep_range(2000, 5000);
+}
+
+void vehicle_ad_channel_set(struct vehicle_ad_dev *ad, int channel)
+{
+	if (sensor_cb && sensor_cb->sensor_set_channel)
+		sensor_cb->sensor_set_channel(ad, channel);
+}
+
+int vehicle_ad_init(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+	//WARN_ON(1);
+	VEHICLE_DGERR("%s(%d) ad_name:%s!", __func__, __LINE__, ad->ad_name);
+
+	vehicle_ad_mclk_set(ad, 1);
+	if (sensor_cb && sensor_cb->sensor_init) {
+		ret = sensor_cb->sensor_init(ad);
+		if (ret < 0) {
+			VEHICLE_DGERR("%s sensor_init failed!\n", ad->ad_name);
+			goto end;
+		}
+	} else {
+		VEHICLE_DGERR("%s sensor_init is NULL!\n", ad->ad_name);
+		ret = -1;
+		goto end;
+	}
+
+	if (sensor_cb && sensor_cb->sensor_check_id_cb) {
+		ret = sensor_cb->sensor_check_id_cb(ad);
+		if (ret < 0)
+			VEHICLE_DGERR("%s check id failed!\n", ad->ad_name);
+	}
+
+end:
+	return ret;
+}
+
+int vehicle_ad_deinit(struct vehicle_ad_dev *ad)
+{
+	int ret = 0;
+
+	if (sensor_cb && sensor_cb->sensor_deinit)
+		ret = sensor_cb->sensor_deinit();
+	else
+		ret = -EINVAL;
+
+	clk_disable_unprepare(ad->xvclk);
+	clk_put(ad->xvclk);
+
+	return ret;
+}
+
+int vehicle_to_v4l2_drv_init(void)
+{
+	int ret = 0;
+
+	VEHICLE_DG("%s(%d) enter!", __func__, __LINE__);
+	if (sensor_cb && sensor_cb->sensor_mod_init)
+		ret = sensor_cb->sensor_mod_init();
+	else
+		ret = -EINVAL;
+
+	return ret;
+}
+
+struct vehicle_cfg *vehicle_ad_get_vehicle_cfg(void)
+{
+	struct vehicle_cfg *cfg = NULL;
+
+	if (sensor_cb && sensor_cb->sensor_get_cfg)
+		sensor_cb->sensor_get_cfg(&cfg);
+
+	return cfg;
+}
+
+void vehicle_ad_check_cif_error(struct vehicle_ad_dev *ad, int last_line)
+{
+	if (sensor_cb && sensor_cb->sensor_get_cfg)
+		sensor_cb->sensor_check_cif_error(ad, last_line);
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_gpio.c b/drivers/video/rockchip/vehicle/vehicle_gpio.c
new file mode 100644
index 0000000000000..54d728b002faf
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_gpio.c
@@ -0,0 +1,178 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_gpio.c
+ *
+ * Copyright (C) 2020 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *	Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/platform_device.h>
+#include <linux/kthread.h>
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/completion.h>
+#include <linux/wakelock.h>
+#include <linux/of_gpio.h>
+#include <linux/of_irq.h>
+#include <linux/interrupt.h>
+
+#include "vehicle_gpio.h"
+#include "vehicle_main.h"
+
+static void gpio_det_work_func(struct work_struct *work)
+{
+	struct gpio_detect *gpiod = container_of(work, struct gpio_detect,
+			work.work);
+	int val = gpio_get_value(gpiod->gpio);
+
+	VEHICLE_DG("%s: gpiod->old val(%d), new val(%d)\n",
+			__func__, gpiod->val, val);
+
+	if (gpiod->val != val) {
+		gpiod->val = val;
+		vehicle_gpio_stat_change_notify();
+	}
+}
+
+static irqreturn_t gpio_det_interrupt(int irq, void *dev_id)
+{
+	struct gpio_detect *gpiod = dev_id;
+	int val = gpio_get_value(gpiod->gpio);
+	unsigned int irqflags = IRQF_ONESHOT;
+
+	if (val)
+		irqflags |= IRQ_TYPE_EDGE_FALLING;
+	else
+		irqflags |= IRQ_TYPE_EDGE_RISING;
+	irq_set_irq_type(gpiod->irq, irqflags);
+
+	mod_delayed_work(system_wq, &gpiod->work,
+			 msecs_to_jiffies(gpiod->debounce_ms));
+
+	return IRQ_HANDLED;
+}
+
+static int vehicle_gpio_init_check(struct gpio_detect *gpiod)
+{
+	gpiod->val = gpio_get_value(gpiod->gpio);
+
+	dev_info(gpiod->dev, "%s: gpiod->atv_val(%d), gpiod->val(%d)\n",
+			__func__, gpiod->atv_val, gpiod->val);
+
+	if (gpiod->atv_val == gpiod->val) {
+		vehicle_gpio_stat_change_notify();
+		return 1;
+	} else {
+		return 0;
+	}
+}
+
+bool vehicle_gpio_reverse_check(struct gpio_detect *gpiod)
+{
+	int val = gpiod->val ^ gpiod->atv_val;
+
+	if (gpiod->num == 0)
+		return true;
+	else
+		return (val == 0) ? true : false;
+}
+
+static int gpio_parse_dt(struct gpio_detect *gpiod, const char *ad_name)
+{
+	struct device *dev = gpiod->dev;
+	struct device_node *gpiod_node;
+	struct device_node *node;
+	const char *name;
+	int ret = 0;
+
+	gpiod_node = of_parse_phandle(dev->of_node, "rockchip,gpio-det", 0);
+	if (!gpiod_node) {
+		VEHICLE_DGERR("phase gpio-det from dts failed, maybe no use!\n");
+		return -EINVAL;
+	}
+
+	gpiod->num = of_get_child_count(gpiod_node);
+	if (gpiod->num == 0) {
+		VEHICLE_DGERR("gpio-det child count is 0, maybe no use!\n");
+		return -EINVAL;
+	}
+
+	for_each_child_of_node(gpiod_node, node) {
+		enum of_gpio_flags flags;
+
+		name = of_get_property(node, "label", NULL);
+		if (!strcmp(name, "car-reverse")) {
+			gpiod->gpio = of_get_named_gpio_flags(node, "car-reverse-gpios", 0, &flags);
+			if (!gpio_is_valid(gpiod->gpio)) {
+				dev_err(dev, "failed to get car reverse gpio\n");
+				ret = -ENOMEM;
+			}
+			gpiod->atv_val = !(flags & OF_GPIO_ACTIVE_LOW);
+			of_property_read_u32(node, "linux,debounce-ms",
+						  &gpiod->debounce_ms);
+			break;
+		}
+	}
+
+	VEHICLE_DG("%s:gpio %d, act_val %d, mirror %d, debounce_ms %d\n",
+		__func__, gpiod->gpio, gpiod->atv_val, gpiod->mirror, gpiod->debounce_ms);
+	return ret;
+}
+
+int vehicle_gpio_init(struct gpio_detect *gpiod, const char *ad_name)
+{
+	int gpio;
+	int ret;
+	unsigned long irqflags = IRQF_ONESHOT;
+
+	if (gpio_parse_dt(gpiod, ad_name) < 0) {
+		VEHICLE_INFO("%s, gpio parse dt failed, maybe unuse gpio-det\n", __func__);
+	} else {
+		gpio = gpiod->gpio;
+
+		ret = gpio_request(gpio, "vehicle");
+		if (ret < 0)
+			VEHICLE_DGERR("%s:failed to request gpio %d, maybe no use\n",
+					__func__, ret);
+
+		dev_info(gpiod->dev, "%s: request irq gpio(%d)\n", __func__, gpio);
+		gpio_direction_input(gpio);
+
+		gpiod->irq = gpio_to_irq(gpio);
+		if (gpiod->irq < 0)
+			VEHICLE_DGERR("failed to get irq, GPIO %d, maybe no use\n", gpio);
+
+		gpiod->val = gpio_get_value(gpio);
+		if (gpiod->val)
+			irqflags |= IRQ_TYPE_EDGE_FALLING;
+		else
+			irqflags |= IRQ_TYPE_EDGE_RISING;
+		ret = devm_request_threaded_irq(gpiod->dev, gpiod->irq,
+					NULL, gpio_det_interrupt,
+					irqflags, "vehicle gpio", gpiod);
+		if (ret < 0)
+			VEHICLE_DGERR("request irq(%s) failed:%d\n",
+				"vehicle", ret);
+	}
+
+	//if not add in create_workqueue only execute once;
+	INIT_DELAYED_WORK(&gpiod->work, gpio_det_work_func);
+
+	vehicle_gpio_init_check(gpiod);
+
+	return 0;
+}
+
+int vehicle_gpio_deinit(struct gpio_detect *gpiod)
+{
+	gpio_free(gpiod->gpio);
+	return 0;
+}
diff --git a/drivers/video/rockchip/vehicle/vehicle_gpio.h b/drivers/video/rockchip/vehicle/vehicle_gpio.h
new file mode 100644
index 0000000000000..6c36392cd6b78
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_gpio.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_GPIO_H
+#define __VEHICLE_GPIO_H
+
+#include "vehicle_cfg.h"
+
+struct gpio_detect {
+	int gpio;
+	int atv_val;
+	int val;
+	int irq;
+	int mirror;
+	int num;
+	unsigned int debounce_ms;
+	struct delayed_work work;
+	struct device *dev;
+};
+/*
+ * true : reverse on
+ * false : reverse over
+ */
+bool vehicle_gpio_reverse_check(struct gpio_detect *gpiod);
+
+int vehicle_gpio_init(struct gpio_detect *gpiod, const char *ad_name);
+
+int vehicle_gpio_deinit(struct gpio_detect *gpiod);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_main.c b/drivers/video/rockchip/vehicle/vehicle_main.c
new file mode 100644
index 0000000000000..e306c40ab16ee
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_main.c
@@ -0,0 +1,501 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/video/rockchip/video/vehicle_main.c
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ * Authors:
+ *	Zhiqin Wei <wzq@rock-chips.com>
+ *      <randy.wang@rock-chips.com>
+ *	Jianwei Fan <jianwei.fan@rock-chips.com>
+ *
+ */
+
+#define CAMMODULE_NAME    "vehicle_main"
+
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/platform_device.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/fb.h>
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/completion.h>
+#include <linux/wakelock.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/pm_runtime.h>
+#include <linux/interrupt.h>
+#include "vehicle_flinger.h"
+#include "vehicle_cfg.h"
+#include "vehicle_ad.h"
+#include "vehicle_main.h"
+#include "vehicle_cif.h"
+#include "vehicle_gpio.h"
+#include <linux/version.h>
+#include "../../../media/platform/rockchip/cif/dev.h"
+#include "../../../phy/rockchip/phy-rockchip-csi2-dphy-common.h"
+
+#define DRIVER_VERSION		KERNEL_VERSION(0, 0x03, 0x02)
+
+static bool flinger_inited;
+static bool TEST_GPIO = true;
+static bool dvr_apk_need_start;
+
+enum {
+	STATE_CLOSE = 0,
+	STATE_OPEN,
+};
+
+struct vehicle {
+	struct device	*dev;
+	struct pinctrl *pinctrl;
+	struct pinctrl_state *pins_default;
+	struct wake_lock wake_lock;
+	struct gpio_detect gpio_data;
+	struct vehicle_cif cif;
+	struct vehicle_ad_dev ad;
+	int mirror;
+	wait_queue_head_t vehicle_wait;
+	atomic_t vehicle_atomic;
+	int state;
+	bool android_is_ready;
+	bool gpio_over;
+};
+
+static struct vehicle *g_vehicle;
+
+static int vehicle_parse_dt(struct vehicle *vehicle_info)
+{
+	struct device	*dev = vehicle_info->dev;
+
+	/*  1. pinctrl */
+	vehicle_info->pinctrl = devm_pinctrl_get(dev);
+
+	if (IS_ERR(vehicle_info->pinctrl)) {
+		dev_err(dev, "pinctrl get failed, maybe unuse\n");
+	} else {
+		vehicle_info->pins_default = pinctrl_lookup_state(vehicle_info->pinctrl,
+				"default");
+
+		if (IS_ERR(vehicle_info->pins_default))
+			dev_err(dev, "get default pinstate failed\n");
+	}
+
+	return 0;
+}
+
+void vehicle_ad_stat_change_notify(void)
+{
+	if (g_vehicle) {
+		VEHICLE_INFO("ad state change! set atpmic to 1!\n");
+		atomic_set(&g_vehicle->vehicle_atomic, 1);
+	}
+}
+
+void vehicle_cif_stat_change_notify(void)
+{
+	if (g_vehicle) {
+		VEHICLE_INFO("cif state change! set atpmic to 1!\n");
+		atomic_set(&g_vehicle->vehicle_atomic, 1);
+	}
+}
+
+void vehicle_gpio_stat_change_notify(void)
+{
+	if (g_vehicle && !g_vehicle->gpio_over) {
+		VEHICLE_INFO("reverse gpio state change! set atpmic to 1!\n");
+		atomic_set(&g_vehicle->vehicle_atomic, 1);
+	}
+}
+
+void vehicle_cif_error_notify(int last_line)
+{
+	if (g_vehicle) {
+		VEHICLE_INFO("cif error notify\n");
+		vehicle_ad_check_cif_error(&g_vehicle->ad, last_line);
+	}
+}
+
+static void vehicle_open(struct vehicle_cfg *v_cfg)
+{
+	VEHICLE_INFO("%s enter: android_is_ready ?= %d",
+			__func__, g_vehicle->android_is_ready);
+	vehicle_flinger_reverse_open(v_cfg, g_vehicle->android_is_ready);
+	vehicle_cif_reverse_open(v_cfg);
+}
+
+static void vehicle_close(void)
+{
+	vehicle_cif_reverse_close();
+	vehicle_flinger_reverse_close(g_vehicle->android_is_ready);
+}
+
+static void vehicle_open_close(void)
+{
+	vehicle_cif_reverse_close();
+}
+
+static int vehicle_state_change(struct vehicle *v)
+{
+	struct vehicle_cfg *v_cfg;
+	struct gpio_detect *gpiod = &v->gpio_data;
+	bool gpio_reverse_on;
+	int ret = 0;
+
+	/*  1. get ad sensor cfg */
+	v_cfg = vehicle_ad_get_vehicle_cfg();
+
+	if (!v_cfg) {
+		VEHICLE_DGERR("v_cfg is NULL, if for test continue.\n");
+		return -ENODEV;
+	}
+
+	if (!flinger_inited) {
+		do {
+			/*  2. flinger */
+			VEHICLE_DG("%s: flinger init start\r\n", __func__);
+			ret = vehicle_flinger_init(v->dev, v_cfg);
+			if (ret < 0) {
+				VEHICLE_DG("rk_vehicle_system_main: flinger init failed\r\n");
+				msleep(20);
+			}
+		} while (ret);
+	}
+	VEHICLE_DG("%s: flinger init success\r\n", __func__);
+	flinger_inited = true;
+
+	gpio_reverse_on = vehicle_gpio_reverse_check(gpiod);
+	gpio_reverse_on = TEST_GPIO & gpio_reverse_on;
+	VEHICLE_INFO(
+	"%s, gpio = reverse %s, width = %d, sensor_ready = %d, state=%d dvr_apk_need_start = %d\n",
+	__func__, gpio_reverse_on ? "on" : "over",
+	v_cfg->width, v_cfg->ad_ready, v->state, dvr_apk_need_start);
+	if (v_cfg->mbus_flags & V4L2_MBUS_CSI2_CONTINUOUS_CLOCK) {
+		switch (v->state) {
+		case STATE_CLOSE:
+			if (dvr_apk_need_start) {
+				vehicle_open(v_cfg);
+				msleep(20);
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				v->state = STATE_OPEN;
+			}
+			if (gpio_reverse_on) {
+				vehicle_open(v_cfg);
+				msleep(20);
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				v->state = STATE_OPEN;
+			}
+			break;
+		case STATE_OPEN:
+			/*  reverse exit || video loss */
+			if (!dvr_apk_need_start && (!gpio_reverse_on || !v_cfg->ad_ready)) {
+				vehicle_close();
+				vehicle_ad_stream(&v->ad, 0);
+				v->state = STATE_CLOSE;
+			} else if (gpio_reverse_on && !v->android_is_ready) { //video fmt change
+				vehicle_open_close();
+				vehicle_open(v_cfg);
+				msleep(100);
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+			} else if (!gpio_reverse_on && dvr_apk_need_start) {
+				vehicle_close();
+				vehicle_open(v_cfg);
+				msleep(20);
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+			}
+			break;
+		}
+	} else if (v_cfg->mbus_flags & V4L2_MBUS_CSI2_NONCONTINUOUS_CLOCK) {
+		switch (v->state) {
+		case STATE_CLOSE:
+			if (dvr_apk_need_start) {
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+				v->state = STATE_OPEN;
+			}
+			if (gpio_reverse_on) {
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+				v->state = STATE_OPEN;
+			}
+			break;
+		case STATE_OPEN:
+			/*  reverse exit || video loss */
+			if (!dvr_apk_need_start && (!gpio_reverse_on || !v_cfg->ad_ready)) {
+				vehicle_close();
+				vehicle_ad_stream(&v->ad, 0);
+				v->state = STATE_CLOSE;
+			} else if (gpio_reverse_on && !v->android_is_ready) { //video fmt change
+				vehicle_open_close();
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(100);
+				vehicle_open(v_cfg);
+			} else if (!gpio_reverse_on && dvr_apk_need_start) {
+				vehicle_close();
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+			}
+			break;
+		}
+	} else {
+		switch (v->state) {
+		case STATE_CLOSE:
+			if (dvr_apk_need_start) {
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+				v->state = STATE_OPEN;
+			}
+			if (gpio_reverse_on) {
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+				v->state = STATE_OPEN;
+			}
+			break;
+		case STATE_OPEN:
+			/*  reverse exit || video loss */
+			if (!dvr_apk_need_start && (!gpio_reverse_on || !v_cfg->ad_ready)) {
+				vehicle_close();
+				vehicle_ad_stream(&v->ad, 0);
+				v->state = STATE_CLOSE;
+			} else if (gpio_reverse_on && !v->android_is_ready) { //video fmt change
+				vehicle_open_close();
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(100);
+				vehicle_open(v_cfg);
+			} else if (!gpio_reverse_on && dvr_apk_need_start) {
+				vehicle_close();
+				vehicle_ad_stream(&v->ad, 0);
+				vehicle_ad_channel_set(&g_vehicle->ad, 0);
+				vehicle_ad_stream(&v->ad, 1);
+				msleep(20);
+				vehicle_open(v_cfg);
+			}
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static int vehicle_probe(struct platform_device *pdev)
+{
+	struct vehicle *vehicle_info;
+
+	dev_info(&pdev->dev, "driver version: %02x.%02x.%02x",
+		 DRIVER_VERSION >> 16,
+		 (DRIVER_VERSION & 0xff00) >> 8,
+		 DRIVER_VERSION & 0x00ff);
+
+	vehicle_info = devm_kzalloc(&pdev->dev,
+				    sizeof(struct vehicle), GFP_KERNEL);
+	if (!vehicle_info)
+		return -ENOMEM;
+
+	vehicle_info->dev = &pdev->dev;
+	vehicle_info->gpio_data.dev = &pdev->dev;
+	vehicle_info->cif.dev = &pdev->dev;
+	vehicle_info->ad.dev = &pdev->dev;
+
+	dev_set_name(vehicle_info->dev, "vehicle_main");
+	if (!pdev->dev.of_node)
+		return -EINVAL;
+
+	vehicle_parse_dt(vehicle_info);
+
+	if (vehicle_parse_sensor(&vehicle_info->ad) < 0) {
+		VEHICLE_DGERR("parse sensor failed!\n");
+		return -EINVAL;
+	}
+
+	wake_lock_init(&vehicle_info->wake_lock, WAKE_LOCK_SUSPEND, "vehicle");
+
+	dev_info(vehicle_info->dev, "vehicle driver probe success\n");
+
+	init_waitqueue_head(&vehicle_info->vehicle_wait);
+	atomic_set(&vehicle_info->vehicle_atomic, 0);
+	vehicle_info->state = STATE_CLOSE;
+	vehicle_info->android_is_ready = false;
+	vehicle_info->gpio_over = false;
+
+	g_vehicle = vehicle_info;
+
+	return 0;
+}
+
+#if defined(CONFIG_OF)
+static const struct of_device_id vehicle_of_match[] = {
+	{ .compatible = "rockchip,vehicle", },
+	{},
+};
+#endif
+
+static struct platform_driver vehicle_driver = {
+	.driver     = {
+		.name   = "vehicle",
+		.owner  = THIS_MODULE,
+		.of_match_table = of_match_ptr(vehicle_of_match),
+	},
+	.probe      = vehicle_probe,
+};
+
+void vehicle_android_is_ready_notify(void)
+{
+	if (g_vehicle)
+		g_vehicle->android_is_ready = true;
+	TEST_GPIO = !TEST_GPIO;
+	atomic_set(&g_vehicle->vehicle_atomic, 1);
+}
+
+void vehicle_apk_state_change(char data[22])
+{
+	if (memcmp(data, "11", 2) == 0)
+		dvr_apk_need_start = true;
+	else if (memcmp(data, "10", 2) == 0)
+		dvr_apk_need_start = false;
+
+	if (g_vehicle)
+		atomic_set(&g_vehicle->vehicle_atomic, 1);
+}
+
+static void vehicle_exit_complete_notify(struct vehicle *v)
+{
+	char *status = NULL;
+	char *envp[2];
+
+	if (!v)
+		return;
+	status = kasprintf(GFP_KERNEL, "vehicle_exit=done");
+	envp[0] = status;
+	envp[1] = NULL;
+	wake_lock_timeout(&v->wake_lock, 5 * HZ);
+	kobject_uevent_env(&v->dev->kobj, KOBJ_CHANGE, envp);
+
+	kfree(status);
+}
+
+static int rk_vehicle_system_main(void *arg)
+{
+	int ret = -1;
+	struct vehicle *v = g_vehicle;
+	int loop_times = 0;
+
+	if (!g_vehicle) {
+		VEHICLE_DGERR("vehicle probe failed, g_vehicle is NULL.\n");
+		goto VEHICLE_EXIT;
+	}
+
+	/*  0. gpio init and check state */
+	ret = vehicle_gpio_init(&v->gpio_data, v->ad.ad_name);
+	if (ret < 0) {
+		VEHICLE_DGERR("%s: gpio init failed\r\n", __func__);
+		goto VEHICLE_GPIO_DEINIT;
+	}
+	VEHICLE_DG("vehicle_gpio_init ok!\n");
+
+	/*  1.ad */
+	VEHICLE_DG("%s: vehicle_ad_init start\r\n", __func__);
+
+	ret = vehicle_ad_init(&v->ad);
+	if (ret < 0) {
+		VEHICLE_DGERR("%s: ad init failed\r\n", __func__);
+		goto VEHICLE_AD_DEINIT;
+	}
+	VEHICLE_DG("vehicle_ad_init ok!\r\n");
+
+	/*  3. cif init */
+	ret = vehicle_cif_init(&v->cif);
+	if (ret < 0) {
+		VEHICLE_DGERR("%s: cif init failed\r\n", __func__);
+		goto VEHICLE_CIF_DEINIT;
+	}
+	VEHICLE_DG("%s: vehicle_cif_init ok!\r\n", __func__);
+	pm_runtime_enable(v->dev);
+	pm_runtime_get_sync(v->dev);
+
+	//while (STATE_OPEN == v->state || !v->vehicle_need_exit) {
+	while (v->state == STATE_OPEN || !v->android_is_ready) {
+		if (v->android_is_ready && !v->state)
+			v->gpio_over = true;
+		wait_event_timeout(v->vehicle_wait,
+				   atomic_read(&v->vehicle_atomic),
+				   msecs_to_jiffies(100));
+		if (atomic_read(&v->vehicle_atomic)) {
+			atomic_set(&v->vehicle_atomic, 0);
+			vehicle_state_change(v);
+		}
+		VEHICLE_DG("loop time(%d) \r\n", loop_times);
+		loop_times++;
+	}
+
+VEHICLE_CIF_DEINIT:
+	vehicle_cif_deinit(&v->cif);
+
+VEHICLE_AD_DEINIT:
+	vehicle_ad_deinit(&v->ad);
+
+VEHICLE_GPIO_DEINIT:
+	vehicle_gpio_deinit(&v->gpio_data);
+
+	/*Init normal drivers*/
+VEHICLE_EXIT:
+	if (flinger_inited)
+		vehicle_flinger_deinit();
+	// if (v && v->pinctrl)
+	//	pinctrl_put(v->pinctrl);
+	vehicle_to_v4l2_drv_init();
+	msleep(500);
+	rockchip_csi2_dphy_hw_init();
+	rockchip_csi2_dphy_init();
+	rk_cif_plat_drv_init();
+	// rkcif_csi2_plat_drv_init();
+	rkcif_clr_unready_dev();
+#ifdef CONFIG_GPIO_DET
+	//gpio_det_init();
+#endif
+	// msleep(1000);
+	vehicle_exit_complete_notify(v);
+	vechile_module_exit();
+	return 0;
+}
+
+static int __init vehicle_system_start(void)
+{
+	platform_driver_register(&vehicle_driver);
+	kthread_run(rk_vehicle_system_main, NULL, "vehicle main");
+
+	return 0;
+}
+
+subsys_initcall_sync(vehicle_system_start);
diff --git a/drivers/video/rockchip/vehicle/vehicle_main.h b/drivers/video/rockchip/vehicle/vehicle_main.h
new file mode 100644
index 0000000000000..4d66db14a978c
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_main.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __VEHICLE_MAIN_H
+#define __VEHICLE_MAIN_H
+
+/* impl by vehicle_main, call by ad detect */
+void vehicle_ad_stat_change_notify(void);
+void vehicle_cif_stat_change_notify(void);
+void vehicle_gpio_stat_change_notify(void);
+void vehicle_cif_error_notify(int last_line);
+void vehicle_android_is_ready_notify(void);
+void vehicle_apk_state_change(char crtc[22]);
+void vechile_module_exit(void);
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_samsung_dcphy_common.h b/drivers/video/rockchip/vehicle/vehicle_samsung_dcphy_common.h
new file mode 100644
index 0000000000000..65189b720a02e
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_samsung_dcphy_common.h
@@ -0,0 +1,246 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ *
+ */
+
+#ifndef _VEHICLE_SAMSUNG_DCPHY_COMMON_H_
+#define _VEHICLE_SAMSUNG_DCPHY_COMMON_H_
+
+#define MAX_NUM_CSI2_DPHY	(0x2)
+
+/*redefine samsung_mipi_dcphy info*/
+struct samsung_mipi_dcphy {
+	struct device *dev;
+	struct clk *ref_clk;
+	struct clk *pclk;
+	struct regmap *regmap;
+	struct regmap *grf_regmap;
+	struct reset_control *m_phy_rst;
+	struct reset_control *s_phy_rst;
+	struct reset_control *apb_rst;
+	struct reset_control *grf_apb_rst;
+	struct mutex mutex;
+	struct csi2_dphy *dphy_dev[MAX_NUM_CSI2_DPHY];
+	atomic_t stream_cnt;
+	int dphy_dev_num;
+	bool c_option;
+
+	unsigned int lanes;
+
+	struct {
+		unsigned long long rate;
+		u8 prediv;
+		u16 fbdiv;
+		long dsm;
+		u8 scaler;
+
+		bool ssc_en;
+		u8 mfr;
+		u8 mrr;
+	} pll;
+
+	int (*stream_on)(struct csi2_dphy *dphy, struct v4l2_subdev *sd);
+	int (*stream_off)(struct csi2_dphy *dphy, struct v4l2_subdev *sd);
+
+	/*for vehicle*/
+	struct csi2_dphy_hw *dphy_vehicle[MAX_NUM_CSI2_DPHY];
+	int dphy_vehicle_num;
+};
+
+#define UPDATE(x, h, l)	(((x) << (l)) & GENMASK((h), (l)))
+
+/*samsung mipi dcphy register*/
+#define BIAS_CON0		0x0000
+#define BIAS_CON1		0x0004
+#define BIAS_CON2		0x0008
+#define BIAS_CON4		0x0010
+#define I_MUX_SEL_MASK		GENMASK(6, 5)
+#define I_MUX_SEL(x)		UPDATE(x, 6, 5)
+
+#define PLL_CON0		0x0100
+#define PLL_EN			BIT(12)
+#define S_MASK			GENMASK(10, 8)
+#define S(x)			UPDATE(x, 10, 8)
+#define P_MASK			GENMASK(5, 0)
+#define P(x)			UPDATE(x, 5, 0)
+#define PLL_CON1		0x0104
+#define PLL_CON2		0x0108
+#define M_MASK			GENMASK(9, 0)
+#define M(x)			UPDATE(x, 9, 0)
+#define PLL_CON3		0x010c
+#define MRR_MASK		GENMASK(13, 8)
+#define MRR(x)			UPDATE(x, 13, 8)
+#define MFR_MASK                GENMASK(7, 0)
+#define MFR(x)			UPDATE(x, 7, 0)
+#define PLL_CON4		0x0110
+#define SSCG_EN			BIT(11)
+#define PLL_CON5		0x0114
+#define RESET_N_SEL		BIT(10)
+#define PLL_ENABLE_SEL		BIT(8)
+#define PLL_CON6		0x0118
+#define PLL_CON7		0x011c
+#define PLL_LOCK_CNT(x)		UPDATE(x, 15, 0)
+#define PLL_CON8		0x0120
+#define PLL_STB_CNT(x)		UPDATE(x, 15, 0)
+#define PLL_STAT0		0x0140
+#define PLL_LOCK		BIT(0)
+
+#define DPHY_MC_GNR_CON0	0x0300
+#define PHY_READY		BIT(1)
+#define PHY_ENABLE		BIT(0)
+#define DPHY_MC_GNR_CON1	0x0304
+#define T_PHY_READY(x)		UPDATE(x, 15, 0)
+#define DPHY_MC_ANA_CON0	0x0308
+#define DPHY_MC_ANA_CON1	0x030c
+#define DPHY_MC_ANA_CON2	0x0310
+#define HS_VREG_AMP_ICON(x)	UPDATE(x, 1, 0)
+#define DPHY_MC_TIME_CON0	0x0330
+#define HSTX_CLK_SEL		BIT(12)
+#define T_LPX(x)		UPDATE(x, 11, 4)
+#define DPHY_MC_TIME_CON1	0x0334
+#define T_CLK_ZERO(x)		UPDATE(x, 15, 8)
+#define T_CLK_PREPARE(x)	UPDATE(x, 7, 0)
+#define DPHY_MC_TIME_CON2	0x0338
+#define T_HS_EXIT(x)		UPDATE(x, 15, 8)
+#define T_CLK_TRAIL(x)		UPDATE(x, 7, 0)
+#define DPHY_MC_TIME_CON3	0x033c
+#define T_CLK_POST(x)		UPDATE(x, 7, 0)
+#define DPHY_MC_TIME_CON4	0x0340
+#define T_ULPS_EXIT(x)		UPDATE(x, 9, 0)
+#define DPHY_MC_DESKEW_CON0	0x0350
+#define SKEW_CAL_RUN_TIME(x)	UPDATE(x, 15, 12)
+
+#define SKEW_CAL_INIT_RUN_TIME(x)	UPDATE(x, 11, 8)
+#define SKEW_CAL_INIT_WAIT_TIME(x)	UPDATE(x, 7, 4)
+#define SKEW_CAL_EN			BIT(0)
+
+#define COMBO_MD0_GNR_CON0	0x0400
+#define COMBO_MD0_GNR_CON1	0x0404
+#define COMBO_MD0_ANA_CON0	0x0408
+#define COMBO_MD0_ANA_CON1      0x040C
+#define COMBO_MD0_ANA_CON2	0x0410
+
+#define COMBO_MD0_TIME_CON0	0x0430
+#define COMBO_MD0_TIME_CON1	0x0434
+#define COMBO_MD0_TIME_CON2	0x0438
+#define COMBO_MD0_TIME_CON3	0x043C
+#define COMBO_MD0_TIME_CON4	0x0440
+#define COMBO_MD0_DATA_CON0	0x0444
+
+#define COMBO_MD1_GNR_CON0	0x0500
+#define COMBO_MD1_GNR_CON1	0x0504
+#define COMBO_MD1_ANA_CON0	0x0508
+#define COMBO_MD1_ANA_CON1	0x050c
+#define COMBO_MD1_ANA_CON2	0x0510
+#define COMBO_MD1_TIME_CON0	0x0530
+#define COMBO_MD1_TIME_CON1	0x0534
+#define COMBO_MD1_TIME_CON2	0x0538
+#define COMBO_MD1_TIME_CON3	0x053C
+#define COMBO_MD1_TIME_CON4	0x0540
+#define COMBO_MD1_DATA_CON0	0x0544
+
+#define COMBO_MD2_GNR_CON0	0x0600
+#define COMBO_MD2_GNR_CON1	0x0604
+#define COMBO_MD2_ANA_CON0	0X0608
+#define COMBO_MD2_ANA_CON1	0X060C
+#define COMBO_MD2_ANA_CON2	0X0610
+#define COMBO_MD2_TIME_CON0	0x0630
+#define COMBO_MD2_TIME_CON1	0x0634
+#define COMBO_MD2_TIME_CON2	0x0638
+#define COMBO_MD2_TIME_CON3	0x063C
+#define COMBO_MD2_TIME_CON4	0x0640
+#define COMBO_MD2_DATA_CON0	0x0644
+
+#define DPHY_MD3_GNR_CON0	0x0700
+#define DPHY_MD3_GNR_CON1	0x0704
+#define DPHY_MD3_ANA_CON0	0X0708
+#define DPHY_MD3_ANA_CON1	0X070C
+#define DPHY_MD3_ANA_CON2	0X0710
+#define DPHY_MD3_TIME_CON0	0x0730
+#define DPHY_MD3_TIME_CON1	0x0734
+#define DPHY_MD3_TIME_CON2	0x0738
+#define DPHY_MD3_TIME_CON3	0x073C
+#define DPHY_MD3_TIME_CON4	0x0740
+#define DPHY_MD3_DATA_CON0	0x0744
+
+#define T_LP_EXIT_SKEW(x)	UPDATE(x, 3, 2)
+#define T_LP_ENTRY_SKEW(x)	UPDATE(x, 1, 0)
+#define T_HS_ZERO(x)		UPDATE(x, 15, 8)
+#define T_HS_PREPARE(x)		UPDATE(x, 7, 0)
+#define T_HS_EXIT(x)		UPDATE(x, 15, 8)
+#define T_HS_TRAIL(x)		UPDATE(x, 7, 0)
+#define T_TA_GET(x)		UPDATE(x, 7, 4)
+#define T_TA_GO(x)		UPDATE(x, 3, 0)
+
+/* MIPI_CDPHY_GRF registers */
+#define MIPI_DCPHY_GRF_CON0	0x0000
+#define S_CPHY_MODE		HIWORD_UPDATE(1, 3, 3)
+#define M_CPHY_MODE		HIWORD_UPDATE(1, 0, 0)
+
+#define MAX_DPHY_BW		4500000L
+#define MAX_CPHY_BW		2000000L
+
+#define RX_CLK_THS_SETTLE		(0xb30)
+#define RX_LANE0_THS_SETTLE		(0xC30)
+#define RX_LANE0_ERR_SOT_SYNC		(0xC34)
+#define RX_LANE1_THS_SETTLE		(0xD30)
+#define RX_LANE1_ERR_SOT_SYNC		(0xD34)
+#define RX_LANE2_THS_SETTLE		(0xE30)
+#define RX_LANE2_ERR_SOT_SYNC		(0xE34)
+#define RX_LANE3_THS_SETTLE		(0xF30)
+#define RX_LANE3_ERR_SOT_SYNC		(0xF34)
+#define RX_CLK_LANE_ENABLE		(0xB00)
+#define RX_DATA_LANE0_ENABLE		(0xC00)
+#define RX_DATA_LANE1_ENABLE		(0xD00)
+#define RX_DATA_LANE2_ENABLE		(0xE00)
+#define RX_DATA_LANE3_ENABLE		(0xF00)
+
+#define RX_S0C_GNR_CON1			(0xB04)
+#define RX_S0C_ANA_CON1			(0xB0c)
+#define RX_S0C_ANA_CON2			(0xB10)
+#define RX_S0C_ANA_CON3			(0xB14)
+#define RX_COMBO_S0D0_GNR_CON1		(0xC04)
+#define RX_COMBO_S0D0_ANA_CON1		(0xC0c)
+#define RX_COMBO_S0D0_ANA_CON2		(0xC10)
+#define RX_COMBO_S0D0_ANA_CON3		(0xC14)
+#define RX_COMBO_S0D0_ANA_CON6		(0xC20)
+#define RX_COMBO_S0D0_ANA_CON7		(0xC24)
+#define RX_COMBO_S0D0_DESKEW_CON0	(0xC40)
+#define RX_COMBO_S0D0_DESKEW_CON2	(0xC48)
+#define RX_COMBO_S0D0_DESKEW_CON4	(0xC50)
+#define RX_COMBO_S0D0_CRC_CON1		(0xC64)
+#define RX_COMBO_S0D0_CRC_CON2		(0xC68)
+#define RX_COMBO_S0D1_GNR_CON1		(0xD04)
+#define RX_COMBO_S0D1_ANA_CON1		(0xD0c)
+#define RX_COMBO_S0D1_ANA_CON2		(0xD10)
+#define RX_COMBO_S0D1_ANA_CON3		(0xD14)
+#define RX_COMBO_S0D1_ANA_CON6		(0xD20)
+#define RX_COMBO_S0D1_ANA_CON7		(0xD24)
+#define RX_COMBO_S0D1_DESKEW_CON0	(0xD40)
+#define RX_COMBO_S0D1_DESKEW_CON2	(0xD48)
+#define RX_COMBO_S0D1_DESKEW_CON4	(0xD50)
+#define RX_COMBO_S0D1_CRC_CON1		(0xD64)
+#define RX_COMBO_S0D1_CRC_CON2		(0xD68)
+#define RX_COMBO_S0D2_GNR_CON1		(0xE04)
+#define RX_COMBO_S0D2_ANA_CON1		(0xE0c)
+#define RX_COMBO_S0D2_ANA_CON2		(0xE10)
+#define RX_COMBO_S0D2_ANA_CON3		(0xE14)
+#define RX_COMBO_S0D2_ANA_CON6		(0xE20)
+#define RX_COMBO_S0D2_ANA_CON7		(0xE24)
+#define RX_COMBO_S0D2_DESKEW_CON0	(0xE40)
+#define RX_COMBO_S0D2_DESKEW_CON2	(0xE48)
+#define RX_COMBO_S0D2_DESKEW_CON4	(0xE50)
+#define RX_COMBO_S0D2_CRC_CON1		(0xE64)
+#define RX_COMBO_S0D2_CRC_CON2		(0xE68)
+#define RX_S0D3_GNR_CON1		(0xF04)
+#define RX_S0D3_ANA_CON1		(0xF0c)
+#define RX_S0D3_ANA_CON2		(0xF10)
+#define RX_S0D3_ANA_CON3		(0xF14)
+#define RX_S0D3_DESKEW_CON0		(0xF40)
+#define RX_S0D3_DESKEW_CON2		(0xF48)
+#define RX_S0D3_DESKEW_CON4		(0xF50)
+
+#endif
diff --git a/drivers/video/rockchip/vehicle/vehicle_version.h b/drivers/video/rockchip/vehicle/vehicle_version.h
new file mode 100644
index 0000000000000..9e5da4de3c75f
--- /dev/null
+++ b/drivers/video/rockchip/vehicle/vehicle_version.h
@@ -0,0 +1,78 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Rockchip Vehicle driver
+ *
+ * Copyright (C) 2022 Rockchip Electronics Co., Ltd.
+ */
+
+#ifndef _RKVEHICLE_VERSION_H
+#define _RKVEHICLE_VERSION_H
+
+#include <linux/version.h>
+
+/*
+ *RKVEHICLE DRIVER VERSION NOTE
+ *
+ * V0.0X01.0X00 first version.
+ *  1. add support rk356x dvp/mipi fast vehicle reverse
+ *  2. add sample dvp interface sensor gc2145 for test
+ *  3. add sample mipi interface sensor nvp6314 one channel for test
+ *  4. fixup rga old/new format transform issue
+ * V0.0X01.0X01 fixup rga yuvtorgb transform issue
+ * V0.0X01.0X02 modify debug log issue
+ * V0.0X01.0X03 fix vehicle reverse close crash issue
+ * V0.0X01.0X04 fix vehicle reverse reopen not ok issue
+ * V0.0X01.0X05 fix after add hwc reserved plane patch, but not use reverse display issue.
+ * V0.0X01.0X06 fix reverse open/close probably stay in reverse preview issue.
+ * V0.0X01.0X07 rename function & remove deprecated code.
+ * V0.0X01.0X08 use Esmart0-win0 plane for vehicle, for Esmart1 depend on Esmart0 open first
+ * V0.0X01.0X09
+ *  1. fix vehicle plane zpos not update issue
+ *  2. use vop_drm_zpos 0x7 & not use drm_direct_disable_kernel_logo to fix kernel logo issue
+ * V0.0X01.0Xa
+ *  1. add cif output nv16 format to display support
+ *  2. use parameter vehicle_dump_data to control dump data
+ * V0.0X01.0Xb add cvbs in PAL/NTSC I format to mipi csi support
+ * V0.0X01.0Xc fix format switch split issue:
+ *     such as: PAL/NTSC I format switch to 720P, cause split problem;
+ * V0.0X01.0Xd fix rk356x vehicle 1080P alloc_buffer_failed issue
+ *     nvp6324 default use 1080p for test.
+ * V0.0X01.0Xe use dummy buffer when request buffer failed case
+ *     fix flicker issue
+ * V0.0X01.0Xf set ddr scene to fix reverse sys stuck issue
+ * V0.0X02.0X0
+ *  1. add mipi csi2 hw soft reset
+ *  2. add ahd hot plug support, sample driver: vehicle_ad_nvp6324.c
+ * V0.0X02.0X1
+ *  1. support quit vehicle, switch to normal v4l2 driver
+ *  2. sample: vehicle_ad_nvp6324.c, vehicle_ad_gc2145.c
+ *  3. switch cmd: echo 88 > /dev/vehicle
+ * V0.0x02.0x2 support rk3588 csi2_dphy in kernel-5.10
+ * V0.0x02.0x3 support rk3588 csi2_dcphy
+ * V0.0x02.0x4 fix some rga3 ioctl and drm interface in kernel-5.10 for rk3588
+ * V0.0X02.0X5 support rk3588 dvp interface sensor
+ * V0.0X02.0X6 add dts phy_node to adapt different csi2_dphy or dvp sensor
+ * V0.0X02.0X7 adapt flinger driver to drm direct show interface
+ * V0.0X02.0X8 remove rockchip_ion falloc buf
+ * V0.0X02.0X9 fix RGA rotation error
+ * V0.0X02.0Xa add support MIPI CONTINUOUS CLOCK
+ * V0.0X02.0Xb add support config crtc and plane from dts
+ *  1.default crtc video_port3
+ *  2.default plane Esmart0-win0
+ * V0.0X02.0Xc remove some gpio unnecessary code
+ * V0.0X02.0Xd support samsung mipi_dcphy combo one driver
+ * V0.0X02.0Xe add GMSL to MIPI max96714 driver support
+ * V0.0X02.0Xf add nvp6188 driver support
+ * V0.0X03.0X00 update driver
+ *  1.fix some code errors
+ *  2.default palne Esmart3-win0
+ *  3.fix rotation parameters config from dts
+ *  4.add vehicle_version.h
+ * V0.0X03.0X01
+ *  1.fix bug of gpio-det if not use
+ *  2.fix some head file error
+ * V0.0X03.0X02
+ *  add rk3562 support
+ */
+
+#endif
diff --git a/drivers/video/rockchip/vtunnel/Kconfig b/drivers/video/rockchip/vtunnel/Kconfig
new file mode 100644
index 0000000000000..49ad2a592bb0d
--- /dev/null
+++ b/drivers/video/rockchip/vtunnel/Kconfig
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+
+menu "Rockchip video tunnel support"
+
+config ROCKCHIP_VIDEO_TUNNEL
+	tristate "Rockchip video tunnel device support"
+	depends on ARCH_ROCKCHIP
+	default n
+	help
+	  Rockchip videotunnel device support.
+
+endmenu
diff --git a/drivers/video/rockchip/vtunnel/Makefile b/drivers/video/rockchip/vtunnel/Makefile
new file mode 100644
index 0000000000000..fdfd79a1c0a27
--- /dev/null
+++ b/drivers/video/rockchip/vtunnel/Makefile
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: GPL-2.0
+
+obj-$(CONFIG_ROCKCHIP_VIDEO_TUNNEL) += rkvtunnel.o
diff --git a/drivers/video/rockchip/vtunnel/rkvtunnel.c b/drivers/video/rockchip/vtunnel/rkvtunnel.c
new file mode 100644
index 0000000000000..b188928e20844
--- /dev/null
+++ b/drivers/video/rockchip/vtunnel/rkvtunnel.c
@@ -0,0 +1,1537 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) Rockchip Electronics Co., Ltd.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/fdtable.h>
+#include <linux/file.h>
+#include <linux/freezer.h>
+#include <linux/miscdevice.h>
+#include <linux/of.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/dma-buf.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/jiffies.h>
+#include <linux/kfifo.h>
+#include <linux/debugfs.h>
+#include <linux/random.h>
+#include <linux/sync_file.h>
+#include <linux/sched/task.h>
+#include <linux/sched/clock.h>
+
+#include <asm-generic/bug.h>
+
+#include "rkvtunnel.h"
+
+#define DEVICE_NAME				"rkvtunnel"
+#define RKVT_MAX_NAME_LENGTH			128
+#define RKVT_POOL_SIZE				32
+#define RKVT_MAX_WAIT_MS			4
+#define RKVT_FENCE_WAIT_MS			3000
+
+#define RKVT_DBG_USER				(1U << 0)
+#define RKVT_DBG_BUFFERS			(1U << 1)
+#define RKVT_DBG_CMD				(1U << 2)
+#define RKVT_DBG_FILE				(1U << 3)
+
+#define rkvt_dbg(mask, x...)\
+	do { if (unlikely(vt_dev_dbg & mask)) pr_info(x); } while (0)
+
+enum rkvt_buf_status_e {
+	RKVT_BUF_QUEUE,
+	RKVT_BUF_DEQUEUE,
+	RKVT_BUF_ACQUIRE,
+	RKVT_BUF_RELEASE,
+	RKVT_BUF_FREE,
+	RKVT_BUF_BUTT,
+};
+
+union rkvt_ioc_arg {
+	struct rkvt_alloc_id_data alloc_data;
+	struct rkvt_ctrl_data ctrl_data;
+	struct rkvt_buf_data buffer_data;
+};
+
+struct rkvt_dev {
+	struct device *dev;
+	struct miscdevice mdev;
+	struct mutex inst_lock; /* protect inst_list and ints_idr */
+	struct idr inst_idr;
+	struct list_head list_inst; /* manage all instances */
+
+	struct mutex session_lock; /* protect sessions */
+	struct list_head list_session;
+
+	char *dev_name;
+	int inst_id_generator;
+	atomic64_t cid_generator;
+	struct dentry *debug_root;
+};
+
+struct rkvt_session {
+	struct list_head dev_link;
+	struct rkvt_dev *vt_dev;
+	struct list_head list_inst; /* manage instance in session */
+
+	enum rkvt_caller_e caller;
+	pid_t pid;
+	char name[RKVT_MAX_NAME_LENGTH];
+	char disp_name[RKVT_MAX_NAME_LENGTH];
+	int disp_serial;
+	int cid;
+	struct task_struct *task;
+	struct dentry *debug_root;
+};
+
+struct rkvt_buffer {
+	struct file *file_buf[MAX_BUF_HANDLE_FDS];
+	int fds_pro[MAX_BUF_HANDLE_FDS];
+	int fds_con[MAX_BUF_HANDLE_FDS];
+
+	struct file *ready_render_fence;
+	struct dma_fence *rendered_fence;
+	struct rkvt_session *session_pro;
+	int cid_pro;
+	struct rkvt_buf_base base;
+};
+
+struct rkvt_instance {
+	struct kref ref;
+	int id;
+	struct rkvt_dev *vt_dev;
+
+	struct mutex lock;
+	struct list_head dev_link;
+	struct list_head session_link;
+	struct rkvt_session *consumer;
+	struct rkvt_session *producer;
+	wait_queue_head_t wait_consumer;
+	wait_queue_head_t wait_producer;
+
+	struct dentry *debug_root;
+	int fcount;
+
+	DECLARE_KFIFO_PTR(fifo_to_consumer, struct rkvt_buffer*);
+	DECLARE_KFIFO_PTR(fifo_to_producer, struct rkvt_buffer*);
+
+	struct rkvt_buffer vt_buffers[RKVT_POOL_SIZE];
+
+	atomic64_t buf_id_generator;
+};
+
+static unsigned int vt_dev_dbg;
+
+module_param(vt_dev_dbg, uint, 0644);
+MODULE_PARM_DESC(vt_dev_dbg, "bit switch for vt debug information");
+
+static const char *
+rkvt_dbg_buf_status_to_string(int status)
+{
+	const char *status_str;
+
+	switch (status) {
+	case RKVT_BUF_QUEUE:
+		status_str = "queued";
+		break;
+	case RKVT_BUF_DEQUEUE:
+		status_str = "dequeued";
+		break;
+	case RKVT_BUF_ACQUIRE:
+		status_str = "acquired";
+		break;
+	case RKVT_BUF_RELEASE:
+		status_str = "released";
+		break;
+	case RKVT_BUF_FREE:
+		status_str = "free";
+		break;
+	default:
+		status_str = "unknown";
+	}
+
+	return status_str;
+}
+
+static int rkvt_dbg_instance_show(struct seq_file *s, void *unused)
+{
+	struct rkvt_instance *inst = s->private;
+	int i;
+	int size_to_con;
+	int size_to_pro;
+	int ref_count;
+
+	mutex_lock(&inst->lock);
+	size_to_con = kfifo_len(&inst->fifo_to_consumer);
+	size_to_pro = kfifo_len(&inst->fifo_to_producer);
+	ref_count = kref_read(&inst->ref);
+
+	seq_printf(s, "tunnel (%p) id=%d, ref=%d, fcount=%d\n",
+		   inst, inst->id, ref_count, inst->fcount);
+	seq_puts(s, "-----------------------------------------------\n");
+	if (inst->consumer)
+		seq_printf(s, "consumer session (%s) %p\n",
+			   inst->consumer->disp_name, inst->consumer);
+	if (inst->producer)
+		seq_printf(s, "producer session (%s) %p\n",
+			   inst->producer->disp_name, inst->producer);
+	seq_puts(s, "-----------------------------------------------\n");
+
+	seq_printf(s, "to consumer fifo size:%d\n", size_to_con);
+	seq_printf(s, "to producer fifo size:%d\n", size_to_pro);
+	seq_puts(s, "-----------------------------------------------\n");
+
+	seq_puts(s, "buffers:\n");
+
+	for (i = 0; i < RKVT_POOL_SIZE; i++) {
+		struct rkvt_buffer *buffer = &inst->vt_buffers[i];
+		int status = buffer->base.buf_status;
+
+		seq_printf(s, "    buffer produce_fd[0](%d) status(%s)\n",
+			   buffer->fds_pro[0],
+			   rkvt_dbg_buf_status_to_string(status));
+	}
+	seq_puts(s, "-----------------------------------------------\n");
+	mutex_unlock(&inst->lock);
+
+	return 0;
+}
+
+static int
+rkvt_dbg_instance_open(struct inode *inode, struct file *file)
+{
+	return single_open(file,
+			   rkvt_dbg_instance_show,
+			   inode->i_private);
+}
+
+static const struct file_operations dbg_instance_fops = {
+	.open = rkvt_dbg_instance_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int rkvt_dbg_session_show(struct seq_file *s, void *unused)
+{
+	struct rkvt_session *session = s->private;
+
+	seq_printf(s, "session(%s) %p role %s cid %d\n",
+		   session->disp_name, session,
+		   session->caller == RKVT_CALLER_PRODUCER ?
+		   "producer" : (session->caller == RKVT_CALLER_CONSUMER ?
+		   "consumer" : "invalid"), session->cid);
+	seq_puts(s, "-----------------------------------------------\n");
+
+	return 0;
+}
+
+static int rkvt_dbg_session_open(struct inode *inode, struct file *file)
+{
+	return single_open(file,
+			   rkvt_dbg_session_show,
+			   inode->i_private);
+}
+
+static const struct file_operations debug_session_fops = {
+	.open = rkvt_dbg_session_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int __rkvt_close_fd(struct files_struct *files, unsigned int fd)
+{
+	struct file *file;
+	struct fdtable *fdt;
+
+	spin_lock(&files->file_lock);
+
+	fdt = files_fdtable(files);
+	if (fd >= fdt->max_fds)
+		goto out_unlock;
+	file = fdt->fd[fd];
+	if (!file)
+		goto out_unlock;
+
+	rcu_assign_pointer(fdt->fd[fd], NULL);
+	spin_unlock(&files->file_lock);
+
+	put_unused_fd(fd);
+	return filp_close(file, files);
+
+out_unlock:
+	spin_unlock(&files->file_lock);
+	return -EBADF;
+}
+
+static int rkvt_close_fd(struct rkvt_session *session, unsigned int fd)
+{
+	int ret;
+
+	if (!session->task)
+		return -ESRCH;
+
+	ret = __rkvt_close_fd(session->task->files, fd);
+	if (unlikely(ret == -ERESTARTSYS ||
+		     ret == -ERESTARTNOINTR ||
+		     ret == -ERESTARTNOHAND ||
+		     ret == -ERESTART_RESTARTBLOCK))
+		ret = -EINTR;
+
+	return ret;
+}
+
+/* The function is responsible for fifo_to_consumer fifo operation
+ * requires external use of rkvt_instance.lock protection
+ */
+static void rkvt_inst_clear_consumer(struct rkvt_instance *inst)
+{
+	struct rkvt_buffer *buffer = NULL;
+	int i = 0;
+
+	if (!inst)
+		return;
+
+	while (kfifo_get(&inst->fifo_to_consumer, &buffer)) {
+		/* put file */
+		for (i = 0; i < buffer->base.num_fds; i++) {
+			if (buffer->file_buf[i]) {
+				fput(buffer->file_buf[i]);
+				buffer->file_buf[i] = NULL;
+			}
+			inst->fcount--;
+		}
+		if (buffer->ready_render_fence) {
+			fput(buffer->ready_render_fence);
+			buffer->ready_render_fence = NULL;
+		}
+		rkvt_dbg(RKVT_DBG_FILE,
+			 "vt [%d] instance trim file(%p) buffer(%p) ino(%08lu) fcount=%d\n",
+			 inst->id, buffer->file_buf, buffer,
+			 buffer->file_buf[i] ?
+			 file_inode(buffer->file_buf[i])->i_ino : 0,
+			 inst->fcount);
+		if (inst->producer != NULL) {
+			buffer->base.buf_status = RKVT_BUF_RELEASE;
+			kfifo_put(&inst->fifo_to_producer, buffer);
+			wake_up_interruptible(&inst->wait_producer);
+		} else {
+			buffer->base.buf_status = RKVT_BUF_FREE;
+		}
+	}
+}
+
+/* The function is responsible for fifo_to_consumer fifo operation
+ * requires external use of rkvt_instance.lock protection.
+ */
+static void rkvt_inst_clear_producer(struct rkvt_instance *inst)
+{
+	struct rkvt_buffer *buffer = NULL;
+
+	if (!inst)
+		return;
+
+	while (kfifo_get(&inst->fifo_to_producer, &buffer)) {
+		if (buffer->rendered_fence) {
+			dma_fence_put(buffer->rendered_fence);
+			buffer->rendered_fence = NULL;
+		}
+		buffer->base.buf_status = RKVT_BUF_FREE;
+	}
+}
+
+static void rkvt_inst_destroy(struct kref *kref)
+{
+	struct rkvt_instance *inst =
+		container_of(kref, struct rkvt_instance, ref);
+	struct rkvt_dev *vt_dev = inst->vt_dev;
+
+	list_del_init(&inst->dev_link);
+	idr_remove(&vt_dev->inst_idr, inst->id);
+
+	rkvt_dbg(RKVT_DBG_USER, "vt [%d] destroy\n", inst->id);
+
+	mutex_lock(&inst->lock);
+	rkvt_inst_clear_consumer(inst);
+	rkvt_inst_clear_producer(inst);
+	kfifo_free(&inst->fifo_to_consumer);
+	kfifo_free(&inst->fifo_to_producer);
+	mutex_unlock(&inst->lock);
+
+	debugfs_remove_recursive(inst->debug_root);
+
+	devm_kfree(vt_dev->dev, inst);
+}
+
+static struct rkvt_instance *rkvt_inst_create(struct rkvt_dev *vt_dev)
+{
+	struct rkvt_instance *inst;
+	int status;
+	int i;
+
+	inst = devm_kzalloc(vt_dev->dev, sizeof(*inst), GFP_KERNEL);
+	if (!inst)
+		return ERR_PTR(-ENOMEM);
+
+	inst->vt_dev = vt_dev;
+	mutex_init(&inst->lock);
+	INIT_LIST_HEAD(&inst->dev_link);
+	INIT_LIST_HEAD(&inst->session_link);
+	kref_init(&inst->ref);
+
+	status = kfifo_alloc(&inst->fifo_to_consumer,
+			     RKVT_POOL_SIZE, GFP_KERNEL);
+	if (status)
+		goto setup_fail;
+
+	status = kfifo_alloc(&inst->fifo_to_producer,
+			     RKVT_POOL_SIZE, GFP_KERNEL);
+	if (status)
+		goto fifo_alloc_fail;
+
+	init_waitqueue_head(&inst->wait_producer);
+	init_waitqueue_head(&inst->wait_consumer);
+
+	for (i = 0; i < RKVT_POOL_SIZE; i++)
+		inst->vt_buffers[i].base.buf_status = RKVT_BUF_FREE;
+
+	/* insert it to dev instances list */
+	mutex_lock(&vt_dev->inst_lock);
+	list_add_tail(&inst->dev_link, &vt_dev->list_inst);
+	mutex_unlock(&vt_dev->inst_lock);
+
+	return inst;
+fifo_alloc_fail:
+	kfifo_free(&inst->fifo_to_consumer);
+setup_fail:
+	devm_kfree(vt_dev->dev, inst);
+	return ERR_PTR(status);
+}
+
+/* The function protected by rkvt_dev.session_lock by caller */
+static int
+rkvt_get_session_serial(const struct list_head *sessions,
+			const unsigned char *name)
+{
+	int serial = -1;
+	struct rkvt_session *session, *n;
+
+	list_for_each_entry_safe(session, n, sessions, dev_link) {
+		if (strcmp(session->name, name))
+			continue;
+		serial = max(serial, session->disp_serial);
+	}
+
+	return serial + 1;
+}
+
+/* The function protected by rkvt_instance.lock by caller */
+static void
+rkvt_session_trim_locked(struct rkvt_session *session, struct rkvt_instance *inst)
+{
+	if (!session || !inst)
+		return;
+
+	if (inst->producer && inst->producer == session) {
+		rkvt_inst_clear_producer(inst);
+		inst->producer = NULL;
+	}
+
+	if (inst->consumer && inst->consumer == session) {
+		rkvt_inst_clear_consumer(inst);
+		inst->consumer = NULL;
+	}
+}
+
+static int rkvt_inst_trim(struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst, *n;
+	int i;
+
+	mutex_lock(&vt_dev->inst_lock);
+	list_for_each_entry_safe(inst, n, &vt_dev->list_inst, dev_link) {
+		mutex_lock(&inst->lock);
+		rkvt_session_trim_locked(session, inst);
+
+		if (!inst->consumer && !inst->producer) {
+			rkvt_inst_clear_producer(inst);
+			rkvt_inst_clear_consumer(inst);
+
+			for (i = 0; i < RKVT_POOL_SIZE; i++)
+				inst->vt_buffers[i].base.buf_status = RKVT_BUF_FREE;
+		}
+		mutex_unlock(&inst->lock);
+	}
+	mutex_unlock(&vt_dev->inst_lock);
+
+	return 0;
+}
+
+static struct rkvt_session *
+rkvt_session_create(struct rkvt_dev *vt_dev, const char *name)
+{
+	struct rkvt_session *session;
+	struct task_struct *task = NULL;
+
+	if (!name) {
+		dev_err(vt_dev->dev, "%s: Name can not be null\n", __func__);
+		return ERR_PTR(-EINVAL);
+	}
+
+	session = devm_kzalloc(vt_dev->dev, sizeof(*session), GFP_KERNEL);
+	if (!session)
+		return ERR_PTR(-ENOMEM);
+
+	get_task_struct(current->group_leader);
+	task_lock(current->group_leader);
+	session->pid = task_pid_nr(current->group_leader);
+
+	if (current->group_leader->flags & PF_KTHREAD) {
+		put_task_struct(current->group_leader);
+		task = NULL;
+	} else {
+		task = current->group_leader;
+	}
+
+	task_unlock(current->group_leader);
+
+	session->vt_dev = vt_dev;
+	session->task = task;
+	session->caller = RKVT_CALLER_BUTT;
+	INIT_LIST_HEAD(&session->dev_link);
+	INIT_LIST_HEAD(&session->list_inst);
+	snprintf(session->name, RKVT_MAX_NAME_LENGTH, "%s", name);
+
+	mutex_lock(&vt_dev->session_lock);
+	session->disp_serial = rkvt_get_session_serial(&vt_dev->list_session, name);
+	snprintf(session->disp_name, RKVT_MAX_NAME_LENGTH, "%s-%d",
+			 name, session->disp_serial);
+
+	list_add_tail(&session->dev_link, &vt_dev->list_session);
+
+	/* add debug fs */
+	session->debug_root = debugfs_create_file(session->disp_name,
+						  0664,
+						  vt_dev->debug_root,
+						  session,
+						  &debug_session_fops);
+
+	mutex_unlock(&vt_dev->session_lock);
+
+	rkvt_dbg(RKVT_DBG_USER, "vt session %s create\n", session->disp_name);
+
+	return session;
+}
+
+static void rkvt_session_destroy(struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+
+	rkvt_dbg(RKVT_DBG_USER, "vt session %s destroy\n", session->disp_name);
+
+	mutex_lock(&vt_dev->inst_lock);
+	while ((inst = list_first_entry_or_null(&session->list_inst,
+						struct rkvt_instance, session_link))) {
+		list_del_init(&inst->session_link);
+		kref_put(&inst->ref, rkvt_inst_destroy);
+	}
+	mutex_unlock(&vt_dev->inst_lock);
+
+	mutex_lock(&vt_dev->session_lock);
+	if (session->task)
+		put_task_struct(session->task);
+	list_del_init(&session->dev_link);
+	debugfs_remove_recursive(session->debug_root);
+	mutex_unlock(&vt_dev->session_lock);
+
+	rkvt_inst_trim(session);
+	devm_kfree(vt_dev->dev, session);
+}
+
+static int rkvt_open(struct inode *inode, struct file *filep)
+{
+	struct miscdevice *miscdev = filep->private_data;
+	struct rkvt_dev *vt_dev = container_of(miscdev, struct rkvt_dev, mdev);
+	struct rkvt_session *session;
+	char debug_name[64];
+
+	snprintf(debug_name, sizeof(debug_name), "%u", task_pid_nr(current->group_leader));
+	session = rkvt_session_create(vt_dev, debug_name);
+	if (IS_ERR(session))
+		return PTR_ERR(session);
+
+	filep->private_data = session;
+
+	return 0;
+}
+
+static int rkvt_release(struct inode *inode, struct file *filep)
+{
+	struct rkvt_session *session = filep->private_data;
+
+	rkvt_session_destroy(session);
+	filep->private_data = NULL;
+
+	return 0;
+}
+
+static int rkvt_get_connected_id(struct rkvt_dev *vt_dev)
+{
+	return atomic64_inc_return(&vt_dev->cid_generator);
+}
+
+static struct rkvt_instance *
+rkvt_inst_get_by_tid(struct rkvt_dev *vt_dev, int id)
+{
+	struct rkvt_instance *inst;
+
+	mutex_lock(&vt_dev->inst_lock);
+	inst = idr_find(&vt_dev->inst_idr, id);
+	if (!inst) {
+		mutex_unlock(&vt_dev->inst_lock);
+		dev_err(vt_dev->dev, "find rkvt [%d] by device idr err, instance is null\n", id);
+		return NULL;
+	}
+	kref_get(&inst->ref);
+	mutex_unlock(&vt_dev->inst_lock);
+
+	return inst;
+}
+
+static void rkvt_inst_put(struct rkvt_instance *inst)
+{
+	struct rkvt_dev *vt_dev;
+
+	if (!inst)
+		return;
+
+	vt_dev = inst->vt_dev;
+
+	mutex_lock(&vt_dev->inst_lock);
+	kref_put(&inst->ref, rkvt_inst_destroy);
+	mutex_unlock(&vt_dev->inst_lock);
+}
+
+static int
+rkvt_connect_proc(struct rkvt_ctrl_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst;
+	int ret = 0;
+
+	// ref get not put in function end, because connect need hold 1 refs.
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+
+	mutex_lock(&inst->lock);
+	if (data->caller == RKVT_CALLER_PRODUCER) {
+		if (inst->producer && inst->producer != session) {
+			dev_err(vt_dev->dev, "Connect to rkvt [%d] err, already has producer\n",
+					data->vt_id);
+			ret = -EINVAL;
+			goto connect_fail;
+		}
+		inst->producer = session;
+	} else if (data->caller == RKVT_CALLER_CONSUMER) {
+		if (inst->consumer && inst->consumer != session) {
+			dev_err(vt_dev->dev, "Connect to rkvt [%d] err, already has consumer\n",
+					data->vt_id);
+			ret = -EINVAL;
+			goto connect_fail;
+		}
+		inst->consumer = session;
+	}
+	mutex_unlock(&inst->lock);
+	session->cid = rkvt_get_connected_id(vt_dev);
+	session->caller = data->caller;
+
+	rkvt_dbg(RKVT_DBG_USER, "rkvt [%d] %s-%d connect, instance ref %d\n",
+		 inst->id,
+		 data->caller == RKVT_CALLER_PRODUCER ? "producer" : "consumer",
+		 session->pid,
+		 kref_read(&inst->ref));
+
+	return 0;
+
+connect_fail:
+	mutex_unlock(&inst->lock);
+	// ref put for rkvt_instance_get_by_tid
+	rkvt_inst_put(inst);
+
+	return ret;
+}
+
+static int
+rkvt_disconnect_proc(struct rkvt_ctrl_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (session->caller != data->caller)
+		goto session_invail;
+
+	mutex_lock(&inst->lock);
+	if (data->caller == RKVT_CALLER_PRODUCER) {
+		if (!inst->producer)
+			goto disconnect_fail;
+		if (inst->producer != session)
+			goto disconnect_fail;
+
+		rkvt_session_trim_locked(session, inst);
+		inst->producer = NULL;
+		wake_up_interruptible(&inst->wait_producer);
+	} else if (data->caller == RKVT_CALLER_CONSUMER) {
+		if (!inst->consumer)
+			goto disconnect_fail;
+		if (inst->consumer != session)
+			goto disconnect_fail;
+
+		rkvt_session_trim_locked(session, inst);
+		inst->consumer = NULL;
+		wake_up_interruptible(&inst->wait_consumer);
+	}
+	mutex_unlock(&inst->lock);
+
+	rkvt_dbg(RKVT_DBG_USER, "rkvt [%d] %s-%d disconnect, instance ref %d\n",
+		 inst->id,
+		 data->caller == RKVT_CALLER_PRODUCER ? "producer" : "consumer",
+		 session->pid,
+		 kref_read(&inst->ref));
+	// ref put for rkvt_instance_get_by_tid
+	rkvt_inst_put(inst);
+	// ref put for connect proc
+	rkvt_inst_put(inst);
+	session->cid = -1;
+
+	return 0;
+
+disconnect_fail:
+	mutex_unlock(&inst->lock);
+session_invail:
+	// ref put for rkvt_instance_get_by_tid
+	rkvt_inst_put(inst);
+
+	return -EINVAL;
+}
+
+static int
+rkvt_reset_proc(struct rkvt_ctrl_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst;
+	long long read_buf_id;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+
+	mutex_lock(&inst->lock);
+	rkvt_inst_clear_consumer(inst);
+	rkvt_inst_clear_producer(inst);
+	read_buf_id = atomic64_read(&inst->buf_id_generator);
+	read_buf_id += 0x100;
+	read_buf_id &= ~0xff;
+	atomic64_set(&inst->buf_id_generator, read_buf_id);
+	mutex_unlock(&inst->lock);
+
+	rkvt_inst_put(inst);
+
+	return 0;
+}
+
+static int
+rkvt_has_consumer_proc(struct rkvt_ctrl_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+
+	mutex_lock(&inst->lock);
+	data->ctrl_data = inst->consumer != NULL ? 1 : 0;
+	mutex_unlock(&inst->lock);
+
+	rkvt_inst_put(inst);
+
+	return 0;
+}
+
+static int
+rkvt_ctrl_proc(struct rkvt_ctrl_data *data, struct rkvt_session *session)
+{
+	int id = data->vt_id;
+	int ret = 0;
+
+	if (id < 0)
+		return -EINVAL;
+	if (data->caller == RKVT_CALLER_BUTT)
+		return -EINVAL;
+
+	switch (data->ctrl_cmd) {
+	case RKVT_CTRL_CONNECT: {
+		ret = rkvt_connect_proc(data, session);
+		break;
+	}
+	case RKVT_CTRL_DISCONNECT: {
+		ret = rkvt_disconnect_proc(data, session);
+		break;
+	}
+	case RKVT_CTRL_RESET: {
+		ret = rkvt_reset_proc(data, session);
+		break;
+	}
+	case RKVT_CTRL_HAS_CONSUMER: {
+		ret = rkvt_has_consumer_proc(data, session);
+		break;
+	}
+	default:
+		pr_err("unknown rkvt cmd:%d\n", data->ctrl_cmd);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+static struct
+rkvt_buffer *rkvt_buf_get(struct rkvt_instance *inst, int key)
+{
+	struct rkvt_buffer *buffer = NULL;
+	int i;
+
+	mutex_lock(&inst->lock);
+	for (i = 0; i < RKVT_POOL_SIZE; i++) {
+		buffer = &inst->vt_buffers[i];
+
+		if (buffer->base.buf_status == RKVT_BUF_ACQUIRE &&
+		    buffer->fds_con[0] == key)
+			break;
+	}
+	mutex_unlock(&inst->lock);
+
+	return buffer;
+}
+
+static int
+rkvt_has_buf(struct rkvt_instance *inst, enum rkvt_caller_e caller)
+{
+	int ret = 0;
+
+	if (caller == RKVT_CALLER_PRODUCER)
+		ret = !kfifo_is_empty(&inst->fifo_to_producer);
+	else
+		ret = !kfifo_is_empty(&inst->fifo_to_consumer);
+
+	return ret;
+}
+
+static int
+rkvt_query_buf_and_wait(struct rkvt_instance *inst,
+			enum rkvt_caller_e caller,
+			int timeout_ms)
+{
+	int ret;
+	wait_queue_head_t *wait_queue;
+
+	if (caller == RKVT_CALLER_PRODUCER)
+		wait_queue = &inst->wait_producer;
+	else
+		wait_queue = &inst->wait_consumer;
+	if (caller == RKVT_CALLER_PRODUCER &&
+	    !kfifo_is_empty(&inst->fifo_to_producer))
+		return 0;
+	if (caller == RKVT_CALLER_CONSUMER &&
+	    !kfifo_is_empty(&inst->fifo_to_consumer))
+		return 0;
+
+	if (timeout_ms < 0)
+		wait_event_interruptible(*wait_queue,
+					 rkvt_has_buf(inst, caller));
+	else if (timeout_ms > 0) {
+		ret = wait_event_interruptible_timeout(*wait_queue,
+							rkvt_has_buf(inst, caller),
+							msecs_to_jiffies(timeout_ms));
+		/* timeout */
+		if (ret == 0)
+			return -EAGAIN;
+	} else
+		return -EAGAIN;
+
+	if (caller == RKVT_CALLER_PRODUCER &&
+	    kfifo_is_empty(&inst->fifo_to_producer))
+		return -EAGAIN;
+	if (caller == RKVT_CALLER_CONSUMER &&
+	    kfifo_is_empty(&inst->fifo_to_consumer))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static struct rkvt_buffer *rkvt_get_free_buf(struct rkvt_instance *inst)
+{
+	struct rkvt_buffer *buffer = NULL;
+	int i, status;
+
+	mutex_lock(&inst->lock);
+	for (i = 0; i < RKVT_POOL_SIZE; i++) {
+		status = inst->vt_buffers[i].base.buf_status;
+		if (status == RKVT_BUF_FREE || status == RKVT_BUF_DEQUEUE) {
+			buffer = &inst->vt_buffers[i];
+			memset(buffer->file_buf, 0, sizeof(buffer->file_buf));
+			buffer->rendered_fence = NULL;
+			break;
+		}
+	}
+	mutex_unlock(&inst->lock);
+
+	return buffer;
+}
+
+static int
+rkvt_queue_buf(struct rkvt_buf_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+	struct rkvt_buf_base *base = NULL;
+	struct rkvt_buffer *buffer = NULL;
+	int i;
+	int ret = 0;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (!inst->producer || inst->producer != session) {
+		ret = -EINVAL;
+		goto queue_fail;
+	}
+	if ((data->base.num_fds > MAX_BUF_HANDLE_FDS) ||
+		(data->base.num_ints > MAX_BUF_HANDLE_INTS)) {
+		ret = -EINVAL;
+		goto queue_fail;
+	}
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTQB [%d] start\n", inst->id);
+
+	base = &data->base;
+	buffer = rkvt_get_free_buf(inst);
+	if (!buffer) {
+		dev_err(vt_dev->dev, "VTQB [%d] no unused buffer.\n", inst->id);
+		ret = -EINVAL;
+		goto queue_fail;
+	}
+	for (i = 0; i < base->num_fds; i++) {
+		buffer->fds_con[i] = -1;
+		buffer->fds_pro[i] = base->fds[i];
+		buffer->file_buf[i] = fget(base->fds[i]);
+
+		if (!buffer->file_buf[i]) {
+			ret = -EBADF;
+			goto buf_fget_fail;
+		}
+
+		inst->fcount++;
+		rkvt_dbg(RKVT_DBG_FILE,
+			"VTQB [%d] fget file(%p) buf(%p) buf session(%p) ino(%08lu) fcount=%d\n",
+			inst->id, buffer->file_buf[i], buffer, buffer->session_pro,
+			buffer->file_buf[i] ? file_inode(buffer->file_buf[i])->i_ino : 0,
+			inst->fcount);
+	}
+
+	if (base->fence_fd >= 0)
+		buffer->ready_render_fence = fget(base->fence_fd);
+
+	// buffer id is empty, generate a new id
+	if (base->buffer_id == 0)
+		base->buffer_id = atomic64_inc_return(&inst->buf_id_generator);
+	buffer->base = *base;
+	buffer->base.buf_status = RKVT_BUF_QUEUE;
+	buffer->session_pro = session;
+	buffer->cid_pro = session->cid;
+
+	mutex_lock(&inst->lock);
+	if (inst->consumer) {
+		kfifo_put(&inst->fifo_to_consumer, buffer);
+	} else {
+		for (i = 0; i < buffer->base.num_fds; i++) {
+			if (buffer->file_buf[i]) {
+				fput(buffer->file_buf[i]);
+				buffer->file_buf[i] = NULL;
+			}
+			inst->fcount--;
+		}
+		if (buffer->ready_render_fence) {
+			fput(buffer->ready_render_fence);
+			buffer->ready_render_fence = NULL;
+		}
+		buffer->base.buf_status = RKVT_BUF_RELEASE;
+		kfifo_put(&inst->fifo_to_producer, buffer);
+	}
+	mutex_unlock(&inst->lock);
+
+	if (inst->consumer)
+		wake_up_interruptible(&inst->wait_consumer);
+	else if (inst->producer)
+		wake_up_interruptible(&inst->wait_producer);
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTQB [%d] pfd[0]:%d end\n", inst->id, buffer->fds_pro[0]);
+
+queue_fail:
+	rkvt_inst_put(inst);
+
+	return ret;
+buf_fget_fail:
+	for (i = 0; i < base->num_fds; i++) {
+		if (buffer->file_buf[i]) {
+			fput(buffer->file_buf[i]);
+			buffer->file_buf[i] = NULL;
+			inst->fcount--;
+		}
+	}
+	rkvt_inst_put(inst);
+
+	return ret;
+}
+
+static int
+rkvt_deque_buf(struct rkvt_buf_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+	struct rkvt_buffer *buffer = NULL;
+	int ret = 0;
+	unsigned long long cur_time, wait_time;
+	int i;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (!inst->producer || inst->producer != session) {
+		ret = -EINVAL;
+		goto deque_fail;
+	}
+
+	/* empty need wait */
+	ret = rkvt_query_buf_and_wait(inst,
+				      RKVT_CALLER_PRODUCER,
+				      data->timeout_ms);
+	if (ret)
+		goto deque_fail;
+
+	mutex_lock(&inst->lock);
+	ret = kfifo_get(&inst->fifo_to_producer, &buffer);
+	if (!ret || !buffer) {
+		dev_err(vt_dev->dev, "VTDB [%d] got null buffer ret(%d)\n", inst->id, ret);
+		mutex_unlock(&inst->lock);
+		ret = -EAGAIN;
+		goto deque_fail;
+	}
+	mutex_unlock(&inst->lock);
+
+	/* it's previous connect buffer */
+	if (buffer->cid_pro != session->cid) {
+		if (buffer->rendered_fence) {
+			dma_fence_put(buffer->rendered_fence);
+			buffer->rendered_fence = NULL;
+		}
+
+		ret = -EAGAIN;
+		goto deque_fail;
+	}
+
+	if (buffer->rendered_fence) {
+		cur_time = sched_clock();
+		ret = dma_fence_wait_timeout(buffer->rendered_fence, false,
+					     msecs_to_jiffies(RKVT_FENCE_WAIT_MS));
+		wait_time = sched_clock() - cur_time;
+		rkvt_dbg(RKVT_DBG_BUFFERS,
+			 "VTDB [%d] pfd[0]:%d rendered fence:%p fence_wait time %llu\n",
+			 inst->id, buffer->fds_pro[0], buffer->rendered_fence, wait_time);
+
+		if (ret < 0)
+			dev_err(vt_dev->dev, "VTDB [%d] wait fence timeout\n", inst->id);
+
+		dma_fence_put(buffer->rendered_fence);
+		buffer->rendered_fence = NULL;
+	}
+	for (i = 0; i < buffer->base.num_fds; i++)
+		rkvt_dbg(RKVT_DBG_FILE,
+			"VTDB [%d] fget file(%p) buf(%p) buf session(%p) ino(%08lu) fcount=%d\n",
+			inst->id, buffer->file_buf[i],
+			buffer, buffer->session_pro,
+			buffer->file_buf[i] ? file_inode(buffer->file_buf[i])->i_ino : 0,
+			inst->fcount);
+
+	buffer->base.vt_id = inst->id;
+	/* return the buffer */
+	data->base = buffer->base;
+	buffer->base.buf_status = RKVT_BUF_DEQUEUE;
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTDB [%d] end pfd[0]:%d\n", inst->id, buffer->fds_pro[0]);
+
+deque_fail:
+	rkvt_inst_put(inst);
+
+	return ret;
+}
+
+static int
+rkvt_acquire_buf(struct rkvt_buf_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+	struct rkvt_buffer *buffer = NULL;
+	int fd, ret = -1;
+	int i;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (!inst->consumer || inst->consumer != session) {
+		ret = -EINVAL;
+		goto acquire_fail;
+	}
+	if ((data->base.num_fds > MAX_BUF_HANDLE_FDS) ||
+		(data->base.num_ints > MAX_BUF_HANDLE_INTS)) {
+		ret = -EINVAL;
+		goto acquire_fail;
+	}
+
+	/* empty need wait */
+	ret = rkvt_query_buf_and_wait(inst,
+				      RKVT_CALLER_CONSUMER,
+				      data->timeout_ms);
+	if (ret)
+		goto acquire_fail;
+
+	mutex_lock(&inst->lock);
+	ret = kfifo_get(&inst->fifo_to_consumer, &buffer);
+	mutex_unlock(&inst->lock);
+	if (!ret || !buffer) {
+		dev_err(vt_dev->dev, "VTAB [%d] got null buffer\n", inst->id);
+		ret = -EAGAIN;
+		goto acquire_fail;
+	}
+
+	/* get the fd in consumer */
+	for (i = 0; i < buffer->base.num_fds; i++) {
+		if (buffer->fds_con[i] <= 0) {
+			fd = get_unused_fd_flags(O_CLOEXEC);
+			if (fd < 0)
+				goto no_memory;
+
+			fd_install(fd, buffer->file_buf[i]);
+			buffer->fds_con[i] = fd;
+			buffer->base.fds[i] = fd;
+		}
+	}
+	if (buffer->ready_render_fence) {
+		fd = get_unused_fd_flags(O_CLOEXEC);
+		if (fd < 0)
+			goto no_memory;
+		fd_install(fd, buffer->ready_render_fence);
+		buffer->base.fence_fd = fd;
+		buffer->ready_render_fence = NULL;
+	} else {
+		buffer->base.fence_fd = -1;
+	}
+	buffer->base.vt_id = inst->id;
+	data->base = buffer->base;
+	buffer->base.buf_status = RKVT_BUF_ACQUIRE;
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTAB [%d] pfd[0](%d) buf(%p) buf session(%p)\n",
+			inst->id, buffer->fds_pro[0], buffer, buffer->session_pro);
+
+	rkvt_inst_put(inst);
+
+	return 0;
+
+no_memory:
+	pr_info("VTAB [%d] install fd error\n", inst->id);
+	mutex_lock(&inst->lock);
+	for (i = 0; i < buffer->base.num_fds; i++) {
+		rkvt_dbg(RKVT_DBG_FILE,
+				"VTAB [%d] install fd error file(%p) buf(%p) ino(%08lu) fcount=%d\n",
+				inst->id, buffer->file_buf[i], buffer,
+				file_inode(buffer->file_buf[i])->i_ino, inst->fcount);
+		if (buffer->file_buf[i]) {
+			fput(buffer->file_buf[i]);
+			buffer->file_buf[i] = NULL;
+			inst->fcount--;
+		}
+	}
+	if (buffer->ready_render_fence) {
+		fput(buffer->ready_render_fence);
+		buffer->ready_render_fence = NULL;
+	}
+	buffer->base.buf_status = RKVT_BUF_RELEASE;
+
+	kfifo_put(&inst->fifo_to_producer, buffer);
+	mutex_unlock(&inst->lock);
+	if (inst->producer)
+		wake_up_interruptible(&inst->wait_producer);
+	ret = -ENOMEM;
+
+acquire_fail:
+	rkvt_inst_put(inst);
+
+	return ret;
+}
+
+static int
+rkvt_release_buf(struct rkvt_buf_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+	struct rkvt_buf_base *buf_base = NULL;
+	struct rkvt_buffer *buffer = NULL;
+	int i;
+	int ret = 0;
+	long long read_buf_id;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (!inst->consumer || inst->consumer != session) {
+		ret = -EINVAL;
+		goto release_fail;
+	}
+
+	buf_base = &data->base;
+	buffer = rkvt_buf_get(inst, buf_base->fds[0]);
+	if (!buffer) {
+		ret = -EINVAL;
+		goto release_fail;
+	}
+
+	if (buf_base->fence_fd >= 0)
+		buffer->rendered_fence = sync_file_get_fence(buf_base->fence_fd);
+
+	if (!buffer->rendered_fence)
+		rkvt_dbg(RKVT_DBG_BUFFERS, "VTRB [%d] rendered fence file is null\n", inst->id);
+
+	/* close the fds in consumer side */
+	for (i = 0; i < buf_base->num_fds; i++) {
+		rkvt_dbg(RKVT_DBG_FILE,
+			"VTRB [%d] file(%p) buf(%p) buf session(%p) ino(%08lu) fcount=%d\n",
+			inst->id, buffer->file_buf[i], buffer, buffer->session_pro,
+			buffer->file_buf[i] ? file_inode(buffer->file_buf[i])->i_ino : 0,
+			inst->fcount);
+		rkvt_close_fd(session, buffer->fds_con[i]);
+		inst->fcount--;
+		buffer->base.fds[i] = buffer->fds_pro[i];
+	}
+	if (buffer->ready_render_fence) {
+		fput(buffer->ready_render_fence);
+		buffer->ready_render_fence = NULL;
+	}
+
+	buffer->base.crop = buf_base->crop;
+	buffer->base.buf_status = RKVT_BUF_RELEASE;
+
+	mutex_lock(&inst->lock);
+	read_buf_id = atomic64_read(&inst->buf_id_generator);
+	/* if producer has disconnect */
+	if (!inst->producer) {
+		rkvt_dbg(RKVT_DBG_BUFFERS, "VTRB [%d], buffer no producer\n", inst->id);
+		buffer->base.buf_status = RKVT_BUF_FREE;
+	} else if ((buffer->base.buffer_id >> 8) != (read_buf_id >> 8)) {
+		dev_err(vt_dev->dev, "VTRB [%d] generation is different. cur(%lld) VS exp(%lld)\n",
+			inst->id, buffer->base.buffer_id >> 8, read_buf_id >> 8);
+		buffer->base.buf_status = RKVT_BUF_FREE;
+	} else {
+		if (buffer->session_pro &&
+		    buffer->session_pro != inst->producer) {
+			rkvt_dbg(RKVT_DBG_BUFFERS,
+				"VTRB [%d] producer not valid, producer(%p), buf session(%p)\n",
+				inst->id, inst->producer, buffer->session_pro);
+			buffer->base.buf_status = RKVT_BUF_FREE;
+		}
+
+		kfifo_put(&inst->fifo_to_producer, buffer);
+	}
+	mutex_unlock(&inst->lock);
+
+	if (inst->producer)
+		wake_up_interruptible(&inst->wait_producer);
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTRB [%d] pfd[0]:%d end\n", inst->id, buffer->fds_pro[0]);
+
+release_fail:
+	rkvt_inst_put(inst);
+
+	return ret;
+}
+
+static int
+rkvt_cancel_buf(struct rkvt_buf_data *data, struct rkvt_session *session)
+{
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+	struct rkvt_buf_base *buf_base = NULL;
+	struct rkvt_buffer *buffer = NULL;
+	int i;
+
+	inst = rkvt_inst_get_by_tid(vt_dev, data->vt_id);
+	if (!inst)
+		return -EINVAL;
+	if (!inst->producer || inst->producer != session) {
+		rkvt_inst_put(inst);
+		return -EINVAL;
+	}
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTCB [%d] start\n", inst->id);
+
+	buf_base = &data->base;
+	buffer = rkvt_get_free_buf(inst);
+	if (!buffer) {
+		dev_err(vt_dev->dev, "VTCB [%d] no unused buffer.\n", inst->id);
+		rkvt_inst_put(inst);
+		return -EINVAL;
+	}
+	for (i = 0; i < buf_base->num_fds; i++) {
+		buffer->fds_con[i] = -1;
+		buffer->fds_pro[i] = buf_base->fds[i];
+		rkvt_dbg(RKVT_DBG_FILE,
+			"VTCB [%d] fget file(%p) buf(%p) buf session(%p) fcount=%d\n",
+			inst->id, buffer->file_buf[i], buffer,
+			buffer->session_pro, inst->fcount);
+	}
+	// buffer id is empty, generate a new id
+	if (buf_base->buffer_id == 0)
+		buf_base->buffer_id = atomic64_inc_return(&inst->buf_id_generator);
+	buffer->base = *buf_base;
+	buffer->base.buf_status = RKVT_BUF_RELEASE;
+	buffer->session_pro = session;
+	buffer->cid_pro = session->cid;
+
+	mutex_lock(&inst->lock);
+	kfifo_put(&inst->fifo_to_producer, buffer);
+	mutex_unlock(&inst->lock);
+
+	if (inst->producer)
+		wake_up_interruptible(&inst->wait_producer);
+
+	rkvt_dbg(RKVT_DBG_BUFFERS, "VTCB [%d] pfd[0]:%d end\n", inst->id, buffer->fds_pro[0]);
+	rkvt_inst_put(inst);
+
+	return 0;
+}
+
+static unsigned int rkvt_ioctl_dir(unsigned int cmd)
+{
+	switch (cmd) {
+	case RKVT_IOC_ALLOC_ID:
+	case RKVT_IOC_DEQUE_BUF:
+	case RKVT_IOC_ACQUIRE_BUF:
+	case RKVT_IOC_CTRL:
+		return _IOC_READ;
+	case RKVT_IOC_QUEUE_BUF:
+	case RKVT_IOC_RELEASE_BUF:
+	case RKVT_IOC_CANCEL_BUF:
+	case RKVT_IOC_FREE_ID:
+		return _IOC_WRITE;
+	default:
+		return _IOC_DIR(cmd);
+	}
+}
+
+static long rkvt_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
+{
+	int ret = 0;
+	union rkvt_ioc_arg data;
+	struct rkvt_session *session = filep->private_data;
+	unsigned int dir = rkvt_ioctl_dir(cmd);
+	struct rkvt_dev *vt_dev = session->vt_dev;
+	struct rkvt_instance *inst = NULL;
+
+	rkvt_dbg(RKVT_DBG_CMD, "rkvt ioctl cmd 0x%x size %d in\n", cmd, _IOC_SIZE(cmd));
+
+	if (_IOC_SIZE(cmd) > sizeof(data))
+		return -EINVAL;
+
+	if (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))
+		return -EFAULT;
+
+	switch (cmd) {
+	case RKVT_IOC_ALLOC_ID: {
+		char name[64];
+
+		inst = rkvt_inst_create(session->vt_dev);
+		if (IS_ERR(inst))
+			return PTR_ERR(inst);
+
+		mutex_lock(&vt_dev->inst_lock);
+		++vt_dev->inst_id_generator;
+		ret = idr_alloc(&vt_dev->inst_idr, inst,
+				vt_dev->inst_id_generator, 0, GFP_KERNEL);
+		mutex_unlock(&vt_dev->inst_lock);
+		if (ret < 0) {
+			rkvt_inst_put(inst);
+			return ret;
+		}
+
+		inst->id = ret;
+		snprintf(name, sizeof(name), "instance-%d", inst->id);
+		inst->debug_root =
+			debugfs_create_file(name, 0664, vt_dev->debug_root,
+					    inst, &dbg_instance_fops);
+
+		mutex_lock(&vt_dev->inst_lock);
+		list_add_tail(&inst->session_link, &session->list_inst);
+		mutex_unlock(&vt_dev->inst_lock);
+
+		data.alloc_data.vt_id = inst->id;
+		rkvt_dbg(RKVT_DBG_USER, "rkvt alloc instance [%d], ref %d\n",
+			 inst->id, kref_read(&inst->ref));
+		break;
+	}
+	case RKVT_IOC_FREE_ID: {
+		inst = rkvt_inst_get_by_tid(vt_dev, data.alloc_data.vt_id);
+		/* to do free id operation check */
+		if (!inst) {
+			dev_err(vt_dev->dev, "destroy unknown videotunnel instance:%d\n",
+			       data.alloc_data.vt_id);
+			ret = -EINVAL;
+		} else {
+			rkvt_dbg(RKVT_DBG_USER, "rkvt free instance [%d], ref %d\n",
+				 inst->id, kref_read(&inst->ref));
+
+			mutex_lock(&vt_dev->inst_lock);
+			list_del_init(&inst->session_link);
+			mutex_unlock(&vt_dev->inst_lock);
+			// ref put for rkvt_instance_get_by_tid
+			rkvt_inst_put(inst);
+			// ref put for kref_init in rkvt_inst_create
+			rkvt_inst_put(inst);
+		}
+		break;
+	}
+	case RKVT_IOC_CTRL:
+		ret = rkvt_ctrl_proc(&data.ctrl_data, session);
+		break;
+	case RKVT_IOC_QUEUE_BUF:
+		ret = rkvt_queue_buf(&data.buffer_data, session);
+		break;
+	case RKVT_IOC_DEQUE_BUF:
+		ret = rkvt_deque_buf(&data.buffer_data, session);
+		break;
+	case RKVT_IOC_RELEASE_BUF:
+		ret = rkvt_release_buf(&data.buffer_data, session);
+		break;
+	case RKVT_IOC_ACQUIRE_BUF:
+		ret = rkvt_acquire_buf(&data.buffer_data, session);
+		break;
+	case RKVT_IOC_CANCEL_BUF:
+		ret = rkvt_cancel_buf(&data.buffer_data, session);
+		break;
+	default:
+		dev_err(vt_dev->dev, "%s: cmd 0x%x not found.\n", __func__, cmd);
+		return -ENOTTY;
+	}
+
+	if (dir & _IOC_READ) {
+		if (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd)))
+			return -EFAULT;
+	}
+
+	return ret;
+}
+
+static const struct file_operations vt_fops = {
+	.owner = THIS_MODULE,
+	.open = rkvt_open,
+	.release = rkvt_release,
+	.unlocked_ioctl = rkvt_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = rkvt_ioctl,
+#endif
+};
+
+static int rkvt_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct rkvt_dev *vdev = NULL;
+
+	dev_info(dev, "probe start\n");
+	vdev = devm_kzalloc(dev, sizeof(*vdev), GFP_KERNEL);
+	if (!vdev)
+		return -ENOMEM;
+
+	vdev->dev = dev;
+	vdev->dev_name = DEVICE_NAME;
+	vdev->mdev.minor = MISC_DYNAMIC_MINOR;
+	vdev->mdev.name = DEVICE_NAME;
+	vdev->mdev.fops = &vt_fops;
+	platform_set_drvdata(pdev, vdev);
+
+	ret = misc_register(&vdev->mdev);
+	if (ret) {
+		dev_err(dev, "misc_register fail.\n");
+		return ret;
+	}
+
+	mutex_init(&vdev->inst_lock);
+	mutex_init(&vdev->session_lock);
+	idr_init(&vdev->inst_idr);
+	atomic64_set(&vdev->cid_generator, 0);
+	INIT_LIST_HEAD(&vdev->list_inst);
+	INIT_LIST_HEAD(&vdev->list_session);
+	vdev->debug_root = debugfs_create_dir(DEVICE_NAME, NULL);
+	if (!vdev->debug_root)
+		dev_err(dev, "failed to create debugfs root directory.\n");
+
+	dev_info(dev, "probe success\n");
+
+	return 0;
+}
+
+static int rkvt_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct rkvt_dev *vdev = platform_get_drvdata(pdev);
+
+	dev_info(dev, "remove device\n");
+
+	idr_destroy(&vdev->inst_idr);
+	debugfs_remove_recursive(vdev->debug_root);
+	misc_deregister(&vdev->mdev);
+
+	return 0;
+}
+
+static const struct of_device_id rk_vt_match[] = {
+	{
+		.compatible = "rockchip,video-tunnel",
+	},
+	{ },
+};
+
+static struct platform_driver rk_vt_driver = {
+	.probe = rkvt_probe,
+	.remove = rkvt_remove,
+	.driver = {
+		.name = "rk_videotunnel_driver",
+		.owner = THIS_MODULE,
+		.of_match_table = rk_vt_match,
+	},
+};
+
+module_platform_driver(rk_vt_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ROCKCHIP videotunnel driver");
diff --git a/drivers/video/rockchip/vtunnel/rkvtunnel.h b/drivers/video/rockchip/vtunnel/rkvtunnel.h
new file mode 100644
index 0000000000000..d173412115c4f
--- /dev/null
+++ b/drivers/video/rockchip/vtunnel/rkvtunnel.h
@@ -0,0 +1,81 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Copyright (c) 2022 Rockchip Electronics Co., Ltd.
+ */
+#ifndef __ROCKCHIP_VIDEO_TUNNEL_H__
+#define __ROCKCHIP_VIDEO_TUNNEL_H__
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+#define MAX_BUF_HANDLE_FDS		16
+#define MAX_BUF_HANDLE_INTS		128
+
+#define RKVT_IOC_MAGIC			'V'
+#define RKVT_IOWR(nr, type)		_IOWR(RKVT_IOC_MAGIC, nr, type)
+
+#define RKVT_IOC_ALLOC_ID		RKVT_IOWR(0x0, struct rkvt_alloc_id_data)
+#define RKVT_IOC_FREE_ID		RKVT_IOWR(0x1, struct rkvt_alloc_id_data)
+#define RKVT_IOC_CTRL			RKVT_IOWR(0x2, struct rkvt_ctrl_data)
+#define RKVT_IOC_QUEUE_BUF		RKVT_IOWR(0x3, struct rkvt_buf_data)
+#define RKVT_IOC_DEQUE_BUF		RKVT_IOWR(0x4, struct rkvt_buf_data)
+#define RKVT_IOC_CANCEL_BUF		RKVT_IOWR(0x5, struct rkvt_buf_data)
+#define RKVT_IOC_ACQUIRE_BUF		RKVT_IOWR(0x6, struct rkvt_buf_data)
+#define RKVT_IOC_RELEASE_BUF		RKVT_IOWR(0x7, struct rkvt_buf_data)
+
+// caller type
+enum rkvt_caller_e {
+	RKVT_CALLER_PRODUCER,
+	RKVT_CALLER_CONSUMER,
+	RKVT_CALLER_BUTT,
+};
+
+// video tunnel caller control
+enum rkvt_ctrl_cmd_e {
+	RKVT_CTRL_CONNECT,
+	RKVT_CTRL_DISCONNECT,
+	RKVT_CTRL_RESET,
+	RKVT_CTRL_HAS_CONSUMER,
+	RKVT_CTRL_BUTT,
+};
+
+struct rkvt_alloc_id_data {
+	int vt_id;
+};
+
+struct rkvt_ctrl_data {
+	int vt_id;
+	enum rkvt_caller_e caller;
+	enum rkvt_ctrl_cmd_e ctrl_cmd;
+	int ctrl_data;
+};
+
+struct rkvt_rect {
+	int left;
+	int top;
+	int right;
+	int bottom;
+};
+
+struct rkvt_buf_base {
+	int vt_id;
+	int fence_fd;
+	int buf_status;
+	int num_fds;     /* number of file-descriptors at &data[0] */
+	int num_ints;    /* number of ints at &data[numFds] */
+	int reserved;
+	int fds[MAX_BUF_HANDLE_FDS];
+	int ints[MAX_BUF_HANDLE_INTS];
+	int64_t priv_data;
+	uint64_t expected_present_time;
+	uint64_t buffer_id;
+	struct rkvt_rect crop;
+};
+
+struct rkvt_buf_data {
+	int vt_id;
+	int timeout_ms;		/* 0: non block, negative: block, other: timeout ms */
+	struct rkvt_buf_base base;
+};
+
+#endif
-- 
2.42.0

